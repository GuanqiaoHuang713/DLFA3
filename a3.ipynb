{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Pre-Processing Data ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, I load the Google Stock Price dataset to preprocess the data for further analysis and model training. First, I handle the formatting inconsistencies by replacing any ',' symbols present in the data, which are often used as thousand separators. Next, I convert the relevant columns to float format. Following this, I apply normalization to scale the values between 0 and 1. Normalization is an essential step to ensure that all features contribute equally during model training, preventing features with larger numerical ranges from dominating the optimization process. This step also improves the numerical stability and convergence speed of machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Dataset\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Read dataset\n",
    "train_path = \"/home/hgq/Projects/DLFA3/dataset/Google_Stock_Price_Train.csv\"\n",
    "test_path = \"/home/hgq/Projects/DLFA3/dataset/Google_Stock_Price_Test.csv\"\n",
    "train_data = pd.read_csv(train_path)\n",
    "test_data = pd.read_csv(test_path)\n",
    "features = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "train_data = train_data[features]\n",
    "test_data = test_data[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train=(873, 30, 5), y_train=(873, 5)\n",
      "X_val=(187, 30, 5), y_val=(187, 5)\n",
      "X_test=(188, 30, 5), y_test=(188, 5)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Data Preprocessing\n",
    "# Transform to float\n",
    "for column in features:\n",
    "    if train_data[column].dtype == 'object':\n",
    "        train_data[column] = train_data[column].str.replace(',', '').astype(float)\n",
    "    if test_data[column].dtype == 'object':\n",
    "        test_data[column] = test_data[column].str.replace(',', '').astype(float)\n",
    "\n",
    "# Combine train and test datasets\n",
    "combined_data = pd.concat([train_data, test_data], axis=0).reset_index(drop=True)\n",
    "\n",
    "# Normalization\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "combined_scaled = scaler.fit_transform(combined_data)\n",
    "\n",
    "# Look-back window size\n",
    "look_back = 30\n",
    "\n",
    "# Create input-output pairs\n",
    "X, y = [], []\n",
    "for i in range(look_back, len(combined_scaled)):\n",
    "    X.append(combined_scaled[i - look_back:i])  # Input: Previous `look_back` days\n",
    "    y.append(combined_scaled[i])               # Output: the data of the current day\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Split into train, validation, and test sets\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "train_end = int(len(X) * train_ratio)\n",
    "val_end = train_end + int(len(X) * val_ratio)\n",
    "\n",
    "X_train, y_train = X[:train_end], y[:train_end]\n",
    "X_val, y_val = X[train_end:val_end], y[train_end:val_end]\n",
    "X_test, y_test = X[val_end:], y[val_end:]\n",
    "\n",
    "# Output shapes\n",
    "print(f\"X_train={X_train.shape}, y_train={y_train.shape}\")\n",
    "print(f\"X_val={X_val.shape}, y_val={y_val.shape}\")\n",
    "print(f\"X_test={X_test.shape}, y_test={y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step that I deployed LSTM for stock prediction, I applied grid search to optimize hyperparameters such as learning rate, optimizer type, hidden layer size, number of LSTM layers, and dropout rate. Furthermore, I also utilized early stopping to prevent overfitting. Grid search evaluates all combinations of hyperparameters on the validation set, saving the best model based on the lowest validation loss. The best model is reloaded for further evaluation or testing. This approach ensures systematic exploration of hyperparameters for optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=16, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1448, Val Loss: 0.2135\n",
      "Epoch [2/50], Train Loss: 0.0639, Val Loss: 0.0429\n",
      "Epoch [3/50], Train Loss: 0.0418, Val Loss: 0.0474\n",
      "Epoch [4/50], Train Loss: 0.0314, Val Loss: 0.0433\n",
      "Epoch [5/50], Train Loss: 0.0275, Val Loss: 0.0353\n",
      "Epoch [6/50], Train Loss: 0.0243, Val Loss: 0.0283\n",
      "Epoch [7/50], Train Loss: 0.0212, Val Loss: 0.0229\n",
      "Epoch [8/50], Train Loss: 0.0184, Val Loss: 0.0190\n",
      "Epoch [9/50], Train Loss: 0.0158, Val Loss: 0.0162\n",
      "Epoch [10/50], Train Loss: 0.0129, Val Loss: 0.0151\n",
      "Epoch [11/50], Train Loss: 0.0098, Val Loss: 0.0113\n",
      "Epoch [12/50], Train Loss: 0.0074, Val Loss: 0.0064\n",
      "Epoch [13/50], Train Loss: 0.0065, Val Loss: 0.0047\n",
      "Epoch [14/50], Train Loss: 0.0049, Val Loss: 0.0049\n",
      "Epoch [15/50], Train Loss: 0.0045, Val Loss: 0.0028\n",
      "Epoch [16/50], Train Loss: 0.0105, Val Loss: 0.0058\n",
      "Epoch [17/50], Train Loss: 0.0035, Val Loss: 0.0024\n",
      "Epoch [18/50], Train Loss: 0.0050, Val Loss: 0.0062\n",
      "Epoch [19/50], Train Loss: 0.0042, Val Loss: 0.0023\n",
      "Epoch [20/50], Train Loss: 0.0044, Val Loss: 0.0020\n",
      "Epoch [21/50], Train Loss: 0.0029, Val Loss: 0.0034\n",
      "Epoch [22/50], Train Loss: 0.0031, Val Loss: 0.0048\n",
      "Epoch [23/50], Train Loss: 0.0031, Val Loss: 0.0022\n",
      "Epoch [24/50], Train Loss: 0.0026, Val Loss: 0.0024\n",
      "Epoch [25/50], Train Loss: 0.0030, Val Loss: 0.0055\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=16, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1377, Val Loss: 0.3200\n",
      "Epoch [2/50], Train Loss: 0.0785, Val Loss: 0.1721\n",
      "Epoch [3/50], Train Loss: 0.0362, Val Loss: 0.0600\n",
      "Epoch [4/50], Train Loss: 0.0368, Val Loss: 0.0562\n",
      "Epoch [5/50], Train Loss: 0.0298, Val Loss: 0.0370\n",
      "Epoch [6/50], Train Loss: 0.0232, Val Loss: 0.0199\n",
      "Epoch [7/50], Train Loss: 0.0164, Val Loss: 0.0167\n",
      "Epoch [8/50], Train Loss: 0.0126, Val Loss: 0.0150\n",
      "Epoch [9/50], Train Loss: 0.0111, Val Loss: 0.0123\n",
      "Epoch [10/50], Train Loss: 0.0110, Val Loss: 0.0125\n",
      "Epoch [11/50], Train Loss: 0.0102, Val Loss: 0.0123\n",
      "Epoch [12/50], Train Loss: 0.0106, Val Loss: 0.0189\n",
      "Epoch [13/50], Train Loss: 0.0102, Val Loss: 0.0137\n",
      "Epoch [14/50], Train Loss: 0.0088, Val Loss: 0.0113\n",
      "Epoch [15/50], Train Loss: 0.0095, Val Loss: 0.0149\n",
      "Epoch [16/50], Train Loss: 0.0086, Val Loss: 0.0111\n",
      "Epoch [17/50], Train Loss: 0.0084, Val Loss: 0.0103\n",
      "Epoch [18/50], Train Loss: 0.0079, Val Loss: 0.0122\n",
      "Epoch [19/50], Train Loss: 0.0087, Val Loss: 0.0123\n",
      "Epoch [20/50], Train Loss: 0.0080, Val Loss: 0.0088\n",
      "Epoch [21/50], Train Loss: 0.0077, Val Loss: 0.0098\n",
      "Epoch [22/50], Train Loss: 0.0084, Val Loss: 0.0144\n",
      "Epoch [23/50], Train Loss: 0.0080, Val Loss: 0.0092\n",
      "Epoch [24/50], Train Loss: 0.0088, Val Loss: 0.0100\n",
      "Epoch [25/50], Train Loss: 0.0079, Val Loss: 0.0135\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=16, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0905, Val Loss: 0.2312\n",
      "Epoch [2/50], Train Loss: 0.0600, Val Loss: 0.1400\n",
      "Epoch [3/50], Train Loss: 0.0448, Val Loss: 0.0750\n",
      "Epoch [4/50], Train Loss: 0.0391, Val Loss: 0.0492\n",
      "Epoch [5/50], Train Loss: 0.0338, Val Loss: 0.0357\n",
      "Epoch [6/50], Train Loss: 0.0304, Val Loss: 0.0203\n",
      "Epoch [7/50], Train Loss: 0.0256, Val Loss: 0.0141\n",
      "Epoch [8/50], Train Loss: 0.0230, Val Loss: 0.0087\n",
      "Epoch [9/50], Train Loss: 0.0202, Val Loss: 0.0142\n",
      "Epoch [10/50], Train Loss: 0.0177, Val Loss: 0.0140\n",
      "Epoch [11/50], Train Loss: 0.0167, Val Loss: 0.0118\n",
      "Epoch [12/50], Train Loss: 0.0170, Val Loss: 0.0107\n",
      "Epoch [13/50], Train Loss: 0.0158, Val Loss: 0.0150\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=16, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1152, Val Loss: 0.1807\n",
      "Epoch [2/50], Train Loss: 0.0343, Val Loss: 0.0369\n",
      "Epoch [3/50], Train Loss: 0.0431, Val Loss: 0.0566\n",
      "Epoch [4/50], Train Loss: 0.0314, Val Loss: 0.0403\n",
      "Epoch [5/50], Train Loss: 0.0286, Val Loss: 0.0280\n",
      "Epoch [6/50], Train Loss: 0.0248, Val Loss: 0.0212\n",
      "Epoch [7/50], Train Loss: 0.0223, Val Loss: 0.0226\n",
      "Epoch [8/50], Train Loss: 0.0202, Val Loss: 0.0230\n",
      "Epoch [9/50], Train Loss: 0.0187, Val Loss: 0.0183\n",
      "Epoch [10/50], Train Loss: 0.0163, Val Loss: 0.0098\n",
      "Epoch [11/50], Train Loss: 0.0088, Val Loss: 0.0173\n",
      "Epoch [12/50], Train Loss: 0.0054, Val Loss: 0.0128\n",
      "Epoch [13/50], Train Loss: 0.0112, Val Loss: 0.0057\n",
      "Epoch [14/50], Train Loss: 0.0049, Val Loss: 0.0226\n",
      "Epoch [15/50], Train Loss: 0.0043, Val Loss: 0.0210\n",
      "Epoch [16/50], Train Loss: 0.0035, Val Loss: 0.0196\n",
      "Epoch [17/50], Train Loss: 0.0041, Val Loss: 0.0152\n",
      "Epoch [18/50], Train Loss: 0.0048, Val Loss: 0.0220\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=16, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0801, Val Loss: 0.2180\n",
      "Epoch [2/50], Train Loss: 0.0493, Val Loss: 0.1438\n",
      "Epoch [3/50], Train Loss: 0.0429, Val Loss: 0.1086\n",
      "Epoch [4/50], Train Loss: 0.0404, Val Loss: 0.0888\n",
      "Epoch [5/50], Train Loss: 0.0375, Val Loss: 0.0691\n",
      "Epoch [6/50], Train Loss: 0.0343, Val Loss: 0.0408\n",
      "Epoch [7/50], Train Loss: 0.0271, Val Loss: 0.0175\n",
      "Epoch [8/50], Train Loss: 0.0231, Val Loss: 0.0105\n",
      "Epoch [9/50], Train Loss: 0.0141, Val Loss: 0.0223\n",
      "Epoch [10/50], Train Loss: 0.0100, Val Loss: 0.0201\n",
      "Epoch [11/50], Train Loss: 0.0102, Val Loss: 0.0213\n",
      "Epoch [12/50], Train Loss: 0.0103, Val Loss: 0.0138\n",
      "Epoch [13/50], Train Loss: 0.0089, Val Loss: 0.0104\n",
      "Epoch [14/50], Train Loss: 0.0095, Val Loss: 0.0287\n",
      "Epoch [15/50], Train Loss: 0.0090, Val Loss: 0.0086\n",
      "Epoch [16/50], Train Loss: 0.0081, Val Loss: 0.0174\n",
      "Epoch [17/50], Train Loss: 0.0113, Val Loss: 0.0262\n",
      "Epoch [18/50], Train Loss: 0.0103, Val Loss: 0.0131\n",
      "Epoch [19/50], Train Loss: 0.0085, Val Loss: 0.0231\n",
      "Epoch [20/50], Train Loss: 0.0104, Val Loss: 0.0134\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=16, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1051, Val Loss: 0.2263\n",
      "Epoch [2/50], Train Loss: 0.0587, Val Loss: 0.1005\n",
      "Epoch [3/50], Train Loss: 0.0554, Val Loss: 0.0970\n",
      "Epoch [4/50], Train Loss: 0.0489, Val Loss: 0.0811\n",
      "Epoch [5/50], Train Loss: 0.0453, Val Loss: 0.0702\n",
      "Epoch [6/50], Train Loss: 0.0405, Val Loss: 0.0529\n",
      "Epoch [7/50], Train Loss: 0.0371, Val Loss: 0.0325\n",
      "Epoch [8/50], Train Loss: 0.0291, Val Loss: 0.0272\n",
      "Epoch [9/50], Train Loss: 0.0270, Val Loss: 0.0330\n",
      "Epoch [10/50], Train Loss: 0.0230, Val Loss: 0.0258\n",
      "Epoch [11/50], Train Loss: 0.0243, Val Loss: 0.0220\n",
      "Epoch [12/50], Train Loss: 0.0203, Val Loss: 0.0376\n",
      "Epoch [13/50], Train Loss: 0.0207, Val Loss: 0.0223\n",
      "Epoch [14/50], Train Loss: 0.0192, Val Loss: 0.0272\n",
      "Epoch [15/50], Train Loss: 0.0215, Val Loss: 0.0374\n",
      "Epoch [16/50], Train Loss: 0.0221, Val Loss: 0.0281\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=16, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1018, Val Loss: 0.1881\n",
      "Epoch [2/50], Train Loss: 0.0417, Val Loss: 0.1012\n",
      "Epoch [3/50], Train Loss: 0.0426, Val Loss: 0.1015\n",
      "Epoch [4/50], Train Loss: 0.0402, Val Loss: 0.0957\n",
      "Epoch [5/50], Train Loss: 0.0394, Val Loss: 0.0881\n",
      "Epoch [6/50], Train Loss: 0.0372, Val Loss: 0.0714\n",
      "Epoch [7/50], Train Loss: 0.0332, Val Loss: 0.0550\n",
      "Epoch [8/50], Train Loss: 0.0288, Val Loss: 0.0423\n",
      "Epoch [9/50], Train Loss: 0.0243, Val Loss: 0.0516\n",
      "Epoch [10/50], Train Loss: 0.0203, Val Loss: 0.0375\n",
      "Epoch [11/50], Train Loss: 0.0197, Val Loss: 0.0362\n",
      "Epoch [12/50], Train Loss: 0.0138, Val Loss: 0.0388\n",
      "Epoch [13/50], Train Loss: 0.0125, Val Loss: 0.0221\n",
      "Epoch [14/50], Train Loss: 0.0092, Val Loss: 0.0364\n",
      "Epoch [15/50], Train Loss: 0.0124, Val Loss: 0.0353\n",
      "Epoch [16/50], Train Loss: 0.0131, Val Loss: 0.0212\n",
      "Epoch [17/50], Train Loss: 0.0087, Val Loss: 0.0552\n",
      "Epoch [18/50], Train Loss: 0.0097, Val Loss: 0.0393\n",
      "Epoch [19/50], Train Loss: 0.0069, Val Loss: 0.0408\n",
      "Epoch [20/50], Train Loss: 0.0091, Val Loss: 0.0383\n",
      "Epoch [21/50], Train Loss: 0.0067, Val Loss: 0.0397\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=16, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1111, Val Loss: 0.2673\n",
      "Epoch [2/50], Train Loss: 0.0524, Val Loss: 0.1003\n",
      "Epoch [3/50], Train Loss: 0.0539, Val Loss: 0.1084\n",
      "Epoch [4/50], Train Loss: 0.0469, Val Loss: 0.0914\n",
      "Epoch [5/50], Train Loss: 0.0458, Val Loss: 0.0852\n",
      "Epoch [6/50], Train Loss: 0.0431, Val Loss: 0.0710\n",
      "Epoch [7/50], Train Loss: 0.0393, Val Loss: 0.0511\n",
      "Epoch [8/50], Train Loss: 0.0314, Val Loss: 0.0371\n",
      "Epoch [9/50], Train Loss: 0.0277, Val Loss: 0.0485\n",
      "Epoch [10/50], Train Loss: 0.0304, Val Loss: 0.0492\n",
      "Epoch [11/50], Train Loss: 0.0210, Val Loss: 0.0525\n",
      "Epoch [12/50], Train Loss: 0.0169, Val Loss: 0.0548\n",
      "Epoch [13/50], Train Loss: 0.0144, Val Loss: 0.0557\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=16, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1038, Val Loss: 0.2061\n",
      "Epoch [2/50], Train Loss: 0.0643, Val Loss: 0.1111\n",
      "Epoch [3/50], Train Loss: 0.0604, Val Loss: 0.1177\n",
      "Epoch [4/50], Train Loss: 0.0523, Val Loss: 0.1028\n",
      "Epoch [5/50], Train Loss: 0.0517, Val Loss: 0.1008\n",
      "Epoch [6/50], Train Loss: 0.0503, Val Loss: 0.0914\n",
      "Epoch [7/50], Train Loss: 0.0465, Val Loss: 0.0694\n",
      "Epoch [8/50], Train Loss: 0.0417, Val Loss: 0.0507\n",
      "Epoch [9/50], Train Loss: 0.0329, Val Loss: 0.0509\n",
      "Epoch [10/50], Train Loss: 0.0271, Val Loss: 0.0412\n",
      "Epoch [11/50], Train Loss: 0.0299, Val Loss: 0.0271\n",
      "Epoch [12/50], Train Loss: 0.0240, Val Loss: 0.0480\n",
      "Epoch [13/50], Train Loss: 0.0226, Val Loss: 0.0267\n",
      "Epoch [14/50], Train Loss: 0.0245, Val Loss: 0.0438\n",
      "Epoch [15/50], Train Loss: 0.0240, Val Loss: 0.0362\n",
      "Epoch [16/50], Train Loss: 0.0191, Val Loss: 0.0341\n",
      "Epoch [17/50], Train Loss: 0.0190, Val Loss: 0.0403\n",
      "Epoch [18/50], Train Loss: 0.0174, Val Loss: 0.0353\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=32, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0800, Val Loss: 0.0960\n",
      "Epoch [2/50], Train Loss: 0.0306, Val Loss: 0.0506\n",
      "Epoch [3/50], Train Loss: 0.0306, Val Loss: 0.0349\n",
      "Epoch [4/50], Train Loss: 0.0267, Val Loss: 0.0328\n",
      "Epoch [5/50], Train Loss: 0.0221, Val Loss: 0.0225\n",
      "Epoch [6/50], Train Loss: 0.0181, Val Loss: 0.0121\n",
      "Epoch [7/50], Train Loss: 0.0094, Val Loss: 0.0028\n",
      "Epoch [8/50], Train Loss: 0.0110, Val Loss: 0.0064\n",
      "Epoch [9/50], Train Loss: 0.0044, Val Loss: 0.0095\n",
      "Epoch [10/50], Train Loss: 0.0056, Val Loss: 0.0036\n",
      "Epoch [11/50], Train Loss: 0.0035, Val Loss: 0.0058\n",
      "Epoch [12/50], Train Loss: 0.0035, Val Loss: 0.0055\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=32, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1023, Val Loss: 0.1910\n",
      "Epoch [2/50], Train Loss: 0.0320, Val Loss: 0.0314\n",
      "Epoch [3/50], Train Loss: 0.0372, Val Loss: 0.0285\n",
      "Epoch [4/50], Train Loss: 0.0316, Val Loss: 0.0229\n",
      "Epoch [5/50], Train Loss: 0.0240, Val Loss: 0.0095\n",
      "Epoch [6/50], Train Loss: 0.0206, Val Loss: 0.0042\n",
      "Epoch [7/50], Train Loss: 0.0158, Val Loss: 0.0062\n",
      "Epoch [8/50], Train Loss: 0.0106, Val Loss: 0.0131\n",
      "Epoch [9/50], Train Loss: 0.0085, Val Loss: 0.0078\n",
      "Epoch [10/50], Train Loss: 0.0076, Val Loss: 0.0081\n",
      "Epoch [11/50], Train Loss: 0.0071, Val Loss: 0.0092\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=32, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1157, Val Loss: 0.1715\n",
      "Epoch [2/50], Train Loss: 0.0454, Val Loss: 0.0542\n",
      "Epoch [3/50], Train Loss: 0.0445, Val Loss: 0.0344\n",
      "Epoch [4/50], Train Loss: 0.0383, Val Loss: 0.0267\n",
      "Epoch [5/50], Train Loss: 0.0314, Val Loss: 0.0105\n",
      "Epoch [6/50], Train Loss: 0.0279, Val Loss: 0.0063\n",
      "Epoch [7/50], Train Loss: 0.0231, Val Loss: 0.0070\n",
      "Epoch [8/50], Train Loss: 0.0207, Val Loss: 0.0090\n",
      "Epoch [9/50], Train Loss: 0.0161, Val Loss: 0.0128\n",
      "Epoch [10/50], Train Loss: 0.0153, Val Loss: 0.0123\n",
      "Epoch [11/50], Train Loss: 0.0137, Val Loss: 0.0194\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=32, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0469, Val Loss: 0.0325\n",
      "Epoch [2/50], Train Loss: 0.0530, Val Loss: 0.0771\n",
      "Epoch [3/50], Train Loss: 0.0298, Val Loss: 0.0160\n",
      "Epoch [4/50], Train Loss: 0.0276, Val Loss: 0.0072\n",
      "Epoch [5/50], Train Loss: 0.0198, Val Loss: 0.0067\n",
      "Epoch [6/50], Train Loss: 0.0177, Val Loss: 0.0074\n",
      "Epoch [7/50], Train Loss: 0.0128, Val Loss: 0.0474\n",
      "Epoch [8/50], Train Loss: 0.0075, Val Loss: 0.0284\n",
      "Epoch [9/50], Train Loss: 0.0104, Val Loss: 0.0160\n",
      "Epoch [10/50], Train Loss: 0.0062, Val Loss: 0.0332\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=32, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0681, Val Loss: 0.1362\n",
      "Epoch [2/50], Train Loss: 0.0357, Val Loss: 0.0503\n",
      "Epoch [3/50], Train Loss: 0.0404, Val Loss: 0.0383\n",
      "Epoch [4/50], Train Loss: 0.0318, Val Loss: 0.0194\n",
      "Epoch [5/50], Train Loss: 0.0278, Val Loss: 0.0227\n",
      "Epoch [6/50], Train Loss: 0.0232, Val Loss: 0.0132\n",
      "Epoch [7/50], Train Loss: 0.0179, Val Loss: 0.0084\n",
      "Epoch [8/50], Train Loss: 0.0142, Val Loss: 0.0316\n",
      "Epoch [9/50], Train Loss: 0.0129, Val Loss: 0.0301\n",
      "Epoch [10/50], Train Loss: 0.0139, Val Loss: 0.0277\n",
      "Epoch [11/50], Train Loss: 0.0086, Val Loss: 0.0177\n",
      "Epoch [12/50], Train Loss: 0.0097, Val Loss: 0.0203\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=32, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0701, Val Loss: 0.1658\n",
      "Epoch [2/50], Train Loss: 0.0429, Val Loss: 0.0718\n",
      "Epoch [3/50], Train Loss: 0.0459, Val Loss: 0.0666\n",
      "Epoch [4/50], Train Loss: 0.0365, Val Loss: 0.0238\n",
      "Epoch [5/50], Train Loss: 0.0285, Val Loss: 0.0206\n",
      "Epoch [6/50], Train Loss: 0.0227, Val Loss: 0.0400\n",
      "Epoch [7/50], Train Loss: 0.0196, Val Loss: 0.0356\n",
      "Epoch [8/50], Train Loss: 0.0188, Val Loss: 0.0187\n",
      "Epoch [9/50], Train Loss: 0.0149, Val Loss: 0.0401\n",
      "Epoch [10/50], Train Loss: 0.0193, Val Loss: 0.0348\n",
      "Epoch [11/50], Train Loss: 0.0177, Val Loss: 0.0236\n",
      "Epoch [12/50], Train Loss: 0.0127, Val Loss: 0.0260\n",
      "Epoch [13/50], Train Loss: 0.0194, Val Loss: 0.0295\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=32, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0983, Val Loss: 0.0998\n",
      "Epoch [2/50], Train Loss: 0.0458, Val Loss: 0.0558\n",
      "Epoch [3/50], Train Loss: 0.0472, Val Loss: 0.0553\n",
      "Epoch [4/50], Train Loss: 0.0373, Val Loss: 0.0395\n",
      "Epoch [5/50], Train Loss: 0.0297, Val Loss: 0.0325\n",
      "Epoch [6/50], Train Loss: 0.0264, Val Loss: 0.0245\n",
      "Epoch [7/50], Train Loss: 0.0222, Val Loss: 0.0261\n",
      "Epoch [8/50], Train Loss: 0.0217, Val Loss: 0.0354\n",
      "Epoch [9/50], Train Loss: 0.0204, Val Loss: 0.0533\n",
      "Epoch [10/50], Train Loss: 0.0204, Val Loss: 0.0361\n",
      "Epoch [11/50], Train Loss: 0.0203, Val Loss: 0.0310\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=32, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0738, Val Loss: 0.0783\n",
      "Epoch [2/50], Train Loss: 0.0546, Val Loss: 0.0888\n",
      "Epoch [3/50], Train Loss: 0.0449, Val Loss: 0.0659\n",
      "Epoch [4/50], Train Loss: 0.0437, Val Loss: 0.0397\n",
      "Epoch [5/50], Train Loss: 0.0357, Val Loss: 0.0311\n",
      "Epoch [6/50], Train Loss: 0.0287, Val Loss: 0.0280\n",
      "Epoch [7/50], Train Loss: 0.0274, Val Loss: 0.0315\n",
      "Epoch [8/50], Train Loss: 0.0247, Val Loss: 0.0358\n",
      "Epoch [9/50], Train Loss: 0.0237, Val Loss: 0.0385\n",
      "Epoch [10/50], Train Loss: 0.0218, Val Loss: 0.0426\n",
      "Epoch [11/50], Train Loss: 0.0185, Val Loss: 0.0416\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=32, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0931, Val Loss: 0.1085\n",
      "Epoch [2/50], Train Loss: 0.0642, Val Loss: 0.0899\n",
      "Epoch [3/50], Train Loss: 0.0538, Val Loss: 0.0806\n",
      "Epoch [4/50], Train Loss: 0.0500, Val Loss: 0.0622\n",
      "Epoch [5/50], Train Loss: 0.0461, Val Loss: 0.0372\n",
      "Epoch [6/50], Train Loss: 0.0361, Val Loss: 0.0195\n",
      "Epoch [7/50], Train Loss: 0.0311, Val Loss: 0.0267\n",
      "Epoch [8/50], Train Loss: 0.0282, Val Loss: 0.0340\n",
      "Epoch [9/50], Train Loss: 0.0232, Val Loss: 0.0468\n",
      "Epoch [10/50], Train Loss: 0.0221, Val Loss: 0.0655\n",
      "Epoch [11/50], Train Loss: 0.0162, Val Loss: 0.0347\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=64, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0586, Val Loss: 0.0274\n",
      "Epoch [2/50], Train Loss: 0.0479, Val Loss: 0.0293\n",
      "Epoch [3/50], Train Loss: 0.0309, Val Loss: 0.0144\n",
      "Epoch [4/50], Train Loss: 0.0223, Val Loss: 0.0059\n",
      "Epoch [5/50], Train Loss: 0.0174, Val Loss: 0.0150\n",
      "Epoch [6/50], Train Loss: 0.0130, Val Loss: 0.0220\n",
      "Epoch [7/50], Train Loss: 0.0106, Val Loss: 0.0401\n",
      "Epoch [8/50], Train Loss: 0.0072, Val Loss: 0.0085\n",
      "Epoch [9/50], Train Loss: 0.0038, Val Loss: 0.0128\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=64, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0816, Val Loss: 0.0539\n",
      "Epoch [2/50], Train Loss: 0.0335, Val Loss: 0.0204\n",
      "Epoch [3/50], Train Loss: 0.0282, Val Loss: 0.0097\n",
      "Epoch [4/50], Train Loss: 0.0238, Val Loss: 0.0096\n",
      "Epoch [5/50], Train Loss: 0.0160, Val Loss: 0.0168\n",
      "Epoch [6/50], Train Loss: 0.0105, Val Loss: 0.0139\n",
      "Epoch [7/50], Train Loss: 0.0102, Val Loss: 0.0265\n",
      "Epoch [8/50], Train Loss: 0.0060, Val Loss: 0.0093\n",
      "Epoch [9/50], Train Loss: 0.0065, Val Loss: 0.0098\n",
      "Epoch [10/50], Train Loss: 0.0058, Val Loss: 0.0075\n",
      "Epoch [11/50], Train Loss: 0.0059, Val Loss: 0.0231\n",
      "Epoch [12/50], Train Loss: 0.0062, Val Loss: 0.0073\n",
      "Epoch [13/50], Train Loss: 0.0059, Val Loss: 0.0050\n",
      "Epoch [14/50], Train Loss: 0.0056, Val Loss: 0.0112\n",
      "Epoch [15/50], Train Loss: 0.0062, Val Loss: 0.0126\n",
      "Epoch [16/50], Train Loss: 0.0097, Val Loss: 0.0034\n",
      "Epoch [17/50], Train Loss: 0.0053, Val Loss: 0.0075\n",
      "Epoch [18/50], Train Loss: 0.0122, Val Loss: 0.0130\n",
      "Epoch [19/50], Train Loss: 0.0045, Val Loss: 0.0090\n",
      "Epoch [20/50], Train Loss: 0.0054, Val Loss: 0.0027\n",
      "Epoch [21/50], Train Loss: 0.0044, Val Loss: 0.0141\n",
      "Epoch [22/50], Train Loss: 0.0057, Val Loss: 0.0058\n",
      "Epoch [23/50], Train Loss: 0.0070, Val Loss: 0.0031\n",
      "Epoch [24/50], Train Loss: 0.0056, Val Loss: 0.0127\n",
      "Epoch [25/50], Train Loss: 0.0092, Val Loss: 0.0043\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=64, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0775, Val Loss: 0.0711\n",
      "Epoch [2/50], Train Loss: 0.0407, Val Loss: 0.0266\n",
      "Epoch [3/50], Train Loss: 0.0346, Val Loss: 0.0067\n",
      "Epoch [4/50], Train Loss: 0.0274, Val Loss: 0.0036\n",
      "Epoch [5/50], Train Loss: 0.0213, Val Loss: 0.0191\n",
      "Epoch [6/50], Train Loss: 0.0140, Val Loss: 0.0253\n",
      "Epoch [7/50], Train Loss: 0.0140, Val Loss: 0.0209\n",
      "Epoch [8/50], Train Loss: 0.0119, Val Loss: 0.0157\n",
      "Epoch [9/50], Train Loss: 0.0131, Val Loss: 0.0100\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=64, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0453, Val Loss: 0.0975\n",
      "Epoch [2/50], Train Loss: 0.0565, Val Loss: 0.0740\n",
      "Epoch [3/50], Train Loss: 0.0436, Val Loss: 0.0505\n",
      "Epoch [4/50], Train Loss: 0.0367, Val Loss: 0.0204\n",
      "Epoch [5/50], Train Loss: 0.0259, Val Loss: 0.0053\n",
      "Epoch [6/50], Train Loss: 0.0234, Val Loss: 0.0076\n",
      "Epoch [7/50], Train Loss: 0.0111, Val Loss: 0.0775\n",
      "Epoch [8/50], Train Loss: 0.0097, Val Loss: 0.0255\n",
      "Epoch [9/50], Train Loss: 0.0128, Val Loss: 0.0158\n",
      "Epoch [10/50], Train Loss: 0.0064, Val Loss: 0.0360\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=64, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0491, Val Loss: 0.1014\n",
      "Epoch [2/50], Train Loss: 0.0635, Val Loss: 0.0623\n",
      "Epoch [3/50], Train Loss: 0.0452, Val Loss: 0.0308\n",
      "Epoch [4/50], Train Loss: 0.0332, Val Loss: 0.0125\n",
      "Epoch [5/50], Train Loss: 0.0242, Val Loss: 0.0143\n",
      "Epoch [6/50], Train Loss: 0.0223, Val Loss: 0.0143\n",
      "Epoch [7/50], Train Loss: 0.0165, Val Loss: 0.0166\n",
      "Epoch [8/50], Train Loss: 0.0111, Val Loss: 0.0261\n",
      "Epoch [9/50], Train Loss: 0.0208, Val Loss: 0.0097\n",
      "Epoch [10/50], Train Loss: 0.0129, Val Loss: 0.0095\n",
      "Epoch [11/50], Train Loss: 0.0081, Val Loss: 0.0333\n",
      "Epoch [12/50], Train Loss: 0.0077, Val Loss: 0.0114\n",
      "Epoch [13/50], Train Loss: 0.0089, Val Loss: 0.0071\n",
      "Epoch [14/50], Train Loss: 0.0058, Val Loss: 0.0256\n",
      "Epoch [15/50], Train Loss: 0.0058, Val Loss: 0.0175\n",
      "Epoch [16/50], Train Loss: 0.0061, Val Loss: 0.0048\n",
      "Epoch [17/50], Train Loss: 0.0060, Val Loss: 0.0183\n",
      "Epoch [18/50], Train Loss: 0.0071, Val Loss: 0.0176\n",
      "Epoch [19/50], Train Loss: 0.0075, Val Loss: 0.0025\n",
      "Epoch [20/50], Train Loss: 0.0052, Val Loss: 0.0108\n",
      "Epoch [21/50], Train Loss: 0.0082, Val Loss: 0.0305\n",
      "Epoch [22/50], Train Loss: 0.0062, Val Loss: 0.0030\n",
      "Epoch [23/50], Train Loss: 0.0057, Val Loss: 0.0065\n",
      "Epoch [24/50], Train Loss: 0.0072, Val Loss: 0.0248\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=64, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0719, Val Loss: 0.0890\n",
      "Epoch [2/50], Train Loss: 0.0608, Val Loss: 0.0262\n",
      "Epoch [3/50], Train Loss: 0.0595, Val Loss: 0.0486\n",
      "Epoch [4/50], Train Loss: 0.0387, Val Loss: 0.0229\n",
      "Epoch [5/50], Train Loss: 0.0332, Val Loss: 0.0114\n",
      "Epoch [6/50], Train Loss: 0.0293, Val Loss: 0.0088\n",
      "Epoch [7/50], Train Loss: 0.0229, Val Loss: 0.0062\n",
      "Epoch [8/50], Train Loss: 0.0141, Val Loss: 0.0098\n",
      "Epoch [9/50], Train Loss: 0.0184, Val Loss: 0.0186\n",
      "Epoch [10/50], Train Loss: 0.0133, Val Loss: 0.0097\n",
      "Epoch [11/50], Train Loss: 0.0140, Val Loss: 0.0303\n",
      "Epoch [12/50], Train Loss: 0.0109, Val Loss: 0.0145\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=64, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0417, Val Loss: 0.0705\n",
      "Epoch [2/50], Train Loss: 0.0648, Val Loss: 0.1210\n",
      "Epoch [3/50], Train Loss: 0.0395, Val Loss: 0.0580\n",
      "Epoch [4/50], Train Loss: 0.0441, Val Loss: 0.0166\n",
      "Epoch [5/50], Train Loss: 0.0350, Val Loss: 0.0472\n",
      "Epoch [6/50], Train Loss: 0.0229, Val Loss: 0.0759\n",
      "Epoch [7/50], Train Loss: 0.0226, Val Loss: 0.0305\n",
      "Epoch [8/50], Train Loss: 0.0214, Val Loss: 0.0315\n",
      "Epoch [9/50], Train Loss: 0.0187, Val Loss: 0.0416\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=64, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0421, Val Loss: 0.0982\n",
      "Epoch [2/50], Train Loss: 0.0581, Val Loss: 0.0555\n",
      "Epoch [3/50], Train Loss: 0.0559, Val Loss: 0.0703\n",
      "Epoch [4/50], Train Loss: 0.0401, Val Loss: 0.0282\n",
      "Epoch [5/50], Train Loss: 0.0299, Val Loss: 0.0182\n",
      "Epoch [6/50], Train Loss: 0.0256, Val Loss: 0.0154\n",
      "Epoch [7/50], Train Loss: 0.0219, Val Loss: 0.0175\n",
      "Epoch [8/50], Train Loss: 0.0213, Val Loss: 0.0189\n",
      "Epoch [9/50], Train Loss: 0.0214, Val Loss: 0.0301\n",
      "Epoch [10/50], Train Loss: 0.0193, Val Loss: 0.0410\n",
      "Epoch [11/50], Train Loss: 0.0191, Val Loss: 0.0363\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=64, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0649, Val Loss: 0.1357\n",
      "Epoch [2/50], Train Loss: 0.0632, Val Loss: 0.0579\n",
      "Epoch [3/50], Train Loss: 0.0639, Val Loss: 0.0915\n",
      "Epoch [4/50], Train Loss: 0.0466, Val Loss: 0.0667\n",
      "Epoch [5/50], Train Loss: 0.0468, Val Loss: 0.0368\n",
      "Epoch [6/50], Train Loss: 0.0354, Val Loss: 0.0403\n",
      "Epoch [7/50], Train Loss: 0.0280, Val Loss: 0.0224\n",
      "Epoch [8/50], Train Loss: 0.0249, Val Loss: 0.0216\n",
      "Epoch [9/50], Train Loss: 0.0250, Val Loss: 0.0443\n",
      "Epoch [10/50], Train Loss: 0.0234, Val Loss: 0.0398\n",
      "Epoch [11/50], Train Loss: 0.0183, Val Loss: 0.0184\n",
      "Epoch [12/50], Train Loss: 0.0176, Val Loss: 0.0290\n",
      "Epoch [13/50], Train Loss: 0.0180, Val Loss: 0.0133\n",
      "Epoch [14/50], Train Loss: 0.0177, Val Loss: 0.0395\n",
      "Epoch [15/50], Train Loss: 0.0357, Val Loss: 0.0133\n",
      "Epoch [16/50], Train Loss: 0.0158, Val Loss: 0.0379\n",
      "Epoch [17/50], Train Loss: 0.0221, Val Loss: 0.0472\n",
      "Epoch [18/50], Train Loss: 0.0134, Val Loss: 0.0093\n",
      "Epoch [19/50], Train Loss: 0.0122, Val Loss: 0.0186\n",
      "Epoch [20/50], Train Loss: 0.0225, Val Loss: 0.0633\n",
      "Epoch [21/50], Train Loss: 0.0200, Val Loss: 0.0273\n",
      "Epoch [22/50], Train Loss: 0.0138, Val Loss: 0.0181\n",
      "Epoch [23/50], Train Loss: 0.0134, Val Loss: 0.0348\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=16, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1325, Val Loss: 0.3571\n",
      "Epoch [2/50], Train Loss: 0.1296, Val Loss: 0.3516\n",
      "Epoch [3/50], Train Loss: 0.1268, Val Loss: 0.3463\n",
      "Epoch [4/50], Train Loss: 0.1240, Val Loss: 0.3411\n",
      "Epoch [5/50], Train Loss: 0.1214, Val Loss: 0.3360\n",
      "Epoch [6/50], Train Loss: 0.1188, Val Loss: 0.3309\n",
      "Epoch [7/50], Train Loss: 0.1163, Val Loss: 0.3261\n",
      "Epoch [8/50], Train Loss: 0.1138, Val Loss: 0.3213\n",
      "Epoch [9/50], Train Loss: 0.1115, Val Loss: 0.3167\n",
      "Epoch [10/50], Train Loss: 0.1092, Val Loss: 0.3121\n",
      "Epoch [11/50], Train Loss: 0.1070, Val Loss: 0.3076\n",
      "Epoch [12/50], Train Loss: 0.1048, Val Loss: 0.3033\n",
      "Epoch [13/50], Train Loss: 0.1027, Val Loss: 0.2990\n",
      "Epoch [14/50], Train Loss: 0.1007, Val Loss: 0.2949\n",
      "Epoch [15/50], Train Loss: 0.0987, Val Loss: 0.2908\n",
      "Epoch [16/50], Train Loss: 0.0968, Val Loss: 0.2868\n",
      "Epoch [17/50], Train Loss: 0.0949, Val Loss: 0.2829\n",
      "Epoch [18/50], Train Loss: 0.0931, Val Loss: 0.2791\n",
      "Epoch [19/50], Train Loss: 0.0914, Val Loss: 0.2754\n",
      "Epoch [20/50], Train Loss: 0.0897, Val Loss: 0.2717\n",
      "Epoch [21/50], Train Loss: 0.0880, Val Loss: 0.2682\n",
      "Epoch [22/50], Train Loss: 0.0864, Val Loss: 0.2647\n",
      "Epoch [23/50], Train Loss: 0.0849, Val Loss: 0.2613\n",
      "Epoch [24/50], Train Loss: 0.0834, Val Loss: 0.2579\n",
      "Epoch [25/50], Train Loss: 0.0819, Val Loss: 0.2546\n",
      "Epoch [26/50], Train Loss: 0.0805, Val Loss: 0.2515\n",
      "Epoch [27/50], Train Loss: 0.0792, Val Loss: 0.2483\n",
      "Epoch [28/50], Train Loss: 0.0778, Val Loss: 0.2453\n",
      "Epoch [29/50], Train Loss: 0.0765, Val Loss: 0.2423\n",
      "Epoch [30/50], Train Loss: 0.0753, Val Loss: 0.2394\n",
      "Epoch [31/50], Train Loss: 0.0741, Val Loss: 0.2365\n",
      "Epoch [32/50], Train Loss: 0.0729, Val Loss: 0.2337\n",
      "Epoch [33/50], Train Loss: 0.0718, Val Loss: 0.2309\n",
      "Epoch [34/50], Train Loss: 0.0706, Val Loss: 0.2283\n",
      "Epoch [35/50], Train Loss: 0.0696, Val Loss: 0.2256\n",
      "Epoch [36/50], Train Loss: 0.0685, Val Loss: 0.2231\n",
      "Epoch [37/50], Train Loss: 0.0675, Val Loss: 0.2206\n",
      "Epoch [38/50], Train Loss: 0.0665, Val Loss: 0.2181\n",
      "Epoch [39/50], Train Loss: 0.0656, Val Loss: 0.2157\n",
      "Epoch [40/50], Train Loss: 0.0647, Val Loss: 0.2133\n",
      "Epoch [41/50], Train Loss: 0.0638, Val Loss: 0.2110\n",
      "Epoch [42/50], Train Loss: 0.0629, Val Loss: 0.2088\n",
      "Epoch [43/50], Train Loss: 0.0621, Val Loss: 0.2066\n",
      "Epoch [44/50], Train Loss: 0.0613, Val Loss: 0.2044\n",
      "Epoch [45/50], Train Loss: 0.0605, Val Loss: 0.2023\n",
      "Epoch [46/50], Train Loss: 0.0597, Val Loss: 0.2002\n",
      "Epoch [47/50], Train Loss: 0.0590, Val Loss: 0.1982\n",
      "Epoch [48/50], Train Loss: 0.0583, Val Loss: 0.1962\n",
      "Epoch [49/50], Train Loss: 0.0576, Val Loss: 0.1943\n",
      "Epoch [50/50], Train Loss: 0.0569, Val Loss: 0.1924\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=16, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1234, Val Loss: 0.3422\n",
      "Epoch [2/50], Train Loss: 0.1215, Val Loss: 0.3371\n",
      "Epoch [3/50], Train Loss: 0.1185, Val Loss: 0.3321\n",
      "Epoch [4/50], Train Loss: 0.1165, Val Loss: 0.3272\n",
      "Epoch [5/50], Train Loss: 0.1139, Val Loss: 0.3225\n",
      "Epoch [6/50], Train Loss: 0.1113, Val Loss: 0.3178\n",
      "Epoch [7/50], Train Loss: 0.1096, Val Loss: 0.3132\n",
      "Epoch [8/50], Train Loss: 0.1075, Val Loss: 0.3087\n",
      "Epoch [9/50], Train Loss: 0.1049, Val Loss: 0.3044\n",
      "Epoch [10/50], Train Loss: 0.1023, Val Loss: 0.3000\n",
      "Epoch [11/50], Train Loss: 0.1000, Val Loss: 0.2958\n",
      "Epoch [12/50], Train Loss: 0.0986, Val Loss: 0.2917\n",
      "Epoch [13/50], Train Loss: 0.0966, Val Loss: 0.2877\n",
      "Epoch [14/50], Train Loss: 0.0945, Val Loss: 0.2837\n",
      "Epoch [15/50], Train Loss: 0.0931, Val Loss: 0.2799\n",
      "Epoch [16/50], Train Loss: 0.0912, Val Loss: 0.2761\n",
      "Epoch [17/50], Train Loss: 0.0894, Val Loss: 0.2723\n",
      "Epoch [18/50], Train Loss: 0.0883, Val Loss: 0.2687\n",
      "Epoch [19/50], Train Loss: 0.0866, Val Loss: 0.2651\n",
      "Epoch [20/50], Train Loss: 0.0845, Val Loss: 0.2617\n",
      "Epoch [21/50], Train Loss: 0.0837, Val Loss: 0.2582\n",
      "Epoch [22/50], Train Loss: 0.0817, Val Loss: 0.2549\n",
      "Epoch [23/50], Train Loss: 0.0807, Val Loss: 0.2516\n",
      "Epoch [24/50], Train Loss: 0.0790, Val Loss: 0.2484\n",
      "Epoch [25/50], Train Loss: 0.0777, Val Loss: 0.2452\n",
      "Epoch [26/50], Train Loss: 0.0762, Val Loss: 0.2421\n",
      "Epoch [27/50], Train Loss: 0.0758, Val Loss: 0.2391\n",
      "Epoch [28/50], Train Loss: 0.0732, Val Loss: 0.2361\n",
      "Epoch [29/50], Train Loss: 0.0729, Val Loss: 0.2332\n",
      "Epoch [30/50], Train Loss: 0.0713, Val Loss: 0.2304\n",
      "Epoch [31/50], Train Loss: 0.0701, Val Loss: 0.2276\n",
      "Epoch [32/50], Train Loss: 0.0694, Val Loss: 0.2248\n",
      "Epoch [33/50], Train Loss: 0.0685, Val Loss: 0.2222\n",
      "Epoch [34/50], Train Loss: 0.0671, Val Loss: 0.2195\n",
      "Epoch [35/50], Train Loss: 0.0661, Val Loss: 0.2170\n",
      "Epoch [36/50], Train Loss: 0.0650, Val Loss: 0.2144\n",
      "Epoch [37/50], Train Loss: 0.0638, Val Loss: 0.2119\n",
      "Epoch [38/50], Train Loss: 0.0631, Val Loss: 0.2095\n",
      "Epoch [39/50], Train Loss: 0.0626, Val Loss: 0.2071\n",
      "Epoch [40/50], Train Loss: 0.0609, Val Loss: 0.2048\n",
      "Epoch [41/50], Train Loss: 0.0604, Val Loss: 0.2025\n",
      "Epoch [42/50], Train Loss: 0.0592, Val Loss: 0.2003\n",
      "Epoch [43/50], Train Loss: 0.0589, Val Loss: 0.1981\n",
      "Epoch [44/50], Train Loss: 0.0586, Val Loss: 0.1959\n",
      "Epoch [45/50], Train Loss: 0.0574, Val Loss: 0.1938\n",
      "Epoch [46/50], Train Loss: 0.0565, Val Loss: 0.1917\n",
      "Epoch [47/50], Train Loss: 0.0564, Val Loss: 0.1897\n",
      "Epoch [48/50], Train Loss: 0.0554, Val Loss: 0.1877\n",
      "Epoch [49/50], Train Loss: 0.0547, Val Loss: 0.1857\n",
      "Epoch [50/50], Train Loss: 0.0536, Val Loss: 0.1838\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=16, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1554, Val Loss: 0.3725\n",
      "Epoch [2/50], Train Loss: 0.1510, Val Loss: 0.3667\n",
      "Epoch [3/50], Train Loss: 0.1505, Val Loss: 0.3611\n",
      "Epoch [4/50], Train Loss: 0.1450, Val Loss: 0.3557\n",
      "Epoch [5/50], Train Loss: 0.1425, Val Loss: 0.3503\n",
      "Epoch [6/50], Train Loss: 0.1397, Val Loss: 0.3451\n",
      "Epoch [7/50], Train Loss: 0.1350, Val Loss: 0.3401\n",
      "Epoch [8/50], Train Loss: 0.1346, Val Loss: 0.3351\n",
      "Epoch [9/50], Train Loss: 0.1306, Val Loss: 0.3302\n",
      "Epoch [10/50], Train Loss: 0.1291, Val Loss: 0.3255\n",
      "Epoch [11/50], Train Loss: 0.1246, Val Loss: 0.3209\n",
      "Epoch [12/50], Train Loss: 0.1223, Val Loss: 0.3164\n",
      "Epoch [13/50], Train Loss: 0.1199, Val Loss: 0.3120\n",
      "Epoch [14/50], Train Loss: 0.1179, Val Loss: 0.3077\n",
      "Epoch [15/50], Train Loss: 0.1150, Val Loss: 0.3034\n",
      "Epoch [16/50], Train Loss: 0.1126, Val Loss: 0.2993\n",
      "Epoch [17/50], Train Loss: 0.1106, Val Loss: 0.2953\n",
      "Epoch [18/50], Train Loss: 0.1077, Val Loss: 0.2914\n",
      "Epoch [19/50], Train Loss: 0.1074, Val Loss: 0.2875\n",
      "Epoch [20/50], Train Loss: 0.1064, Val Loss: 0.2838\n",
      "Epoch [21/50], Train Loss: 0.1026, Val Loss: 0.2801\n",
      "Epoch [22/50], Train Loss: 0.1012, Val Loss: 0.2765\n",
      "Epoch [23/50], Train Loss: 0.0984, Val Loss: 0.2731\n",
      "Epoch [24/50], Train Loss: 0.0975, Val Loss: 0.2696\n",
      "Epoch [25/50], Train Loss: 0.0962, Val Loss: 0.2662\n",
      "Epoch [26/50], Train Loss: 0.0941, Val Loss: 0.2629\n",
      "Epoch [27/50], Train Loss: 0.0924, Val Loss: 0.2597\n",
      "Epoch [28/50], Train Loss: 0.0908, Val Loss: 0.2566\n",
      "Epoch [29/50], Train Loss: 0.0890, Val Loss: 0.2535\n",
      "Epoch [30/50], Train Loss: 0.0872, Val Loss: 0.2505\n",
      "Epoch [31/50], Train Loss: 0.0865, Val Loss: 0.2476\n",
      "Epoch [32/50], Train Loss: 0.0847, Val Loss: 0.2447\n",
      "Epoch [33/50], Train Loss: 0.0842, Val Loss: 0.2419\n",
      "Epoch [34/50], Train Loss: 0.0823, Val Loss: 0.2392\n",
      "Epoch [35/50], Train Loss: 0.0816, Val Loss: 0.2365\n",
      "Epoch [36/50], Train Loss: 0.0808, Val Loss: 0.2338\n",
      "Epoch [37/50], Train Loss: 0.0789, Val Loss: 0.2312\n",
      "Epoch [38/50], Train Loss: 0.0773, Val Loss: 0.2287\n",
      "Epoch [39/50], Train Loss: 0.0759, Val Loss: 0.2263\n",
      "Epoch [40/50], Train Loss: 0.0751, Val Loss: 0.2238\n",
      "Epoch [41/50], Train Loss: 0.0739, Val Loss: 0.2215\n",
      "Epoch [42/50], Train Loss: 0.0731, Val Loss: 0.2192\n",
      "Epoch [43/50], Train Loss: 0.0727, Val Loss: 0.2169\n",
      "Epoch [44/50], Train Loss: 0.0710, Val Loss: 0.2147\n",
      "Epoch [45/50], Train Loss: 0.0707, Val Loss: 0.2125\n",
      "Epoch [46/50], Train Loss: 0.0697, Val Loss: 0.2104\n",
      "Epoch [47/50], Train Loss: 0.0683, Val Loss: 0.2083\n",
      "Epoch [48/50], Train Loss: 0.0681, Val Loss: 0.2062\n",
      "Epoch [49/50], Train Loss: 0.0668, Val Loss: 0.2042\n",
      "Epoch [50/50], Train Loss: 0.0652, Val Loss: 0.2023\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=16, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0879, Val Loss: 0.2597\n",
      "Epoch [2/50], Train Loss: 0.0862, Val Loss: 0.2563\n",
      "Epoch [3/50], Train Loss: 0.0845, Val Loss: 0.2530\n",
      "Epoch [4/50], Train Loss: 0.0829, Val Loss: 0.2498\n",
      "Epoch [5/50], Train Loss: 0.0814, Val Loss: 0.2467\n",
      "Epoch [6/50], Train Loss: 0.0799, Val Loss: 0.2436\n",
      "Epoch [7/50], Train Loss: 0.0784, Val Loss: 0.2406\n",
      "Epoch [8/50], Train Loss: 0.0770, Val Loss: 0.2377\n",
      "Epoch [9/50], Train Loss: 0.0756, Val Loss: 0.2348\n",
      "Epoch [10/50], Train Loss: 0.0743, Val Loss: 0.2320\n",
      "Epoch [11/50], Train Loss: 0.0731, Val Loss: 0.2293\n",
      "Epoch [12/50], Train Loss: 0.0718, Val Loss: 0.2266\n",
      "Epoch [13/50], Train Loss: 0.0706, Val Loss: 0.2240\n",
      "Epoch [14/50], Train Loss: 0.0695, Val Loss: 0.2215\n",
      "Epoch [15/50], Train Loss: 0.0684, Val Loss: 0.2190\n",
      "Epoch [16/50], Train Loss: 0.0673, Val Loss: 0.2166\n",
      "Epoch [17/50], Train Loss: 0.0663, Val Loss: 0.2142\n",
      "Epoch [18/50], Train Loss: 0.0652, Val Loss: 0.2119\n",
      "Epoch [19/50], Train Loss: 0.0643, Val Loss: 0.2096\n",
      "Epoch [20/50], Train Loss: 0.0633, Val Loss: 0.2074\n",
      "Epoch [21/50], Train Loss: 0.0624, Val Loss: 0.2053\n",
      "Epoch [22/50], Train Loss: 0.0616, Val Loss: 0.2032\n",
      "Epoch [23/50], Train Loss: 0.0607, Val Loss: 0.2011\n",
      "Epoch [24/50], Train Loss: 0.0599, Val Loss: 0.1991\n",
      "Epoch [25/50], Train Loss: 0.0591, Val Loss: 0.1971\n",
      "Epoch [26/50], Train Loss: 0.0583, Val Loss: 0.1952\n",
      "Epoch [27/50], Train Loss: 0.0576, Val Loss: 0.1934\n",
      "Epoch [28/50], Train Loss: 0.0569, Val Loss: 0.1915\n",
      "Epoch [29/50], Train Loss: 0.0562, Val Loss: 0.1897\n",
      "Epoch [30/50], Train Loss: 0.0555, Val Loss: 0.1880\n",
      "Epoch [31/50], Train Loss: 0.0549, Val Loss: 0.1863\n",
      "Epoch [32/50], Train Loss: 0.0542, Val Loss: 0.1846\n",
      "Epoch [33/50], Train Loss: 0.0536, Val Loss: 0.1830\n",
      "Epoch [34/50], Train Loss: 0.0531, Val Loss: 0.1814\n",
      "Epoch [35/50], Train Loss: 0.0525, Val Loss: 0.1799\n",
      "Epoch [36/50], Train Loss: 0.0520, Val Loss: 0.1783\n",
      "Epoch [37/50], Train Loss: 0.0514, Val Loss: 0.1769\n",
      "Epoch [38/50], Train Loss: 0.0509, Val Loss: 0.1754\n",
      "Epoch [39/50], Train Loss: 0.0504, Val Loss: 0.1740\n",
      "Epoch [40/50], Train Loss: 0.0500, Val Loss: 0.1726\n",
      "Epoch [41/50], Train Loss: 0.0495, Val Loss: 0.1713\n",
      "Epoch [42/50], Train Loss: 0.0491, Val Loss: 0.1699\n",
      "Epoch [43/50], Train Loss: 0.0486, Val Loss: 0.1686\n",
      "Epoch [44/50], Train Loss: 0.0482, Val Loss: 0.1674\n",
      "Epoch [45/50], Train Loss: 0.0478, Val Loss: 0.1661\n",
      "Epoch [46/50], Train Loss: 0.0475, Val Loss: 0.1649\n",
      "Epoch [47/50], Train Loss: 0.0471, Val Loss: 0.1638\n",
      "Epoch [48/50], Train Loss: 0.0467, Val Loss: 0.1626\n",
      "Epoch [49/50], Train Loss: 0.0464, Val Loss: 0.1615\n",
      "Epoch [50/50], Train Loss: 0.0461, Val Loss: 0.1604\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=16, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.2069, Val Loss: 0.5181\n",
      "Epoch [2/50], Train Loss: 0.1997, Val Loss: 0.5069\n",
      "Epoch [3/50], Train Loss: 0.1937, Val Loss: 0.4961\n",
      "Epoch [4/50], Train Loss: 0.1883, Val Loss: 0.4857\n",
      "Epoch [5/50], Train Loss: 0.1818, Val Loss: 0.4757\n",
      "Epoch [6/50], Train Loss: 0.1773, Val Loss: 0.4658\n",
      "Epoch [7/50], Train Loss: 0.1706, Val Loss: 0.4564\n",
      "Epoch [8/50], Train Loss: 0.1651, Val Loss: 0.4473\n",
      "Epoch [9/50], Train Loss: 0.1622, Val Loss: 0.4384\n",
      "Epoch [10/50], Train Loss: 0.1579, Val Loss: 0.4298\n",
      "Epoch [11/50], Train Loss: 0.1523, Val Loss: 0.4215\n",
      "Epoch [12/50], Train Loss: 0.1481, Val Loss: 0.4134\n",
      "Epoch [13/50], Train Loss: 0.1436, Val Loss: 0.4056\n",
      "Epoch [14/50], Train Loss: 0.1395, Val Loss: 0.3981\n",
      "Epoch [15/50], Train Loss: 0.1360, Val Loss: 0.3908\n",
      "Epoch [16/50], Train Loss: 0.1333, Val Loss: 0.3836\n",
      "Epoch [17/50], Train Loss: 0.1303, Val Loss: 0.3767\n",
      "Epoch [18/50], Train Loss: 0.1264, Val Loss: 0.3700\n",
      "Epoch [19/50], Train Loss: 0.1235, Val Loss: 0.3635\n",
      "Epoch [20/50], Train Loss: 0.1195, Val Loss: 0.3571\n",
      "Epoch [21/50], Train Loss: 0.1170, Val Loss: 0.3510\n",
      "Epoch [22/50], Train Loss: 0.1135, Val Loss: 0.3450\n",
      "Epoch [23/50], Train Loss: 0.1124, Val Loss: 0.3392\n",
      "Epoch [24/50], Train Loss: 0.1081, Val Loss: 0.3335\n",
      "Epoch [25/50], Train Loss: 0.1052, Val Loss: 0.3281\n",
      "Epoch [26/50], Train Loss: 0.1031, Val Loss: 0.3227\n",
      "Epoch [27/50], Train Loss: 0.1008, Val Loss: 0.3175\n",
      "Epoch [28/50], Train Loss: 0.0994, Val Loss: 0.3125\n",
      "Epoch [29/50], Train Loss: 0.0970, Val Loss: 0.3076\n",
      "Epoch [30/50], Train Loss: 0.0941, Val Loss: 0.3028\n",
      "Epoch [31/50], Train Loss: 0.0925, Val Loss: 0.2981\n",
      "Epoch [32/50], Train Loss: 0.0900, Val Loss: 0.2936\n",
      "Epoch [33/50], Train Loss: 0.0884, Val Loss: 0.2893\n",
      "Epoch [34/50], Train Loss: 0.0866, Val Loss: 0.2850\n",
      "Epoch [35/50], Train Loss: 0.0851, Val Loss: 0.2808\n",
      "Epoch [36/50], Train Loss: 0.0832, Val Loss: 0.2767\n",
      "Epoch [37/50], Train Loss: 0.0819, Val Loss: 0.2727\n",
      "Epoch [38/50], Train Loss: 0.0805, Val Loss: 0.2689\n",
      "Epoch [39/50], Train Loss: 0.0790, Val Loss: 0.2651\n",
      "Epoch [40/50], Train Loss: 0.0756, Val Loss: 0.2615\n",
      "Epoch [41/50], Train Loss: 0.0759, Val Loss: 0.2579\n",
      "Epoch [42/50], Train Loss: 0.0740, Val Loss: 0.2545\n",
      "Epoch [43/50], Train Loss: 0.0732, Val Loss: 0.2511\n",
      "Epoch [44/50], Train Loss: 0.0723, Val Loss: 0.2478\n",
      "Epoch [45/50], Train Loss: 0.0713, Val Loss: 0.2446\n",
      "Epoch [46/50], Train Loss: 0.0701, Val Loss: 0.2415\n",
      "Epoch [47/50], Train Loss: 0.0686, Val Loss: 0.2384\n",
      "Epoch [48/50], Train Loss: 0.0676, Val Loss: 0.2354\n",
      "Epoch [49/50], Train Loss: 0.0671, Val Loss: 0.2325\n",
      "Epoch [50/50], Train Loss: 0.0647, Val Loss: 0.2297\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=16, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1275, Val Loss: 0.3187\n",
      "Epoch [2/50], Train Loss: 0.1237, Val Loss: 0.3137\n",
      "Epoch [3/50], Train Loss: 0.1216, Val Loss: 0.3090\n",
      "Epoch [4/50], Train Loss: 0.1207, Val Loss: 0.3042\n",
      "Epoch [5/50], Train Loss: 0.1164, Val Loss: 0.2996\n",
      "Epoch [6/50], Train Loss: 0.1154, Val Loss: 0.2951\n",
      "Epoch [7/50], Train Loss: 0.1102, Val Loss: 0.2909\n",
      "Epoch [8/50], Train Loss: 0.1103, Val Loss: 0.2866\n",
      "Epoch [9/50], Train Loss: 0.1065, Val Loss: 0.2825\n",
      "Epoch [10/50], Train Loss: 0.1065, Val Loss: 0.2785\n",
      "Epoch [11/50], Train Loss: 0.1017, Val Loss: 0.2746\n",
      "Epoch [12/50], Train Loss: 0.0997, Val Loss: 0.2708\n",
      "Epoch [13/50], Train Loss: 0.1005, Val Loss: 0.2671\n",
      "Epoch [14/50], Train Loss: 0.0982, Val Loss: 0.2634\n",
      "Epoch [15/50], Train Loss: 0.0946, Val Loss: 0.2599\n",
      "Epoch [16/50], Train Loss: 0.0933, Val Loss: 0.2565\n",
      "Epoch [17/50], Train Loss: 0.0913, Val Loss: 0.2532\n",
      "Epoch [18/50], Train Loss: 0.0904, Val Loss: 0.2499\n",
      "Epoch [19/50], Train Loss: 0.0896, Val Loss: 0.2467\n",
      "Epoch [20/50], Train Loss: 0.0884, Val Loss: 0.2436\n",
      "Epoch [21/50], Train Loss: 0.0867, Val Loss: 0.2406\n",
      "Epoch [22/50], Train Loss: 0.0835, Val Loss: 0.2376\n",
      "Epoch [23/50], Train Loss: 0.0833, Val Loss: 0.2348\n",
      "Epoch [24/50], Train Loss: 0.0813, Val Loss: 0.2320\n",
      "Epoch [25/50], Train Loss: 0.0804, Val Loss: 0.2293\n",
      "Epoch [26/50], Train Loss: 0.0777, Val Loss: 0.2267\n",
      "Epoch [27/50], Train Loss: 0.0779, Val Loss: 0.2241\n",
      "Epoch [28/50], Train Loss: 0.0768, Val Loss: 0.2216\n",
      "Epoch [29/50], Train Loss: 0.0765, Val Loss: 0.2191\n",
      "Epoch [30/50], Train Loss: 0.0746, Val Loss: 0.2168\n",
      "Epoch [31/50], Train Loss: 0.0735, Val Loss: 0.2144\n",
      "Epoch [32/50], Train Loss: 0.0726, Val Loss: 0.2121\n",
      "Epoch [33/50], Train Loss: 0.0715, Val Loss: 0.2098\n",
      "Epoch [34/50], Train Loss: 0.0714, Val Loss: 0.2076\n",
      "Epoch [35/50], Train Loss: 0.0686, Val Loss: 0.2055\n",
      "Epoch [36/50], Train Loss: 0.0685, Val Loss: 0.2034\n",
      "Epoch [37/50], Train Loss: 0.0680, Val Loss: 0.2013\n",
      "Epoch [38/50], Train Loss: 0.0680, Val Loss: 0.1993\n",
      "Epoch [39/50], Train Loss: 0.0651, Val Loss: 0.1975\n",
      "Epoch [40/50], Train Loss: 0.0669, Val Loss: 0.1955\n",
      "Epoch [41/50], Train Loss: 0.0646, Val Loss: 0.1937\n",
      "Epoch [42/50], Train Loss: 0.0645, Val Loss: 0.1919\n",
      "Epoch [43/50], Train Loss: 0.0637, Val Loss: 0.1901\n",
      "Epoch [44/50], Train Loss: 0.0627, Val Loss: 0.1884\n",
      "Epoch [45/50], Train Loss: 0.0619, Val Loss: 0.1868\n",
      "Epoch [46/50], Train Loss: 0.0636, Val Loss: 0.1851\n",
      "Epoch [47/50], Train Loss: 0.0614, Val Loss: 0.1836\n",
      "Epoch [48/50], Train Loss: 0.0627, Val Loss: 0.1820\n",
      "Epoch [49/50], Train Loss: 0.0602, Val Loss: 0.1804\n",
      "Epoch [50/50], Train Loss: 0.0607, Val Loss: 0.1790\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=16, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1473, Val Loss: 0.3760\n",
      "Epoch [2/50], Train Loss: 0.1441, Val Loss: 0.3706\n",
      "Epoch [3/50], Train Loss: 0.1411, Val Loss: 0.3654\n",
      "Epoch [4/50], Train Loss: 0.1381, Val Loss: 0.3602\n",
      "Epoch [5/50], Train Loss: 0.1352, Val Loss: 0.3552\n",
      "Epoch [6/50], Train Loss: 0.1324, Val Loss: 0.3503\n",
      "Epoch [7/50], Train Loss: 0.1296, Val Loss: 0.3455\n",
      "Epoch [8/50], Train Loss: 0.1270, Val Loss: 0.3408\n",
      "Epoch [9/50], Train Loss: 0.1244, Val Loss: 0.3362\n",
      "Epoch [10/50], Train Loss: 0.1219, Val Loss: 0.3317\n",
      "Epoch [11/50], Train Loss: 0.1194, Val Loss: 0.3273\n",
      "Epoch [12/50], Train Loss: 0.1171, Val Loss: 0.3230\n",
      "Epoch [13/50], Train Loss: 0.1148, Val Loss: 0.3188\n",
      "Epoch [14/50], Train Loss: 0.1125, Val Loss: 0.3147\n",
      "Epoch [15/50], Train Loss: 0.1104, Val Loss: 0.3107\n",
      "Epoch [16/50], Train Loss: 0.1083, Val Loss: 0.3067\n",
      "Epoch [17/50], Train Loss: 0.1062, Val Loss: 0.3029\n",
      "Epoch [18/50], Train Loss: 0.1042, Val Loss: 0.2991\n",
      "Epoch [19/50], Train Loss: 0.1023, Val Loss: 0.2954\n",
      "Epoch [20/50], Train Loss: 0.1004, Val Loss: 0.2918\n",
      "Epoch [21/50], Train Loss: 0.0986, Val Loss: 0.2883\n",
      "Epoch [22/50], Train Loss: 0.0968, Val Loss: 0.2848\n",
      "Epoch [23/50], Train Loss: 0.0951, Val Loss: 0.2815\n",
      "Epoch [24/50], Train Loss: 0.0934, Val Loss: 0.2781\n",
      "Epoch [25/50], Train Loss: 0.0918, Val Loss: 0.2749\n",
      "Epoch [26/50], Train Loss: 0.0902, Val Loss: 0.2717\n",
      "Epoch [27/50], Train Loss: 0.0886, Val Loss: 0.2686\n",
      "Epoch [28/50], Train Loss: 0.0871, Val Loss: 0.2656\n",
      "Epoch [29/50], Train Loss: 0.0857, Val Loss: 0.2626\n",
      "Epoch [30/50], Train Loss: 0.0843, Val Loss: 0.2597\n",
      "Epoch [31/50], Train Loss: 0.0829, Val Loss: 0.2568\n",
      "Epoch [32/50], Train Loss: 0.0816, Val Loss: 0.2540\n",
      "Epoch [33/50], Train Loss: 0.0803, Val Loss: 0.2513\n",
      "Epoch [34/50], Train Loss: 0.0790, Val Loss: 0.2486\n",
      "Epoch [35/50], Train Loss: 0.0778, Val Loss: 0.2460\n",
      "Epoch [36/50], Train Loss: 0.0766, Val Loss: 0.2434\n",
      "Epoch [37/50], Train Loss: 0.0754, Val Loss: 0.2409\n",
      "Epoch [38/50], Train Loss: 0.0743, Val Loss: 0.2385\n",
      "Epoch [39/50], Train Loss: 0.0732, Val Loss: 0.2360\n",
      "Epoch [40/50], Train Loss: 0.0722, Val Loss: 0.2337\n",
      "Epoch [41/50], Train Loss: 0.0711, Val Loss: 0.2314\n",
      "Epoch [42/50], Train Loss: 0.0701, Val Loss: 0.2291\n",
      "Epoch [43/50], Train Loss: 0.0692, Val Loss: 0.2269\n",
      "Epoch [44/50], Train Loss: 0.0682, Val Loss: 0.2247\n",
      "Epoch [45/50], Train Loss: 0.0673, Val Loss: 0.2225\n",
      "Epoch [46/50], Train Loss: 0.0664, Val Loss: 0.2204\n",
      "Epoch [47/50], Train Loss: 0.0656, Val Loss: 0.2184\n",
      "Epoch [48/50], Train Loss: 0.0647, Val Loss: 0.2164\n",
      "Epoch [49/50], Train Loss: 0.0639, Val Loss: 0.2144\n",
      "Epoch [50/50], Train Loss: 0.0631, Val Loss: 0.2125\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=16, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1541, Val Loss: 0.4122\n",
      "Epoch [2/50], Train Loss: 0.1500, Val Loss: 0.4055\n",
      "Epoch [3/50], Train Loss: 0.1466, Val Loss: 0.3990\n",
      "Epoch [4/50], Train Loss: 0.1427, Val Loss: 0.3926\n",
      "Epoch [5/50], Train Loss: 0.1393, Val Loss: 0.3864\n",
      "Epoch [6/50], Train Loss: 0.1364, Val Loss: 0.3803\n",
      "Epoch [7/50], Train Loss: 0.1329, Val Loss: 0.3744\n",
      "Epoch [8/50], Train Loss: 0.1299, Val Loss: 0.3686\n",
      "Epoch [9/50], Train Loss: 0.1267, Val Loss: 0.3629\n",
      "Epoch [10/50], Train Loss: 0.1237, Val Loss: 0.3574\n",
      "Epoch [11/50], Train Loss: 0.1217, Val Loss: 0.3520\n",
      "Epoch [12/50], Train Loss: 0.1184, Val Loss: 0.3468\n",
      "Epoch [13/50], Train Loss: 0.1156, Val Loss: 0.3416\n",
      "Epoch [14/50], Train Loss: 0.1137, Val Loss: 0.3366\n",
      "Epoch [15/50], Train Loss: 0.1117, Val Loss: 0.3317\n",
      "Epoch [16/50], Train Loss: 0.1089, Val Loss: 0.3269\n",
      "Epoch [17/50], Train Loss: 0.1067, Val Loss: 0.3223\n",
      "Epoch [18/50], Train Loss: 0.1042, Val Loss: 0.3177\n",
      "Epoch [19/50], Train Loss: 0.1020, Val Loss: 0.3133\n",
      "Epoch [20/50], Train Loss: 0.0993, Val Loss: 0.3089\n",
      "Epoch [21/50], Train Loss: 0.0980, Val Loss: 0.3047\n",
      "Epoch [22/50], Train Loss: 0.0963, Val Loss: 0.3005\n",
      "Epoch [23/50], Train Loss: 0.0941, Val Loss: 0.2965\n",
      "Epoch [24/50], Train Loss: 0.0920, Val Loss: 0.2925\n",
      "Epoch [25/50], Train Loss: 0.0907, Val Loss: 0.2887\n",
      "Epoch [26/50], Train Loss: 0.0887, Val Loss: 0.2849\n",
      "Epoch [27/50], Train Loss: 0.0869, Val Loss: 0.2812\n",
      "Epoch [28/50], Train Loss: 0.0850, Val Loss: 0.2776\n",
      "Epoch [29/50], Train Loss: 0.0837, Val Loss: 0.2741\n",
      "Epoch [30/50], Train Loss: 0.0820, Val Loss: 0.2707\n",
      "Epoch [31/50], Train Loss: 0.0808, Val Loss: 0.2673\n",
      "Epoch [32/50], Train Loss: 0.0795, Val Loss: 0.2640\n",
      "Epoch [33/50], Train Loss: 0.0788, Val Loss: 0.2608\n",
      "Epoch [34/50], Train Loss: 0.0768, Val Loss: 0.2577\n",
      "Epoch [35/50], Train Loss: 0.0756, Val Loss: 0.2546\n",
      "Epoch [36/50], Train Loss: 0.0740, Val Loss: 0.2516\n",
      "Epoch [37/50], Train Loss: 0.0728, Val Loss: 0.2487\n",
      "Epoch [38/50], Train Loss: 0.0717, Val Loss: 0.2458\n",
      "Epoch [39/50], Train Loss: 0.0709, Val Loss: 0.2430\n",
      "Epoch [40/50], Train Loss: 0.0703, Val Loss: 0.2402\n",
      "Epoch [41/50], Train Loss: 0.0690, Val Loss: 0.2376\n",
      "Epoch [42/50], Train Loss: 0.0677, Val Loss: 0.2350\n",
      "Epoch [43/50], Train Loss: 0.0667, Val Loss: 0.2324\n",
      "Epoch [44/50], Train Loss: 0.0657, Val Loss: 0.2299\n",
      "Epoch [45/50], Train Loss: 0.0654, Val Loss: 0.2274\n",
      "Epoch [46/50], Train Loss: 0.0647, Val Loss: 0.2250\n",
      "Epoch [47/50], Train Loss: 0.0632, Val Loss: 0.2227\n",
      "Epoch [48/50], Train Loss: 0.0623, Val Loss: 0.2204\n",
      "Epoch [49/50], Train Loss: 0.0617, Val Loss: 0.2181\n",
      "Epoch [50/50], Train Loss: 0.0605, Val Loss: 0.2160\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=16, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1483, Val Loss: 0.4006\n",
      "Epoch [2/50], Train Loss: 0.1435, Val Loss: 0.3943\n",
      "Epoch [3/50], Train Loss: 0.1408, Val Loss: 0.3880\n",
      "Epoch [4/50], Train Loss: 0.1382, Val Loss: 0.3819\n",
      "Epoch [5/50], Train Loss: 0.1344, Val Loss: 0.3760\n",
      "Epoch [6/50], Train Loss: 0.1311, Val Loss: 0.3703\n",
      "Epoch [7/50], Train Loss: 0.1286, Val Loss: 0.3646\n",
      "Epoch [8/50], Train Loss: 0.1263, Val Loss: 0.3592\n",
      "Epoch [9/50], Train Loss: 0.1238, Val Loss: 0.3538\n",
      "Epoch [10/50], Train Loss: 0.1210, Val Loss: 0.3486\n",
      "Epoch [11/50], Train Loss: 0.1182, Val Loss: 0.3435\n",
      "Epoch [12/50], Train Loss: 0.1159, Val Loss: 0.3386\n",
      "Epoch [13/50], Train Loss: 0.1129, Val Loss: 0.3337\n",
      "Epoch [14/50], Train Loss: 0.1106, Val Loss: 0.3290\n",
      "Epoch [15/50], Train Loss: 0.1084, Val Loss: 0.3244\n",
      "Epoch [16/50], Train Loss: 0.1069, Val Loss: 0.3199\n",
      "Epoch [17/50], Train Loss: 0.1043, Val Loss: 0.3155\n",
      "Epoch [18/50], Train Loss: 0.1021, Val Loss: 0.3112\n",
      "Epoch [19/50], Train Loss: 0.0990, Val Loss: 0.3070\n",
      "Epoch [20/50], Train Loss: 0.0974, Val Loss: 0.3029\n",
      "Epoch [21/50], Train Loss: 0.0968, Val Loss: 0.2990\n",
      "Epoch [22/50], Train Loss: 0.0959, Val Loss: 0.2951\n",
      "Epoch [23/50], Train Loss: 0.0925, Val Loss: 0.2913\n",
      "Epoch [24/50], Train Loss: 0.0912, Val Loss: 0.2876\n",
      "Epoch [25/50], Train Loss: 0.0889, Val Loss: 0.2839\n",
      "Epoch [26/50], Train Loss: 0.0882, Val Loss: 0.2804\n",
      "Epoch [27/50], Train Loss: 0.0869, Val Loss: 0.2769\n",
      "Epoch [28/50], Train Loss: 0.0853, Val Loss: 0.2735\n",
      "Epoch [29/50], Train Loss: 0.0836, Val Loss: 0.2702\n",
      "Epoch [30/50], Train Loss: 0.0818, Val Loss: 0.2670\n",
      "Epoch [31/50], Train Loss: 0.0813, Val Loss: 0.2638\n",
      "Epoch [32/50], Train Loss: 0.0807, Val Loss: 0.2608\n",
      "Epoch [33/50], Train Loss: 0.0790, Val Loss: 0.2578\n",
      "Epoch [34/50], Train Loss: 0.0777, Val Loss: 0.2548\n",
      "Epoch [35/50], Train Loss: 0.0765, Val Loss: 0.2519\n",
      "Epoch [36/50], Train Loss: 0.0758, Val Loss: 0.2491\n",
      "Epoch [37/50], Train Loss: 0.0741, Val Loss: 0.2464\n",
      "Epoch [38/50], Train Loss: 0.0743, Val Loss: 0.2437\n",
      "Epoch [39/50], Train Loss: 0.0722, Val Loss: 0.2411\n",
      "Epoch [40/50], Train Loss: 0.0706, Val Loss: 0.2385\n",
      "Epoch [41/50], Train Loss: 0.0707, Val Loss: 0.2360\n",
      "Epoch [42/50], Train Loss: 0.0707, Val Loss: 0.2336\n",
      "Epoch [43/50], Train Loss: 0.0684, Val Loss: 0.2311\n",
      "Epoch [44/50], Train Loss: 0.0675, Val Loss: 0.2288\n",
      "Epoch [45/50], Train Loss: 0.0665, Val Loss: 0.2265\n",
      "Epoch [46/50], Train Loss: 0.0675, Val Loss: 0.2242\n",
      "Epoch [47/50], Train Loss: 0.0657, Val Loss: 0.2220\n",
      "Epoch [48/50], Train Loss: 0.0641, Val Loss: 0.2199\n",
      "Epoch [49/50], Train Loss: 0.0635, Val Loss: 0.2178\n",
      "Epoch [50/50], Train Loss: 0.0633, Val Loss: 0.2158\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=32, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1261, Val Loss: 0.3689\n",
      "Epoch [2/50], Train Loss: 0.1232, Val Loss: 0.3631\n",
      "Epoch [3/50], Train Loss: 0.1204, Val Loss: 0.3573\n",
      "Epoch [4/50], Train Loss: 0.1176, Val Loss: 0.3518\n",
      "Epoch [5/50], Train Loss: 0.1150, Val Loss: 0.3463\n",
      "Epoch [6/50], Train Loss: 0.1125, Val Loss: 0.3410\n",
      "Epoch [7/50], Train Loss: 0.1100, Val Loss: 0.3358\n",
      "Epoch [8/50], Train Loss: 0.1076, Val Loss: 0.3307\n",
      "Epoch [9/50], Train Loss: 0.1053, Val Loss: 0.3257\n",
      "Epoch [10/50], Train Loss: 0.1030, Val Loss: 0.3209\n",
      "Epoch [11/50], Train Loss: 0.1008, Val Loss: 0.3162\n",
      "Epoch [12/50], Train Loss: 0.0987, Val Loss: 0.3116\n",
      "Epoch [13/50], Train Loss: 0.0967, Val Loss: 0.3070\n",
      "Epoch [14/50], Train Loss: 0.0947, Val Loss: 0.3026\n",
      "Epoch [15/50], Train Loss: 0.0927, Val Loss: 0.2983\n",
      "Epoch [16/50], Train Loss: 0.0909, Val Loss: 0.2941\n",
      "Epoch [17/50], Train Loss: 0.0891, Val Loss: 0.2900\n",
      "Epoch [18/50], Train Loss: 0.0873, Val Loss: 0.2859\n",
      "Epoch [19/50], Train Loss: 0.0856, Val Loss: 0.2820\n",
      "Epoch [20/50], Train Loss: 0.0840, Val Loss: 0.2781\n",
      "Epoch [21/50], Train Loss: 0.0824, Val Loss: 0.2744\n",
      "Epoch [22/50], Train Loss: 0.0808, Val Loss: 0.2707\n",
      "Epoch [23/50], Train Loss: 0.0794, Val Loss: 0.2671\n",
      "Epoch [24/50], Train Loss: 0.0779, Val Loss: 0.2636\n",
      "Epoch [25/50], Train Loss: 0.0765, Val Loss: 0.2601\n",
      "Epoch [26/50], Train Loss: 0.0751, Val Loss: 0.2568\n",
      "Epoch [27/50], Train Loss: 0.0738, Val Loss: 0.2535\n",
      "Epoch [28/50], Train Loss: 0.0725, Val Loss: 0.2502\n",
      "Epoch [29/50], Train Loss: 0.0713, Val Loss: 0.2471\n",
      "Epoch [30/50], Train Loss: 0.0701, Val Loss: 0.2440\n",
      "Epoch [31/50], Train Loss: 0.0689, Val Loss: 0.2410\n",
      "Epoch [32/50], Train Loss: 0.0678, Val Loss: 0.2380\n",
      "Epoch [33/50], Train Loss: 0.0667, Val Loss: 0.2351\n",
      "Epoch [34/50], Train Loss: 0.0656, Val Loss: 0.2323\n",
      "Epoch [35/50], Train Loss: 0.0646, Val Loss: 0.2295\n",
      "Epoch [36/50], Train Loss: 0.0636, Val Loss: 0.2268\n",
      "Epoch [37/50], Train Loss: 0.0626, Val Loss: 0.2242\n",
      "Epoch [38/50], Train Loss: 0.0617, Val Loss: 0.2215\n",
      "Epoch [39/50], Train Loss: 0.0608, Val Loss: 0.2190\n",
      "Epoch [40/50], Train Loss: 0.0599, Val Loss: 0.2165\n",
      "Epoch [41/50], Train Loss: 0.0591, Val Loss: 0.2141\n",
      "Epoch [42/50], Train Loss: 0.0582, Val Loss: 0.2117\n",
      "Epoch [43/50], Train Loss: 0.0574, Val Loss: 0.2093\n",
      "Epoch [44/50], Train Loss: 0.0567, Val Loss: 0.2071\n",
      "Epoch [45/50], Train Loss: 0.0559, Val Loss: 0.2048\n",
      "Epoch [46/50], Train Loss: 0.0552, Val Loss: 0.2026\n",
      "Epoch [47/50], Train Loss: 0.0545, Val Loss: 0.2005\n",
      "Epoch [48/50], Train Loss: 0.0538, Val Loss: 0.1984\n",
      "Epoch [49/50], Train Loss: 0.0531, Val Loss: 0.1963\n",
      "Epoch [50/50], Train Loss: 0.0525, Val Loss: 0.1943\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=32, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1354, Val Loss: 0.3622\n",
      "Epoch [2/50], Train Loss: 0.1321, Val Loss: 0.3564\n",
      "Epoch [3/50], Train Loss: 0.1292, Val Loss: 0.3508\n",
      "Epoch [4/50], Train Loss: 0.1262, Val Loss: 0.3452\n",
      "Epoch [5/50], Train Loss: 0.1231, Val Loss: 0.3399\n",
      "Epoch [6/50], Train Loss: 0.1204, Val Loss: 0.3346\n",
      "Epoch [7/50], Train Loss: 0.1175, Val Loss: 0.3295\n",
      "Epoch [8/50], Train Loss: 0.1153, Val Loss: 0.3244\n",
      "Epoch [9/50], Train Loss: 0.1124, Val Loss: 0.3195\n",
      "Epoch [10/50], Train Loss: 0.1100, Val Loss: 0.3147\n",
      "Epoch [11/50], Train Loss: 0.1077, Val Loss: 0.3100\n",
      "Epoch [12/50], Train Loss: 0.1054, Val Loss: 0.3054\n",
      "Epoch [13/50], Train Loss: 0.1033, Val Loss: 0.3010\n",
      "Epoch [14/50], Train Loss: 0.1011, Val Loss: 0.2966\n",
      "Epoch [15/50], Train Loss: 0.0988, Val Loss: 0.2923\n",
      "Epoch [16/50], Train Loss: 0.0969, Val Loss: 0.2881\n",
      "Epoch [17/50], Train Loss: 0.0953, Val Loss: 0.2840\n",
      "Epoch [18/50], Train Loss: 0.0934, Val Loss: 0.2800\n",
      "Epoch [19/50], Train Loss: 0.0912, Val Loss: 0.2762\n",
      "Epoch [20/50], Train Loss: 0.0896, Val Loss: 0.2723\n",
      "Epoch [21/50], Train Loss: 0.0877, Val Loss: 0.2686\n",
      "Epoch [22/50], Train Loss: 0.0866, Val Loss: 0.2650\n",
      "Epoch [23/50], Train Loss: 0.0848, Val Loss: 0.2614\n",
      "Epoch [24/50], Train Loss: 0.0830, Val Loss: 0.2579\n",
      "Epoch [25/50], Train Loss: 0.0815, Val Loss: 0.2545\n",
      "Epoch [26/50], Train Loss: 0.0800, Val Loss: 0.2511\n",
      "Epoch [27/50], Train Loss: 0.0788, Val Loss: 0.2479\n",
      "Epoch [28/50], Train Loss: 0.0770, Val Loss: 0.2447\n",
      "Epoch [29/50], Train Loss: 0.0761, Val Loss: 0.2416\n",
      "Epoch [30/50], Train Loss: 0.0744, Val Loss: 0.2385\n",
      "Epoch [31/50], Train Loss: 0.0729, Val Loss: 0.2355\n",
      "Epoch [32/50], Train Loss: 0.0717, Val Loss: 0.2326\n",
      "Epoch [33/50], Train Loss: 0.0709, Val Loss: 0.2297\n",
      "Epoch [34/50], Train Loss: 0.0697, Val Loss: 0.2269\n",
      "Epoch [35/50], Train Loss: 0.0693, Val Loss: 0.2242\n",
      "Epoch [36/50], Train Loss: 0.0673, Val Loss: 0.2215\n",
      "Epoch [37/50], Train Loss: 0.0665, Val Loss: 0.2189\n",
      "Epoch [38/50], Train Loss: 0.0658, Val Loss: 0.2163\n",
      "Epoch [39/50], Train Loss: 0.0646, Val Loss: 0.2138\n",
      "Epoch [40/50], Train Loss: 0.0640, Val Loss: 0.2113\n",
      "Epoch [41/50], Train Loss: 0.0625, Val Loss: 0.2089\n",
      "Epoch [42/50], Train Loss: 0.0613, Val Loss: 0.2066\n",
      "Epoch [43/50], Train Loss: 0.0611, Val Loss: 0.2043\n",
      "Epoch [44/50], Train Loss: 0.0600, Val Loss: 0.2020\n",
      "Epoch [45/50], Train Loss: 0.0600, Val Loss: 0.1998\n",
      "Epoch [46/50], Train Loss: 0.0583, Val Loss: 0.1977\n",
      "Epoch [47/50], Train Loss: 0.0578, Val Loss: 0.1956\n",
      "Epoch [48/50], Train Loss: 0.0568, Val Loss: 0.1935\n",
      "Epoch [49/50], Train Loss: 0.0560, Val Loss: 0.1915\n",
      "Epoch [50/50], Train Loss: 0.0559, Val Loss: 0.1895\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=32, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1279, Val Loss: 0.3039\n",
      "Epoch [2/50], Train Loss: 0.1251, Val Loss: 0.2986\n",
      "Epoch [3/50], Train Loss: 0.1220, Val Loss: 0.2933\n",
      "Epoch [4/50], Train Loss: 0.1184, Val Loss: 0.2882\n",
      "Epoch [5/50], Train Loss: 0.1167, Val Loss: 0.2832\n",
      "Epoch [6/50], Train Loss: 0.1144, Val Loss: 0.2784\n",
      "Epoch [7/50], Train Loss: 0.1108, Val Loss: 0.2737\n",
      "Epoch [8/50], Train Loss: 0.1088, Val Loss: 0.2691\n",
      "Epoch [9/50], Train Loss: 0.1062, Val Loss: 0.2646\n",
      "Epoch [10/50], Train Loss: 0.1044, Val Loss: 0.2603\n",
      "Epoch [11/50], Train Loss: 0.1012, Val Loss: 0.2560\n",
      "Epoch [12/50], Train Loss: 0.0993, Val Loss: 0.2519\n",
      "Epoch [13/50], Train Loss: 0.0978, Val Loss: 0.2478\n",
      "Epoch [14/50], Train Loss: 0.0954, Val Loss: 0.2439\n",
      "Epoch [15/50], Train Loss: 0.0931, Val Loss: 0.2401\n",
      "Epoch [16/50], Train Loss: 0.0920, Val Loss: 0.2363\n",
      "Epoch [17/50], Train Loss: 0.0906, Val Loss: 0.2326\n",
      "Epoch [18/50], Train Loss: 0.0879, Val Loss: 0.2291\n",
      "Epoch [19/50], Train Loss: 0.0853, Val Loss: 0.2256\n",
      "Epoch [20/50], Train Loss: 0.0849, Val Loss: 0.2222\n",
      "Epoch [21/50], Train Loss: 0.0830, Val Loss: 0.2189\n",
      "Epoch [22/50], Train Loss: 0.0820, Val Loss: 0.2157\n",
      "Epoch [23/50], Train Loss: 0.0800, Val Loss: 0.2125\n",
      "Epoch [24/50], Train Loss: 0.0782, Val Loss: 0.2094\n",
      "Epoch [25/50], Train Loss: 0.0776, Val Loss: 0.2064\n",
      "Epoch [26/50], Train Loss: 0.0768, Val Loss: 0.2035\n",
      "Epoch [27/50], Train Loss: 0.0758, Val Loss: 0.2006\n",
      "Epoch [28/50], Train Loss: 0.0729, Val Loss: 0.1978\n",
      "Epoch [29/50], Train Loss: 0.0714, Val Loss: 0.1951\n",
      "Epoch [30/50], Train Loss: 0.0710, Val Loss: 0.1924\n",
      "Epoch [31/50], Train Loss: 0.0698, Val Loss: 0.1898\n",
      "Epoch [32/50], Train Loss: 0.0687, Val Loss: 0.1873\n",
      "Epoch [33/50], Train Loss: 0.0679, Val Loss: 0.1848\n",
      "Epoch [34/50], Train Loss: 0.0662, Val Loss: 0.1824\n",
      "Epoch [35/50], Train Loss: 0.0645, Val Loss: 0.1800\n",
      "Epoch [36/50], Train Loss: 0.0651, Val Loss: 0.1778\n",
      "Epoch [37/50], Train Loss: 0.0630, Val Loss: 0.1755\n",
      "Epoch [38/50], Train Loss: 0.0624, Val Loss: 0.1733\n",
      "Epoch [39/50], Train Loss: 0.0606, Val Loss: 0.1712\n",
      "Epoch [40/50], Train Loss: 0.0610, Val Loss: 0.1691\n",
      "Epoch [41/50], Train Loss: 0.0599, Val Loss: 0.1670\n",
      "Epoch [42/50], Train Loss: 0.0589, Val Loss: 0.1651\n",
      "Epoch [43/50], Train Loss: 0.0580, Val Loss: 0.1631\n",
      "Epoch [44/50], Train Loss: 0.0569, Val Loss: 0.1612\n",
      "Epoch [45/50], Train Loss: 0.0566, Val Loss: 0.1593\n",
      "Epoch [46/50], Train Loss: 0.0559, Val Loss: 0.1575\n",
      "Epoch [47/50], Train Loss: 0.0549, Val Loss: 0.1558\n",
      "Epoch [48/50], Train Loss: 0.0545, Val Loss: 0.1540\n",
      "Epoch [49/50], Train Loss: 0.0536, Val Loss: 0.1523\n",
      "Epoch [50/50], Train Loss: 0.0536, Val Loss: 0.1506\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=32, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1011, Val Loss: 0.3257\n",
      "Epoch [2/50], Train Loss: 0.0989, Val Loss: 0.3209\n",
      "Epoch [3/50], Train Loss: 0.0968, Val Loss: 0.3162\n",
      "Epoch [4/50], Train Loss: 0.0948, Val Loss: 0.3116\n",
      "Epoch [5/50], Train Loss: 0.0928, Val Loss: 0.3071\n",
      "Epoch [6/50], Train Loss: 0.0909, Val Loss: 0.3027\n",
      "Epoch [7/50], Train Loss: 0.0890, Val Loss: 0.2985\n",
      "Epoch [8/50], Train Loss: 0.0873, Val Loss: 0.2943\n",
      "Epoch [9/50], Train Loss: 0.0855, Val Loss: 0.2903\n",
      "Epoch [10/50], Train Loss: 0.0839, Val Loss: 0.2863\n",
      "Epoch [11/50], Train Loss: 0.0823, Val Loss: 0.2825\n",
      "Epoch [12/50], Train Loss: 0.0807, Val Loss: 0.2788\n",
      "Epoch [13/50], Train Loss: 0.0793, Val Loss: 0.2751\n",
      "Epoch [14/50], Train Loss: 0.0778, Val Loss: 0.2715\n",
      "Epoch [15/50], Train Loss: 0.0764, Val Loss: 0.2681\n",
      "Epoch [16/50], Train Loss: 0.0751, Val Loss: 0.2647\n",
      "Epoch [17/50], Train Loss: 0.0738, Val Loss: 0.2614\n",
      "Epoch [18/50], Train Loss: 0.0726, Val Loss: 0.2582\n",
      "Epoch [19/50], Train Loss: 0.0714, Val Loss: 0.2550\n",
      "Epoch [20/50], Train Loss: 0.0702, Val Loss: 0.2519\n",
      "Epoch [21/50], Train Loss: 0.0691, Val Loss: 0.2489\n",
      "Epoch [22/50], Train Loss: 0.0680, Val Loss: 0.2460\n",
      "Epoch [23/50], Train Loss: 0.0670, Val Loss: 0.2431\n",
      "Epoch [24/50], Train Loss: 0.0659, Val Loss: 0.2403\n",
      "Epoch [25/50], Train Loss: 0.0650, Val Loss: 0.2376\n",
      "Epoch [26/50], Train Loss: 0.0640, Val Loss: 0.2350\n",
      "Epoch [27/50], Train Loss: 0.0631, Val Loss: 0.2324\n",
      "Epoch [28/50], Train Loss: 0.0622, Val Loss: 0.2298\n",
      "Epoch [29/50], Train Loss: 0.0614, Val Loss: 0.2273\n",
      "Epoch [30/50], Train Loss: 0.0606, Val Loss: 0.2249\n",
      "Epoch [31/50], Train Loss: 0.0598, Val Loss: 0.2225\n",
      "Epoch [32/50], Train Loss: 0.0590, Val Loss: 0.2202\n",
      "Epoch [33/50], Train Loss: 0.0583, Val Loss: 0.2180\n",
      "Epoch [34/50], Train Loss: 0.0576, Val Loss: 0.2158\n",
      "Epoch [35/50], Train Loss: 0.0569, Val Loss: 0.2136\n",
      "Epoch [36/50], Train Loss: 0.0562, Val Loss: 0.2115\n",
      "Epoch [37/50], Train Loss: 0.0556, Val Loss: 0.2094\n",
      "Epoch [38/50], Train Loss: 0.0550, Val Loss: 0.2074\n",
      "Epoch [39/50], Train Loss: 0.0543, Val Loss: 0.2054\n",
      "Epoch [40/50], Train Loss: 0.0538, Val Loss: 0.2035\n",
      "Epoch [41/50], Train Loss: 0.0532, Val Loss: 0.2016\n",
      "Epoch [42/50], Train Loss: 0.0527, Val Loss: 0.1998\n",
      "Epoch [43/50], Train Loss: 0.0521, Val Loss: 0.1980\n",
      "Epoch [44/50], Train Loss: 0.0516, Val Loss: 0.1962\n",
      "Epoch [45/50], Train Loss: 0.0512, Val Loss: 0.1945\n",
      "Epoch [46/50], Train Loss: 0.0507, Val Loss: 0.1928\n",
      "Epoch [47/50], Train Loss: 0.0502, Val Loss: 0.1911\n",
      "Epoch [48/50], Train Loss: 0.0498, Val Loss: 0.1895\n",
      "Epoch [49/50], Train Loss: 0.0494, Val Loss: 0.1879\n",
      "Epoch [50/50], Train Loss: 0.0489, Val Loss: 0.1864\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=32, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1165, Val Loss: 0.3267\n",
      "Epoch [2/50], Train Loss: 0.1144, Val Loss: 0.3223\n",
      "Epoch [3/50], Train Loss: 0.1116, Val Loss: 0.3180\n",
      "Epoch [4/50], Train Loss: 0.1098, Val Loss: 0.3138\n",
      "Epoch [5/50], Train Loss: 0.1078, Val Loss: 0.3098\n",
      "Epoch [6/50], Train Loss: 0.1055, Val Loss: 0.3058\n",
      "Epoch [7/50], Train Loss: 0.1035, Val Loss: 0.3019\n",
      "Epoch [8/50], Train Loss: 0.1018, Val Loss: 0.2981\n",
      "Epoch [9/50], Train Loss: 0.0997, Val Loss: 0.2943\n",
      "Epoch [10/50], Train Loss: 0.0982, Val Loss: 0.2907\n",
      "Epoch [11/50], Train Loss: 0.0963, Val Loss: 0.2871\n",
      "Epoch [12/50], Train Loss: 0.0945, Val Loss: 0.2836\n",
      "Epoch [13/50], Train Loss: 0.0928, Val Loss: 0.2802\n",
      "Epoch [14/50], Train Loss: 0.0910, Val Loss: 0.2769\n",
      "Epoch [15/50], Train Loss: 0.0894, Val Loss: 0.2736\n",
      "Epoch [16/50], Train Loss: 0.0882, Val Loss: 0.2705\n",
      "Epoch [17/50], Train Loss: 0.0869, Val Loss: 0.2673\n",
      "Epoch [18/50], Train Loss: 0.0852, Val Loss: 0.2643\n",
      "Epoch [19/50], Train Loss: 0.0836, Val Loss: 0.2613\n",
      "Epoch [20/50], Train Loss: 0.0828, Val Loss: 0.2584\n",
      "Epoch [21/50], Train Loss: 0.0812, Val Loss: 0.2555\n",
      "Epoch [22/50], Train Loss: 0.0803, Val Loss: 0.2527\n",
      "Epoch [23/50], Train Loss: 0.0788, Val Loss: 0.2499\n",
      "Epoch [24/50], Train Loss: 0.0774, Val Loss: 0.2472\n",
      "Epoch [25/50], Train Loss: 0.0763, Val Loss: 0.2446\n",
      "Epoch [26/50], Train Loss: 0.0756, Val Loss: 0.2421\n",
      "Epoch [27/50], Train Loss: 0.0740, Val Loss: 0.2395\n",
      "Epoch [28/50], Train Loss: 0.0731, Val Loss: 0.2371\n",
      "Epoch [29/50], Train Loss: 0.0720, Val Loss: 0.2347\n",
      "Epoch [30/50], Train Loss: 0.0711, Val Loss: 0.2323\n",
      "Epoch [31/50], Train Loss: 0.0697, Val Loss: 0.2300\n",
      "Epoch [32/50], Train Loss: 0.0694, Val Loss: 0.2277\n",
      "Epoch [33/50], Train Loss: 0.0683, Val Loss: 0.2255\n",
      "Epoch [34/50], Train Loss: 0.0675, Val Loss: 0.2233\n",
      "Epoch [35/50], Train Loss: 0.0665, Val Loss: 0.2212\n",
      "Epoch [36/50], Train Loss: 0.0658, Val Loss: 0.2191\n",
      "Epoch [37/50], Train Loss: 0.0646, Val Loss: 0.2171\n",
      "Epoch [38/50], Train Loss: 0.0639, Val Loss: 0.2151\n",
      "Epoch [39/50], Train Loss: 0.0633, Val Loss: 0.2131\n",
      "Epoch [40/50], Train Loss: 0.0629, Val Loss: 0.2112\n",
      "Epoch [41/50], Train Loss: 0.0617, Val Loss: 0.2093\n",
      "Epoch [42/50], Train Loss: 0.0611, Val Loss: 0.2075\n",
      "Epoch [43/50], Train Loss: 0.0604, Val Loss: 0.2057\n",
      "Epoch [44/50], Train Loss: 0.0600, Val Loss: 0.2039\n",
      "Epoch [45/50], Train Loss: 0.0591, Val Loss: 0.2022\n",
      "Epoch [46/50], Train Loss: 0.0584, Val Loss: 0.2005\n",
      "Epoch [47/50], Train Loss: 0.0581, Val Loss: 0.1989\n",
      "Epoch [48/50], Train Loss: 0.0572, Val Loss: 0.1972\n",
      "Epoch [49/50], Train Loss: 0.0567, Val Loss: 0.1956\n",
      "Epoch [50/50], Train Loss: 0.0560, Val Loss: 0.1941\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=32, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1792, Val Loss: 0.4633\n",
      "Epoch [2/50], Train Loss: 0.1750, Val Loss: 0.4550\n",
      "Epoch [3/50], Train Loss: 0.1688, Val Loss: 0.4471\n",
      "Epoch [4/50], Train Loss: 0.1650, Val Loss: 0.4393\n",
      "Epoch [5/50], Train Loss: 0.1611, Val Loss: 0.4317\n",
      "Epoch [6/50], Train Loss: 0.1590, Val Loss: 0.4244\n",
      "Epoch [7/50], Train Loss: 0.1532, Val Loss: 0.4172\n",
      "Epoch [8/50], Train Loss: 0.1494, Val Loss: 0.4103\n",
      "Epoch [9/50], Train Loss: 0.1448, Val Loss: 0.4035\n",
      "Epoch [10/50], Train Loss: 0.1422, Val Loss: 0.3969\n",
      "Epoch [11/50], Train Loss: 0.1388, Val Loss: 0.3905\n",
      "Epoch [12/50], Train Loss: 0.1361, Val Loss: 0.3843\n",
      "Epoch [13/50], Train Loss: 0.1324, Val Loss: 0.3782\n",
      "Epoch [14/50], Train Loss: 0.1295, Val Loss: 0.3722\n",
      "Epoch [15/50], Train Loss: 0.1262, Val Loss: 0.3664\n",
      "Epoch [16/50], Train Loss: 0.1228, Val Loss: 0.3608\n",
      "Epoch [17/50], Train Loss: 0.1222, Val Loss: 0.3553\n",
      "Epoch [18/50], Train Loss: 0.1178, Val Loss: 0.3499\n",
      "Epoch [19/50], Train Loss: 0.1159, Val Loss: 0.3446\n",
      "Epoch [20/50], Train Loss: 0.1124, Val Loss: 0.3396\n",
      "Epoch [21/50], Train Loss: 0.1102, Val Loss: 0.3346\n",
      "Epoch [22/50], Train Loss: 0.1089, Val Loss: 0.3298\n",
      "Epoch [23/50], Train Loss: 0.1069, Val Loss: 0.3250\n",
      "Epoch [24/50], Train Loss: 0.1045, Val Loss: 0.3204\n",
      "Epoch [25/50], Train Loss: 0.1024, Val Loss: 0.3159\n",
      "Epoch [26/50], Train Loss: 0.1004, Val Loss: 0.3115\n",
      "Epoch [27/50], Train Loss: 0.0983, Val Loss: 0.3072\n",
      "Epoch [28/50], Train Loss: 0.0973, Val Loss: 0.3030\n",
      "Epoch [29/50], Train Loss: 0.0951, Val Loss: 0.2989\n",
      "Epoch [30/50], Train Loss: 0.0926, Val Loss: 0.2949\n",
      "Epoch [31/50], Train Loss: 0.0909, Val Loss: 0.2910\n",
      "Epoch [32/50], Train Loss: 0.0898, Val Loss: 0.2873\n",
      "Epoch [33/50], Train Loss: 0.0882, Val Loss: 0.2835\n",
      "Epoch [34/50], Train Loss: 0.0864, Val Loss: 0.2799\n",
      "Epoch [35/50], Train Loss: 0.0850, Val Loss: 0.2764\n",
      "Epoch [36/50], Train Loss: 0.0823, Val Loss: 0.2730\n",
      "Epoch [37/50], Train Loss: 0.0819, Val Loss: 0.2696\n",
      "Epoch [38/50], Train Loss: 0.0810, Val Loss: 0.2663\n",
      "Epoch [39/50], Train Loss: 0.0785, Val Loss: 0.2630\n",
      "Epoch [40/50], Train Loss: 0.0778, Val Loss: 0.2599\n",
      "Epoch [41/50], Train Loss: 0.0763, Val Loss: 0.2568\n",
      "Epoch [42/50], Train Loss: 0.0759, Val Loss: 0.2538\n",
      "Epoch [43/50], Train Loss: 0.0747, Val Loss: 0.2509\n",
      "Epoch [44/50], Train Loss: 0.0727, Val Loss: 0.2480\n",
      "Epoch [45/50], Train Loss: 0.0721, Val Loss: 0.2452\n",
      "Epoch [46/50], Train Loss: 0.0709, Val Loss: 0.2425\n",
      "Epoch [47/50], Train Loss: 0.0708, Val Loss: 0.2398\n",
      "Epoch [48/50], Train Loss: 0.0688, Val Loss: 0.2372\n",
      "Epoch [49/50], Train Loss: 0.0682, Val Loss: 0.2346\n",
      "Epoch [50/50], Train Loss: 0.0671, Val Loss: 0.2322\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=32, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1046, Val Loss: 0.2924\n",
      "Epoch [2/50], Train Loss: 0.1024, Val Loss: 0.2883\n",
      "Epoch [3/50], Train Loss: 0.1003, Val Loss: 0.2843\n",
      "Epoch [4/50], Train Loss: 0.0982, Val Loss: 0.2804\n",
      "Epoch [5/50], Train Loss: 0.0962, Val Loss: 0.2766\n",
      "Epoch [6/50], Train Loss: 0.0942, Val Loss: 0.2729\n",
      "Epoch [7/50], Train Loss: 0.0924, Val Loss: 0.2693\n",
      "Epoch [8/50], Train Loss: 0.0905, Val Loss: 0.2658\n",
      "Epoch [9/50], Train Loss: 0.0888, Val Loss: 0.2623\n",
      "Epoch [10/50], Train Loss: 0.0871, Val Loss: 0.2590\n",
      "Epoch [11/50], Train Loss: 0.0854, Val Loss: 0.2557\n",
      "Epoch [12/50], Train Loss: 0.0838, Val Loss: 0.2525\n",
      "Epoch [13/50], Train Loss: 0.0823, Val Loss: 0.2494\n",
      "Epoch [14/50], Train Loss: 0.0808, Val Loss: 0.2463\n",
      "Epoch [15/50], Train Loss: 0.0794, Val Loss: 0.2434\n",
      "Epoch [16/50], Train Loss: 0.0780, Val Loss: 0.2405\n",
      "Epoch [17/50], Train Loss: 0.0766, Val Loss: 0.2376\n",
      "Epoch [18/50], Train Loss: 0.0753, Val Loss: 0.2349\n",
      "Epoch [19/50], Train Loss: 0.0741, Val Loss: 0.2322\n",
      "Epoch [20/50], Train Loss: 0.0728, Val Loss: 0.2296\n",
      "Epoch [21/50], Train Loss: 0.0717, Val Loss: 0.2270\n",
      "Epoch [22/50], Train Loss: 0.0705, Val Loss: 0.2245\n",
      "Epoch [23/50], Train Loss: 0.0694, Val Loss: 0.2220\n",
      "Epoch [24/50], Train Loss: 0.0684, Val Loss: 0.2196\n",
      "Epoch [25/50], Train Loss: 0.0673, Val Loss: 0.2173\n",
      "Epoch [26/50], Train Loss: 0.0663, Val Loss: 0.2150\n",
      "Epoch [27/50], Train Loss: 0.0654, Val Loss: 0.2128\n",
      "Epoch [28/50], Train Loss: 0.0644, Val Loss: 0.2106\n",
      "Epoch [29/50], Train Loss: 0.0635, Val Loss: 0.2085\n",
      "Epoch [30/50], Train Loss: 0.0627, Val Loss: 0.2064\n",
      "Epoch [31/50], Train Loss: 0.0618, Val Loss: 0.2044\n",
      "Epoch [32/50], Train Loss: 0.0610, Val Loss: 0.2024\n",
      "Epoch [33/50], Train Loss: 0.0602, Val Loss: 0.2005\n",
      "Epoch [34/50], Train Loss: 0.0594, Val Loss: 0.1986\n",
      "Epoch [35/50], Train Loss: 0.0587, Val Loss: 0.1967\n",
      "Epoch [36/50], Train Loss: 0.0580, Val Loss: 0.1949\n",
      "Epoch [37/50], Train Loss: 0.0573, Val Loss: 0.1931\n",
      "Epoch [38/50], Train Loss: 0.0566, Val Loss: 0.1914\n",
      "Epoch [39/50], Train Loss: 0.0560, Val Loss: 0.1897\n",
      "Epoch [40/50], Train Loss: 0.0554, Val Loss: 0.1881\n",
      "Epoch [41/50], Train Loss: 0.0547, Val Loss: 0.1865\n",
      "Epoch [42/50], Train Loss: 0.0542, Val Loss: 0.1849\n",
      "Epoch [43/50], Train Loss: 0.0536, Val Loss: 0.1833\n",
      "Epoch [44/50], Train Loss: 0.0531, Val Loss: 0.1818\n",
      "Epoch [45/50], Train Loss: 0.0525, Val Loss: 0.1804\n",
      "Epoch [46/50], Train Loss: 0.0520, Val Loss: 0.1789\n",
      "Epoch [47/50], Train Loss: 0.0515, Val Loss: 0.1775\n",
      "Epoch [48/50], Train Loss: 0.0510, Val Loss: 0.1761\n",
      "Epoch [49/50], Train Loss: 0.0506, Val Loss: 0.1748\n",
      "Epoch [50/50], Train Loss: 0.0501, Val Loss: 0.1735\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=32, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.2002, Val Loss: 0.4837\n",
      "Epoch [2/50], Train Loss: 0.1950, Val Loss: 0.4754\n",
      "Epoch [3/50], Train Loss: 0.1898, Val Loss: 0.4672\n",
      "Epoch [4/50], Train Loss: 0.1856, Val Loss: 0.4593\n",
      "Epoch [5/50], Train Loss: 0.1806, Val Loss: 0.4516\n",
      "Epoch [6/50], Train Loss: 0.1762, Val Loss: 0.4440\n",
      "Epoch [7/50], Train Loss: 0.1721, Val Loss: 0.4367\n",
      "Epoch [8/50], Train Loss: 0.1675, Val Loss: 0.4296\n",
      "Epoch [9/50], Train Loss: 0.1639, Val Loss: 0.4226\n",
      "Epoch [10/50], Train Loss: 0.1600, Val Loss: 0.4159\n",
      "Epoch [11/50], Train Loss: 0.1564, Val Loss: 0.4092\n",
      "Epoch [12/50], Train Loss: 0.1525, Val Loss: 0.4028\n",
      "Epoch [13/50], Train Loss: 0.1487, Val Loss: 0.3965\n",
      "Epoch [14/50], Train Loss: 0.1453, Val Loss: 0.3904\n",
      "Epoch [15/50], Train Loss: 0.1419, Val Loss: 0.3845\n",
      "Epoch [16/50], Train Loss: 0.1388, Val Loss: 0.3787\n",
      "Epoch [17/50], Train Loss: 0.1360, Val Loss: 0.3730\n",
      "Epoch [18/50], Train Loss: 0.1330, Val Loss: 0.3674\n",
      "Epoch [19/50], Train Loss: 0.1301, Val Loss: 0.3620\n",
      "Epoch [20/50], Train Loss: 0.1268, Val Loss: 0.3568\n",
      "Epoch [21/50], Train Loss: 0.1244, Val Loss: 0.3516\n",
      "Epoch [22/50], Train Loss: 0.1217, Val Loss: 0.3466\n",
      "Epoch [23/50], Train Loss: 0.1191, Val Loss: 0.3417\n",
      "Epoch [24/50], Train Loss: 0.1168, Val Loss: 0.3369\n",
      "Epoch [25/50], Train Loss: 0.1139, Val Loss: 0.3323\n",
      "Epoch [26/50], Train Loss: 0.1120, Val Loss: 0.3277\n",
      "Epoch [27/50], Train Loss: 0.1099, Val Loss: 0.3232\n",
      "Epoch [28/50], Train Loss: 0.1073, Val Loss: 0.3189\n",
      "Epoch [29/50], Train Loss: 0.1054, Val Loss: 0.3146\n",
      "Epoch [30/50], Train Loss: 0.1035, Val Loss: 0.3105\n",
      "Epoch [31/50], Train Loss: 0.1013, Val Loss: 0.3064\n",
      "Epoch [32/50], Train Loss: 0.0992, Val Loss: 0.3025\n",
      "Epoch [33/50], Train Loss: 0.0974, Val Loss: 0.2986\n",
      "Epoch [34/50], Train Loss: 0.0955, Val Loss: 0.2948\n",
      "Epoch [35/50], Train Loss: 0.0938, Val Loss: 0.2912\n",
      "Epoch [36/50], Train Loss: 0.0924, Val Loss: 0.2875\n",
      "Epoch [37/50], Train Loss: 0.0906, Val Loss: 0.2840\n",
      "Epoch [38/50], Train Loss: 0.0890, Val Loss: 0.2806\n",
      "Epoch [39/50], Train Loss: 0.0873, Val Loss: 0.2772\n",
      "Epoch [40/50], Train Loss: 0.0855, Val Loss: 0.2739\n",
      "Epoch [41/50], Train Loss: 0.0842, Val Loss: 0.2707\n",
      "Epoch [42/50], Train Loss: 0.0829, Val Loss: 0.2675\n",
      "Epoch [43/50], Train Loss: 0.0813, Val Loss: 0.2644\n",
      "Epoch [44/50], Train Loss: 0.0801, Val Loss: 0.2614\n",
      "Epoch [45/50], Train Loss: 0.0791, Val Loss: 0.2584\n",
      "Epoch [46/50], Train Loss: 0.0780, Val Loss: 0.2555\n",
      "Epoch [47/50], Train Loss: 0.0766, Val Loss: 0.2527\n",
      "Epoch [48/50], Train Loss: 0.0755, Val Loss: 0.2500\n",
      "Epoch [49/50], Train Loss: 0.0742, Val Loss: 0.2472\n",
      "Epoch [50/50], Train Loss: 0.0730, Val Loss: 0.2446\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=32, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.2212, Val Loss: 0.4945\n",
      "Epoch [2/50], Train Loss: 0.2150, Val Loss: 0.4858\n",
      "Epoch [3/50], Train Loss: 0.2098, Val Loss: 0.4773\n",
      "Epoch [4/50], Train Loss: 0.2036, Val Loss: 0.4690\n",
      "Epoch [5/50], Train Loss: 0.1988, Val Loss: 0.4610\n",
      "Epoch [6/50], Train Loss: 0.1943, Val Loss: 0.4531\n",
      "Epoch [7/50], Train Loss: 0.1892, Val Loss: 0.4455\n",
      "Epoch [8/50], Train Loss: 0.1848, Val Loss: 0.4381\n",
      "Epoch [9/50], Train Loss: 0.1802, Val Loss: 0.4308\n",
      "Epoch [10/50], Train Loss: 0.1754, Val Loss: 0.4237\n",
      "Epoch [11/50], Train Loss: 0.1714, Val Loss: 0.4168\n",
      "Epoch [12/50], Train Loss: 0.1670, Val Loss: 0.4101\n",
      "Epoch [13/50], Train Loss: 0.1627, Val Loss: 0.4036\n",
      "Epoch [14/50], Train Loss: 0.1589, Val Loss: 0.3972\n",
      "Epoch [15/50], Train Loss: 0.1551, Val Loss: 0.3910\n",
      "Epoch [16/50], Train Loss: 0.1518, Val Loss: 0.3849\n",
      "Epoch [17/50], Train Loss: 0.1485, Val Loss: 0.3790\n",
      "Epoch [18/50], Train Loss: 0.1449, Val Loss: 0.3733\n",
      "Epoch [19/50], Train Loss: 0.1423, Val Loss: 0.3676\n",
      "Epoch [20/50], Train Loss: 0.1393, Val Loss: 0.3621\n",
      "Epoch [21/50], Train Loss: 0.1352, Val Loss: 0.3567\n",
      "Epoch [22/50], Train Loss: 0.1326, Val Loss: 0.3515\n",
      "Epoch [23/50], Train Loss: 0.1294, Val Loss: 0.3464\n",
      "Epoch [24/50], Train Loss: 0.1283, Val Loss: 0.3414\n",
      "Epoch [25/50], Train Loss: 0.1240, Val Loss: 0.3366\n",
      "Epoch [26/50], Train Loss: 0.1216, Val Loss: 0.3318\n",
      "Epoch [27/50], Train Loss: 0.1188, Val Loss: 0.3272\n",
      "Epoch [28/50], Train Loss: 0.1169, Val Loss: 0.3226\n",
      "Epoch [29/50], Train Loss: 0.1145, Val Loss: 0.3182\n",
      "Epoch [30/50], Train Loss: 0.1114, Val Loss: 0.3139\n",
      "Epoch [31/50], Train Loss: 0.1097, Val Loss: 0.3097\n",
      "Epoch [32/50], Train Loss: 0.1073, Val Loss: 0.3055\n",
      "Epoch [33/50], Train Loss: 0.1050, Val Loss: 0.3015\n",
      "Epoch [34/50], Train Loss: 0.1036, Val Loss: 0.2976\n",
      "Epoch [35/50], Train Loss: 0.1007, Val Loss: 0.2937\n",
      "Epoch [36/50], Train Loss: 0.0991, Val Loss: 0.2900\n",
      "Epoch [37/50], Train Loss: 0.0978, Val Loss: 0.2863\n",
      "Epoch [38/50], Train Loss: 0.0958, Val Loss: 0.2827\n",
      "Epoch [39/50], Train Loss: 0.0938, Val Loss: 0.2792\n",
      "Epoch [40/50], Train Loss: 0.0928, Val Loss: 0.2757\n",
      "Epoch [41/50], Train Loss: 0.0906, Val Loss: 0.2723\n",
      "Epoch [42/50], Train Loss: 0.0886, Val Loss: 0.2691\n",
      "Epoch [43/50], Train Loss: 0.0871, Val Loss: 0.2658\n",
      "Epoch [44/50], Train Loss: 0.0861, Val Loss: 0.2627\n",
      "Epoch [45/50], Train Loss: 0.0842, Val Loss: 0.2597\n",
      "Epoch [46/50], Train Loss: 0.0828, Val Loss: 0.2567\n",
      "Epoch [47/50], Train Loss: 0.0821, Val Loss: 0.2538\n",
      "Epoch [48/50], Train Loss: 0.0800, Val Loss: 0.2509\n",
      "Epoch [49/50], Train Loss: 0.0796, Val Loss: 0.2481\n",
      "Epoch [50/50], Train Loss: 0.0783, Val Loss: 0.2453\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=64, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1485, Val Loss: 0.3873\n",
      "Epoch [2/50], Train Loss: 0.1450, Val Loss: 0.3809\n",
      "Epoch [3/50], Train Loss: 0.1416, Val Loss: 0.3748\n",
      "Epoch [4/50], Train Loss: 0.1383, Val Loss: 0.3687\n",
      "Epoch [5/50], Train Loss: 0.1351, Val Loss: 0.3628\n",
      "Epoch [6/50], Train Loss: 0.1320, Val Loss: 0.3571\n",
      "Epoch [7/50], Train Loss: 0.1290, Val Loss: 0.3515\n",
      "Epoch [8/50], Train Loss: 0.1261, Val Loss: 0.3460\n",
      "Epoch [9/50], Train Loss: 0.1233, Val Loss: 0.3407\n",
      "Epoch [10/50], Train Loss: 0.1206, Val Loss: 0.3355\n",
      "Epoch [11/50], Train Loss: 0.1179, Val Loss: 0.3304\n",
      "Epoch [12/50], Train Loss: 0.1153, Val Loss: 0.3254\n",
      "Epoch [13/50], Train Loss: 0.1129, Val Loss: 0.3205\n",
      "Epoch [14/50], Train Loss: 0.1105, Val Loss: 0.3158\n",
      "Epoch [15/50], Train Loss: 0.1081, Val Loss: 0.3111\n",
      "Epoch [16/50], Train Loss: 0.1059, Val Loss: 0.3066\n",
      "Epoch [17/50], Train Loss: 0.1037, Val Loss: 0.3021\n",
      "Epoch [18/50], Train Loss: 0.1015, Val Loss: 0.2978\n",
      "Epoch [19/50], Train Loss: 0.0995, Val Loss: 0.2936\n",
      "Epoch [20/50], Train Loss: 0.0975, Val Loss: 0.2894\n",
      "Epoch [21/50], Train Loss: 0.0955, Val Loss: 0.2854\n",
      "Epoch [22/50], Train Loss: 0.0936, Val Loss: 0.2814\n",
      "Epoch [23/50], Train Loss: 0.0918, Val Loss: 0.2775\n",
      "Epoch [24/50], Train Loss: 0.0900, Val Loss: 0.2737\n",
      "Epoch [25/50], Train Loss: 0.0883, Val Loss: 0.2700\n",
      "Epoch [26/50], Train Loss: 0.0866, Val Loss: 0.2664\n",
      "Epoch [27/50], Train Loss: 0.0850, Val Loss: 0.2628\n",
      "Epoch [28/50], Train Loss: 0.0835, Val Loss: 0.2594\n",
      "Epoch [29/50], Train Loss: 0.0819, Val Loss: 0.2560\n",
      "Epoch [30/50], Train Loss: 0.0805, Val Loss: 0.2527\n",
      "Epoch [31/50], Train Loss: 0.0790, Val Loss: 0.2494\n",
      "Epoch [32/50], Train Loss: 0.0776, Val Loss: 0.2462\n",
      "Epoch [33/50], Train Loss: 0.0763, Val Loss: 0.2431\n",
      "Epoch [34/50], Train Loss: 0.0750, Val Loss: 0.2401\n",
      "Epoch [35/50], Train Loss: 0.0737, Val Loss: 0.2371\n",
      "Epoch [36/50], Train Loss: 0.0725, Val Loss: 0.2341\n",
      "Epoch [37/50], Train Loss: 0.0713, Val Loss: 0.2313\n",
      "Epoch [38/50], Train Loss: 0.0701, Val Loss: 0.2285\n",
      "Epoch [39/50], Train Loss: 0.0690, Val Loss: 0.2258\n",
      "Epoch [40/50], Train Loss: 0.0679, Val Loss: 0.2231\n",
      "Epoch [41/50], Train Loss: 0.0669, Val Loss: 0.2204\n",
      "Epoch [42/50], Train Loss: 0.0658, Val Loss: 0.2179\n",
      "Epoch [43/50], Train Loss: 0.0648, Val Loss: 0.2153\n",
      "Epoch [44/50], Train Loss: 0.0639, Val Loss: 0.2129\n",
      "Epoch [45/50], Train Loss: 0.0629, Val Loss: 0.2105\n",
      "Epoch [46/50], Train Loss: 0.0620, Val Loss: 0.2081\n",
      "Epoch [47/50], Train Loss: 0.0612, Val Loss: 0.2058\n",
      "Epoch [48/50], Train Loss: 0.0603, Val Loss: 0.2035\n",
      "Epoch [49/50], Train Loss: 0.0595, Val Loss: 0.2013\n",
      "Epoch [50/50], Train Loss: 0.0587, Val Loss: 0.1991\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=64, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1650, Val Loss: 0.4369\n",
      "Epoch [2/50], Train Loss: 0.1606, Val Loss: 0.4291\n",
      "Epoch [3/50], Train Loss: 0.1566, Val Loss: 0.4215\n",
      "Epoch [4/50], Train Loss: 0.1525, Val Loss: 0.4141\n",
      "Epoch [5/50], Train Loss: 0.1494, Val Loss: 0.4069\n",
      "Epoch [6/50], Train Loss: 0.1451, Val Loss: 0.3998\n",
      "Epoch [7/50], Train Loss: 0.1418, Val Loss: 0.3930\n",
      "Epoch [8/50], Train Loss: 0.1384, Val Loss: 0.3863\n",
      "Epoch [9/50], Train Loss: 0.1347, Val Loss: 0.3798\n",
      "Epoch [10/50], Train Loss: 0.1319, Val Loss: 0.3734\n",
      "Epoch [11/50], Train Loss: 0.1285, Val Loss: 0.3672\n",
      "Epoch [12/50], Train Loss: 0.1256, Val Loss: 0.3611\n",
      "Epoch [13/50], Train Loss: 0.1228, Val Loss: 0.3553\n",
      "Epoch [14/50], Train Loss: 0.1200, Val Loss: 0.3495\n",
      "Epoch [15/50], Train Loss: 0.1167, Val Loss: 0.3439\n",
      "Epoch [16/50], Train Loss: 0.1148, Val Loss: 0.3384\n",
      "Epoch [17/50], Train Loss: 0.1117, Val Loss: 0.3331\n",
      "Epoch [18/50], Train Loss: 0.1094, Val Loss: 0.3279\n",
      "Epoch [19/50], Train Loss: 0.1072, Val Loss: 0.3227\n",
      "Epoch [20/50], Train Loss: 0.1048, Val Loss: 0.3178\n",
      "Epoch [21/50], Train Loss: 0.1022, Val Loss: 0.3129\n",
      "Epoch [22/50], Train Loss: 0.1003, Val Loss: 0.3082\n",
      "Epoch [23/50], Train Loss: 0.0978, Val Loss: 0.3035\n",
      "Epoch [24/50], Train Loss: 0.0959, Val Loss: 0.2990\n",
      "Epoch [25/50], Train Loss: 0.0942, Val Loss: 0.2946\n",
      "Epoch [26/50], Train Loss: 0.0922, Val Loss: 0.2903\n",
      "Epoch [27/50], Train Loss: 0.0904, Val Loss: 0.2860\n",
      "Epoch [28/50], Train Loss: 0.0884, Val Loss: 0.2819\n",
      "Epoch [29/50], Train Loss: 0.0867, Val Loss: 0.2779\n",
      "Epoch [30/50], Train Loss: 0.0849, Val Loss: 0.2740\n",
      "Epoch [31/50], Train Loss: 0.0831, Val Loss: 0.2701\n",
      "Epoch [32/50], Train Loss: 0.0821, Val Loss: 0.2664\n",
      "Epoch [33/50], Train Loss: 0.0802, Val Loss: 0.2627\n",
      "Epoch [34/50], Train Loss: 0.0786, Val Loss: 0.2591\n",
      "Epoch [35/50], Train Loss: 0.0772, Val Loss: 0.2556\n",
      "Epoch [36/50], Train Loss: 0.0757, Val Loss: 0.2522\n",
      "Epoch [37/50], Train Loss: 0.0742, Val Loss: 0.2488\n",
      "Epoch [38/50], Train Loss: 0.0732, Val Loss: 0.2455\n",
      "Epoch [39/50], Train Loss: 0.0721, Val Loss: 0.2424\n",
      "Epoch [40/50], Train Loss: 0.0706, Val Loss: 0.2392\n",
      "Epoch [41/50], Train Loss: 0.0696, Val Loss: 0.2362\n",
      "Epoch [42/50], Train Loss: 0.0682, Val Loss: 0.2332\n",
      "Epoch [43/50], Train Loss: 0.0676, Val Loss: 0.2303\n",
      "Epoch [44/50], Train Loss: 0.0662, Val Loss: 0.2274\n",
      "Epoch [45/50], Train Loss: 0.0651, Val Loss: 0.2246\n",
      "Epoch [46/50], Train Loss: 0.0640, Val Loss: 0.2219\n",
      "Epoch [47/50], Train Loss: 0.0632, Val Loss: 0.2192\n",
      "Epoch [48/50], Train Loss: 0.0622, Val Loss: 0.2166\n",
      "Epoch [49/50], Train Loss: 0.0613, Val Loss: 0.2140\n",
      "Epoch [50/50], Train Loss: 0.0604, Val Loss: 0.2115\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=64, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1397, Val Loss: 0.3832\n",
      "Epoch [2/50], Train Loss: 0.1372, Val Loss: 0.3768\n",
      "Epoch [3/50], Train Loss: 0.1333, Val Loss: 0.3706\n",
      "Epoch [4/50], Train Loss: 0.1297, Val Loss: 0.3646\n",
      "Epoch [5/50], Train Loss: 0.1265, Val Loss: 0.3587\n",
      "Epoch [6/50], Train Loss: 0.1243, Val Loss: 0.3529\n",
      "Epoch [7/50], Train Loss: 0.1209, Val Loss: 0.3473\n",
      "Epoch [8/50], Train Loss: 0.1183, Val Loss: 0.3419\n",
      "Epoch [9/50], Train Loss: 0.1165, Val Loss: 0.3365\n",
      "Epoch [10/50], Train Loss: 0.1137, Val Loss: 0.3313\n",
      "Epoch [11/50], Train Loss: 0.1113, Val Loss: 0.3262\n",
      "Epoch [12/50], Train Loss: 0.1088, Val Loss: 0.3212\n",
      "Epoch [13/50], Train Loss: 0.1066, Val Loss: 0.3164\n",
      "Epoch [14/50], Train Loss: 0.1043, Val Loss: 0.3117\n",
      "Epoch [15/50], Train Loss: 0.1018, Val Loss: 0.3071\n",
      "Epoch [16/50], Train Loss: 0.0999, Val Loss: 0.3026\n",
      "Epoch [17/50], Train Loss: 0.0976, Val Loss: 0.2982\n",
      "Epoch [18/50], Train Loss: 0.0959, Val Loss: 0.2939\n",
      "Epoch [19/50], Train Loss: 0.0944, Val Loss: 0.2897\n",
      "Epoch [20/50], Train Loss: 0.0927, Val Loss: 0.2856\n",
      "Epoch [21/50], Train Loss: 0.0905, Val Loss: 0.2816\n",
      "Epoch [22/50], Train Loss: 0.0886, Val Loss: 0.2777\n",
      "Epoch [23/50], Train Loss: 0.0862, Val Loss: 0.2739\n",
      "Epoch [24/50], Train Loss: 0.0856, Val Loss: 0.2701\n",
      "Epoch [25/50], Train Loss: 0.0837, Val Loss: 0.2665\n",
      "Epoch [26/50], Train Loss: 0.0815, Val Loss: 0.2629\n",
      "Epoch [27/50], Train Loss: 0.0797, Val Loss: 0.2594\n",
      "Epoch [28/50], Train Loss: 0.0786, Val Loss: 0.2560\n",
      "Epoch [29/50], Train Loss: 0.0776, Val Loss: 0.2527\n",
      "Epoch [30/50], Train Loss: 0.0768, Val Loss: 0.2494\n",
      "Epoch [31/50], Train Loss: 0.0754, Val Loss: 0.2463\n",
      "Epoch [32/50], Train Loss: 0.0735, Val Loss: 0.2432\n",
      "Epoch [33/50], Train Loss: 0.0726, Val Loss: 0.2402\n",
      "Epoch [34/50], Train Loss: 0.0716, Val Loss: 0.2372\n",
      "Epoch [35/50], Train Loss: 0.0704, Val Loss: 0.2343\n",
      "Epoch [36/50], Train Loss: 0.0687, Val Loss: 0.2314\n",
      "Epoch [37/50], Train Loss: 0.0681, Val Loss: 0.2287\n",
      "Epoch [38/50], Train Loss: 0.0669, Val Loss: 0.2259\n",
      "Epoch [39/50], Train Loss: 0.0663, Val Loss: 0.2233\n",
      "Epoch [40/50], Train Loss: 0.0649, Val Loss: 0.2207\n",
      "Epoch [41/50], Train Loss: 0.0634, Val Loss: 0.2181\n",
      "Epoch [42/50], Train Loss: 0.0630, Val Loss: 0.2157\n",
      "Epoch [43/50], Train Loss: 0.0623, Val Loss: 0.2132\n",
      "Epoch [44/50], Train Loss: 0.0614, Val Loss: 0.2108\n",
      "Epoch [45/50], Train Loss: 0.0610, Val Loss: 0.2085\n",
      "Epoch [46/50], Train Loss: 0.0599, Val Loss: 0.2062\n",
      "Epoch [47/50], Train Loss: 0.0591, Val Loss: 0.2040\n",
      "Epoch [48/50], Train Loss: 0.0581, Val Loss: 0.2018\n",
      "Epoch [49/50], Train Loss: 0.0579, Val Loss: 0.1996\n",
      "Epoch [50/50], Train Loss: 0.0577, Val Loss: 0.1975\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=64, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1311, Val Loss: 0.3686\n",
      "Epoch [2/50], Train Loss: 0.1282, Val Loss: 0.3632\n",
      "Epoch [3/50], Train Loss: 0.1254, Val Loss: 0.3579\n",
      "Epoch [4/50], Train Loss: 0.1226, Val Loss: 0.3528\n",
      "Epoch [5/50], Train Loss: 0.1200, Val Loss: 0.3477\n",
      "Epoch [6/50], Train Loss: 0.1174, Val Loss: 0.3428\n",
      "Epoch [7/50], Train Loss: 0.1149, Val Loss: 0.3380\n",
      "Epoch [8/50], Train Loss: 0.1125, Val Loss: 0.3333\n",
      "Epoch [9/50], Train Loss: 0.1102, Val Loss: 0.3287\n",
      "Epoch [10/50], Train Loss: 0.1079, Val Loss: 0.3242\n",
      "Epoch [11/50], Train Loss: 0.1057, Val Loss: 0.3198\n",
      "Epoch [12/50], Train Loss: 0.1036, Val Loss: 0.3155\n",
      "Epoch [13/50], Train Loss: 0.1015, Val Loss: 0.3113\n",
      "Epoch [14/50], Train Loss: 0.0995, Val Loss: 0.3072\n",
      "Epoch [15/50], Train Loss: 0.0975, Val Loss: 0.3031\n",
      "Epoch [16/50], Train Loss: 0.0957, Val Loss: 0.2992\n",
      "Epoch [17/50], Train Loss: 0.0938, Val Loss: 0.2954\n",
      "Epoch [18/50], Train Loss: 0.0920, Val Loss: 0.2916\n",
      "Epoch [19/50], Train Loss: 0.0903, Val Loss: 0.2879\n",
      "Epoch [20/50], Train Loss: 0.0887, Val Loss: 0.2843\n",
      "Epoch [21/50], Train Loss: 0.0870, Val Loss: 0.2808\n",
      "Epoch [22/50], Train Loss: 0.0855, Val Loss: 0.2774\n",
      "Epoch [23/50], Train Loss: 0.0839, Val Loss: 0.2740\n",
      "Epoch [24/50], Train Loss: 0.0825, Val Loss: 0.2707\n",
      "Epoch [25/50], Train Loss: 0.0810, Val Loss: 0.2675\n",
      "Epoch [26/50], Train Loss: 0.0797, Val Loss: 0.2643\n",
      "Epoch [27/50], Train Loss: 0.0783, Val Loss: 0.2613\n",
      "Epoch [28/50], Train Loss: 0.0770, Val Loss: 0.2582\n",
      "Epoch [29/50], Train Loss: 0.0757, Val Loss: 0.2553\n",
      "Epoch [30/50], Train Loss: 0.0745, Val Loss: 0.2524\n",
      "Epoch [31/50], Train Loss: 0.0733, Val Loss: 0.2496\n",
      "Epoch [32/50], Train Loss: 0.0722, Val Loss: 0.2468\n",
      "Epoch [33/50], Train Loss: 0.0711, Val Loss: 0.2441\n",
      "Epoch [34/50], Train Loss: 0.0700, Val Loss: 0.2414\n",
      "Epoch [35/50], Train Loss: 0.0689, Val Loss: 0.2388\n",
      "Epoch [36/50], Train Loss: 0.0679, Val Loss: 0.2363\n",
      "Epoch [37/50], Train Loss: 0.0669, Val Loss: 0.2338\n",
      "Epoch [38/50], Train Loss: 0.0660, Val Loss: 0.2314\n",
      "Epoch [39/50], Train Loss: 0.0650, Val Loss: 0.2290\n",
      "Epoch [40/50], Train Loss: 0.0641, Val Loss: 0.2266\n",
      "Epoch [41/50], Train Loss: 0.0633, Val Loss: 0.2243\n",
      "Epoch [42/50], Train Loss: 0.0624, Val Loss: 0.2221\n",
      "Epoch [43/50], Train Loss: 0.0616, Val Loss: 0.2199\n",
      "Epoch [44/50], Train Loss: 0.0608, Val Loss: 0.2178\n",
      "Epoch [45/50], Train Loss: 0.0600, Val Loss: 0.2157\n",
      "Epoch [46/50], Train Loss: 0.0593, Val Loss: 0.2136\n",
      "Epoch [47/50], Train Loss: 0.0586, Val Loss: 0.2116\n",
      "Epoch [48/50], Train Loss: 0.0579, Val Loss: 0.2096\n",
      "Epoch [49/50], Train Loss: 0.0572, Val Loss: 0.2077\n",
      "Epoch [50/50], Train Loss: 0.0566, Val Loss: 0.2058\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=64, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1383, Val Loss: 0.3735\n",
      "Epoch [2/50], Train Loss: 0.1345, Val Loss: 0.3675\n",
      "Epoch [3/50], Train Loss: 0.1311, Val Loss: 0.3616\n",
      "Epoch [4/50], Train Loss: 0.1285, Val Loss: 0.3558\n",
      "Epoch [5/50], Train Loss: 0.1251, Val Loss: 0.3502\n",
      "Epoch [6/50], Train Loss: 0.1224, Val Loss: 0.3447\n",
      "Epoch [7/50], Train Loss: 0.1194, Val Loss: 0.3393\n",
      "Epoch [8/50], Train Loss: 0.1169, Val Loss: 0.3341\n",
      "Epoch [9/50], Train Loss: 0.1141, Val Loss: 0.3290\n",
      "Epoch [10/50], Train Loss: 0.1119, Val Loss: 0.3240\n",
      "Epoch [11/50], Train Loss: 0.1093, Val Loss: 0.3192\n",
      "Epoch [12/50], Train Loss: 0.1066, Val Loss: 0.3145\n",
      "Epoch [13/50], Train Loss: 0.1043, Val Loss: 0.3098\n",
      "Epoch [14/50], Train Loss: 0.1018, Val Loss: 0.3053\n",
      "Epoch [15/50], Train Loss: 0.1001, Val Loss: 0.3009\n",
      "Epoch [16/50], Train Loss: 0.0979, Val Loss: 0.2966\n",
      "Epoch [17/50], Train Loss: 0.0962, Val Loss: 0.2924\n",
      "Epoch [18/50], Train Loss: 0.0937, Val Loss: 0.2883\n",
      "Epoch [19/50], Train Loss: 0.0918, Val Loss: 0.2843\n",
      "Epoch [20/50], Train Loss: 0.0898, Val Loss: 0.2804\n",
      "Epoch [21/50], Train Loss: 0.0881, Val Loss: 0.2766\n",
      "Epoch [22/50], Train Loss: 0.0867, Val Loss: 0.2728\n",
      "Epoch [23/50], Train Loss: 0.0847, Val Loss: 0.2692\n",
      "Epoch [24/50], Train Loss: 0.0834, Val Loss: 0.2656\n",
      "Epoch [25/50], Train Loss: 0.0814, Val Loss: 0.2621\n",
      "Epoch [26/50], Train Loss: 0.0799, Val Loss: 0.2587\n",
      "Epoch [27/50], Train Loss: 0.0791, Val Loss: 0.2554\n",
      "Epoch [28/50], Train Loss: 0.0773, Val Loss: 0.2522\n",
      "Epoch [29/50], Train Loss: 0.0758, Val Loss: 0.2490\n",
      "Epoch [30/50], Train Loss: 0.0747, Val Loss: 0.2459\n",
      "Epoch [31/50], Train Loss: 0.0731, Val Loss: 0.2428\n",
      "Epoch [32/50], Train Loss: 0.0721, Val Loss: 0.2399\n",
      "Epoch [33/50], Train Loss: 0.0707, Val Loss: 0.2370\n",
      "Epoch [34/50], Train Loss: 0.0696, Val Loss: 0.2341\n",
      "Epoch [35/50], Train Loss: 0.0686, Val Loss: 0.2314\n",
      "Epoch [36/50], Train Loss: 0.0675, Val Loss: 0.2286\n",
      "Epoch [37/50], Train Loss: 0.0663, Val Loss: 0.2260\n",
      "Epoch [38/50], Train Loss: 0.0655, Val Loss: 0.2234\n",
      "Epoch [39/50], Train Loss: 0.0646, Val Loss: 0.2209\n",
      "Epoch [40/50], Train Loss: 0.0635, Val Loss: 0.2184\n",
      "Epoch [41/50], Train Loss: 0.0624, Val Loss: 0.2160\n",
      "Epoch [42/50], Train Loss: 0.0617, Val Loss: 0.2136\n",
      "Epoch [43/50], Train Loss: 0.0610, Val Loss: 0.2113\n",
      "Epoch [44/50], Train Loss: 0.0599, Val Loss: 0.2090\n",
      "Epoch [45/50], Train Loss: 0.0589, Val Loss: 0.2068\n",
      "Epoch [46/50], Train Loss: 0.0583, Val Loss: 0.2046\n",
      "Epoch [47/50], Train Loss: 0.0576, Val Loss: 0.2025\n",
      "Epoch [48/50], Train Loss: 0.0569, Val Loss: 0.2004\n",
      "Epoch [49/50], Train Loss: 0.0562, Val Loss: 0.1984\n",
      "Epoch [50/50], Train Loss: 0.0556, Val Loss: 0.1964\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=64, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1067, Val Loss: 0.3252\n",
      "Epoch [2/50], Train Loss: 0.1047, Val Loss: 0.3206\n",
      "Epoch [3/50], Train Loss: 0.1024, Val Loss: 0.3161\n",
      "Epoch [4/50], Train Loss: 0.1004, Val Loss: 0.3117\n",
      "Epoch [5/50], Train Loss: 0.0987, Val Loss: 0.3074\n",
      "Epoch [6/50], Train Loss: 0.0971, Val Loss: 0.3032\n",
      "Epoch [7/50], Train Loss: 0.0951, Val Loss: 0.2991\n",
      "Epoch [8/50], Train Loss: 0.0927, Val Loss: 0.2951\n",
      "Epoch [9/50], Train Loss: 0.0908, Val Loss: 0.2912\n",
      "Epoch [10/50], Train Loss: 0.0898, Val Loss: 0.2874\n",
      "Epoch [11/50], Train Loss: 0.0879, Val Loss: 0.2837\n",
      "Epoch [12/50], Train Loss: 0.0859, Val Loss: 0.2801\n",
      "Epoch [13/50], Train Loss: 0.0848, Val Loss: 0.2765\n",
      "Epoch [14/50], Train Loss: 0.0834, Val Loss: 0.2730\n",
      "Epoch [15/50], Train Loss: 0.0819, Val Loss: 0.2696\n",
      "Epoch [16/50], Train Loss: 0.0797, Val Loss: 0.2663\n",
      "Epoch [17/50], Train Loss: 0.0794, Val Loss: 0.2631\n",
      "Epoch [18/50], Train Loss: 0.0778, Val Loss: 0.2599\n",
      "Epoch [19/50], Train Loss: 0.0766, Val Loss: 0.2568\n",
      "Epoch [20/50], Train Loss: 0.0753, Val Loss: 0.2537\n",
      "Epoch [21/50], Train Loss: 0.0737, Val Loss: 0.2508\n",
      "Epoch [22/50], Train Loss: 0.0726, Val Loss: 0.2479\n",
      "Epoch [23/50], Train Loss: 0.0712, Val Loss: 0.2451\n",
      "Epoch [24/50], Train Loss: 0.0703, Val Loss: 0.2423\n",
      "Epoch [25/50], Train Loss: 0.0689, Val Loss: 0.2396\n",
      "Epoch [26/50], Train Loss: 0.0685, Val Loss: 0.2370\n",
      "Epoch [27/50], Train Loss: 0.0675, Val Loss: 0.2344\n",
      "Epoch [28/50], Train Loss: 0.0662, Val Loss: 0.2318\n",
      "Epoch [29/50], Train Loss: 0.0651, Val Loss: 0.2294\n",
      "Epoch [30/50], Train Loss: 0.0649, Val Loss: 0.2269\n",
      "Epoch [31/50], Train Loss: 0.0641, Val Loss: 0.2246\n",
      "Epoch [32/50], Train Loss: 0.0633, Val Loss: 0.2222\n",
      "Epoch [33/50], Train Loss: 0.0618, Val Loss: 0.2200\n",
      "Epoch [34/50], Train Loss: 0.0614, Val Loss: 0.2177\n",
      "Epoch [35/50], Train Loss: 0.0599, Val Loss: 0.2156\n",
      "Epoch [36/50], Train Loss: 0.0599, Val Loss: 0.2134\n",
      "Epoch [37/50], Train Loss: 0.0593, Val Loss: 0.2114\n",
      "Epoch [38/50], Train Loss: 0.0585, Val Loss: 0.2093\n",
      "Epoch [39/50], Train Loss: 0.0577, Val Loss: 0.2073\n",
      "Epoch [40/50], Train Loss: 0.0575, Val Loss: 0.2054\n",
      "Epoch [41/50], Train Loss: 0.0567, Val Loss: 0.2035\n",
      "Epoch [42/50], Train Loss: 0.0562, Val Loss: 0.2016\n",
      "Epoch [43/50], Train Loss: 0.0556, Val Loss: 0.1998\n",
      "Epoch [44/50], Train Loss: 0.0550, Val Loss: 0.1980\n",
      "Epoch [45/50], Train Loss: 0.0542, Val Loss: 0.1962\n",
      "Epoch [46/50], Train Loss: 0.0538, Val Loss: 0.1945\n",
      "Epoch [47/50], Train Loss: 0.0537, Val Loss: 0.1928\n",
      "Epoch [48/50], Train Loss: 0.0531, Val Loss: 0.1912\n",
      "Epoch [49/50], Train Loss: 0.0525, Val Loss: 0.1896\n",
      "Epoch [50/50], Train Loss: 0.0520, Val Loss: 0.1880\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=64, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1448, Val Loss: 0.3654\n",
      "Epoch [2/50], Train Loss: 0.1415, Val Loss: 0.3599\n",
      "Epoch [3/50], Train Loss: 0.1383, Val Loss: 0.3546\n",
      "Epoch [4/50], Train Loss: 0.1352, Val Loss: 0.3495\n",
      "Epoch [5/50], Train Loss: 0.1322, Val Loss: 0.3444\n",
      "Epoch [6/50], Train Loss: 0.1293, Val Loss: 0.3395\n",
      "Epoch [7/50], Train Loss: 0.1265, Val Loss: 0.3346\n",
      "Epoch [8/50], Train Loss: 0.1238, Val Loss: 0.3299\n",
      "Epoch [9/50], Train Loss: 0.1211, Val Loss: 0.3253\n",
      "Epoch [10/50], Train Loss: 0.1186, Val Loss: 0.3208\n",
      "Epoch [11/50], Train Loss: 0.1161, Val Loss: 0.3164\n",
      "Epoch [12/50], Train Loss: 0.1136, Val Loss: 0.3121\n",
      "Epoch [13/50], Train Loss: 0.1113, Val Loss: 0.3079\n",
      "Epoch [14/50], Train Loss: 0.1090, Val Loss: 0.3038\n",
      "Epoch [15/50], Train Loss: 0.1068, Val Loss: 0.2998\n",
      "Epoch [16/50], Train Loss: 0.1047, Val Loss: 0.2959\n",
      "Epoch [17/50], Train Loss: 0.1026, Val Loss: 0.2920\n",
      "Epoch [18/50], Train Loss: 0.1006, Val Loss: 0.2883\n",
      "Epoch [19/50], Train Loss: 0.0986, Val Loss: 0.2846\n",
      "Epoch [20/50], Train Loss: 0.0967, Val Loss: 0.2811\n",
      "Epoch [21/50], Train Loss: 0.0949, Val Loss: 0.2776\n",
      "Epoch [22/50], Train Loss: 0.0931, Val Loss: 0.2741\n",
      "Epoch [23/50], Train Loss: 0.0914, Val Loss: 0.2708\n",
      "Epoch [24/50], Train Loss: 0.0897, Val Loss: 0.2675\n",
      "Epoch [25/50], Train Loss: 0.0881, Val Loss: 0.2643\n",
      "Epoch [26/50], Train Loss: 0.0865, Val Loss: 0.2612\n",
      "Epoch [27/50], Train Loss: 0.0849, Val Loss: 0.2581\n",
      "Epoch [28/50], Train Loss: 0.0835, Val Loss: 0.2551\n",
      "Epoch [29/50], Train Loss: 0.0820, Val Loss: 0.2522\n",
      "Epoch [30/50], Train Loss: 0.0806, Val Loss: 0.2493\n",
      "Epoch [31/50], Train Loss: 0.0793, Val Loss: 0.2465\n",
      "Epoch [32/50], Train Loss: 0.0780, Val Loss: 0.2438\n",
      "Epoch [33/50], Train Loss: 0.0767, Val Loss: 0.2411\n",
      "Epoch [34/50], Train Loss: 0.0754, Val Loss: 0.2385\n",
      "Epoch [35/50], Train Loss: 0.0742, Val Loss: 0.2359\n",
      "Epoch [36/50], Train Loss: 0.0731, Val Loss: 0.2334\n",
      "Epoch [37/50], Train Loss: 0.0720, Val Loss: 0.2309\n",
      "Epoch [38/50], Train Loss: 0.0709, Val Loss: 0.2285\n",
      "Epoch [39/50], Train Loss: 0.0698, Val Loss: 0.2261\n",
      "Epoch [40/50], Train Loss: 0.0688, Val Loss: 0.2238\n",
      "Epoch [41/50], Train Loss: 0.0678, Val Loss: 0.2216\n",
      "Epoch [42/50], Train Loss: 0.0668, Val Loss: 0.2194\n",
      "Epoch [43/50], Train Loss: 0.0659, Val Loss: 0.2172\n",
      "Epoch [44/50], Train Loss: 0.0650, Val Loss: 0.2151\n",
      "Epoch [45/50], Train Loss: 0.0641, Val Loss: 0.2130\n",
      "Epoch [46/50], Train Loss: 0.0633, Val Loss: 0.2110\n",
      "Epoch [47/50], Train Loss: 0.0624, Val Loss: 0.2090\n",
      "Epoch [48/50], Train Loss: 0.0616, Val Loss: 0.2071\n",
      "Epoch [49/50], Train Loss: 0.0609, Val Loss: 0.2052\n",
      "Epoch [50/50], Train Loss: 0.0601, Val Loss: 0.2033\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=64, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1401, Val Loss: 0.3853\n",
      "Epoch [2/50], Train Loss: 0.1370, Val Loss: 0.3787\n",
      "Epoch [3/50], Train Loss: 0.1331, Val Loss: 0.3723\n",
      "Epoch [4/50], Train Loss: 0.1301, Val Loss: 0.3661\n",
      "Epoch [5/50], Train Loss: 0.1268, Val Loss: 0.3601\n",
      "Epoch [6/50], Train Loss: 0.1236, Val Loss: 0.3542\n",
      "Epoch [7/50], Train Loss: 0.1209, Val Loss: 0.3484\n",
      "Epoch [8/50], Train Loss: 0.1174, Val Loss: 0.3428\n",
      "Epoch [9/50], Train Loss: 0.1153, Val Loss: 0.3374\n",
      "Epoch [10/50], Train Loss: 0.1122, Val Loss: 0.3321\n",
      "Epoch [11/50], Train Loss: 0.1099, Val Loss: 0.3269\n",
      "Epoch [12/50], Train Loss: 0.1069, Val Loss: 0.3219\n",
      "Epoch [13/50], Train Loss: 0.1046, Val Loss: 0.3170\n",
      "Epoch [14/50], Train Loss: 0.1026, Val Loss: 0.3122\n",
      "Epoch [15/50], Train Loss: 0.0998, Val Loss: 0.3075\n",
      "Epoch [16/50], Train Loss: 0.0981, Val Loss: 0.3030\n",
      "Epoch [17/50], Train Loss: 0.0963, Val Loss: 0.2986\n",
      "Epoch [18/50], Train Loss: 0.0943, Val Loss: 0.2942\n",
      "Epoch [19/50], Train Loss: 0.0921, Val Loss: 0.2900\n",
      "Epoch [20/50], Train Loss: 0.0902, Val Loss: 0.2859\n",
      "Epoch [21/50], Train Loss: 0.0882, Val Loss: 0.2819\n",
      "Epoch [22/50], Train Loss: 0.0867, Val Loss: 0.2780\n",
      "Epoch [23/50], Train Loss: 0.0845, Val Loss: 0.2742\n",
      "Epoch [24/50], Train Loss: 0.0830, Val Loss: 0.2705\n",
      "Epoch [25/50], Train Loss: 0.0817, Val Loss: 0.2668\n",
      "Epoch [26/50], Train Loss: 0.0802, Val Loss: 0.2633\n",
      "Epoch [27/50], Train Loss: 0.0783, Val Loss: 0.2599\n",
      "Epoch [28/50], Train Loss: 0.0769, Val Loss: 0.2565\n",
      "Epoch [29/50], Train Loss: 0.0753, Val Loss: 0.2532\n",
      "Epoch [30/50], Train Loss: 0.0739, Val Loss: 0.2500\n",
      "Epoch [31/50], Train Loss: 0.0731, Val Loss: 0.2469\n",
      "Epoch [32/50], Train Loss: 0.0718, Val Loss: 0.2438\n",
      "Epoch [33/50], Train Loss: 0.0704, Val Loss: 0.2408\n",
      "Epoch [34/50], Train Loss: 0.0699, Val Loss: 0.2379\n",
      "Epoch [35/50], Train Loss: 0.0684, Val Loss: 0.2351\n",
      "Epoch [36/50], Train Loss: 0.0671, Val Loss: 0.2323\n",
      "Epoch [37/50], Train Loss: 0.0661, Val Loss: 0.2296\n",
      "Epoch [38/50], Train Loss: 0.0652, Val Loss: 0.2269\n",
      "Epoch [39/50], Train Loss: 0.0639, Val Loss: 0.2244\n",
      "Epoch [40/50], Train Loss: 0.0633, Val Loss: 0.2218\n",
      "Epoch [41/50], Train Loss: 0.0625, Val Loss: 0.2194\n",
      "Epoch [42/50], Train Loss: 0.0615, Val Loss: 0.2170\n",
      "Epoch [43/50], Train Loss: 0.0608, Val Loss: 0.2146\n",
      "Epoch [44/50], Train Loss: 0.0605, Val Loss: 0.2123\n",
      "Epoch [45/50], Train Loss: 0.0591, Val Loss: 0.2101\n",
      "Epoch [46/50], Train Loss: 0.0581, Val Loss: 0.2079\n",
      "Epoch [47/50], Train Loss: 0.0576, Val Loss: 0.2057\n",
      "Epoch [48/50], Train Loss: 0.0568, Val Loss: 0.2036\n",
      "Epoch [49/50], Train Loss: 0.0561, Val Loss: 0.2016\n",
      "Epoch [50/50], Train Loss: 0.0557, Val Loss: 0.1996\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=64, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1573, Val Loss: 0.4247\n",
      "Epoch [2/50], Train Loss: 0.1523, Val Loss: 0.4181\n",
      "Epoch [3/50], Train Loss: 0.1490, Val Loss: 0.4116\n",
      "Epoch [4/50], Train Loss: 0.1458, Val Loss: 0.4052\n",
      "Epoch [5/50], Train Loss: 0.1426, Val Loss: 0.3990\n",
      "Epoch [6/50], Train Loss: 0.1392, Val Loss: 0.3930\n",
      "Epoch [7/50], Train Loss: 0.1368, Val Loss: 0.3871\n",
      "Epoch [8/50], Train Loss: 0.1332, Val Loss: 0.3813\n",
      "Epoch [9/50], Train Loss: 0.1296, Val Loss: 0.3757\n",
      "Epoch [10/50], Train Loss: 0.1277, Val Loss: 0.3702\n",
      "Epoch [11/50], Train Loss: 0.1246, Val Loss: 0.3648\n",
      "Epoch [12/50], Train Loss: 0.1224, Val Loss: 0.3596\n",
      "Epoch [13/50], Train Loss: 0.1193, Val Loss: 0.3544\n",
      "Epoch [14/50], Train Loss: 0.1175, Val Loss: 0.3494\n",
      "Epoch [15/50], Train Loss: 0.1151, Val Loss: 0.3445\n",
      "Epoch [16/50], Train Loss: 0.1125, Val Loss: 0.3397\n",
      "Epoch [17/50], Train Loss: 0.1098, Val Loss: 0.3351\n",
      "Epoch [18/50], Train Loss: 0.1079, Val Loss: 0.3305\n",
      "Epoch [19/50], Train Loss: 0.1059, Val Loss: 0.3260\n",
      "Epoch [20/50], Train Loss: 0.1041, Val Loss: 0.3216\n",
      "Epoch [21/50], Train Loss: 0.1016, Val Loss: 0.3173\n",
      "Epoch [22/50], Train Loss: 0.0997, Val Loss: 0.3132\n",
      "Epoch [23/50], Train Loss: 0.0972, Val Loss: 0.3091\n",
      "Epoch [24/50], Train Loss: 0.0961, Val Loss: 0.3051\n",
      "Epoch [25/50], Train Loss: 0.0944, Val Loss: 0.3012\n",
      "Epoch [26/50], Train Loss: 0.0927, Val Loss: 0.2974\n",
      "Epoch [27/50], Train Loss: 0.0906, Val Loss: 0.2937\n",
      "Epoch [28/50], Train Loss: 0.0888, Val Loss: 0.2901\n",
      "Epoch [29/50], Train Loss: 0.0876, Val Loss: 0.2865\n",
      "Epoch [30/50], Train Loss: 0.0859, Val Loss: 0.2830\n",
      "Epoch [31/50], Train Loss: 0.0845, Val Loss: 0.2796\n",
      "Epoch [32/50], Train Loss: 0.0831, Val Loss: 0.2763\n",
      "Epoch [33/50], Train Loss: 0.0823, Val Loss: 0.2730\n",
      "Epoch [34/50], Train Loss: 0.0806, Val Loss: 0.2698\n",
      "Epoch [35/50], Train Loss: 0.0791, Val Loss: 0.2667\n",
      "Epoch [36/50], Train Loss: 0.0784, Val Loss: 0.2636\n",
      "Epoch [37/50], Train Loss: 0.0763, Val Loss: 0.2606\n",
      "Epoch [38/50], Train Loss: 0.0760, Val Loss: 0.2577\n",
      "Epoch [39/50], Train Loss: 0.0744, Val Loss: 0.2548\n",
      "Epoch [40/50], Train Loss: 0.0730, Val Loss: 0.2520\n",
      "Epoch [41/50], Train Loss: 0.0729, Val Loss: 0.2493\n",
      "Epoch [42/50], Train Loss: 0.0707, Val Loss: 0.2466\n",
      "Epoch [43/50], Train Loss: 0.0702, Val Loss: 0.2439\n",
      "Epoch [44/50], Train Loss: 0.0696, Val Loss: 0.2414\n",
      "Epoch [45/50], Train Loss: 0.0684, Val Loss: 0.2388\n",
      "Epoch [46/50], Train Loss: 0.0675, Val Loss: 0.2364\n",
      "Epoch [47/50], Train Loss: 0.0665, Val Loss: 0.2339\n",
      "Epoch [48/50], Train Loss: 0.0654, Val Loss: 0.2316\n",
      "Epoch [49/50], Train Loss: 0.0648, Val Loss: 0.2292\n",
      "Epoch [50/50], Train Loss: 0.0635, Val Loss: 0.2270\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=16, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1209, Val Loss: 0.2691\n",
      "Epoch [2/50], Train Loss: 0.0475, Val Loss: 0.0837\n",
      "Epoch [3/50], Train Loss: 0.0282, Val Loss: 0.0434\n",
      "Epoch [4/50], Train Loss: 0.0280, Val Loss: 0.0346\n",
      "Epoch [5/50], Train Loss: 0.0249, Val Loss: 0.0260\n",
      "Epoch [6/50], Train Loss: 0.0228, Val Loss: 0.0205\n",
      "Epoch [7/50], Train Loss: 0.0207, Val Loss: 0.0165\n",
      "Epoch [8/50], Train Loss: 0.0190, Val Loss: 0.0135\n",
      "Epoch [9/50], Train Loss: 0.0174, Val Loss: 0.0112\n",
      "Epoch [10/50], Train Loss: 0.0160, Val Loss: 0.0094\n",
      "Epoch [11/50], Train Loss: 0.0144, Val Loss: 0.0079\n",
      "Epoch [12/50], Train Loss: 0.0124, Val Loss: 0.0065\n",
      "Epoch [13/50], Train Loss: 0.0095, Val Loss: 0.0046\n",
      "Epoch [14/50], Train Loss: 0.0062, Val Loss: 0.0040\n",
      "Epoch [15/50], Train Loss: 0.0045, Val Loss: 0.0053\n",
      "Epoch [16/50], Train Loss: 0.0033, Val Loss: 0.0054\n",
      "Epoch [17/50], Train Loss: 0.0029, Val Loss: 0.0046\n",
      "Epoch [18/50], Train Loss: 0.0029, Val Loss: 0.0029\n",
      "Epoch [19/50], Train Loss: 0.0032, Val Loss: 0.0038\n",
      "Epoch [20/50], Train Loss: 0.0025, Val Loss: 0.0029\n",
      "Epoch [21/50], Train Loss: 0.0025, Val Loss: 0.0029\n",
      "Epoch [22/50], Train Loss: 0.0024, Val Loss: 0.0029\n",
      "Epoch [23/50], Train Loss: 0.0026, Val Loss: 0.0038\n",
      "Epoch [24/50], Train Loss: 0.0023, Val Loss: 0.0030\n",
      "Epoch [25/50], Train Loss: 0.0027, Val Loss: 0.0033\n",
      "Epoch [26/50], Train Loss: 0.0026, Val Loss: 0.0034\n",
      "Epoch [27/50], Train Loss: 0.0048, Val Loss: 0.0087\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=16, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0787, Val Loss: 0.2123\n",
      "Epoch [2/50], Train Loss: 0.0405, Val Loss: 0.0999\n",
      "Epoch [3/50], Train Loss: 0.0323, Val Loss: 0.0593\n",
      "Epoch [4/50], Train Loss: 0.0283, Val Loss: 0.0377\n",
      "Epoch [5/50], Train Loss: 0.0219, Val Loss: 0.0198\n",
      "Epoch [6/50], Train Loss: 0.0157, Val Loss: 0.0176\n",
      "Epoch [7/50], Train Loss: 0.0131, Val Loss: 0.0142\n",
      "Epoch [8/50], Train Loss: 0.0119, Val Loss: 0.0121\n",
      "Epoch [9/50], Train Loss: 0.0106, Val Loss: 0.0129\n",
      "Epoch [10/50], Train Loss: 0.0098, Val Loss: 0.0170\n",
      "Epoch [11/50], Train Loss: 0.0102, Val Loss: 0.0087\n",
      "Epoch [12/50], Train Loss: 0.0107, Val Loss: 0.0115\n",
      "Epoch [13/50], Train Loss: 0.0102, Val Loss: 0.0125\n",
      "Epoch [14/50], Train Loss: 0.0087, Val Loss: 0.0097\n",
      "Epoch [15/50], Train Loss: 0.0100, Val Loss: 0.0089\n",
      "Epoch [16/50], Train Loss: 0.0080, Val Loss: 0.0128\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=16, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1012, Val Loss: 0.2346\n",
      "Epoch [2/50], Train Loss: 0.0606, Val Loss: 0.1259\n",
      "Epoch [3/50], Train Loss: 0.0461, Val Loss: 0.0756\n",
      "Epoch [4/50], Train Loss: 0.0422, Val Loss: 0.0523\n",
      "Epoch [5/50], Train Loss: 0.0343, Val Loss: 0.0293\n",
      "Epoch [6/50], Train Loss: 0.0291, Val Loss: 0.0222\n",
      "Epoch [7/50], Train Loss: 0.0242, Val Loss: 0.0223\n",
      "Epoch [8/50], Train Loss: 0.0225, Val Loss: 0.0161\n",
      "Epoch [9/50], Train Loss: 0.0202, Val Loss: 0.0192\n",
      "Epoch [10/50], Train Loss: 0.0207, Val Loss: 0.0136\n",
      "Epoch [11/50], Train Loss: 0.0191, Val Loss: 0.0123\n",
      "Epoch [12/50], Train Loss: 0.0173, Val Loss: 0.0217\n",
      "Epoch [13/50], Train Loss: 0.0176, Val Loss: 0.0091\n",
      "Epoch [14/50], Train Loss: 0.0160, Val Loss: 0.0154\n",
      "Epoch [15/50], Train Loss: 0.0149, Val Loss: 0.0095\n",
      "Epoch [16/50], Train Loss: 0.0136, Val Loss: 0.0132\n",
      "Epoch [17/50], Train Loss: 0.0139, Val Loss: 0.0091\n",
      "Epoch [18/50], Train Loss: 0.0132, Val Loss: 0.0113\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=16, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1267, Val Loss: 0.2115\n",
      "Epoch [2/50], Train Loss: 0.0397, Val Loss: 0.0511\n",
      "Epoch [3/50], Train Loss: 0.0495, Val Loss: 0.0843\n",
      "Epoch [4/50], Train Loss: 0.0373, Val Loss: 0.0661\n",
      "Epoch [5/50], Train Loss: 0.0374, Val Loss: 0.0617\n",
      "Epoch [6/50], Train Loss: 0.0338, Val Loss: 0.0494\n",
      "Epoch [7/50], Train Loss: 0.0298, Val Loss: 0.0364\n",
      "Epoch [8/50], Train Loss: 0.0241, Val Loss: 0.0346\n",
      "Epoch [9/50], Train Loss: 0.0205, Val Loss: 0.0331\n",
      "Epoch [10/50], Train Loss: 0.0186, Val Loss: 0.0304\n",
      "Epoch [11/50], Train Loss: 0.0163, Val Loss: 0.0344\n",
      "Epoch [12/50], Train Loss: 0.0109, Val Loss: 0.0377\n",
      "Epoch [13/50], Train Loss: 0.0090, Val Loss: 0.0262\n",
      "Epoch [14/50], Train Loss: 0.0082, Val Loss: 0.0232\n",
      "Epoch [15/50], Train Loss: 0.0083, Val Loss: 0.0467\n",
      "Epoch [16/50], Train Loss: 0.0053, Val Loss: 0.0234\n",
      "Epoch [17/50], Train Loss: 0.0055, Val Loss: 0.0317\n",
      "Epoch [18/50], Train Loss: 0.0061, Val Loss: 0.0349\n",
      "Epoch [19/50], Train Loss: 0.0044, Val Loss: 0.0239\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=16, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1323, Val Loss: 0.2283\n",
      "Epoch [2/50], Train Loss: 0.0446, Val Loss: 0.0655\n",
      "Epoch [3/50], Train Loss: 0.0479, Val Loss: 0.0511\n",
      "Epoch [4/50], Train Loss: 0.0423, Val Loss: 0.0348\n",
      "Epoch [5/50], Train Loss: 0.0355, Val Loss: 0.0190\n",
      "Epoch [6/50], Train Loss: 0.0280, Val Loss: 0.0130\n",
      "Epoch [7/50], Train Loss: 0.0227, Val Loss: 0.0100\n",
      "Epoch [8/50], Train Loss: 0.0193, Val Loss: 0.0360\n",
      "Epoch [9/50], Train Loss: 0.0202, Val Loss: 0.0113\n",
      "Epoch [10/50], Train Loss: 0.0172, Val Loss: 0.0707\n",
      "Epoch [11/50], Train Loss: 0.0162, Val Loss: 0.0231\n",
      "Epoch [12/50], Train Loss: 0.0138, Val Loss: 0.0469\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=16, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1417, Val Loss: 0.2586\n",
      "Epoch [2/50], Train Loss: 0.0761, Val Loss: 0.1407\n",
      "Epoch [3/50], Train Loss: 0.0583, Val Loss: 0.1157\n",
      "Epoch [4/50], Train Loss: 0.0524, Val Loss: 0.0929\n",
      "Epoch [5/50], Train Loss: 0.0479, Val Loss: 0.0737\n",
      "Epoch [6/50], Train Loss: 0.0443, Val Loss: 0.0680\n",
      "Epoch [7/50], Train Loss: 0.0400, Val Loss: 0.0581\n",
      "Epoch [8/50], Train Loss: 0.0380, Val Loss: 0.0446\n",
      "Epoch [9/50], Train Loss: 0.0332, Val Loss: 0.0435\n",
      "Epoch [10/50], Train Loss: 0.0303, Val Loss: 0.0487\n",
      "Epoch [11/50], Train Loss: 0.0262, Val Loss: 0.0376\n",
      "Epoch [12/50], Train Loss: 0.0260, Val Loss: 0.0478\n",
      "Epoch [13/50], Train Loss: 0.0252, Val Loss: 0.0382\n",
      "Epoch [14/50], Train Loss: 0.0244, Val Loss: 0.0333\n",
      "Epoch [15/50], Train Loss: 0.0209, Val Loss: 0.0401\n",
      "Epoch [16/50], Train Loss: 0.0222, Val Loss: 0.0307\n",
      "Epoch [17/50], Train Loss: 0.0210, Val Loss: 0.0327\n",
      "Epoch [18/50], Train Loss: 0.0189, Val Loss: 0.0347\n",
      "Epoch [19/50], Train Loss: 0.0189, Val Loss: 0.0313\n",
      "Epoch [20/50], Train Loss: 0.0180, Val Loss: 0.0271\n",
      "Epoch [21/50], Train Loss: 0.0181, Val Loss: 0.0320\n",
      "Epoch [22/50], Train Loss: 0.0175, Val Loss: 0.0295\n",
      "Epoch [23/50], Train Loss: 0.0189, Val Loss: 0.0214\n",
      "Epoch [24/50], Train Loss: 0.0164, Val Loss: 0.0330\n",
      "Epoch [25/50], Train Loss: 0.0165, Val Loss: 0.0258\n",
      "Epoch [26/50], Train Loss: 0.0160, Val Loss: 0.0287\n",
      "Epoch [27/50], Train Loss: 0.0173, Val Loss: 0.0308\n",
      "Epoch [28/50], Train Loss: 0.0178, Val Loss: 0.0292\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=16, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0932, Val Loss: 0.1765\n",
      "Epoch [2/50], Train Loss: 0.0384, Val Loss: 0.0676\n",
      "Epoch [3/50], Train Loss: 0.0483, Val Loss: 0.0934\n",
      "Epoch [4/50], Train Loss: 0.0393, Val Loss: 0.0791\n",
      "Epoch [5/50], Train Loss: 0.0388, Val Loss: 0.0716\n",
      "Epoch [6/50], Train Loss: 0.0341, Val Loss: 0.0548\n",
      "Epoch [7/50], Train Loss: 0.0272, Val Loss: 0.0402\n",
      "Epoch [8/50], Train Loss: 0.0230, Val Loss: 0.0360\n",
      "Epoch [9/50], Train Loss: 0.0209, Val Loss: 0.0393\n",
      "Epoch [10/50], Train Loss: 0.0189, Val Loss: 0.0464\n",
      "Epoch [11/50], Train Loss: 0.0140, Val Loss: 0.0451\n",
      "Epoch [12/50], Train Loss: 0.0087, Val Loss: 0.0370\n",
      "Epoch [13/50], Train Loss: 0.0111, Val Loss: 0.0281\n",
      "Epoch [14/50], Train Loss: 0.0060, Val Loss: 0.0511\n",
      "Epoch [15/50], Train Loss: 0.0079, Val Loss: 0.0391\n",
      "Epoch [16/50], Train Loss: 0.0069, Val Loss: 0.0363\n",
      "Epoch [17/50], Train Loss: 0.0058, Val Loss: 0.0382\n",
      "Epoch [18/50], Train Loss: 0.0055, Val Loss: 0.0372\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=16, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1080, Val Loss: 0.2476\n",
      "Epoch [2/50], Train Loss: 0.0455, Val Loss: 0.0708\n",
      "Epoch [3/50], Train Loss: 0.0590, Val Loss: 0.1088\n",
      "Epoch [4/50], Train Loss: 0.0455, Val Loss: 0.0788\n",
      "Epoch [5/50], Train Loss: 0.0428, Val Loss: 0.0576\n",
      "Epoch [6/50], Train Loss: 0.0363, Val Loss: 0.0528\n",
      "Epoch [7/50], Train Loss: 0.0306, Val Loss: 0.0552\n",
      "Epoch [8/50], Train Loss: 0.0298, Val Loss: 0.0453\n",
      "Epoch [9/50], Train Loss: 0.0286, Val Loss: 0.0337\n",
      "Epoch [10/50], Train Loss: 0.0283, Val Loss: 0.0611\n",
      "Epoch [11/50], Train Loss: 0.0265, Val Loss: 0.0487\n",
      "Epoch [12/50], Train Loss: 0.0257, Val Loss: 0.0477\n",
      "Epoch [13/50], Train Loss: 0.0235, Val Loss: 0.0526\n",
      "Epoch [14/50], Train Loss: 0.0213, Val Loss: 0.0469\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=16, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1413, Val Loss: 0.2932\n",
      "Epoch [2/50], Train Loss: 0.0726, Val Loss: 0.1060\n",
      "Epoch [3/50], Train Loss: 0.0708, Val Loss: 0.1078\n",
      "Epoch [4/50], Train Loss: 0.0582, Val Loss: 0.0775\n",
      "Epoch [5/50], Train Loss: 0.0510, Val Loss: 0.0505\n",
      "Epoch [6/50], Train Loss: 0.0399, Val Loss: 0.0457\n",
      "Epoch [7/50], Train Loss: 0.0375, Val Loss: 0.0490\n",
      "Epoch [8/50], Train Loss: 0.0344, Val Loss: 0.0467\n",
      "Epoch [9/50], Train Loss: 0.0332, Val Loss: 0.0429\n",
      "Epoch [10/50], Train Loss: 0.0295, Val Loss: 0.0497\n",
      "Epoch [11/50], Train Loss: 0.0293, Val Loss: 0.0428\n",
      "Epoch [12/50], Train Loss: 0.0301, Val Loss: 0.0434\n",
      "Epoch [13/50], Train Loss: 0.0288, Val Loss: 0.0380\n",
      "Epoch [14/50], Train Loss: 0.0275, Val Loss: 0.0345\n",
      "Epoch [15/50], Train Loss: 0.0267, Val Loss: 0.0350\n",
      "Epoch [16/50], Train Loss: 0.0270, Val Loss: 0.0243\n",
      "Epoch [17/50], Train Loss: 0.0261, Val Loss: 0.0340\n",
      "Epoch [18/50], Train Loss: 0.0244, Val Loss: 0.0496\n",
      "Epoch [19/50], Train Loss: 0.0233, Val Loss: 0.0330\n",
      "Epoch [20/50], Train Loss: 0.0220, Val Loss: 0.0352\n",
      "Epoch [21/50], Train Loss: 0.0277, Val Loss: 0.0545\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=32, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1266, Val Loss: 0.1809\n",
      "Epoch [2/50], Train Loss: 0.0270, Val Loss: 0.0413\n",
      "Epoch [3/50], Train Loss: 0.0392, Val Loss: 0.0276\n",
      "Epoch [4/50], Train Loss: 0.0287, Val Loss: 0.0190\n",
      "Epoch [5/50], Train Loss: 0.0180, Val Loss: 0.0124\n",
      "Epoch [6/50], Train Loss: 0.0091, Val Loss: 0.0139\n",
      "Epoch [7/50], Train Loss: 0.0044, Val Loss: 0.0093\n",
      "Epoch [8/50], Train Loss: 0.0040, Val Loss: 0.0097\n",
      "Epoch [9/50], Train Loss: 0.0042, Val Loss: 0.0094\n",
      "Epoch [10/50], Train Loss: 0.0046, Val Loss: 0.0102\n",
      "Epoch [11/50], Train Loss: 0.0044, Val Loss: 0.0171\n",
      "Epoch [12/50], Train Loss: 0.0038, Val Loss: 0.0079\n",
      "Epoch [13/50], Train Loss: 0.0032, Val Loss: 0.0045\n",
      "Epoch [14/50], Train Loss: 0.0028, Val Loss: 0.0046\n",
      "Epoch [15/50], Train Loss: 0.0034, Val Loss: 0.0096\n",
      "Epoch [16/50], Train Loss: 0.0034, Val Loss: 0.0135\n",
      "Epoch [17/50], Train Loss: 0.0026, Val Loss: 0.0063\n",
      "Epoch [18/50], Train Loss: 0.0033, Val Loss: 0.0036\n",
      "Epoch [19/50], Train Loss: 0.0029, Val Loss: 0.0052\n",
      "Epoch [20/50], Train Loss: 0.0038, Val Loss: 0.0145\n",
      "Epoch [21/50], Train Loss: 0.0033, Val Loss: 0.0095\n",
      "Epoch [22/50], Train Loss: 0.0036, Val Loss: 0.0050\n",
      "Epoch [23/50], Train Loss: 0.0047, Val Loss: 0.0029\n",
      "Epoch [24/50], Train Loss: 0.0034, Val Loss: 0.0077\n",
      "Epoch [25/50], Train Loss: 0.0046, Val Loss: 0.0196\n",
      "Epoch [26/50], Train Loss: 0.0030, Val Loss: 0.0061\n",
      "Epoch [27/50], Train Loss: 0.0035, Val Loss: 0.0028\n",
      "Epoch [28/50], Train Loss: 0.0025, Val Loss: 0.0033\n",
      "Epoch [29/50], Train Loss: 0.0034, Val Loss: 0.0129\n",
      "Epoch [30/50], Train Loss: 0.0028, Val Loss: 0.0111\n",
      "Epoch [31/50], Train Loss: 0.0044, Val Loss: 0.0029\n",
      "Epoch [32/50], Train Loss: 0.0045, Val Loss: 0.0081\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=32, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1344, Val Loss: 0.2102\n",
      "Epoch [2/50], Train Loss: 0.0334, Val Loss: 0.0547\n",
      "Epoch [3/50], Train Loss: 0.0345, Val Loss: 0.0239\n",
      "Epoch [4/50], Train Loss: 0.0337, Val Loss: 0.0292\n",
      "Epoch [5/50], Train Loss: 0.0261, Val Loss: 0.0223\n",
      "Epoch [6/50], Train Loss: 0.0219, Val Loss: 0.0143\n",
      "Epoch [7/50], Train Loss: 0.0178, Val Loss: 0.0073\n",
      "Epoch [8/50], Train Loss: 0.0115, Val Loss: 0.0027\n",
      "Epoch [9/50], Train Loss: 0.0100, Val Loss: 0.0091\n",
      "Epoch [10/50], Train Loss: 0.0105, Val Loss: 0.0027\n",
      "Epoch [11/50], Train Loss: 0.0089, Val Loss: 0.0054\n",
      "Epoch [12/50], Train Loss: 0.0103, Val Loss: 0.0042\n",
      "Epoch [13/50], Train Loss: 0.0078, Val Loss: 0.0040\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=32, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0887, Val Loss: 0.1403\n",
      "Epoch [2/50], Train Loss: 0.0394, Val Loss: 0.0645\n",
      "Epoch [3/50], Train Loss: 0.0388, Val Loss: 0.0503\n",
      "Epoch [4/50], Train Loss: 0.0352, Val Loss: 0.0461\n",
      "Epoch [5/50], Train Loss: 0.0298, Val Loss: 0.0284\n",
      "Epoch [6/50], Train Loss: 0.0261, Val Loss: 0.0177\n",
      "Epoch [7/50], Train Loss: 0.0201, Val Loss: 0.0132\n",
      "Epoch [8/50], Train Loss: 0.0198, Val Loss: 0.0087\n",
      "Epoch [9/50], Train Loss: 0.0169, Val Loss: 0.0232\n",
      "Epoch [10/50], Train Loss: 0.0144, Val Loss: 0.0053\n",
      "Epoch [11/50], Train Loss: 0.0179, Val Loss: 0.0036\n",
      "Epoch [12/50], Train Loss: 0.0123, Val Loss: 0.0068\n",
      "Epoch [13/50], Train Loss: 0.0126, Val Loss: 0.0098\n",
      "Epoch [14/50], Train Loss: 0.0113, Val Loss: 0.0063\n",
      "Epoch [15/50], Train Loss: 0.0125, Val Loss: 0.0056\n",
      "Epoch [16/50], Train Loss: 0.0102, Val Loss: 0.0103\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=32, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0900, Val Loss: 0.1756\n",
      "Epoch [2/50], Train Loss: 0.0334, Val Loss: 0.0302\n",
      "Epoch [3/50], Train Loss: 0.0558, Val Loss: 0.0855\n",
      "Epoch [4/50], Train Loss: 0.0294, Val Loss: 0.0278\n",
      "Epoch [5/50], Train Loss: 0.0287, Val Loss: 0.0053\n",
      "Epoch [6/50], Train Loss: 0.0173, Val Loss: 0.0237\n",
      "Epoch [7/50], Train Loss: 0.0111, Val Loss: 0.0246\n",
      "Epoch [8/50], Train Loss: 0.0074, Val Loss: 0.0268\n",
      "Epoch [9/50], Train Loss: 0.0089, Val Loss: 0.0182\n",
      "Epoch [10/50], Train Loss: 0.0052, Val Loss: 0.0256\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=32, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0703, Val Loss: 0.0992\n",
      "Epoch [2/50], Train Loss: 0.0437, Val Loss: 0.0488\n",
      "Epoch [3/50], Train Loss: 0.0412, Val Loss: 0.0277\n",
      "Epoch [4/50], Train Loss: 0.0316, Val Loss: 0.0139\n",
      "Epoch [5/50], Train Loss: 0.0244, Val Loss: 0.0109\n",
      "Epoch [6/50], Train Loss: 0.0177, Val Loss: 0.0313\n",
      "Epoch [7/50], Train Loss: 0.0174, Val Loss: 0.0360\n",
      "Epoch [8/50], Train Loss: 0.0101, Val Loss: 0.0246\n",
      "Epoch [9/50], Train Loss: 0.0092, Val Loss: 0.0349\n",
      "Epoch [10/50], Train Loss: 0.0085, Val Loss: 0.0294\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=32, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0603, Val Loss: 0.1233\n",
      "Epoch [2/50], Train Loss: 0.0405, Val Loss: 0.0577\n",
      "Epoch [3/50], Train Loss: 0.0394, Val Loss: 0.0306\n",
      "Epoch [4/50], Train Loss: 0.0301, Val Loss: 0.0250\n",
      "Epoch [5/50], Train Loss: 0.0237, Val Loss: 0.0430\n",
      "Epoch [6/50], Train Loss: 0.0210, Val Loss: 0.0383\n",
      "Epoch [7/50], Train Loss: 0.0148, Val Loss: 0.0366\n",
      "Epoch [8/50], Train Loss: 0.0147, Val Loss: 0.0471\n",
      "Epoch [9/50], Train Loss: 0.0132, Val Loss: 0.0407\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=32, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1097, Val Loss: 0.0442\n",
      "Epoch [2/50], Train Loss: 0.0671, Val Loss: 0.0956\n",
      "Epoch [3/50], Train Loss: 0.0424, Val Loss: 0.0746\n",
      "Epoch [4/50], Train Loss: 0.0450, Val Loss: 0.0716\n",
      "Epoch [5/50], Train Loss: 0.0412, Val Loss: 0.0591\n",
      "Epoch [6/50], Train Loss: 0.0358, Val Loss: 0.0519\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=32, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0690, Val Loss: 0.0451\n",
      "Epoch [2/50], Train Loss: 0.0652, Val Loss: 0.1017\n",
      "Epoch [3/50], Train Loss: 0.0403, Val Loss: 0.0646\n",
      "Epoch [4/50], Train Loss: 0.0424, Val Loss: 0.0482\n",
      "Epoch [5/50], Train Loss: 0.0360, Val Loss: 0.0322\n",
      "Epoch [6/50], Train Loss: 0.0274, Val Loss: 0.0300\n",
      "Epoch [7/50], Train Loss: 0.0246, Val Loss: 0.0368\n",
      "Epoch [8/50], Train Loss: 0.0230, Val Loss: 0.0298\n",
      "Epoch [9/50], Train Loss: 0.0245, Val Loss: 0.0303\n",
      "Epoch [10/50], Train Loss: 0.0232, Val Loss: 0.0478\n",
      "Epoch [11/50], Train Loss: 0.0222, Val Loss: 0.0448\n",
      "Epoch [12/50], Train Loss: 0.0251, Val Loss: 0.0270\n",
      "Epoch [13/50], Train Loss: 0.0167, Val Loss: 0.0406\n",
      "Epoch [14/50], Train Loss: 0.0137, Val Loss: 0.0230\n",
      "Epoch [15/50], Train Loss: 0.0124, Val Loss: 0.0577\n",
      "Epoch [16/50], Train Loss: 0.0122, Val Loss: 0.0578\n",
      "Epoch [17/50], Train Loss: 0.0149, Val Loss: 0.0327\n",
      "Epoch [18/50], Train Loss: 0.0102, Val Loss: 0.0671\n",
      "Epoch [19/50], Train Loss: 0.0107, Val Loss: 0.0303\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=32, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0785, Val Loss: 0.0451\n",
      "Epoch [2/50], Train Loss: 0.0776, Val Loss: 0.1219\n",
      "Epoch [3/50], Train Loss: 0.0500, Val Loss: 0.0582\n",
      "Epoch [4/50], Train Loss: 0.0558, Val Loss: 0.0417\n",
      "Epoch [5/50], Train Loss: 0.0418, Val Loss: 0.0281\n",
      "Epoch [6/50], Train Loss: 0.0329, Val Loss: 0.0181\n",
      "Epoch [7/50], Train Loss: 0.0329, Val Loss: 0.0130\n",
      "Epoch [8/50], Train Loss: 0.0304, Val Loss: 0.0184\n",
      "Epoch [9/50], Train Loss: 0.0301, Val Loss: 0.0549\n",
      "Epoch [10/50], Train Loss: 0.0291, Val Loss: 0.0255\n",
      "Epoch [11/50], Train Loss: 0.0310, Val Loss: 0.0207\n",
      "Epoch [12/50], Train Loss: 0.0267, Val Loss: 0.0247\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=64, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0503, Val Loss: 0.0437\n",
      "Epoch [2/50], Train Loss: 0.0423, Val Loss: 0.0447\n",
      "Epoch [3/50], Train Loss: 0.0273, Val Loss: 0.0181\n",
      "Epoch [4/50], Train Loss: 0.0215, Val Loss: 0.0059\n",
      "Epoch [5/50], Train Loss: 0.0170, Val Loss: 0.0119\n",
      "Epoch [6/50], Train Loss: 0.0063, Val Loss: 0.0131\n",
      "Epoch [7/50], Train Loss: 0.0071, Val Loss: 0.0208\n",
      "Epoch [8/50], Train Loss: 0.0111, Val Loss: 0.0026\n",
      "Epoch [9/50], Train Loss: 0.0066, Val Loss: 0.0042\n",
      "Epoch [10/50], Train Loss: 0.0138, Val Loss: 0.0468\n",
      "Epoch [11/50], Train Loss: 0.0092, Val Loss: 0.0096\n",
      "Epoch [12/50], Train Loss: 0.0089, Val Loss: 0.0127\n",
      "Epoch [13/50], Train Loss: 0.0048, Val Loss: 0.0074\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=64, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0595, Val Loss: 0.0447\n",
      "Epoch [2/50], Train Loss: 0.0466, Val Loss: 0.0440\n",
      "Epoch [3/50], Train Loss: 0.0322, Val Loss: 0.0298\n",
      "Epoch [4/50], Train Loss: 0.0261, Val Loss: 0.0097\n",
      "Epoch [5/50], Train Loss: 0.0209, Val Loss: 0.0042\n",
      "Epoch [6/50], Train Loss: 0.0125, Val Loss: 0.0229\n",
      "Epoch [7/50], Train Loss: 0.0114, Val Loss: 0.0135\n",
      "Epoch [8/50], Train Loss: 0.0067, Val Loss: 0.0111\n",
      "Epoch [9/50], Train Loss: 0.0072, Val Loss: 0.0156\n",
      "Epoch [10/50], Train Loss: 0.0080, Val Loss: 0.0113\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=64, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0653, Val Loss: 0.0524\n",
      "Epoch [2/50], Train Loss: 0.0518, Val Loss: 0.0392\n",
      "Epoch [3/50], Train Loss: 0.0397, Val Loss: 0.0351\n",
      "Epoch [4/50], Train Loss: 0.0297, Val Loss: 0.0138\n",
      "Epoch [5/50], Train Loss: 0.0267, Val Loss: 0.0049\n",
      "Epoch [6/50], Train Loss: 0.0193, Val Loss: 0.0090\n",
      "Epoch [7/50], Train Loss: 0.0127, Val Loss: 0.0083\n",
      "Epoch [8/50], Train Loss: 0.0129, Val Loss: 0.0058\n",
      "Epoch [9/50], Train Loss: 0.0123, Val Loss: 0.0079\n",
      "Epoch [10/50], Train Loss: 0.0099, Val Loss: 0.0081\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=64, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0525, Val Loss: 0.1074\n",
      "Epoch [2/50], Train Loss: 0.0484, Val Loss: 0.0233\n",
      "Epoch [3/50], Train Loss: 0.0578, Val Loss: 0.0589\n",
      "Epoch [4/50], Train Loss: 0.0329, Val Loss: 0.0284\n",
      "Epoch [5/50], Train Loss: 0.0274, Val Loss: 0.0129\n",
      "Epoch [6/50], Train Loss: 0.0212, Val Loss: 0.0096\n",
      "Epoch [7/50], Train Loss: 0.0183, Val Loss: 0.0041\n",
      "Epoch [8/50], Train Loss: 0.0167, Val Loss: 0.0323\n",
      "Epoch [9/50], Train Loss: 0.0048, Val Loss: 0.0105\n",
      "Epoch [10/50], Train Loss: 0.0045, Val Loss: 0.0300\n",
      "Epoch [11/50], Train Loss: 0.0045, Val Loss: 0.0160\n",
      "Epoch [12/50], Train Loss: 0.0052, Val Loss: 0.0214\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=64, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0547, Val Loss: 0.0668\n",
      "Epoch [2/50], Train Loss: 0.0669, Val Loss: 0.1016\n",
      "Epoch [3/50], Train Loss: 0.0372, Val Loss: 0.0383\n",
      "Epoch [4/50], Train Loss: 0.0405, Val Loss: 0.0148\n",
      "Epoch [5/50], Train Loss: 0.0283, Val Loss: 0.0047\n",
      "Epoch [6/50], Train Loss: 0.0294, Val Loss: 0.0262\n",
      "Epoch [7/50], Train Loss: 0.0183, Val Loss: 0.0236\n",
      "Epoch [8/50], Train Loss: 0.0145, Val Loss: 0.0315\n",
      "Epoch [9/50], Train Loss: 0.0192, Val Loss: 0.0233\n",
      "Epoch [10/50], Train Loss: 0.0081, Val Loss: 0.0255\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=64, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0501, Val Loss: 0.1016\n",
      "Epoch [2/50], Train Loss: 0.0555, Val Loss: 0.0507\n",
      "Epoch [3/50], Train Loss: 0.0475, Val Loss: 0.0439\n",
      "Epoch [4/50], Train Loss: 0.0379, Val Loss: 0.0174\n",
      "Epoch [5/50], Train Loss: 0.0289, Val Loss: 0.0077\n",
      "Epoch [6/50], Train Loss: 0.0273, Val Loss: 0.0098\n",
      "Epoch [7/50], Train Loss: 0.0188, Val Loss: 0.0370\n",
      "Epoch [8/50], Train Loss: 0.0150, Val Loss: 0.0070\n",
      "Epoch [9/50], Train Loss: 0.0188, Val Loss: 0.0083\n",
      "Epoch [10/50], Train Loss: 0.0153, Val Loss: 0.0108\n",
      "Epoch [11/50], Train Loss: 0.0263, Val Loss: 0.0880\n",
      "Epoch [12/50], Train Loss: 0.0163, Val Loss: 0.0199\n",
      "Epoch [13/50], Train Loss: 0.0179, Val Loss: 0.0103\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=64, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0488, Val Loss: 0.1098\n",
      "Epoch [2/50], Train Loss: 0.0575, Val Loss: 0.0696\n",
      "Epoch [3/50], Train Loss: 0.0526, Val Loss: 0.0875\n",
      "Epoch [4/50], Train Loss: 0.0419, Val Loss: 0.0662\n",
      "Epoch [5/50], Train Loss: 0.0398, Val Loss: 0.0348\n",
      "Epoch [6/50], Train Loss: 0.0300, Val Loss: 0.0261\n",
      "Epoch [7/50], Train Loss: 0.0246, Val Loss: 0.0193\n",
      "Epoch [8/50], Train Loss: 0.0207, Val Loss: 0.0494\n",
      "Epoch [9/50], Train Loss: 0.0211, Val Loss: 0.0317\n",
      "Epoch [10/50], Train Loss: 0.0197, Val Loss: 0.0124\n",
      "Epoch [11/50], Train Loss: 0.0147, Val Loss: 0.0225\n",
      "Epoch [12/50], Train Loss: 0.0078, Val Loss: 0.0301\n",
      "Epoch [13/50], Train Loss: 0.0142, Val Loss: 0.0518\n",
      "Epoch [14/50], Train Loss: 0.0111, Val Loss: 0.0255\n",
      "Epoch [15/50], Train Loss: 0.0162, Val Loss: 0.0114\n",
      "Epoch [16/50], Train Loss: 0.0221, Val Loss: 0.0337\n",
      "Epoch [17/50], Train Loss: 0.0137, Val Loss: 0.0132\n",
      "Epoch [18/50], Train Loss: 0.0067, Val Loss: 0.0186\n",
      "Epoch [19/50], Train Loss: 0.0116, Val Loss: 0.0379\n",
      "Epoch [20/50], Train Loss: 0.0077, Val Loss: 0.0133\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=64, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0421, Val Loss: 0.1044\n",
      "Epoch [2/50], Train Loss: 0.0605, Val Loss: 0.0825\n",
      "Epoch [3/50], Train Loss: 0.0473, Val Loss: 0.0464\n",
      "Epoch [4/50], Train Loss: 0.0387, Val Loss: 0.0340\n",
      "Epoch [5/50], Train Loss: 0.0273, Val Loss: 0.0147\n",
      "Epoch [6/50], Train Loss: 0.0216, Val Loss: 0.0172\n",
      "Epoch [7/50], Train Loss: 0.0221, Val Loss: 0.0093\n",
      "Epoch [8/50], Train Loss: 0.0189, Val Loss: 0.0261\n",
      "Epoch [9/50], Train Loss: 0.0201, Val Loss: 0.0635\n",
      "Epoch [10/50], Train Loss: 0.0205, Val Loss: 0.0582\n",
      "Epoch [11/50], Train Loss: 0.0170, Val Loss: 0.0796\n",
      "Epoch [12/50], Train Loss: 0.0080, Val Loss: 0.0567\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=64, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0502, Val Loss: 0.0920\n",
      "Epoch [2/50], Train Loss: 0.0651, Val Loss: 0.0992\n",
      "Epoch [3/50], Train Loss: 0.0486, Val Loss: 0.0761\n",
      "Epoch [4/50], Train Loss: 0.0421, Val Loss: 0.0319\n",
      "Epoch [5/50], Train Loss: 0.0318, Val Loss: 0.0213\n",
      "Epoch [6/50], Train Loss: 0.0245, Val Loss: 0.0213\n",
      "Epoch [7/50], Train Loss: 0.0270, Val Loss: 0.0428\n",
      "Epoch [8/50], Train Loss: 0.0227, Val Loss: 0.0436\n",
      "Epoch [9/50], Train Loss: 0.0181, Val Loss: 0.0460\n",
      "Epoch [10/50], Train Loss: 0.0191, Val Loss: 0.0711\n",
      "Epoch [11/50], Train Loss: 0.0124, Val Loss: 0.0434\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=16, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1605, Val Loss: 0.3967\n",
      "Epoch [2/50], Train Loss: 0.1187, Val Loss: 0.3091\n",
      "Epoch [3/50], Train Loss: 0.0800, Val Loss: 0.2078\n",
      "Epoch [4/50], Train Loss: 0.0458, Val Loss: 0.1068\n",
      "Epoch [5/50], Train Loss: 0.0315, Val Loss: 0.0609\n",
      "Epoch [6/50], Train Loss: 0.0307, Val Loss: 0.0548\n",
      "Epoch [7/50], Train Loss: 0.0286, Val Loss: 0.0511\n",
      "Epoch [8/50], Train Loss: 0.0265, Val Loss: 0.0460\n",
      "Epoch [9/50], Train Loss: 0.0246, Val Loss: 0.0407\n",
      "Epoch [10/50], Train Loss: 0.0227, Val Loss: 0.0352\n",
      "Epoch [11/50], Train Loss: 0.0207, Val Loss: 0.0297\n",
      "Epoch [12/50], Train Loss: 0.0184, Val Loss: 0.0244\n",
      "Epoch [13/50], Train Loss: 0.0161, Val Loss: 0.0198\n",
      "Epoch [14/50], Train Loss: 0.0137, Val Loss: 0.0159\n",
      "Epoch [15/50], Train Loss: 0.0113, Val Loss: 0.0130\n",
      "Epoch [16/50], Train Loss: 0.0089, Val Loss: 0.0130\n",
      "Epoch [17/50], Train Loss: 0.0069, Val Loss: 0.0168\n",
      "Epoch [18/50], Train Loss: 0.0057, Val Loss: 0.0187\n",
      "Epoch [19/50], Train Loss: 0.0049, Val Loss: 0.0176\n",
      "Epoch [20/50], Train Loss: 0.0044, Val Loss: 0.0175\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=16, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1203, Val Loss: 0.3097\n",
      "Epoch [2/50], Train Loss: 0.0849, Val Loss: 0.2313\n",
      "Epoch [3/50], Train Loss: 0.0632, Val Loss: 0.1670\n",
      "Epoch [4/50], Train Loss: 0.0486, Val Loss: 0.1162\n",
      "Epoch [5/50], Train Loss: 0.0405, Val Loss: 0.0815\n",
      "Epoch [6/50], Train Loss: 0.0372, Val Loss: 0.0617\n",
      "Epoch [7/50], Train Loss: 0.0342, Val Loss: 0.0520\n",
      "Epoch [8/50], Train Loss: 0.0316, Val Loss: 0.0428\n",
      "Epoch [9/50], Train Loss: 0.0301, Val Loss: 0.0357\n",
      "Epoch [10/50], Train Loss: 0.0283, Val Loss: 0.0276\n",
      "Epoch [11/50], Train Loss: 0.0259, Val Loss: 0.0231\n",
      "Epoch [12/50], Train Loss: 0.0209, Val Loss: 0.0211\n",
      "Epoch [13/50], Train Loss: 0.0178, Val Loss: 0.0174\n",
      "Epoch [14/50], Train Loss: 0.0163, Val Loss: 0.0138\n",
      "Epoch [15/50], Train Loss: 0.0134, Val Loss: 0.0123\n",
      "Epoch [16/50], Train Loss: 0.0146, Val Loss: 0.0089\n",
      "Epoch [17/50], Train Loss: 0.0137, Val Loss: 0.0086\n",
      "Epoch [18/50], Train Loss: 0.0124, Val Loss: 0.0073\n",
      "Epoch [19/50], Train Loss: 0.0120, Val Loss: 0.0063\n",
      "Epoch [20/50], Train Loss: 0.0107, Val Loss: 0.0053\n",
      "Epoch [21/50], Train Loss: 0.0103, Val Loss: 0.0048\n",
      "Epoch [22/50], Train Loss: 0.0099, Val Loss: 0.0038\n",
      "Epoch [23/50], Train Loss: 0.0098, Val Loss: 0.0040\n",
      "Epoch [24/50], Train Loss: 0.0107, Val Loss: 0.0034\n",
      "Epoch [25/50], Train Loss: 0.0106, Val Loss: 0.0042\n",
      "Epoch [26/50], Train Loss: 0.0097, Val Loss: 0.0038\n",
      "Epoch [27/50], Train Loss: 0.0094, Val Loss: 0.0034\n",
      "Epoch [28/50], Train Loss: 0.0091, Val Loss: 0.0030\n",
      "Epoch [29/50], Train Loss: 0.0093, Val Loss: 0.0031\n",
      "Epoch [30/50], Train Loss: 0.0102, Val Loss: 0.0041\n",
      "Epoch [31/50], Train Loss: 0.0096, Val Loss: 0.0038\n",
      "Epoch [32/50], Train Loss: 0.0086, Val Loss: 0.0034\n",
      "Epoch [33/50], Train Loss: 0.0083, Val Loss: 0.0030\n",
      "Epoch [34/50], Train Loss: 0.0082, Val Loss: 0.0026\n",
      "Epoch [35/50], Train Loss: 0.0075, Val Loss: 0.0024\n",
      "Epoch [36/50], Train Loss: 0.0078, Val Loss: 0.0027\n",
      "Epoch [37/50], Train Loss: 0.0075, Val Loss: 0.0024\n",
      "Epoch [38/50], Train Loss: 0.0081, Val Loss: 0.0027\n",
      "Epoch [39/50], Train Loss: 0.0090, Val Loss: 0.0028\n",
      "Epoch [40/50], Train Loss: 0.0077, Val Loss: 0.0026\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=16, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1940, Val Loss: 0.3626\n",
      "Epoch [2/50], Train Loss: 0.1560, Val Loss: 0.2975\n",
      "Epoch [3/50], Train Loss: 0.1222, Val Loss: 0.2319\n",
      "Epoch [4/50], Train Loss: 0.0930, Val Loss: 0.1664\n",
      "Epoch [5/50], Train Loss: 0.0733, Val Loss: 0.1203\n",
      "Epoch [6/50], Train Loss: 0.0610, Val Loss: 0.0982\n",
      "Epoch [7/50], Train Loss: 0.0547, Val Loss: 0.0858\n",
      "Epoch [8/50], Train Loss: 0.0519, Val Loss: 0.0763\n",
      "Epoch [9/50], Train Loss: 0.0460, Val Loss: 0.0659\n",
      "Epoch [10/50], Train Loss: 0.0418, Val Loss: 0.0624\n",
      "Epoch [11/50], Train Loss: 0.0415, Val Loss: 0.0582\n",
      "Epoch [12/50], Train Loss: 0.0392, Val Loss: 0.0514\n",
      "Epoch [13/50], Train Loss: 0.0364, Val Loss: 0.0474\n",
      "Epoch [14/50], Train Loss: 0.0342, Val Loss: 0.0425\n",
      "Epoch [15/50], Train Loss: 0.0326, Val Loss: 0.0375\n",
      "Epoch [16/50], Train Loss: 0.0303, Val Loss: 0.0344\n",
      "Epoch [17/50], Train Loss: 0.0301, Val Loss: 0.0334\n",
      "Epoch [18/50], Train Loss: 0.0274, Val Loss: 0.0332\n",
      "Epoch [19/50], Train Loss: 0.0239, Val Loss: 0.0321\n",
      "Epoch [20/50], Train Loss: 0.0225, Val Loss: 0.0316\n",
      "Epoch [21/50], Train Loss: 0.0228, Val Loss: 0.0318\n",
      "Epoch [22/50], Train Loss: 0.0223, Val Loss: 0.0300\n",
      "Epoch [23/50], Train Loss: 0.0212, Val Loss: 0.0283\n",
      "Epoch [24/50], Train Loss: 0.0209, Val Loss: 0.0281\n",
      "Epoch [25/50], Train Loss: 0.0185, Val Loss: 0.0318\n",
      "Epoch [26/50], Train Loss: 0.0199, Val Loss: 0.0269\n",
      "Epoch [27/50], Train Loss: 0.0185, Val Loss: 0.0263\n",
      "Epoch [28/50], Train Loss: 0.0184, Val Loss: 0.0273\n",
      "Epoch [29/50], Train Loss: 0.0195, Val Loss: 0.0280\n",
      "Epoch [30/50], Train Loss: 0.0179, Val Loss: 0.0264\n",
      "Epoch [31/50], Train Loss: 0.0177, Val Loss: 0.0271\n",
      "Epoch [32/50], Train Loss: 0.0170, Val Loss: 0.0213\n",
      "Epoch [33/50], Train Loss: 0.0178, Val Loss: 0.0253\n",
      "Epoch [34/50], Train Loss: 0.0159, Val Loss: 0.0213\n",
      "Epoch [35/50], Train Loss: 0.0167, Val Loss: 0.0227\n",
      "Epoch [36/50], Train Loss: 0.0160, Val Loss: 0.0217\n",
      "Epoch [37/50], Train Loss: 0.0155, Val Loss: 0.0205\n",
      "Epoch [38/50], Train Loss: 0.0159, Val Loss: 0.0200\n",
      "Epoch [39/50], Train Loss: 0.0154, Val Loss: 0.0192\n",
      "Epoch [40/50], Train Loss: 0.0151, Val Loss: 0.0217\n",
      "Epoch [41/50], Train Loss: 0.0137, Val Loss: 0.0182\n",
      "Epoch [42/50], Train Loss: 0.0153, Val Loss: 0.0229\n",
      "Epoch [43/50], Train Loss: 0.0153, Val Loss: 0.0184\n",
      "Epoch [44/50], Train Loss: 0.0139, Val Loss: 0.0175\n",
      "Epoch [45/50], Train Loss: 0.0143, Val Loss: 0.0198\n",
      "Epoch [46/50], Train Loss: 0.0131, Val Loss: 0.0162\n",
      "Epoch [47/50], Train Loss: 0.0137, Val Loss: 0.0204\n",
      "Epoch [48/50], Train Loss: 0.0141, Val Loss: 0.0158\n",
      "Epoch [49/50], Train Loss: 0.0134, Val Loss: 0.0171\n",
      "Epoch [50/50], Train Loss: 0.0131, Val Loss: 0.0167\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=16, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0832, Val Loss: 0.2376\n",
      "Epoch [2/50], Train Loss: 0.0596, Val Loss: 0.1742\n",
      "Epoch [3/50], Train Loss: 0.0408, Val Loss: 0.1069\n",
      "Epoch [4/50], Train Loss: 0.0348, Val Loss: 0.0731\n",
      "Epoch [5/50], Train Loss: 0.0349, Val Loss: 0.0642\n",
      "Epoch [6/50], Train Loss: 0.0329, Val Loss: 0.0552\n",
      "Epoch [7/50], Train Loss: 0.0305, Val Loss: 0.0439\n",
      "Epoch [8/50], Train Loss: 0.0276, Val Loss: 0.0299\n",
      "Epoch [9/50], Train Loss: 0.0239, Val Loss: 0.0177\n",
      "Epoch [10/50], Train Loss: 0.0199, Val Loss: 0.0136\n",
      "Epoch [11/50], Train Loss: 0.0171, Val Loss: 0.0127\n",
      "Epoch [12/50], Train Loss: 0.0149, Val Loss: 0.0150\n",
      "Epoch [13/50], Train Loss: 0.0128, Val Loss: 0.0144\n",
      "Epoch [14/50], Train Loss: 0.0105, Val Loss: 0.0080\n",
      "Epoch [15/50], Train Loss: 0.0086, Val Loss: 0.0144\n",
      "Epoch [16/50], Train Loss: 0.0075, Val Loss: 0.0279\n",
      "Epoch [17/50], Train Loss: 0.0058, Val Loss: 0.0073\n",
      "Epoch [18/50], Train Loss: 0.0051, Val Loss: 0.0144\n",
      "Epoch [19/50], Train Loss: 0.0086, Val Loss: 0.0171\n",
      "Epoch [20/50], Train Loss: 0.0071, Val Loss: 0.0219\n",
      "Epoch [21/50], Train Loss: 0.0056, Val Loss: 0.0162\n",
      "Epoch [22/50], Train Loss: 0.0054, Val Loss: 0.0186\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=16, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0907, Val Loss: 0.2561\n",
      "Epoch [2/50], Train Loss: 0.0673, Val Loss: 0.1986\n",
      "Epoch [3/50], Train Loss: 0.0489, Val Loss: 0.1351\n",
      "Epoch [4/50], Train Loss: 0.0401, Val Loss: 0.0940\n",
      "Epoch [5/50], Train Loss: 0.0383, Val Loss: 0.0776\n",
      "Epoch [6/50], Train Loss: 0.0355, Val Loss: 0.0626\n",
      "Epoch [7/50], Train Loss: 0.0306, Val Loss: 0.0492\n",
      "Epoch [8/50], Train Loss: 0.0267, Val Loss: 0.0405\n",
      "Epoch [9/50], Train Loss: 0.0248, Val Loss: 0.0346\n",
      "Epoch [10/50], Train Loss: 0.0220, Val Loss: 0.0318\n",
      "Epoch [11/50], Train Loss: 0.0207, Val Loss: 0.0302\n",
      "Epoch [12/50], Train Loss: 0.0186, Val Loss: 0.0272\n",
      "Epoch [13/50], Train Loss: 0.0174, Val Loss: 0.0262\n",
      "Epoch [14/50], Train Loss: 0.0146, Val Loss: 0.0219\n",
      "Epoch [15/50], Train Loss: 0.0116, Val Loss: 0.0191\n",
      "Epoch [16/50], Train Loss: 0.0107, Val Loss: 0.0137\n",
      "Epoch [17/50], Train Loss: 0.0098, Val Loss: 0.0154\n",
      "Epoch [18/50], Train Loss: 0.0088, Val Loss: 0.0185\n",
      "Epoch [19/50], Train Loss: 0.0090, Val Loss: 0.0122\n",
      "Epoch [20/50], Train Loss: 0.0086, Val Loss: 0.0153\n",
      "Epoch [21/50], Train Loss: 0.0086, Val Loss: 0.0152\n",
      "Epoch [22/50], Train Loss: 0.0079, Val Loss: 0.0129\n",
      "Epoch [23/50], Train Loss: 0.0078, Val Loss: 0.0118\n",
      "Epoch [24/50], Train Loss: 0.0078, Val Loss: 0.0147\n",
      "Epoch [25/50], Train Loss: 0.0076, Val Loss: 0.0103\n",
      "Epoch [26/50], Train Loss: 0.0074, Val Loss: 0.0113\n",
      "Epoch [27/50], Train Loss: 0.0078, Val Loss: 0.0142\n",
      "Epoch [28/50], Train Loss: 0.0073, Val Loss: 0.0104\n",
      "Epoch [29/50], Train Loss: 0.0071, Val Loss: 0.0120\n",
      "Epoch [30/50], Train Loss: 0.0077, Val Loss: 0.0125\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=16, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0979, Val Loss: 0.2753\n",
      "Epoch [2/50], Train Loss: 0.0775, Val Loss: 0.2257\n",
      "Epoch [3/50], Train Loss: 0.0614, Val Loss: 0.1756\n",
      "Epoch [4/50], Train Loss: 0.0532, Val Loss: 0.1418\n",
      "Epoch [5/50], Train Loss: 0.0502, Val Loss: 0.1228\n",
      "Epoch [6/50], Train Loss: 0.0485, Val Loss: 0.1123\n",
      "Epoch [7/50], Train Loss: 0.0454, Val Loss: 0.0992\n",
      "Epoch [8/50], Train Loss: 0.0432, Val Loss: 0.0844\n",
      "Epoch [9/50], Train Loss: 0.0400, Val Loss: 0.0649\n",
      "Epoch [10/50], Train Loss: 0.0366, Val Loss: 0.0511\n",
      "Epoch [11/50], Train Loss: 0.0338, Val Loss: 0.0508\n",
      "Epoch [12/50], Train Loss: 0.0306, Val Loss: 0.0464\n",
      "Epoch [13/50], Train Loss: 0.0293, Val Loss: 0.0309\n",
      "Epoch [14/50], Train Loss: 0.0282, Val Loss: 0.0300\n",
      "Epoch [15/50], Train Loss: 0.0250, Val Loss: 0.0308\n",
      "Epoch [16/50], Train Loss: 0.0244, Val Loss: 0.0274\n",
      "Epoch [17/50], Train Loss: 0.0232, Val Loss: 0.0270\n",
      "Epoch [18/50], Train Loss: 0.0215, Val Loss: 0.0319\n",
      "Epoch [19/50], Train Loss: 0.0207, Val Loss: 0.0273\n",
      "Epoch [20/50], Train Loss: 0.0206, Val Loss: 0.0282\n",
      "Epoch [21/50], Train Loss: 0.0191, Val Loss: 0.0267\n",
      "Epoch [22/50], Train Loss: 0.0183, Val Loss: 0.0242\n",
      "Epoch [23/50], Train Loss: 0.0183, Val Loss: 0.0244\n",
      "Epoch [24/50], Train Loss: 0.0184, Val Loss: 0.0243\n",
      "Epoch [25/50], Train Loss: 0.0181, Val Loss: 0.0271\n",
      "Epoch [26/50], Train Loss: 0.0177, Val Loss: 0.0267\n",
      "Epoch [27/50], Train Loss: 0.0174, Val Loss: 0.0240\n",
      "Epoch [28/50], Train Loss: 0.0176, Val Loss: 0.0292\n",
      "Epoch [29/50], Train Loss: 0.0173, Val Loss: 0.0228\n",
      "Epoch [30/50], Train Loss: 0.0170, Val Loss: 0.0260\n",
      "Epoch [31/50], Train Loss: 0.0164, Val Loss: 0.0211\n",
      "Epoch [32/50], Train Loss: 0.0159, Val Loss: 0.0253\n",
      "Epoch [33/50], Train Loss: 0.0150, Val Loss: 0.0224\n",
      "Epoch [34/50], Train Loss: 0.0152, Val Loss: 0.0234\n",
      "Epoch [35/50], Train Loss: 0.0144, Val Loss: 0.0233\n",
      "Epoch [36/50], Train Loss: 0.0145, Val Loss: 0.0243\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=16, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1580, Val Loss: 0.3714\n",
      "Epoch [2/50], Train Loss: 0.1118, Val Loss: 0.2739\n",
      "Epoch [3/50], Train Loss: 0.0588, Val Loss: 0.1208\n",
      "Epoch [4/50], Train Loss: 0.0476, Val Loss: 0.1004\n",
      "Epoch [5/50], Train Loss: 0.0408, Val Loss: 0.0881\n",
      "Epoch [6/50], Train Loss: 0.0383, Val Loss: 0.0791\n",
      "Epoch [7/50], Train Loss: 0.0352, Val Loss: 0.0651\n",
      "Epoch [8/50], Train Loss: 0.0309, Val Loss: 0.0465\n",
      "Epoch [9/50], Train Loss: 0.0260, Val Loss: 0.0340\n",
      "Epoch [10/50], Train Loss: 0.0228, Val Loss: 0.0300\n",
      "Epoch [11/50], Train Loss: 0.0210, Val Loss: 0.0308\n",
      "Epoch [12/50], Train Loss: 0.0199, Val Loss: 0.0308\n",
      "Epoch [13/50], Train Loss: 0.0192, Val Loss: 0.0278\n",
      "Epoch [14/50], Train Loss: 0.0188, Val Loss: 0.0266\n",
      "Epoch [15/50], Train Loss: 0.0185, Val Loss: 0.0259\n",
      "Epoch [16/50], Train Loss: 0.0182, Val Loss: 0.0263\n",
      "Epoch [17/50], Train Loss: 0.0179, Val Loss: 0.0264\n",
      "Epoch [18/50], Train Loss: 0.0177, Val Loss: 0.0258\n",
      "Epoch [19/50], Train Loss: 0.0176, Val Loss: 0.0250\n",
      "Epoch [20/50], Train Loss: 0.0175, Val Loss: 0.0246\n",
      "Epoch [21/50], Train Loss: 0.0174, Val Loss: 0.0247\n",
      "Epoch [22/50], Train Loss: 0.0173, Val Loss: 0.0248\n",
      "Epoch [23/50], Train Loss: 0.0172, Val Loss: 0.0245\n",
      "Epoch [24/50], Train Loss: 0.0171, Val Loss: 0.0238\n",
      "Epoch [25/50], Train Loss: 0.0170, Val Loss: 0.0233\n",
      "Epoch [26/50], Train Loss: 0.0169, Val Loss: 0.0231\n",
      "Epoch [27/50], Train Loss: 0.0168, Val Loss: 0.0232\n",
      "Epoch [28/50], Train Loss: 0.0167, Val Loss: 0.0233\n",
      "Epoch [29/50], Train Loss: 0.0165, Val Loss: 0.0228\n",
      "Epoch [30/50], Train Loss: 0.0163, Val Loss: 0.0220\n",
      "Epoch [31/50], Train Loss: 0.0161, Val Loss: 0.0212\n",
      "Epoch [32/50], Train Loss: 0.0158, Val Loss: 0.0205\n",
      "Epoch [33/50], Train Loss: 0.0153, Val Loss: 0.0197\n",
      "Epoch [34/50], Train Loss: 0.0125, Val Loss: 0.0188\n",
      "Epoch [35/50], Train Loss: 0.0073, Val Loss: 0.0148\n",
      "Epoch [36/50], Train Loss: 0.0083, Val Loss: 0.0078\n",
      "Epoch [37/50], Train Loss: 0.0084, Val Loss: 0.0299\n",
      "Epoch [38/50], Train Loss: 0.0065, Val Loss: 0.0181\n",
      "Epoch [39/50], Train Loss: 0.0067, Val Loss: 0.0224\n",
      "Epoch [40/50], Train Loss: 0.0074, Val Loss: 0.0168\n",
      "Epoch [41/50], Train Loss: 0.0062, Val Loss: 0.0172\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=16, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1672, Val Loss: 0.3686\n",
      "Epoch [2/50], Train Loss: 0.1004, Val Loss: 0.1951\n",
      "Epoch [3/50], Train Loss: 0.0512, Val Loss: 0.0973\n",
      "Epoch [4/50], Train Loss: 0.0519, Val Loss: 0.0966\n",
      "Epoch [5/50], Train Loss: 0.0492, Val Loss: 0.0876\n",
      "Epoch [6/50], Train Loss: 0.0461, Val Loss: 0.0789\n",
      "Epoch [7/50], Train Loss: 0.0431, Val Loss: 0.0642\n",
      "Epoch [8/50], Train Loss: 0.0392, Val Loss: 0.0527\n",
      "Epoch [9/50], Train Loss: 0.0353, Val Loss: 0.0433\n",
      "Epoch [10/50], Train Loss: 0.0328, Val Loss: 0.0376\n",
      "Epoch [11/50], Train Loss: 0.0314, Val Loss: 0.0390\n",
      "Epoch [12/50], Train Loss: 0.0296, Val Loss: 0.0378\n",
      "Epoch [13/50], Train Loss: 0.0281, Val Loss: 0.0349\n",
      "Epoch [14/50], Train Loss: 0.0265, Val Loss: 0.0310\n",
      "Epoch [15/50], Train Loss: 0.0203, Val Loss: 0.0338\n",
      "Epoch [16/50], Train Loss: 0.0201, Val Loss: 0.0503\n",
      "Epoch [17/50], Train Loss: 0.0162, Val Loss: 0.0355\n",
      "Epoch [18/50], Train Loss: 0.0162, Val Loss: 0.0384\n",
      "Epoch [19/50], Train Loss: 0.0183, Val Loss: 0.0445\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=16, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0906, Val Loss: 0.2185\n",
      "Epoch [2/50], Train Loss: 0.0689, Val Loss: 0.1717\n",
      "Epoch [3/50], Train Loss: 0.0606, Val Loss: 0.1480\n",
      "Epoch [4/50], Train Loss: 0.0595, Val Loss: 0.1376\n",
      "Epoch [5/50], Train Loss: 0.0557, Val Loss: 0.1276\n",
      "Epoch [6/50], Train Loss: 0.0524, Val Loss: 0.1182\n",
      "Epoch [7/50], Train Loss: 0.0517, Val Loss: 0.1065\n",
      "Epoch [8/50], Train Loss: 0.0489, Val Loss: 0.0888\n",
      "Epoch [9/50], Train Loss: 0.0458, Val Loss: 0.0694\n",
      "Epoch [10/50], Train Loss: 0.0411, Val Loss: 0.0543\n",
      "Epoch [11/50], Train Loss: 0.0393, Val Loss: 0.0539\n",
      "Epoch [12/50], Train Loss: 0.0376, Val Loss: 0.0551\n",
      "Epoch [13/50], Train Loss: 0.0343, Val Loss: 0.0451\n",
      "Epoch [14/50], Train Loss: 0.0300, Val Loss: 0.0390\n",
      "Epoch [15/50], Train Loss: 0.0272, Val Loss: 0.0517\n",
      "Epoch [16/50], Train Loss: 0.0272, Val Loss: 0.0435\n",
      "Epoch [17/50], Train Loss: 0.0251, Val Loss: 0.0435\n",
      "Epoch [18/50], Train Loss: 0.0240, Val Loss: 0.0416\n",
      "Epoch [19/50], Train Loss: 0.0234, Val Loss: 0.0396\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=32, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1060, Val Loss: 0.2666\n",
      "Epoch [2/50], Train Loss: 0.0619, Val Loss: 0.1532\n",
      "Epoch [3/50], Train Loss: 0.0306, Val Loss: 0.0579\n",
      "Epoch [4/50], Train Loss: 0.0275, Val Loss: 0.0395\n",
      "Epoch [5/50], Train Loss: 0.0240, Val Loss: 0.0263\n",
      "Epoch [6/50], Train Loss: 0.0200, Val Loss: 0.0148\n",
      "Epoch [7/50], Train Loss: 0.0161, Val Loss: 0.0079\n",
      "Epoch [8/50], Train Loss: 0.0121, Val Loss: 0.0076\n",
      "Epoch [9/50], Train Loss: 0.0080, Val Loss: 0.0169\n",
      "Epoch [10/50], Train Loss: 0.0055, Val Loss: 0.0218\n",
      "Epoch [11/50], Train Loss: 0.0050, Val Loss: 0.0158\n",
      "Epoch [12/50], Train Loss: 0.0044, Val Loss: 0.0183\n",
      "Epoch [13/50], Train Loss: 0.0048, Val Loss: 0.0239\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=32, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1681, Val Loss: 0.3977\n",
      "Epoch [2/50], Train Loss: 0.0960, Val Loss: 0.1875\n",
      "Epoch [3/50], Train Loss: 0.0334, Val Loss: 0.0196\n",
      "Epoch [4/50], Train Loss: 0.0447, Val Loss: 0.0433\n",
      "Epoch [5/50], Train Loss: 0.0300, Val Loss: 0.0218\n",
      "Epoch [6/50], Train Loss: 0.0286, Val Loss: 0.0156\n",
      "Epoch [7/50], Train Loss: 0.0251, Val Loss: 0.0100\n",
      "Epoch [8/50], Train Loss: 0.0234, Val Loss: 0.0067\n",
      "Epoch [9/50], Train Loss: 0.0223, Val Loss: 0.0063\n",
      "Epoch [10/50], Train Loss: 0.0202, Val Loss: 0.0067\n",
      "Epoch [11/50], Train Loss: 0.0204, Val Loss: 0.0054\n",
      "Epoch [12/50], Train Loss: 0.0187, Val Loss: 0.0049\n",
      "Epoch [13/50], Train Loss: 0.0172, Val Loss: 0.0051\n",
      "Epoch [14/50], Train Loss: 0.0160, Val Loss: 0.0045\n",
      "Epoch [15/50], Train Loss: 0.0135, Val Loss: 0.0087\n",
      "Epoch [16/50], Train Loss: 0.0115, Val Loss: 0.0100\n",
      "Epoch [17/50], Train Loss: 0.0098, Val Loss: 0.0157\n",
      "Epoch [18/50], Train Loss: 0.0094, Val Loss: 0.0132\n",
      "Epoch [19/50], Train Loss: 0.0095, Val Loss: 0.0132\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=32, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1106, Val Loss: 0.2403\n",
      "Epoch [2/50], Train Loss: 0.0774, Val Loss: 0.1600\n",
      "Epoch [3/50], Train Loss: 0.0495, Val Loss: 0.0695\n",
      "Epoch [4/50], Train Loss: 0.0382, Val Loss: 0.0412\n",
      "Epoch [5/50], Train Loss: 0.0333, Val Loss: 0.0290\n",
      "Epoch [6/50], Train Loss: 0.0284, Val Loss: 0.0161\n",
      "Epoch [7/50], Train Loss: 0.0230, Val Loss: 0.0101\n",
      "Epoch [8/50], Train Loss: 0.0207, Val Loss: 0.0146\n",
      "Epoch [9/50], Train Loss: 0.0190, Val Loss: 0.0137\n",
      "Epoch [10/50], Train Loss: 0.0163, Val Loss: 0.0132\n",
      "Epoch [11/50], Train Loss: 0.0153, Val Loss: 0.0105\n",
      "Epoch [12/50], Train Loss: 0.0143, Val Loss: 0.0151\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=32, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0991, Val Loss: 0.2281\n",
      "Epoch [2/50], Train Loss: 0.0380, Val Loss: 0.0751\n",
      "Epoch [3/50], Train Loss: 0.0383, Val Loss: 0.0670\n",
      "Epoch [4/50], Train Loss: 0.0316, Val Loss: 0.0435\n",
      "Epoch [5/50], Train Loss: 0.0278, Val Loss: 0.0234\n",
      "Epoch [6/50], Train Loss: 0.0212, Val Loss: 0.0181\n",
      "Epoch [7/50], Train Loss: 0.0158, Val Loss: 0.0301\n",
      "Epoch [8/50], Train Loss: 0.0113, Val Loss: 0.0361\n",
      "Epoch [9/50], Train Loss: 0.0103, Val Loss: 0.0239\n",
      "Epoch [10/50], Train Loss: 0.0079, Val Loss: 0.0271\n",
      "Epoch [11/50], Train Loss: 0.0085, Val Loss: 0.0363\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=32, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1413, Val Loss: 0.2923\n",
      "Epoch [2/50], Train Loss: 0.0612, Val Loss: 0.0875\n",
      "Epoch [3/50], Train Loss: 0.0483, Val Loss: 0.0776\n",
      "Epoch [4/50], Train Loss: 0.0390, Val Loss: 0.0607\n",
      "Epoch [5/50], Train Loss: 0.0373, Val Loss: 0.0497\n",
      "Epoch [6/50], Train Loss: 0.0345, Val Loss: 0.0392\n",
      "Epoch [7/50], Train Loss: 0.0311, Val Loss: 0.0287\n",
      "Epoch [8/50], Train Loss: 0.0278, Val Loss: 0.0246\n",
      "Epoch [9/50], Train Loss: 0.0251, Val Loss: 0.0178\n",
      "Epoch [10/50], Train Loss: 0.0210, Val Loss: 0.0147\n",
      "Epoch [11/50], Train Loss: 0.0181, Val Loss: 0.0289\n",
      "Epoch [12/50], Train Loss: 0.0145, Val Loss: 0.0055\n",
      "Epoch [13/50], Train Loss: 0.0127, Val Loss: 0.0082\n",
      "Epoch [14/50], Train Loss: 0.0149, Val Loss: 0.0060\n",
      "Epoch [15/50], Train Loss: 0.0133, Val Loss: 0.0226\n",
      "Epoch [16/50], Train Loss: 0.0112, Val Loss: 0.0068\n",
      "Epoch [17/50], Train Loss: 0.0104, Val Loss: 0.0108\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=32, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1176, Val Loss: 0.2560\n",
      "Epoch [2/50], Train Loss: 0.0531, Val Loss: 0.0848\n",
      "Epoch [3/50], Train Loss: 0.0491, Val Loss: 0.0742\n",
      "Epoch [4/50], Train Loss: 0.0442, Val Loss: 0.0540\n",
      "Epoch [5/50], Train Loss: 0.0408, Val Loss: 0.0435\n",
      "Epoch [6/50], Train Loss: 0.0353, Val Loss: 0.0319\n",
      "Epoch [7/50], Train Loss: 0.0339, Val Loss: 0.0247\n",
      "Epoch [8/50], Train Loss: 0.0314, Val Loss: 0.0250\n",
      "Epoch [9/50], Train Loss: 0.0303, Val Loss: 0.0268\n",
      "Epoch [10/50], Train Loss: 0.0291, Val Loss: 0.0301\n",
      "Epoch [11/50], Train Loss: 0.0281, Val Loss: 0.0263\n",
      "Epoch [12/50], Train Loss: 0.0274, Val Loss: 0.0202\n",
      "Epoch [13/50], Train Loss: 0.0274, Val Loss: 0.0188\n",
      "Epoch [14/50], Train Loss: 0.0249, Val Loss: 0.0214\n",
      "Epoch [15/50], Train Loss: 0.0235, Val Loss: 0.0134\n",
      "Epoch [16/50], Train Loss: 0.0213, Val Loss: 0.0060\n",
      "Epoch [17/50], Train Loss: 0.0165, Val Loss: 0.0047\n",
      "Epoch [18/50], Train Loss: 0.0169, Val Loss: 0.0190\n",
      "Epoch [19/50], Train Loss: 0.0150, Val Loss: 0.0087\n",
      "Epoch [20/50], Train Loss: 0.0150, Val Loss: 0.0066\n",
      "Epoch [21/50], Train Loss: 0.0160, Val Loss: 0.0320\n",
      "Epoch [22/50], Train Loss: 0.0139, Val Loss: 0.0090\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=32, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1201, Val Loss: 0.2434\n",
      "Epoch [2/50], Train Loss: 0.0299, Val Loss: 0.0366\n",
      "Epoch [3/50], Train Loss: 0.0566, Val Loss: 0.0870\n",
      "Epoch [4/50], Train Loss: 0.0352, Val Loss: 0.0494\n",
      "Epoch [5/50], Train Loss: 0.0377, Val Loss: 0.0408\n",
      "Epoch [6/50], Train Loss: 0.0310, Val Loss: 0.0223\n",
      "Epoch [7/50], Train Loss: 0.0245, Val Loss: 0.0163\n",
      "Epoch [8/50], Train Loss: 0.0209, Val Loss: 0.0166\n",
      "Epoch [9/50], Train Loss: 0.0207, Val Loss: 0.0181\n",
      "Epoch [10/50], Train Loss: 0.0211, Val Loss: 0.0244\n",
      "Epoch [11/50], Train Loss: 0.0210, Val Loss: 0.0413\n",
      "Epoch [12/50], Train Loss: 0.0205, Val Loss: 0.0298\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=32, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0665, Val Loss: 0.1909\n",
      "Epoch [2/50], Train Loss: 0.0418, Val Loss: 0.1131\n",
      "Epoch [3/50], Train Loss: 0.0411, Val Loss: 0.0970\n",
      "Epoch [4/50], Train Loss: 0.0408, Val Loss: 0.0867\n",
      "Epoch [5/50], Train Loss: 0.0374, Val Loss: 0.0476\n",
      "Epoch [6/50], Train Loss: 0.0312, Val Loss: 0.0317\n",
      "Epoch [7/50], Train Loss: 0.0237, Val Loss: 0.0452\n",
      "Epoch [8/50], Train Loss: 0.0208, Val Loss: 0.0337\n",
      "Epoch [9/50], Train Loss: 0.0178, Val Loss: 0.0433\n",
      "Epoch [10/50], Train Loss: 0.0133, Val Loss: 0.0512\n",
      "Epoch [11/50], Train Loss: 0.0154, Val Loss: 0.0274\n",
      "Epoch [12/50], Train Loss: 0.0096, Val Loss: 0.0462\n",
      "Epoch [13/50], Train Loss: 0.0099, Val Loss: 0.0362\n",
      "Epoch [14/50], Train Loss: 0.0093, Val Loss: 0.0332\n",
      "Epoch [15/50], Train Loss: 0.0136, Val Loss: 0.0382\n",
      "Epoch [16/50], Train Loss: 0.0107, Val Loss: 0.0435\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=32, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1206, Val Loss: 0.2834\n",
      "Epoch [2/50], Train Loss: 0.0558, Val Loss: 0.0738\n",
      "Epoch [3/50], Train Loss: 0.0664, Val Loss: 0.1181\n",
      "Epoch [4/50], Train Loss: 0.0497, Val Loss: 0.0643\n",
      "Epoch [5/50], Train Loss: 0.0474, Val Loss: 0.0426\n",
      "Epoch [6/50], Train Loss: 0.0411, Val Loss: 0.0347\n",
      "Epoch [7/50], Train Loss: 0.0361, Val Loss: 0.0415\n",
      "Epoch [8/50], Train Loss: 0.0329, Val Loss: 0.0208\n",
      "Epoch [9/50], Train Loss: 0.0320, Val Loss: 0.0178\n",
      "Epoch [10/50], Train Loss: 0.0322, Val Loss: 0.0295\n",
      "Epoch [11/50], Train Loss: 0.0309, Val Loss: 0.0429\n",
      "Epoch [12/50], Train Loss: 0.0288, Val Loss: 0.0253\n",
      "Epoch [13/50], Train Loss: 0.0287, Val Loss: 0.0235\n",
      "Epoch [14/50], Train Loss: 0.0285, Val Loss: 0.0396\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=64, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0853, Val Loss: 0.2314\n",
      "Epoch [2/50], Train Loss: 0.0389, Val Loss: 0.0618\n",
      "Epoch [3/50], Train Loss: 0.0315, Val Loss: 0.0365\n",
      "Epoch [4/50], Train Loss: 0.0260, Val Loss: 0.0195\n",
      "Epoch [5/50], Train Loss: 0.0211, Val Loss: 0.0100\n",
      "Epoch [6/50], Train Loss: 0.0179, Val Loss: 0.0171\n",
      "Epoch [7/50], Train Loss: 0.0095, Val Loss: 0.0152\n",
      "Epoch [8/50], Train Loss: 0.0074, Val Loss: 0.0176\n",
      "Epoch [9/50], Train Loss: 0.0059, Val Loss: 0.0282\n",
      "Epoch [10/50], Train Loss: 0.0068, Val Loss: 0.0114\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=64, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1110, Val Loss: 0.1973\n",
      "Epoch [2/50], Train Loss: 0.0307, Val Loss: 0.0447\n",
      "Epoch [3/50], Train Loss: 0.0313, Val Loss: 0.0145\n",
      "Epoch [4/50], Train Loss: 0.0308, Val Loss: 0.0195\n",
      "Epoch [5/50], Train Loss: 0.0242, Val Loss: 0.0165\n",
      "Epoch [6/50], Train Loss: 0.0207, Val Loss: 0.0125\n",
      "Epoch [7/50], Train Loss: 0.0199, Val Loss: 0.0112\n",
      "Epoch [8/50], Train Loss: 0.0173, Val Loss: 0.0068\n",
      "Epoch [9/50], Train Loss: 0.0149, Val Loss: 0.0038\n",
      "Epoch [10/50], Train Loss: 0.0132, Val Loss: 0.0279\n",
      "Epoch [11/50], Train Loss: 0.0093, Val Loss: 0.0057\n",
      "Epoch [12/50], Train Loss: 0.0083, Val Loss: 0.0075\n",
      "Epoch [13/50], Train Loss: 0.0081, Val Loss: 0.0157\n",
      "Epoch [14/50], Train Loss: 0.0066, Val Loss: 0.0059\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=64, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1290, Val Loss: 0.2612\n",
      "Epoch [2/50], Train Loss: 0.0450, Val Loss: 0.0503\n",
      "Epoch [3/50], Train Loss: 0.0493, Val Loss: 0.0630\n",
      "Epoch [4/50], Train Loss: 0.0369, Val Loss: 0.0415\n",
      "Epoch [5/50], Train Loss: 0.0329, Val Loss: 0.0253\n",
      "Epoch [6/50], Train Loss: 0.0303, Val Loss: 0.0116\n",
      "Epoch [7/50], Train Loss: 0.0251, Val Loss: 0.0053\n",
      "Epoch [8/50], Train Loss: 0.0183, Val Loss: 0.0162\n",
      "Epoch [9/50], Train Loss: 0.0142, Val Loss: 0.0142\n",
      "Epoch [10/50], Train Loss: 0.0166, Val Loss: 0.0338\n",
      "Epoch [11/50], Train Loss: 0.0185, Val Loss: 0.0037\n",
      "Epoch [12/50], Train Loss: 0.0134, Val Loss: 0.0071\n",
      "Epoch [13/50], Train Loss: 0.0117, Val Loss: 0.0120\n",
      "Epoch [14/50], Train Loss: 0.0118, Val Loss: 0.0152\n",
      "Epoch [15/50], Train Loss: 0.0144, Val Loss: 0.0017\n",
      "Epoch [16/50], Train Loss: 0.0104, Val Loss: 0.0088\n",
      "Epoch [17/50], Train Loss: 0.0103, Val Loss: 0.0071\n",
      "Epoch [18/50], Train Loss: 0.0097, Val Loss: 0.0117\n",
      "Epoch [19/50], Train Loss: 0.0125, Val Loss: 0.0022\n",
      "Epoch [20/50], Train Loss: 0.0089, Val Loss: 0.0089\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=64, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0695, Val Loss: 0.1156\n",
      "Epoch [2/50], Train Loss: 0.0357, Val Loss: 0.0418\n",
      "Epoch [3/50], Train Loss: 0.0386, Val Loss: 0.0336\n",
      "Epoch [4/50], Train Loss: 0.0307, Val Loss: 0.0125\n",
      "Epoch [5/50], Train Loss: 0.0240, Val Loss: 0.0068\n",
      "Epoch [6/50], Train Loss: 0.0160, Val Loss: 0.0389\n",
      "Epoch [7/50], Train Loss: 0.0121, Val Loss: 0.0210\n",
      "Epoch [8/50], Train Loss: 0.0136, Val Loss: 0.0177\n",
      "Epoch [9/50], Train Loss: 0.0089, Val Loss: 0.0348\n",
      "Epoch [10/50], Train Loss: 0.0082, Val Loss: 0.0110\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=64, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0816, Val Loss: 0.0583\n",
      "Epoch [2/50], Train Loss: 0.0498, Val Loss: 0.0503\n",
      "Epoch [3/50], Train Loss: 0.0390, Val Loss: 0.0359\n",
      "Epoch [4/50], Train Loss: 0.0353, Val Loss: 0.0205\n",
      "Epoch [5/50], Train Loss: 0.0303, Val Loss: 0.0146\n",
      "Epoch [6/50], Train Loss: 0.0250, Val Loss: 0.0207\n",
      "Epoch [7/50], Train Loss: 0.0226, Val Loss: 0.0197\n",
      "Epoch [8/50], Train Loss: 0.0192, Val Loss: 0.0153\n",
      "Epoch [9/50], Train Loss: 0.0133, Val Loss: 0.0229\n",
      "Epoch [10/50], Train Loss: 0.0194, Val Loss: 0.0611\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=64, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1143, Val Loss: 0.1767\n",
      "Epoch [2/50], Train Loss: 0.0433, Val Loss: 0.0577\n",
      "Epoch [3/50], Train Loss: 0.0524, Val Loss: 0.0349\n",
      "Epoch [4/50], Train Loss: 0.0457, Val Loss: 0.0278\n",
      "Epoch [5/50], Train Loss: 0.0361, Val Loss: 0.0094\n",
      "Epoch [6/50], Train Loss: 0.0302, Val Loss: 0.0040\n",
      "Epoch [7/50], Train Loss: 0.0259, Val Loss: 0.0053\n",
      "Epoch [8/50], Train Loss: 0.0182, Val Loss: 0.0186\n",
      "Epoch [9/50], Train Loss: 0.0161, Val Loss: 0.0137\n",
      "Epoch [10/50], Train Loss: 0.0190, Val Loss: 0.0581\n",
      "Epoch [11/50], Train Loss: 0.0186, Val Loss: 0.0115\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=64, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0649, Val Loss: 0.0428\n",
      "Epoch [2/50], Train Loss: 0.0642, Val Loss: 0.1154\n",
      "Epoch [3/50], Train Loss: 0.0366, Val Loss: 0.0522\n",
      "Epoch [4/50], Train Loss: 0.0452, Val Loss: 0.0564\n",
      "Epoch [5/50], Train Loss: 0.0363, Val Loss: 0.0238\n",
      "Epoch [6/50], Train Loss: 0.0319, Val Loss: 0.0138\n",
      "Epoch [7/50], Train Loss: 0.0245, Val Loss: 0.0085\n",
      "Epoch [8/50], Train Loss: 0.0228, Val Loss: 0.0111\n",
      "Epoch [9/50], Train Loss: 0.0208, Val Loss: 0.0136\n",
      "Epoch [10/50], Train Loss: 0.0199, Val Loss: 0.0161\n",
      "Epoch [11/50], Train Loss: 0.0204, Val Loss: 0.0294\n",
      "Epoch [12/50], Train Loss: 0.0175, Val Loss: 0.0549\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=64, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0735, Val Loss: 0.0942\n",
      "Epoch [2/50], Train Loss: 0.0520, Val Loss: 0.0789\n",
      "Epoch [3/50], Train Loss: 0.0444, Val Loss: 0.0598\n",
      "Epoch [4/50], Train Loss: 0.0409, Val Loss: 0.0367\n",
      "Epoch [5/50], Train Loss: 0.0349, Val Loss: 0.0287\n",
      "Epoch [6/50], Train Loss: 0.0274, Val Loss: 0.0195\n",
      "Epoch [7/50], Train Loss: 0.0250, Val Loss: 0.0186\n",
      "Epoch [8/50], Train Loss: 0.0234, Val Loss: 0.0176\n",
      "Epoch [9/50], Train Loss: 0.0225, Val Loss: 0.0188\n",
      "Epoch [10/50], Train Loss: 0.0224, Val Loss: 0.0603\n",
      "Epoch [11/50], Train Loss: 0.0213, Val Loss: 0.0332\n",
      "Epoch [12/50], Train Loss: 0.0187, Val Loss: 0.0352\n",
      "Epoch [13/50], Train Loss: 0.0117, Val Loss: 0.0416\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=64, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0823, Val Loss: 0.0840\n",
      "Epoch [2/50], Train Loss: 0.0550, Val Loss: 0.0775\n",
      "Epoch [3/50], Train Loss: 0.0505, Val Loss: 0.0591\n",
      "Epoch [4/50], Train Loss: 0.0449, Val Loss: 0.0313\n",
      "Epoch [5/50], Train Loss: 0.0381, Val Loss: 0.0158\n",
      "Epoch [6/50], Train Loss: 0.0329, Val Loss: 0.0236\n",
      "Epoch [7/50], Train Loss: 0.0331, Val Loss: 0.0295\n",
      "Epoch [8/50], Train Loss: 0.0279, Val Loss: 0.0385\n",
      "Epoch [9/50], Train Loss: 0.0267, Val Loss: 0.0445\n",
      "Epoch [10/50], Train Loss: 0.0247, Val Loss: 0.0279\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=16, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1192, Val Loss: 0.2991\n",
      "Epoch [2/50], Train Loss: 0.1178, Val Loss: 0.2966\n",
      "Epoch [3/50], Train Loss: 0.1164, Val Loss: 0.2942\n",
      "Epoch [4/50], Train Loss: 0.1151, Val Loss: 0.2917\n",
      "Epoch [5/50], Train Loss: 0.1137, Val Loss: 0.2893\n",
      "Epoch [6/50], Train Loss: 0.1124, Val Loss: 0.2869\n",
      "Epoch [7/50], Train Loss: 0.1112, Val Loss: 0.2846\n",
      "Epoch [8/50], Train Loss: 0.1099, Val Loss: 0.2822\n",
      "Epoch [9/50], Train Loss: 0.1087, Val Loss: 0.2799\n",
      "Epoch [10/50], Train Loss: 0.1074, Val Loss: 0.2777\n",
      "Epoch [11/50], Train Loss: 0.1062, Val Loss: 0.2754\n",
      "Epoch [12/50], Train Loss: 0.1050, Val Loss: 0.2732\n",
      "Epoch [13/50], Train Loss: 0.1039, Val Loss: 0.2710\n",
      "Epoch [14/50], Train Loss: 0.1027, Val Loss: 0.2689\n",
      "Epoch [15/50], Train Loss: 0.1016, Val Loss: 0.2667\n",
      "Epoch [16/50], Train Loss: 0.1005, Val Loss: 0.2646\n",
      "Epoch [17/50], Train Loss: 0.0994, Val Loss: 0.2626\n",
      "Epoch [18/50], Train Loss: 0.0983, Val Loss: 0.2605\n",
      "Epoch [19/50], Train Loss: 0.0972, Val Loss: 0.2584\n",
      "Epoch [20/50], Train Loss: 0.0962, Val Loss: 0.2564\n",
      "Epoch [21/50], Train Loss: 0.0952, Val Loss: 0.2545\n",
      "Epoch [22/50], Train Loss: 0.0942, Val Loss: 0.2525\n",
      "Epoch [23/50], Train Loss: 0.0932, Val Loss: 0.2506\n",
      "Epoch [24/50], Train Loss: 0.0922, Val Loss: 0.2487\n",
      "Epoch [25/50], Train Loss: 0.0912, Val Loss: 0.2468\n",
      "Epoch [26/50], Train Loss: 0.0903, Val Loss: 0.2449\n",
      "Epoch [27/50], Train Loss: 0.0893, Val Loss: 0.2430\n",
      "Epoch [28/50], Train Loss: 0.0884, Val Loss: 0.2412\n",
      "Epoch [29/50], Train Loss: 0.0875, Val Loss: 0.2394\n",
      "Epoch [30/50], Train Loss: 0.0866, Val Loss: 0.2376\n",
      "Epoch [31/50], Train Loss: 0.0857, Val Loss: 0.2359\n",
      "Epoch [32/50], Train Loss: 0.0848, Val Loss: 0.2342\n",
      "Epoch [33/50], Train Loss: 0.0840, Val Loss: 0.2324\n",
      "Epoch [34/50], Train Loss: 0.0832, Val Loss: 0.2307\n",
      "Epoch [35/50], Train Loss: 0.0823, Val Loss: 0.2291\n",
      "Epoch [36/50], Train Loss: 0.0815, Val Loss: 0.2274\n",
      "Epoch [37/50], Train Loss: 0.0807, Val Loss: 0.2258\n",
      "Epoch [38/50], Train Loss: 0.0799, Val Loss: 0.2242\n",
      "Epoch [39/50], Train Loss: 0.0791, Val Loss: 0.2226\n",
      "Epoch [40/50], Train Loss: 0.0784, Val Loss: 0.2210\n",
      "Epoch [41/50], Train Loss: 0.0776, Val Loss: 0.2194\n",
      "Epoch [42/50], Train Loss: 0.0769, Val Loss: 0.2179\n",
      "Epoch [43/50], Train Loss: 0.0762, Val Loss: 0.2164\n",
      "Epoch [44/50], Train Loss: 0.0754, Val Loss: 0.2149\n",
      "Epoch [45/50], Train Loss: 0.0747, Val Loss: 0.2134\n",
      "Epoch [46/50], Train Loss: 0.0740, Val Loss: 0.2119\n",
      "Epoch [47/50], Train Loss: 0.0734, Val Loss: 0.2105\n",
      "Epoch [48/50], Train Loss: 0.0727, Val Loss: 0.2091\n",
      "Epoch [49/50], Train Loss: 0.0720, Val Loss: 0.2076\n",
      "Epoch [50/50], Train Loss: 0.0714, Val Loss: 0.2063\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=16, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1165, Val Loss: 0.3352\n",
      "Epoch [2/50], Train Loss: 0.1147, Val Loss: 0.3325\n",
      "Epoch [3/50], Train Loss: 0.1149, Val Loss: 0.3298\n",
      "Epoch [4/50], Train Loss: 0.1130, Val Loss: 0.3271\n",
      "Epoch [5/50], Train Loss: 0.1117, Val Loss: 0.3245\n",
      "Epoch [6/50], Train Loss: 0.1111, Val Loss: 0.3219\n",
      "Epoch [7/50], Train Loss: 0.1086, Val Loss: 0.3194\n",
      "Epoch [8/50], Train Loss: 0.1081, Val Loss: 0.3168\n",
      "Epoch [9/50], Train Loss: 0.1073, Val Loss: 0.3143\n",
      "Epoch [10/50], Train Loss: 0.1057, Val Loss: 0.3119\n",
      "Epoch [11/50], Train Loss: 0.1039, Val Loss: 0.3094\n",
      "Epoch [12/50], Train Loss: 0.1028, Val Loss: 0.3071\n",
      "Epoch [13/50], Train Loss: 0.1018, Val Loss: 0.3047\n",
      "Epoch [14/50], Train Loss: 0.1009, Val Loss: 0.3023\n",
      "Epoch [15/50], Train Loss: 0.0997, Val Loss: 0.3000\n",
      "Epoch [16/50], Train Loss: 0.0990, Val Loss: 0.2977\n",
      "Epoch [17/50], Train Loss: 0.0978, Val Loss: 0.2955\n",
      "Epoch [18/50], Train Loss: 0.0971, Val Loss: 0.2933\n",
      "Epoch [19/50], Train Loss: 0.0962, Val Loss: 0.2910\n",
      "Epoch [20/50], Train Loss: 0.0952, Val Loss: 0.2889\n",
      "Epoch [21/50], Train Loss: 0.0939, Val Loss: 0.2867\n",
      "Epoch [22/50], Train Loss: 0.0929, Val Loss: 0.2846\n",
      "Epoch [23/50], Train Loss: 0.0919, Val Loss: 0.2825\n",
      "Epoch [24/50], Train Loss: 0.0909, Val Loss: 0.2805\n",
      "Epoch [25/50], Train Loss: 0.0909, Val Loss: 0.2784\n",
      "Epoch [26/50], Train Loss: 0.0896, Val Loss: 0.2764\n",
      "Epoch [27/50], Train Loss: 0.0881, Val Loss: 0.2744\n",
      "Epoch [28/50], Train Loss: 0.0873, Val Loss: 0.2725\n",
      "Epoch [29/50], Train Loss: 0.0867, Val Loss: 0.2705\n",
      "Epoch [30/50], Train Loss: 0.0858, Val Loss: 0.2686\n",
      "Epoch [31/50], Train Loss: 0.0850, Val Loss: 0.2667\n",
      "Epoch [32/50], Train Loss: 0.0844, Val Loss: 0.2648\n",
      "Epoch [33/50], Train Loss: 0.0830, Val Loss: 0.2630\n",
      "Epoch [34/50], Train Loss: 0.0824, Val Loss: 0.2612\n",
      "Epoch [35/50], Train Loss: 0.0817, Val Loss: 0.2594\n",
      "Epoch [36/50], Train Loss: 0.0811, Val Loss: 0.2576\n",
      "Epoch [37/50], Train Loss: 0.0805, Val Loss: 0.2558\n",
      "Epoch [38/50], Train Loss: 0.0805, Val Loss: 0.2541\n",
      "Epoch [39/50], Train Loss: 0.0786, Val Loss: 0.2523\n",
      "Epoch [40/50], Train Loss: 0.0786, Val Loss: 0.2506\n",
      "Epoch [41/50], Train Loss: 0.0776, Val Loss: 0.2490\n",
      "Epoch [42/50], Train Loss: 0.0767, Val Loss: 0.2473\n",
      "Epoch [43/50], Train Loss: 0.0762, Val Loss: 0.2456\n",
      "Epoch [44/50], Train Loss: 0.0757, Val Loss: 0.2440\n",
      "Epoch [45/50], Train Loss: 0.0746, Val Loss: 0.2424\n",
      "Epoch [46/50], Train Loss: 0.0734, Val Loss: 0.2408\n",
      "Epoch [47/50], Train Loss: 0.0740, Val Loss: 0.2393\n",
      "Epoch [48/50], Train Loss: 0.0728, Val Loss: 0.2377\n",
      "Epoch [49/50], Train Loss: 0.0718, Val Loss: 0.2362\n",
      "Epoch [50/50], Train Loss: 0.0717, Val Loss: 0.2347\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=16, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1435, Val Loss: 0.3045\n",
      "Epoch [2/50], Train Loss: 0.1416, Val Loss: 0.3021\n",
      "Epoch [3/50], Train Loss: 0.1398, Val Loss: 0.2998\n",
      "Epoch [4/50], Train Loss: 0.1381, Val Loss: 0.2975\n",
      "Epoch [5/50], Train Loss: 0.1377, Val Loss: 0.2952\n",
      "Epoch [6/50], Train Loss: 0.1355, Val Loss: 0.2929\n",
      "Epoch [7/50], Train Loss: 0.1327, Val Loss: 0.2907\n",
      "Epoch [8/50], Train Loss: 0.1328, Val Loss: 0.2885\n",
      "Epoch [9/50], Train Loss: 0.1313, Val Loss: 0.2863\n",
      "Epoch [10/50], Train Loss: 0.1308, Val Loss: 0.2842\n",
      "Epoch [11/50], Train Loss: 0.1291, Val Loss: 0.2820\n",
      "Epoch [12/50], Train Loss: 0.1276, Val Loss: 0.2800\n",
      "Epoch [13/50], Train Loss: 0.1258, Val Loss: 0.2779\n",
      "Epoch [14/50], Train Loss: 0.1245, Val Loss: 0.2759\n",
      "Epoch [15/50], Train Loss: 0.1241, Val Loss: 0.2738\n",
      "Epoch [16/50], Train Loss: 0.1210, Val Loss: 0.2719\n",
      "Epoch [17/50], Train Loss: 0.1198, Val Loss: 0.2699\n",
      "Epoch [18/50], Train Loss: 0.1199, Val Loss: 0.2680\n",
      "Epoch [19/50], Train Loss: 0.1179, Val Loss: 0.2661\n",
      "Epoch [20/50], Train Loss: 0.1179, Val Loss: 0.2642\n",
      "Epoch [21/50], Train Loss: 0.1160, Val Loss: 0.2624\n",
      "Epoch [22/50], Train Loss: 0.1137, Val Loss: 0.2606\n",
      "Epoch [23/50], Train Loss: 0.1123, Val Loss: 0.2588\n",
      "Epoch [24/50], Train Loss: 0.1129, Val Loss: 0.2570\n",
      "Epoch [25/50], Train Loss: 0.1121, Val Loss: 0.2553\n",
      "Epoch [26/50], Train Loss: 0.1111, Val Loss: 0.2535\n",
      "Epoch [27/50], Train Loss: 0.1094, Val Loss: 0.2518\n",
      "Epoch [28/50], Train Loss: 0.1087, Val Loss: 0.2501\n",
      "Epoch [29/50], Train Loss: 0.1080, Val Loss: 0.2485\n",
      "Epoch [30/50], Train Loss: 0.1066, Val Loss: 0.2468\n",
      "Epoch [31/50], Train Loss: 0.1047, Val Loss: 0.2452\n",
      "Epoch [32/50], Train Loss: 0.1040, Val Loss: 0.2436\n",
      "Epoch [33/50], Train Loss: 0.1035, Val Loss: 0.2420\n",
      "Epoch [34/50], Train Loss: 0.1027, Val Loss: 0.2405\n",
      "Epoch [35/50], Train Loss: 0.1003, Val Loss: 0.2389\n",
      "Epoch [36/50], Train Loss: 0.1011, Val Loss: 0.2374\n",
      "Epoch [37/50], Train Loss: 0.0986, Val Loss: 0.2359\n",
      "Epoch [38/50], Train Loss: 0.0982, Val Loss: 0.2344\n",
      "Epoch [39/50], Train Loss: 0.0975, Val Loss: 0.2330\n",
      "Epoch [40/50], Train Loss: 0.0957, Val Loss: 0.2316\n",
      "Epoch [41/50], Train Loss: 0.0967, Val Loss: 0.2301\n",
      "Epoch [42/50], Train Loss: 0.0962, Val Loss: 0.2287\n",
      "Epoch [43/50], Train Loss: 0.0943, Val Loss: 0.2273\n",
      "Epoch [44/50], Train Loss: 0.0933, Val Loss: 0.2259\n",
      "Epoch [45/50], Train Loss: 0.0929, Val Loss: 0.2245\n",
      "Epoch [46/50], Train Loss: 0.0923, Val Loss: 0.2232\n",
      "Epoch [47/50], Train Loss: 0.0909, Val Loss: 0.2219\n",
      "Epoch [48/50], Train Loss: 0.0906, Val Loss: 0.2206\n",
      "Epoch [49/50], Train Loss: 0.0901, Val Loss: 0.2193\n",
      "Epoch [50/50], Train Loss: 0.0889, Val Loss: 0.2180\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=16, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1856, Val Loss: 0.4847\n",
      "Epoch [2/50], Train Loss: 0.1835, Val Loss: 0.4809\n",
      "Epoch [3/50], Train Loss: 0.1814, Val Loss: 0.4772\n",
      "Epoch [4/50], Train Loss: 0.1793, Val Loss: 0.4736\n",
      "Epoch [5/50], Train Loss: 0.1773, Val Loss: 0.4699\n",
      "Epoch [6/50], Train Loss: 0.1753, Val Loss: 0.4664\n",
      "Epoch [7/50], Train Loss: 0.1733, Val Loss: 0.4628\n",
      "Epoch [8/50], Train Loss: 0.1713, Val Loss: 0.4593\n",
      "Epoch [9/50], Train Loss: 0.1694, Val Loss: 0.4558\n",
      "Epoch [10/50], Train Loss: 0.1675, Val Loss: 0.4524\n",
      "Epoch [11/50], Train Loss: 0.1656, Val Loss: 0.4490\n",
      "Epoch [12/50], Train Loss: 0.1638, Val Loss: 0.4456\n",
      "Epoch [13/50], Train Loss: 0.1619, Val Loss: 0.4423\n",
      "Epoch [14/50], Train Loss: 0.1601, Val Loss: 0.4390\n",
      "Epoch [15/50], Train Loss: 0.1584, Val Loss: 0.4358\n",
      "Epoch [16/50], Train Loss: 0.1566, Val Loss: 0.4325\n",
      "Epoch [17/50], Train Loss: 0.1549, Val Loss: 0.4294\n",
      "Epoch [18/50], Train Loss: 0.1532, Val Loss: 0.4262\n",
      "Epoch [19/50], Train Loss: 0.1515, Val Loss: 0.4231\n",
      "Epoch [20/50], Train Loss: 0.1499, Val Loss: 0.4200\n",
      "Epoch [21/50], Train Loss: 0.1483, Val Loss: 0.4170\n",
      "Epoch [22/50], Train Loss: 0.1467, Val Loss: 0.4139\n",
      "Epoch [23/50], Train Loss: 0.1451, Val Loss: 0.4110\n",
      "Epoch [24/50], Train Loss: 0.1435, Val Loss: 0.4080\n",
      "Epoch [25/50], Train Loss: 0.1420, Val Loss: 0.4051\n",
      "Epoch [26/50], Train Loss: 0.1405, Val Loss: 0.4022\n",
      "Epoch [27/50], Train Loss: 0.1390, Val Loss: 0.3993\n",
      "Epoch [28/50], Train Loss: 0.1375, Val Loss: 0.3965\n",
      "Epoch [29/50], Train Loss: 0.1361, Val Loss: 0.3937\n",
      "Epoch [30/50], Train Loss: 0.1346, Val Loss: 0.3909\n",
      "Epoch [31/50], Train Loss: 0.1332, Val Loss: 0.3882\n",
      "Epoch [32/50], Train Loss: 0.1318, Val Loss: 0.3855\n",
      "Epoch [33/50], Train Loss: 0.1305, Val Loss: 0.3828\n",
      "Epoch [34/50], Train Loss: 0.1291, Val Loss: 0.3801\n",
      "Epoch [35/50], Train Loss: 0.1278, Val Loss: 0.3775\n",
      "Epoch [36/50], Train Loss: 0.1265, Val Loss: 0.3749\n",
      "Epoch [37/50], Train Loss: 0.1252, Val Loss: 0.3723\n",
      "Epoch [38/50], Train Loss: 0.1239, Val Loss: 0.3698\n",
      "Epoch [39/50], Train Loss: 0.1226, Val Loss: 0.3672\n",
      "Epoch [40/50], Train Loss: 0.1214, Val Loss: 0.3647\n",
      "Epoch [41/50], Train Loss: 0.1202, Val Loss: 0.3623\n",
      "Epoch [42/50], Train Loss: 0.1190, Val Loss: 0.3598\n",
      "Epoch [43/50], Train Loss: 0.1178, Val Loss: 0.3574\n",
      "Epoch [44/50], Train Loss: 0.1166, Val Loss: 0.3550\n",
      "Epoch [45/50], Train Loss: 0.1154, Val Loss: 0.3526\n",
      "Epoch [46/50], Train Loss: 0.1143, Val Loss: 0.3503\n",
      "Epoch [47/50], Train Loss: 0.1132, Val Loss: 0.3480\n",
      "Epoch [48/50], Train Loss: 0.1121, Val Loss: 0.3457\n",
      "Epoch [49/50], Train Loss: 0.1110, Val Loss: 0.3434\n",
      "Epoch [50/50], Train Loss: 0.1099, Val Loss: 0.3412\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=16, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.2485, Val Loss: 0.5460\n",
      "Epoch [2/50], Train Loss: 0.2460, Val Loss: 0.5409\n",
      "Epoch [3/50], Train Loss: 0.2423, Val Loss: 0.5360\n",
      "Epoch [4/50], Train Loss: 0.2395, Val Loss: 0.5311\n",
      "Epoch [5/50], Train Loss: 0.2356, Val Loss: 0.5263\n",
      "Epoch [6/50], Train Loss: 0.2330, Val Loss: 0.5216\n",
      "Epoch [7/50], Train Loss: 0.2300, Val Loss: 0.5169\n",
      "Epoch [8/50], Train Loss: 0.2256, Val Loss: 0.5123\n",
      "Epoch [9/50], Train Loss: 0.2242, Val Loss: 0.5077\n",
      "Epoch [10/50], Train Loss: 0.2217, Val Loss: 0.5033\n",
      "Epoch [11/50], Train Loss: 0.2176, Val Loss: 0.4989\n",
      "Epoch [12/50], Train Loss: 0.2157, Val Loss: 0.4945\n",
      "Epoch [13/50], Train Loss: 0.2129, Val Loss: 0.4902\n",
      "Epoch [14/50], Train Loss: 0.2088, Val Loss: 0.4860\n",
      "Epoch [15/50], Train Loss: 0.2070, Val Loss: 0.4818\n",
      "Epoch [16/50], Train Loss: 0.2046, Val Loss: 0.4776\n",
      "Epoch [17/50], Train Loss: 0.2015, Val Loss: 0.4736\n",
      "Epoch [18/50], Train Loss: 0.1994, Val Loss: 0.4696\n",
      "Epoch [19/50], Train Loss: 0.1968, Val Loss: 0.4656\n",
      "Epoch [20/50], Train Loss: 0.1943, Val Loss: 0.4617\n",
      "Epoch [21/50], Train Loss: 0.1923, Val Loss: 0.4578\n",
      "Epoch [22/50], Train Loss: 0.1896, Val Loss: 0.4540\n",
      "Epoch [23/50], Train Loss: 0.1879, Val Loss: 0.4502\n",
      "Epoch [24/50], Train Loss: 0.1850, Val Loss: 0.4465\n",
      "Epoch [25/50], Train Loss: 0.1826, Val Loss: 0.4428\n",
      "Epoch [26/50], Train Loss: 0.1805, Val Loss: 0.4392\n",
      "Epoch [27/50], Train Loss: 0.1783, Val Loss: 0.4356\n",
      "Epoch [28/50], Train Loss: 0.1760, Val Loss: 0.4321\n",
      "Epoch [29/50], Train Loss: 0.1745, Val Loss: 0.4286\n",
      "Epoch [30/50], Train Loss: 0.1720, Val Loss: 0.4252\n",
      "Epoch [31/50], Train Loss: 0.1699, Val Loss: 0.4218\n",
      "Epoch [32/50], Train Loss: 0.1678, Val Loss: 0.4184\n",
      "Epoch [33/50], Train Loss: 0.1662, Val Loss: 0.4151\n",
      "Epoch [34/50], Train Loss: 0.1640, Val Loss: 0.4119\n",
      "Epoch [35/50], Train Loss: 0.1622, Val Loss: 0.4086\n",
      "Epoch [36/50], Train Loss: 0.1604, Val Loss: 0.4054\n",
      "Epoch [37/50], Train Loss: 0.1587, Val Loss: 0.4023\n",
      "Epoch [38/50], Train Loss: 0.1563, Val Loss: 0.3992\n",
      "Epoch [39/50], Train Loss: 0.1548, Val Loss: 0.3961\n",
      "Epoch [40/50], Train Loss: 0.1535, Val Loss: 0.3930\n",
      "Epoch [41/50], Train Loss: 0.1511, Val Loss: 0.3900\n",
      "Epoch [42/50], Train Loss: 0.1496, Val Loss: 0.3871\n",
      "Epoch [43/50], Train Loss: 0.1482, Val Loss: 0.3841\n",
      "Epoch [44/50], Train Loss: 0.1466, Val Loss: 0.3812\n",
      "Epoch [45/50], Train Loss: 0.1448, Val Loss: 0.3784\n",
      "Epoch [46/50], Train Loss: 0.1427, Val Loss: 0.3756\n",
      "Epoch [47/50], Train Loss: 0.1409, Val Loss: 0.3728\n",
      "Epoch [48/50], Train Loss: 0.1399, Val Loss: 0.3700\n",
      "Epoch [49/50], Train Loss: 0.1385, Val Loss: 0.3673\n",
      "Epoch [50/50], Train Loss: 0.1371, Val Loss: 0.3646\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=16, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1359, Val Loss: 0.3843\n",
      "Epoch [2/50], Train Loss: 0.1345, Val Loss: 0.3818\n",
      "Epoch [3/50], Train Loss: 0.1329, Val Loss: 0.3793\n",
      "Epoch [4/50], Train Loss: 0.1307, Val Loss: 0.3768\n",
      "Epoch [5/50], Train Loss: 0.1305, Val Loss: 0.3744\n",
      "Epoch [6/50], Train Loss: 0.1295, Val Loss: 0.3719\n",
      "Epoch [7/50], Train Loss: 0.1275, Val Loss: 0.3696\n",
      "Epoch [8/50], Train Loss: 0.1267, Val Loss: 0.3672\n",
      "Epoch [9/50], Train Loss: 0.1256, Val Loss: 0.3649\n",
      "Epoch [10/50], Train Loss: 0.1241, Val Loss: 0.3626\n",
      "Epoch [11/50], Train Loss: 0.1222, Val Loss: 0.3603\n",
      "Epoch [12/50], Train Loss: 0.1224, Val Loss: 0.3580\n",
      "Epoch [13/50], Train Loss: 0.1203, Val Loss: 0.3558\n",
      "Epoch [14/50], Train Loss: 0.1191, Val Loss: 0.3536\n",
      "Epoch [15/50], Train Loss: 0.1185, Val Loss: 0.3514\n",
      "Epoch [16/50], Train Loss: 0.1171, Val Loss: 0.3493\n",
      "Epoch [17/50], Train Loss: 0.1156, Val Loss: 0.3472\n",
      "Epoch [18/50], Train Loss: 0.1148, Val Loss: 0.3451\n",
      "Epoch [19/50], Train Loss: 0.1139, Val Loss: 0.3430\n",
      "Epoch [20/50], Train Loss: 0.1134, Val Loss: 0.3409\n",
      "Epoch [21/50], Train Loss: 0.1123, Val Loss: 0.3388\n",
      "Epoch [22/50], Train Loss: 0.1110, Val Loss: 0.3368\n",
      "Epoch [23/50], Train Loss: 0.1100, Val Loss: 0.3348\n",
      "Epoch [24/50], Train Loss: 0.1096, Val Loss: 0.3328\n",
      "Epoch [25/50], Train Loss: 0.1078, Val Loss: 0.3309\n",
      "Epoch [26/50], Train Loss: 0.1065, Val Loss: 0.3290\n",
      "Epoch [27/50], Train Loss: 0.1066, Val Loss: 0.3270\n",
      "Epoch [28/50], Train Loss: 0.1052, Val Loss: 0.3251\n",
      "Epoch [29/50], Train Loss: 0.1044, Val Loss: 0.3233\n",
      "Epoch [30/50], Train Loss: 0.1040, Val Loss: 0.3214\n",
      "Epoch [31/50], Train Loss: 0.1026, Val Loss: 0.3195\n",
      "Epoch [32/50], Train Loss: 0.1018, Val Loss: 0.3177\n",
      "Epoch [33/50], Train Loss: 0.1009, Val Loss: 0.3159\n",
      "Epoch [34/50], Train Loss: 0.1006, Val Loss: 0.3141\n",
      "Epoch [35/50], Train Loss: 0.1004, Val Loss: 0.3123\n",
      "Epoch [36/50], Train Loss: 0.0984, Val Loss: 0.3106\n",
      "Epoch [37/50], Train Loss: 0.0978, Val Loss: 0.3089\n",
      "Epoch [38/50], Train Loss: 0.0977, Val Loss: 0.3071\n",
      "Epoch [39/50], Train Loss: 0.0965, Val Loss: 0.3054\n",
      "Epoch [40/50], Train Loss: 0.0951, Val Loss: 0.3038\n",
      "Epoch [41/50], Train Loss: 0.0942, Val Loss: 0.3021\n",
      "Epoch [42/50], Train Loss: 0.0940, Val Loss: 0.3005\n",
      "Epoch [43/50], Train Loss: 0.0926, Val Loss: 0.2989\n",
      "Epoch [44/50], Train Loss: 0.0926, Val Loss: 0.2973\n",
      "Epoch [45/50], Train Loss: 0.0921, Val Loss: 0.2957\n",
      "Epoch [46/50], Train Loss: 0.0916, Val Loss: 0.2941\n",
      "Epoch [47/50], Train Loss: 0.0915, Val Loss: 0.2925\n",
      "Epoch [48/50], Train Loss: 0.0893, Val Loss: 0.2909\n",
      "Epoch [49/50], Train Loss: 0.0893, Val Loss: 0.2894\n",
      "Epoch [50/50], Train Loss: 0.0882, Val Loss: 0.2879\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=16, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1359, Val Loss: 0.3681\n",
      "Epoch [2/50], Train Loss: 0.1342, Val Loss: 0.3651\n",
      "Epoch [3/50], Train Loss: 0.1326, Val Loss: 0.3621\n",
      "Epoch [4/50], Train Loss: 0.1310, Val Loss: 0.3592\n",
      "Epoch [5/50], Train Loss: 0.1294, Val Loss: 0.3563\n",
      "Epoch [6/50], Train Loss: 0.1279, Val Loss: 0.3534\n",
      "Epoch [7/50], Train Loss: 0.1263, Val Loss: 0.3505\n",
      "Epoch [8/50], Train Loss: 0.1248, Val Loss: 0.3477\n",
      "Epoch [9/50], Train Loss: 0.1234, Val Loss: 0.3450\n",
      "Epoch [10/50], Train Loss: 0.1219, Val Loss: 0.3422\n",
      "Epoch [11/50], Train Loss: 0.1205, Val Loss: 0.3395\n",
      "Epoch [12/50], Train Loss: 0.1191, Val Loss: 0.3369\n",
      "Epoch [13/50], Train Loss: 0.1177, Val Loss: 0.3342\n",
      "Epoch [14/50], Train Loss: 0.1164, Val Loss: 0.3316\n",
      "Epoch [15/50], Train Loss: 0.1150, Val Loss: 0.3291\n",
      "Epoch [16/50], Train Loss: 0.1137, Val Loss: 0.3265\n",
      "Epoch [17/50], Train Loss: 0.1124, Val Loss: 0.3240\n",
      "Epoch [18/50], Train Loss: 0.1112, Val Loss: 0.3216\n",
      "Epoch [19/50], Train Loss: 0.1099, Val Loss: 0.3191\n",
      "Epoch [20/50], Train Loss: 0.1087, Val Loss: 0.3167\n",
      "Epoch [21/50], Train Loss: 0.1075, Val Loss: 0.3144\n",
      "Epoch [22/50], Train Loss: 0.1063, Val Loss: 0.3120\n",
      "Epoch [23/50], Train Loss: 0.1052, Val Loss: 0.3097\n",
      "Epoch [24/50], Train Loss: 0.1040, Val Loss: 0.3074\n",
      "Epoch [25/50], Train Loss: 0.1029, Val Loss: 0.3051\n",
      "Epoch [26/50], Train Loss: 0.1018, Val Loss: 0.3029\n",
      "Epoch [27/50], Train Loss: 0.1007, Val Loss: 0.3007\n",
      "Epoch [28/50], Train Loss: 0.0996, Val Loss: 0.2985\n",
      "Epoch [29/50], Train Loss: 0.0986, Val Loss: 0.2964\n",
      "Epoch [30/50], Train Loss: 0.0975, Val Loss: 0.2943\n",
      "Epoch [31/50], Train Loss: 0.0965, Val Loss: 0.2922\n",
      "Epoch [32/50], Train Loss: 0.0955, Val Loss: 0.2901\n",
      "Epoch [33/50], Train Loss: 0.0945, Val Loss: 0.2880\n",
      "Epoch [34/50], Train Loss: 0.0936, Val Loss: 0.2860\n",
      "Epoch [35/50], Train Loss: 0.0926, Val Loss: 0.2840\n",
      "Epoch [36/50], Train Loss: 0.0917, Val Loss: 0.2821\n",
      "Epoch [37/50], Train Loss: 0.0908, Val Loss: 0.2801\n",
      "Epoch [38/50], Train Loss: 0.0899, Val Loss: 0.2782\n",
      "Epoch [39/50], Train Loss: 0.0890, Val Loss: 0.2763\n",
      "Epoch [40/50], Train Loss: 0.0881, Val Loss: 0.2744\n",
      "Epoch [41/50], Train Loss: 0.0872, Val Loss: 0.2726\n",
      "Epoch [42/50], Train Loss: 0.0864, Val Loss: 0.2707\n",
      "Epoch [43/50], Train Loss: 0.0856, Val Loss: 0.2689\n",
      "Epoch [44/50], Train Loss: 0.0847, Val Loss: 0.2671\n",
      "Epoch [45/50], Train Loss: 0.0839, Val Loss: 0.2654\n",
      "Epoch [46/50], Train Loss: 0.0832, Val Loss: 0.2636\n",
      "Epoch [47/50], Train Loss: 0.0824, Val Loss: 0.2619\n",
      "Epoch [48/50], Train Loss: 0.0816, Val Loss: 0.2602\n",
      "Epoch [49/50], Train Loss: 0.0809, Val Loss: 0.2585\n",
      "Epoch [50/50], Train Loss: 0.0801, Val Loss: 0.2569\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=16, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0874, Val Loss: 0.2517\n",
      "Epoch [2/50], Train Loss: 0.0861, Val Loss: 0.2501\n",
      "Epoch [3/50], Train Loss: 0.0860, Val Loss: 0.2484\n",
      "Epoch [4/50], Train Loss: 0.0859, Val Loss: 0.2468\n",
      "Epoch [5/50], Train Loss: 0.0846, Val Loss: 0.2452\n",
      "Epoch [6/50], Train Loss: 0.0834, Val Loss: 0.2436\n",
      "Epoch [7/50], Train Loss: 0.0826, Val Loss: 0.2421\n",
      "Epoch [8/50], Train Loss: 0.0824, Val Loss: 0.2405\n",
      "Epoch [9/50], Train Loss: 0.0810, Val Loss: 0.2390\n",
      "Epoch [10/50], Train Loss: 0.0810, Val Loss: 0.2375\n",
      "Epoch [11/50], Train Loss: 0.0803, Val Loss: 0.2361\n",
      "Epoch [12/50], Train Loss: 0.0786, Val Loss: 0.2346\n",
      "Epoch [13/50], Train Loss: 0.0785, Val Loss: 0.2332\n",
      "Epoch [14/50], Train Loss: 0.0774, Val Loss: 0.2318\n",
      "Epoch [15/50], Train Loss: 0.0765, Val Loss: 0.2304\n",
      "Epoch [16/50], Train Loss: 0.0764, Val Loss: 0.2290\n",
      "Epoch [17/50], Train Loss: 0.0758, Val Loss: 0.2276\n",
      "Epoch [18/50], Train Loss: 0.0757, Val Loss: 0.2263\n",
      "Epoch [19/50], Train Loss: 0.0746, Val Loss: 0.2249\n",
      "Epoch [20/50], Train Loss: 0.0741, Val Loss: 0.2236\n",
      "Epoch [21/50], Train Loss: 0.0728, Val Loss: 0.2223\n",
      "Epoch [22/50], Train Loss: 0.0738, Val Loss: 0.2210\n",
      "Epoch [23/50], Train Loss: 0.0719, Val Loss: 0.2197\n",
      "Epoch [24/50], Train Loss: 0.0717, Val Loss: 0.2185\n",
      "Epoch [25/50], Train Loss: 0.0708, Val Loss: 0.2173\n",
      "Epoch [26/50], Train Loss: 0.0706, Val Loss: 0.2160\n",
      "Epoch [27/50], Train Loss: 0.0697, Val Loss: 0.2148\n",
      "Epoch [28/50], Train Loss: 0.0701, Val Loss: 0.2136\n",
      "Epoch [29/50], Train Loss: 0.0687, Val Loss: 0.2125\n",
      "Epoch [30/50], Train Loss: 0.0690, Val Loss: 0.2113\n",
      "Epoch [31/50], Train Loss: 0.0682, Val Loss: 0.2102\n",
      "Epoch [32/50], Train Loss: 0.0676, Val Loss: 0.2090\n",
      "Epoch [33/50], Train Loss: 0.0671, Val Loss: 0.2079\n",
      "Epoch [34/50], Train Loss: 0.0663, Val Loss: 0.2068\n",
      "Epoch [35/50], Train Loss: 0.0665, Val Loss: 0.2057\n",
      "Epoch [36/50], Train Loss: 0.0659, Val Loss: 0.2047\n",
      "Epoch [37/50], Train Loss: 0.0654, Val Loss: 0.2036\n",
      "Epoch [38/50], Train Loss: 0.0648, Val Loss: 0.2025\n",
      "Epoch [39/50], Train Loss: 0.0641, Val Loss: 0.2015\n",
      "Epoch [40/50], Train Loss: 0.0647, Val Loss: 0.2005\n",
      "Epoch [41/50], Train Loss: 0.0636, Val Loss: 0.1995\n",
      "Epoch [42/50], Train Loss: 0.0638, Val Loss: 0.1985\n",
      "Epoch [43/50], Train Loss: 0.0629, Val Loss: 0.1975\n",
      "Epoch [44/50], Train Loss: 0.0617, Val Loss: 0.1965\n",
      "Epoch [45/50], Train Loss: 0.0628, Val Loss: 0.1955\n",
      "Epoch [46/50], Train Loss: 0.0615, Val Loss: 0.1946\n",
      "Epoch [47/50], Train Loss: 0.0612, Val Loss: 0.1936\n",
      "Epoch [48/50], Train Loss: 0.0609, Val Loss: 0.1927\n",
      "Epoch [49/50], Train Loss: 0.0603, Val Loss: 0.1918\n",
      "Epoch [50/50], Train Loss: 0.0601, Val Loss: 0.1909\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=16, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1428, Val Loss: 0.3635\n",
      "Epoch [2/50], Train Loss: 0.1416, Val Loss: 0.3608\n",
      "Epoch [3/50], Train Loss: 0.1399, Val Loss: 0.3582\n",
      "Epoch [4/50], Train Loss: 0.1378, Val Loss: 0.3556\n",
      "Epoch [5/50], Train Loss: 0.1375, Val Loss: 0.3530\n",
      "Epoch [6/50], Train Loss: 0.1354, Val Loss: 0.3505\n",
      "Epoch [7/50], Train Loss: 0.1344, Val Loss: 0.3480\n",
      "Epoch [8/50], Train Loss: 0.1331, Val Loss: 0.3455\n",
      "Epoch [9/50], Train Loss: 0.1314, Val Loss: 0.3431\n",
      "Epoch [10/50], Train Loss: 0.1308, Val Loss: 0.3406\n",
      "Epoch [11/50], Train Loss: 0.1293, Val Loss: 0.3383\n",
      "Epoch [12/50], Train Loss: 0.1269, Val Loss: 0.3359\n",
      "Epoch [13/50], Train Loss: 0.1260, Val Loss: 0.3336\n",
      "Epoch [14/50], Train Loss: 0.1253, Val Loss: 0.3313\n",
      "Epoch [15/50], Train Loss: 0.1237, Val Loss: 0.3290\n",
      "Epoch [16/50], Train Loss: 0.1220, Val Loss: 0.3267\n",
      "Epoch [17/50], Train Loss: 0.1205, Val Loss: 0.3245\n",
      "Epoch [18/50], Train Loss: 0.1199, Val Loss: 0.3223\n",
      "Epoch [19/50], Train Loss: 0.1186, Val Loss: 0.3201\n",
      "Epoch [20/50], Train Loss: 0.1182, Val Loss: 0.3180\n",
      "Epoch [21/50], Train Loss: 0.1166, Val Loss: 0.3159\n",
      "Epoch [22/50], Train Loss: 0.1156, Val Loss: 0.3138\n",
      "Epoch [23/50], Train Loss: 0.1152, Val Loss: 0.3117\n",
      "Epoch [24/50], Train Loss: 0.1129, Val Loss: 0.3097\n",
      "Epoch [25/50], Train Loss: 0.1108, Val Loss: 0.3077\n",
      "Epoch [26/50], Train Loss: 0.1103, Val Loss: 0.3057\n",
      "Epoch [27/50], Train Loss: 0.1088, Val Loss: 0.3037\n",
      "Epoch [28/50], Train Loss: 0.1090, Val Loss: 0.3018\n",
      "Epoch [29/50], Train Loss: 0.1076, Val Loss: 0.2998\n",
      "Epoch [30/50], Train Loss: 0.1066, Val Loss: 0.2979\n",
      "Epoch [31/50], Train Loss: 0.1060, Val Loss: 0.2960\n",
      "Epoch [32/50], Train Loss: 0.1051, Val Loss: 0.2942\n",
      "Epoch [33/50], Train Loss: 0.1031, Val Loss: 0.2923\n",
      "Epoch [34/50], Train Loss: 0.1032, Val Loss: 0.2905\n",
      "Epoch [35/50], Train Loss: 0.1026, Val Loss: 0.2887\n",
      "Epoch [36/50], Train Loss: 0.1009, Val Loss: 0.2870\n",
      "Epoch [37/50], Train Loss: 0.0995, Val Loss: 0.2852\n",
      "Epoch [38/50], Train Loss: 0.0994, Val Loss: 0.2835\n",
      "Epoch [39/50], Train Loss: 0.0982, Val Loss: 0.2818\n",
      "Epoch [40/50], Train Loss: 0.0983, Val Loss: 0.2801\n",
      "Epoch [41/50], Train Loss: 0.0966, Val Loss: 0.2784\n",
      "Epoch [42/50], Train Loss: 0.0956, Val Loss: 0.2767\n",
      "Epoch [43/50], Train Loss: 0.0951, Val Loss: 0.2751\n",
      "Epoch [44/50], Train Loss: 0.0944, Val Loss: 0.2734\n",
      "Epoch [45/50], Train Loss: 0.0935, Val Loss: 0.2718\n",
      "Epoch [46/50], Train Loss: 0.0926, Val Loss: 0.2702\n",
      "Epoch [47/50], Train Loss: 0.0921, Val Loss: 0.2687\n",
      "Epoch [48/50], Train Loss: 0.0920, Val Loss: 0.2671\n",
      "Epoch [49/50], Train Loss: 0.0905, Val Loss: 0.2656\n",
      "Epoch [50/50], Train Loss: 0.0895, Val Loss: 0.2641\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=32, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1712, Val Loss: 0.4111\n",
      "Epoch [2/50], Train Loss: 0.1692, Val Loss: 0.4077\n",
      "Epoch [3/50], Train Loss: 0.1671, Val Loss: 0.4042\n",
      "Epoch [4/50], Train Loss: 0.1651, Val Loss: 0.4009\n",
      "Epoch [5/50], Train Loss: 0.1631, Val Loss: 0.3975\n",
      "Epoch [6/50], Train Loss: 0.1612, Val Loss: 0.3942\n",
      "Epoch [7/50], Train Loss: 0.1592, Val Loss: 0.3909\n",
      "Epoch [8/50], Train Loss: 0.1573, Val Loss: 0.3877\n",
      "Epoch [9/50], Train Loss: 0.1555, Val Loss: 0.3845\n",
      "Epoch [10/50], Train Loss: 0.1536, Val Loss: 0.3814\n",
      "Epoch [11/50], Train Loss: 0.1518, Val Loss: 0.3783\n",
      "Epoch [12/50], Train Loss: 0.1500, Val Loss: 0.3752\n",
      "Epoch [13/50], Train Loss: 0.1483, Val Loss: 0.3722\n",
      "Epoch [14/50], Train Loss: 0.1466, Val Loss: 0.3692\n",
      "Epoch [15/50], Train Loss: 0.1449, Val Loss: 0.3662\n",
      "Epoch [16/50], Train Loss: 0.1432, Val Loss: 0.3633\n",
      "Epoch [17/50], Train Loss: 0.1415, Val Loss: 0.3604\n",
      "Epoch [18/50], Train Loss: 0.1399, Val Loss: 0.3576\n",
      "Epoch [19/50], Train Loss: 0.1383, Val Loss: 0.3547\n",
      "Epoch [20/50], Train Loss: 0.1367, Val Loss: 0.3520\n",
      "Epoch [21/50], Train Loss: 0.1352, Val Loss: 0.3492\n",
      "Epoch [22/50], Train Loss: 0.1336, Val Loss: 0.3465\n",
      "Epoch [23/50], Train Loss: 0.1321, Val Loss: 0.3438\n",
      "Epoch [24/50], Train Loss: 0.1307, Val Loss: 0.3411\n",
      "Epoch [25/50], Train Loss: 0.1292, Val Loss: 0.3385\n",
      "Epoch [26/50], Train Loss: 0.1277, Val Loss: 0.3359\n",
      "Epoch [27/50], Train Loss: 0.1263, Val Loss: 0.3334\n",
      "Epoch [28/50], Train Loss: 0.1249, Val Loss: 0.3309\n",
      "Epoch [29/50], Train Loss: 0.1236, Val Loss: 0.3284\n",
      "Epoch [30/50], Train Loss: 0.1222, Val Loss: 0.3259\n",
      "Epoch [31/50], Train Loss: 0.1209, Val Loss: 0.3234\n",
      "Epoch [32/50], Train Loss: 0.1196, Val Loss: 0.3210\n",
      "Epoch [33/50], Train Loss: 0.1183, Val Loss: 0.3186\n",
      "Epoch [34/50], Train Loss: 0.1170, Val Loss: 0.3163\n",
      "Epoch [35/50], Train Loss: 0.1157, Val Loss: 0.3139\n",
      "Epoch [36/50], Train Loss: 0.1145, Val Loss: 0.3116\n",
      "Epoch [37/50], Train Loss: 0.1133, Val Loss: 0.3094\n",
      "Epoch [38/50], Train Loss: 0.1121, Val Loss: 0.3071\n",
      "Epoch [39/50], Train Loss: 0.1109, Val Loss: 0.3049\n",
      "Epoch [40/50], Train Loss: 0.1097, Val Loss: 0.3027\n",
      "Epoch [41/50], Train Loss: 0.1086, Val Loss: 0.3005\n",
      "Epoch [42/50], Train Loss: 0.1074, Val Loss: 0.2984\n",
      "Epoch [43/50], Train Loss: 0.1063, Val Loss: 0.2963\n",
      "Epoch [44/50], Train Loss: 0.1052, Val Loss: 0.2942\n",
      "Epoch [45/50], Train Loss: 0.1042, Val Loss: 0.2921\n",
      "Epoch [46/50], Train Loss: 0.1031, Val Loss: 0.2900\n",
      "Epoch [47/50], Train Loss: 0.1020, Val Loss: 0.2880\n",
      "Epoch [48/50], Train Loss: 0.1010, Val Loss: 0.2860\n",
      "Epoch [49/50], Train Loss: 0.1000, Val Loss: 0.2840\n",
      "Epoch [50/50], Train Loss: 0.0990, Val Loss: 0.2821\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=32, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0976, Val Loss: 0.2705\n",
      "Epoch [2/50], Train Loss: 0.0961, Val Loss: 0.2687\n",
      "Epoch [3/50], Train Loss: 0.0952, Val Loss: 0.2669\n",
      "Epoch [4/50], Train Loss: 0.0940, Val Loss: 0.2651\n",
      "Epoch [5/50], Train Loss: 0.0936, Val Loss: 0.2634\n",
      "Epoch [6/50], Train Loss: 0.0925, Val Loss: 0.2616\n",
      "Epoch [7/50], Train Loss: 0.0918, Val Loss: 0.2599\n",
      "Epoch [8/50], Train Loss: 0.0904, Val Loss: 0.2582\n",
      "Epoch [9/50], Train Loss: 0.0899, Val Loss: 0.2565\n",
      "Epoch [10/50], Train Loss: 0.0892, Val Loss: 0.2549\n",
      "Epoch [11/50], Train Loss: 0.0884, Val Loss: 0.2532\n",
      "Epoch [12/50], Train Loss: 0.0876, Val Loss: 0.2516\n",
      "Epoch [13/50], Train Loss: 0.0866, Val Loss: 0.2500\n",
      "Epoch [14/50], Train Loss: 0.0856, Val Loss: 0.2484\n",
      "Epoch [15/50], Train Loss: 0.0849, Val Loss: 0.2469\n",
      "Epoch [16/50], Train Loss: 0.0849, Val Loss: 0.2453\n",
      "Epoch [17/50], Train Loss: 0.0836, Val Loss: 0.2438\n",
      "Epoch [18/50], Train Loss: 0.0829, Val Loss: 0.2423\n",
      "Epoch [19/50], Train Loss: 0.0826, Val Loss: 0.2408\n",
      "Epoch [20/50], Train Loss: 0.0813, Val Loss: 0.2393\n",
      "Epoch [21/50], Train Loss: 0.0807, Val Loss: 0.2378\n",
      "Epoch [22/50], Train Loss: 0.0797, Val Loss: 0.2364\n",
      "Epoch [23/50], Train Loss: 0.0791, Val Loss: 0.2350\n",
      "Epoch [24/50], Train Loss: 0.0787, Val Loss: 0.2336\n",
      "Epoch [25/50], Train Loss: 0.0776, Val Loss: 0.2322\n",
      "Epoch [26/50], Train Loss: 0.0771, Val Loss: 0.2308\n",
      "Epoch [27/50], Train Loss: 0.0765, Val Loss: 0.2295\n",
      "Epoch [28/50], Train Loss: 0.0760, Val Loss: 0.2281\n",
      "Epoch [29/50], Train Loss: 0.0757, Val Loss: 0.2268\n",
      "Epoch [30/50], Train Loss: 0.0748, Val Loss: 0.2255\n",
      "Epoch [31/50], Train Loss: 0.0743, Val Loss: 0.2242\n",
      "Epoch [32/50], Train Loss: 0.0734, Val Loss: 0.2229\n",
      "Epoch [33/50], Train Loss: 0.0733, Val Loss: 0.2216\n",
      "Epoch [34/50], Train Loss: 0.0723, Val Loss: 0.2204\n",
      "Epoch [35/50], Train Loss: 0.0719, Val Loss: 0.2191\n",
      "Epoch [36/50], Train Loss: 0.0714, Val Loss: 0.2179\n",
      "Epoch [37/50], Train Loss: 0.0709, Val Loss: 0.2167\n",
      "Epoch [38/50], Train Loss: 0.0699, Val Loss: 0.2155\n",
      "Epoch [39/50], Train Loss: 0.0699, Val Loss: 0.2143\n",
      "Epoch [40/50], Train Loss: 0.0689, Val Loss: 0.2131\n",
      "Epoch [41/50], Train Loss: 0.0684, Val Loss: 0.2120\n",
      "Epoch [42/50], Train Loss: 0.0681, Val Loss: 0.2108\n",
      "Epoch [43/50], Train Loss: 0.0675, Val Loss: 0.2097\n",
      "Epoch [44/50], Train Loss: 0.0672, Val Loss: 0.2086\n",
      "Epoch [45/50], Train Loss: 0.0670, Val Loss: 0.2075\n",
      "Epoch [46/50], Train Loss: 0.0663, Val Loss: 0.2064\n",
      "Epoch [47/50], Train Loss: 0.0660, Val Loss: 0.2053\n",
      "Epoch [48/50], Train Loss: 0.0654, Val Loss: 0.2042\n",
      "Epoch [49/50], Train Loss: 0.0647, Val Loss: 0.2031\n",
      "Epoch [50/50], Train Loss: 0.0644, Val Loss: 0.2021\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=32, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1235, Val Loss: 0.3333\n",
      "Epoch [2/50], Train Loss: 0.1226, Val Loss: 0.3305\n",
      "Epoch [3/50], Train Loss: 0.1208, Val Loss: 0.3277\n",
      "Epoch [4/50], Train Loss: 0.1202, Val Loss: 0.3249\n",
      "Epoch [5/50], Train Loss: 0.1200, Val Loss: 0.3222\n",
      "Epoch [6/50], Train Loss: 0.1167, Val Loss: 0.3195\n",
      "Epoch [7/50], Train Loss: 0.1161, Val Loss: 0.3169\n",
      "Epoch [8/50], Train Loss: 0.1137, Val Loss: 0.3143\n",
      "Epoch [9/50], Train Loss: 0.1126, Val Loss: 0.3117\n",
      "Epoch [10/50], Train Loss: 0.1125, Val Loss: 0.3092\n",
      "Epoch [11/50], Train Loss: 0.1090, Val Loss: 0.3067\n",
      "Epoch [12/50], Train Loss: 0.1092, Val Loss: 0.3042\n",
      "Epoch [13/50], Train Loss: 0.1090, Val Loss: 0.3017\n",
      "Epoch [14/50], Train Loss: 0.1070, Val Loss: 0.2993\n",
      "Epoch [15/50], Train Loss: 0.1061, Val Loss: 0.2970\n",
      "Epoch [16/50], Train Loss: 0.1047, Val Loss: 0.2946\n",
      "Epoch [17/50], Train Loss: 0.1028, Val Loss: 0.2923\n",
      "Epoch [18/50], Train Loss: 0.1029, Val Loss: 0.2900\n",
      "Epoch [19/50], Train Loss: 0.1006, Val Loss: 0.2878\n",
      "Epoch [20/50], Train Loss: 0.1003, Val Loss: 0.2855\n",
      "Epoch [21/50], Train Loss: 0.0991, Val Loss: 0.2834\n",
      "Epoch [22/50], Train Loss: 0.0988, Val Loss: 0.2812\n",
      "Epoch [23/50], Train Loss: 0.0974, Val Loss: 0.2790\n",
      "Epoch [24/50], Train Loss: 0.0967, Val Loss: 0.2769\n",
      "Epoch [25/50], Train Loss: 0.0949, Val Loss: 0.2748\n",
      "Epoch [26/50], Train Loss: 0.0947, Val Loss: 0.2728\n",
      "Epoch [27/50], Train Loss: 0.0933, Val Loss: 0.2707\n",
      "Epoch [28/50], Train Loss: 0.0914, Val Loss: 0.2687\n",
      "Epoch [29/50], Train Loss: 0.0927, Val Loss: 0.2667\n",
      "Epoch [30/50], Train Loss: 0.0905, Val Loss: 0.2648\n",
      "Epoch [31/50], Train Loss: 0.0900, Val Loss: 0.2628\n",
      "Epoch [32/50], Train Loss: 0.0895, Val Loss: 0.2609\n",
      "Epoch [33/50], Train Loss: 0.0879, Val Loss: 0.2590\n",
      "Epoch [34/50], Train Loss: 0.0871, Val Loss: 0.2571\n",
      "Epoch [35/50], Train Loss: 0.0858, Val Loss: 0.2553\n",
      "Epoch [36/50], Train Loss: 0.0848, Val Loss: 0.2535\n",
      "Epoch [37/50], Train Loss: 0.0854, Val Loss: 0.2517\n",
      "Epoch [38/50], Train Loss: 0.0845, Val Loss: 0.2499\n",
      "Epoch [39/50], Train Loss: 0.0835, Val Loss: 0.2481\n",
      "Epoch [40/50], Train Loss: 0.0827, Val Loss: 0.2464\n",
      "Epoch [41/50], Train Loss: 0.0815, Val Loss: 0.2447\n",
      "Epoch [42/50], Train Loss: 0.0808, Val Loss: 0.2430\n",
      "Epoch [43/50], Train Loss: 0.0796, Val Loss: 0.2413\n",
      "Epoch [44/50], Train Loss: 0.0793, Val Loss: 0.2397\n",
      "Epoch [45/50], Train Loss: 0.0782, Val Loss: 0.2380\n",
      "Epoch [46/50], Train Loss: 0.0784, Val Loss: 0.2364\n",
      "Epoch [47/50], Train Loss: 0.0772, Val Loss: 0.2348\n",
      "Epoch [48/50], Train Loss: 0.0761, Val Loss: 0.2332\n",
      "Epoch [49/50], Train Loss: 0.0765, Val Loss: 0.2317\n",
      "Epoch [50/50], Train Loss: 0.0766, Val Loss: 0.2301\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=32, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1487, Val Loss: 0.3898\n",
      "Epoch [2/50], Train Loss: 0.1469, Val Loss: 0.3866\n",
      "Epoch [3/50], Train Loss: 0.1451, Val Loss: 0.3835\n",
      "Epoch [4/50], Train Loss: 0.1433, Val Loss: 0.3804\n",
      "Epoch [5/50], Train Loss: 0.1416, Val Loss: 0.3773\n",
      "Epoch [6/50], Train Loss: 0.1399, Val Loss: 0.3743\n",
      "Epoch [7/50], Train Loss: 0.1382, Val Loss: 0.3713\n",
      "Epoch [8/50], Train Loss: 0.1366, Val Loss: 0.3684\n",
      "Epoch [9/50], Train Loss: 0.1349, Val Loss: 0.3655\n",
      "Epoch [10/50], Train Loss: 0.1334, Val Loss: 0.3626\n",
      "Epoch [11/50], Train Loss: 0.1318, Val Loss: 0.3598\n",
      "Epoch [12/50], Train Loss: 0.1302, Val Loss: 0.3570\n",
      "Epoch [13/50], Train Loss: 0.1287, Val Loss: 0.3543\n",
      "Epoch [14/50], Train Loss: 0.1272, Val Loss: 0.3515\n",
      "Epoch [15/50], Train Loss: 0.1258, Val Loss: 0.3489\n",
      "Epoch [16/50], Train Loss: 0.1243, Val Loss: 0.3462\n",
      "Epoch [17/50], Train Loss: 0.1229, Val Loss: 0.3436\n",
      "Epoch [18/50], Train Loss: 0.1215, Val Loss: 0.3410\n",
      "Epoch [19/50], Train Loss: 0.1202, Val Loss: 0.3384\n",
      "Epoch [20/50], Train Loss: 0.1188, Val Loss: 0.3359\n",
      "Epoch [21/50], Train Loss: 0.1175, Val Loss: 0.3334\n",
      "Epoch [22/50], Train Loss: 0.1162, Val Loss: 0.3310\n",
      "Epoch [23/50], Train Loss: 0.1149, Val Loss: 0.3285\n",
      "Epoch [24/50], Train Loss: 0.1136, Val Loss: 0.3261\n",
      "Epoch [25/50], Train Loss: 0.1124, Val Loss: 0.3238\n",
      "Epoch [26/50], Train Loss: 0.1112, Val Loss: 0.3214\n",
      "Epoch [27/50], Train Loss: 0.1100, Val Loss: 0.3191\n",
      "Epoch [28/50], Train Loss: 0.1088, Val Loss: 0.3168\n",
      "Epoch [29/50], Train Loss: 0.1076, Val Loss: 0.3146\n",
      "Epoch [30/50], Train Loss: 0.1065, Val Loss: 0.3124\n",
      "Epoch [31/50], Train Loss: 0.1053, Val Loss: 0.3101\n",
      "Epoch [32/50], Train Loss: 0.1042, Val Loss: 0.3080\n",
      "Epoch [33/50], Train Loss: 0.1032, Val Loss: 0.3058\n",
      "Epoch [34/50], Train Loss: 0.1021, Val Loss: 0.3037\n",
      "Epoch [35/50], Train Loss: 0.1010, Val Loss: 0.3016\n",
      "Epoch [36/50], Train Loss: 0.1000, Val Loss: 0.2995\n",
      "Epoch [37/50], Train Loss: 0.0990, Val Loss: 0.2975\n",
      "Epoch [38/50], Train Loss: 0.0980, Val Loss: 0.2955\n",
      "Epoch [39/50], Train Loss: 0.0970, Val Loss: 0.2935\n",
      "Epoch [40/50], Train Loss: 0.0960, Val Loss: 0.2915\n",
      "Epoch [41/50], Train Loss: 0.0951, Val Loss: 0.2895\n",
      "Epoch [42/50], Train Loss: 0.0941, Val Loss: 0.2876\n",
      "Epoch [43/50], Train Loss: 0.0932, Val Loss: 0.2857\n",
      "Epoch [44/50], Train Loss: 0.0923, Val Loss: 0.2838\n",
      "Epoch [45/50], Train Loss: 0.0914, Val Loss: 0.2820\n",
      "Epoch [46/50], Train Loss: 0.0905, Val Loss: 0.2801\n",
      "Epoch [47/50], Train Loss: 0.0896, Val Loss: 0.2783\n",
      "Epoch [48/50], Train Loss: 0.0888, Val Loss: 0.2765\n",
      "Epoch [49/50], Train Loss: 0.0879, Val Loss: 0.2747\n",
      "Epoch [50/50], Train Loss: 0.0871, Val Loss: 0.2730\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=32, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.2105, Val Loss: 0.4937\n",
      "Epoch [2/50], Train Loss: 0.2079, Val Loss: 0.4891\n",
      "Epoch [3/50], Train Loss: 0.2048, Val Loss: 0.4846\n",
      "Epoch [4/50], Train Loss: 0.2025, Val Loss: 0.4802\n",
      "Epoch [5/50], Train Loss: 0.1991, Val Loss: 0.4758\n",
      "Epoch [6/50], Train Loss: 0.1972, Val Loss: 0.4715\n",
      "Epoch [7/50], Train Loss: 0.1945, Val Loss: 0.4672\n",
      "Epoch [8/50], Train Loss: 0.1910, Val Loss: 0.4631\n",
      "Epoch [9/50], Train Loss: 0.1890, Val Loss: 0.4590\n",
      "Epoch [10/50], Train Loss: 0.1866, Val Loss: 0.4549\n",
      "Epoch [11/50], Train Loss: 0.1845, Val Loss: 0.4509\n",
      "Epoch [12/50], Train Loss: 0.1816, Val Loss: 0.4470\n",
      "Epoch [13/50], Train Loss: 0.1794, Val Loss: 0.4431\n",
      "Epoch [14/50], Train Loss: 0.1767, Val Loss: 0.4392\n",
      "Epoch [15/50], Train Loss: 0.1746, Val Loss: 0.4355\n",
      "Epoch [16/50], Train Loss: 0.1723, Val Loss: 0.4318\n",
      "Epoch [17/50], Train Loss: 0.1704, Val Loss: 0.4281\n",
      "Epoch [18/50], Train Loss: 0.1672, Val Loss: 0.4245\n",
      "Epoch [19/50], Train Loss: 0.1659, Val Loss: 0.4209\n",
      "Epoch [20/50], Train Loss: 0.1639, Val Loss: 0.4174\n",
      "Epoch [21/50], Train Loss: 0.1622, Val Loss: 0.4139\n",
      "Epoch [22/50], Train Loss: 0.1607, Val Loss: 0.4104\n",
      "Epoch [23/50], Train Loss: 0.1576, Val Loss: 0.4071\n",
      "Epoch [24/50], Train Loss: 0.1563, Val Loss: 0.4037\n",
      "Epoch [25/50], Train Loss: 0.1540, Val Loss: 0.4004\n",
      "Epoch [26/50], Train Loss: 0.1526, Val Loss: 0.3972\n",
      "Epoch [27/50], Train Loss: 0.1501, Val Loss: 0.3940\n",
      "Epoch [28/50], Train Loss: 0.1486, Val Loss: 0.3908\n",
      "Epoch [29/50], Train Loss: 0.1469, Val Loss: 0.3877\n",
      "Epoch [30/50], Train Loss: 0.1447, Val Loss: 0.3846\n",
      "Epoch [31/50], Train Loss: 0.1433, Val Loss: 0.3815\n",
      "Epoch [32/50], Train Loss: 0.1417, Val Loss: 0.3785\n",
      "Epoch [33/50], Train Loss: 0.1401, Val Loss: 0.3756\n",
      "Epoch [34/50], Train Loss: 0.1384, Val Loss: 0.3727\n",
      "Epoch [35/50], Train Loss: 0.1366, Val Loss: 0.3698\n",
      "Epoch [36/50], Train Loss: 0.1347, Val Loss: 0.3669\n",
      "Epoch [37/50], Train Loss: 0.1330, Val Loss: 0.3641\n",
      "Epoch [38/50], Train Loss: 0.1317, Val Loss: 0.3614\n",
      "Epoch [39/50], Train Loss: 0.1302, Val Loss: 0.3586\n",
      "Epoch [40/50], Train Loss: 0.1290, Val Loss: 0.3559\n",
      "Epoch [41/50], Train Loss: 0.1272, Val Loss: 0.3533\n",
      "Epoch [42/50], Train Loss: 0.1261, Val Loss: 0.3506\n",
      "Epoch [43/50], Train Loss: 0.1240, Val Loss: 0.3480\n",
      "Epoch [44/50], Train Loss: 0.1232, Val Loss: 0.3455\n",
      "Epoch [45/50], Train Loss: 0.1223, Val Loss: 0.3429\n",
      "Epoch [46/50], Train Loss: 0.1206, Val Loss: 0.3404\n",
      "Epoch [47/50], Train Loss: 0.1188, Val Loss: 0.3380\n",
      "Epoch [48/50], Train Loss: 0.1176, Val Loss: 0.3355\n",
      "Epoch [49/50], Train Loss: 0.1169, Val Loss: 0.3331\n",
      "Epoch [50/50], Train Loss: 0.1150, Val Loss: 0.3307\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=32, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1348, Val Loss: 0.3619\n",
      "Epoch [2/50], Train Loss: 0.1330, Val Loss: 0.3591\n",
      "Epoch [3/50], Train Loss: 0.1307, Val Loss: 0.3565\n",
      "Epoch [4/50], Train Loss: 0.1308, Val Loss: 0.3538\n",
      "Epoch [5/50], Train Loss: 0.1291, Val Loss: 0.3512\n",
      "Epoch [6/50], Train Loss: 0.1269, Val Loss: 0.3486\n",
      "Epoch [7/50], Train Loss: 0.1262, Val Loss: 0.3461\n",
      "Epoch [8/50], Train Loss: 0.1242, Val Loss: 0.3436\n",
      "Epoch [9/50], Train Loss: 0.1230, Val Loss: 0.3411\n",
      "Epoch [10/50], Train Loss: 0.1212, Val Loss: 0.3386\n",
      "Epoch [11/50], Train Loss: 0.1198, Val Loss: 0.3362\n",
      "Epoch [12/50], Train Loss: 0.1186, Val Loss: 0.3339\n",
      "Epoch [13/50], Train Loss: 0.1176, Val Loss: 0.3315\n",
      "Epoch [14/50], Train Loss: 0.1179, Val Loss: 0.3292\n",
      "Epoch [15/50], Train Loss: 0.1161, Val Loss: 0.3268\n",
      "Epoch [16/50], Train Loss: 0.1146, Val Loss: 0.3246\n",
      "Epoch [17/50], Train Loss: 0.1133, Val Loss: 0.3223\n",
      "Epoch [18/50], Train Loss: 0.1120, Val Loss: 0.3201\n",
      "Epoch [19/50], Train Loss: 0.1111, Val Loss: 0.3179\n",
      "Epoch [20/50], Train Loss: 0.1096, Val Loss: 0.3157\n",
      "Epoch [21/50], Train Loss: 0.1095, Val Loss: 0.3136\n",
      "Epoch [22/50], Train Loss: 0.1084, Val Loss: 0.3114\n",
      "Epoch [23/50], Train Loss: 0.1062, Val Loss: 0.3094\n",
      "Epoch [24/50], Train Loss: 0.1064, Val Loss: 0.3073\n",
      "Epoch [25/50], Train Loss: 0.1050, Val Loss: 0.3052\n",
      "Epoch [26/50], Train Loss: 0.1031, Val Loss: 0.3032\n",
      "Epoch [27/50], Train Loss: 0.1020, Val Loss: 0.3012\n",
      "Epoch [28/50], Train Loss: 0.1012, Val Loss: 0.2992\n",
      "Epoch [29/50], Train Loss: 0.1000, Val Loss: 0.2973\n",
      "Epoch [30/50], Train Loss: 0.0997, Val Loss: 0.2954\n",
      "Epoch [31/50], Train Loss: 0.0983, Val Loss: 0.2935\n",
      "Epoch [32/50], Train Loss: 0.0975, Val Loss: 0.2916\n",
      "Epoch [33/50], Train Loss: 0.0959, Val Loss: 0.2898\n",
      "Epoch [34/50], Train Loss: 0.0960, Val Loss: 0.2879\n",
      "Epoch [35/50], Train Loss: 0.0949, Val Loss: 0.2861\n",
      "Epoch [36/50], Train Loss: 0.0946, Val Loss: 0.2843\n",
      "Epoch [37/50], Train Loss: 0.0927, Val Loss: 0.2825\n",
      "Epoch [38/50], Train Loss: 0.0923, Val Loss: 0.2808\n",
      "Epoch [39/50], Train Loss: 0.0924, Val Loss: 0.2791\n",
      "Epoch [40/50], Train Loss: 0.0910, Val Loss: 0.2773\n",
      "Epoch [41/50], Train Loss: 0.0895, Val Loss: 0.2757\n",
      "Epoch [42/50], Train Loss: 0.0892, Val Loss: 0.2740\n",
      "Epoch [43/50], Train Loss: 0.0882, Val Loss: 0.2723\n",
      "Epoch [44/50], Train Loss: 0.0871, Val Loss: 0.2707\n",
      "Epoch [45/50], Train Loss: 0.0870, Val Loss: 0.2690\n",
      "Epoch [46/50], Train Loss: 0.0856, Val Loss: 0.2675\n",
      "Epoch [47/50], Train Loss: 0.0850, Val Loss: 0.2659\n",
      "Epoch [48/50], Train Loss: 0.0843, Val Loss: 0.2643\n",
      "Epoch [49/50], Train Loss: 0.0837, Val Loss: 0.2628\n",
      "Epoch [50/50], Train Loss: 0.0839, Val Loss: 0.2612\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=32, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1676, Val Loss: 0.4393\n",
      "Epoch [2/50], Train Loss: 0.1656, Val Loss: 0.4357\n",
      "Epoch [3/50], Train Loss: 0.1636, Val Loss: 0.4323\n",
      "Epoch [4/50], Train Loss: 0.1616, Val Loss: 0.4288\n",
      "Epoch [5/50], Train Loss: 0.1597, Val Loss: 0.4254\n",
      "Epoch [6/50], Train Loss: 0.1578, Val Loss: 0.4220\n",
      "Epoch [7/50], Train Loss: 0.1559, Val Loss: 0.4187\n",
      "Epoch [8/50], Train Loss: 0.1540, Val Loss: 0.4154\n",
      "Epoch [9/50], Train Loss: 0.1522, Val Loss: 0.4122\n",
      "Epoch [10/50], Train Loss: 0.1504, Val Loss: 0.4089\n",
      "Epoch [11/50], Train Loss: 0.1487, Val Loss: 0.4058\n",
      "Epoch [12/50], Train Loss: 0.1469, Val Loss: 0.4026\n",
      "Epoch [13/50], Train Loss: 0.1452, Val Loss: 0.3995\n",
      "Epoch [14/50], Train Loss: 0.1435, Val Loss: 0.3965\n",
      "Epoch [15/50], Train Loss: 0.1419, Val Loss: 0.3935\n",
      "Epoch [16/50], Train Loss: 0.1402, Val Loss: 0.3905\n",
      "Epoch [17/50], Train Loss: 0.1386, Val Loss: 0.3875\n",
      "Epoch [18/50], Train Loss: 0.1371, Val Loss: 0.3846\n",
      "Epoch [19/50], Train Loss: 0.1355, Val Loss: 0.3817\n",
      "Epoch [20/50], Train Loss: 0.1340, Val Loss: 0.3789\n",
      "Epoch [21/50], Train Loss: 0.1325, Val Loss: 0.3761\n",
      "Epoch [22/50], Train Loss: 0.1310, Val Loss: 0.3733\n",
      "Epoch [23/50], Train Loss: 0.1295, Val Loss: 0.3705\n",
      "Epoch [24/50], Train Loss: 0.1281, Val Loss: 0.3678\n",
      "Epoch [25/50], Train Loss: 0.1267, Val Loss: 0.3651\n",
      "Epoch [26/50], Train Loss: 0.1253, Val Loss: 0.3625\n",
      "Epoch [27/50], Train Loss: 0.1239, Val Loss: 0.3598\n",
      "Epoch [28/50], Train Loss: 0.1225, Val Loss: 0.3572\n",
      "Epoch [29/50], Train Loss: 0.1212, Val Loss: 0.3547\n",
      "Epoch [30/50], Train Loss: 0.1199, Val Loss: 0.3522\n",
      "Epoch [31/50], Train Loss: 0.1186, Val Loss: 0.3496\n",
      "Epoch [32/50], Train Loss: 0.1173, Val Loss: 0.3472\n",
      "Epoch [33/50], Train Loss: 0.1161, Val Loss: 0.3447\n",
      "Epoch [34/50], Train Loss: 0.1149, Val Loss: 0.3423\n",
      "Epoch [35/50], Train Loss: 0.1136, Val Loss: 0.3399\n",
      "Epoch [36/50], Train Loss: 0.1124, Val Loss: 0.3375\n",
      "Epoch [37/50], Train Loss: 0.1113, Val Loss: 0.3352\n",
      "Epoch [38/50], Train Loss: 0.1101, Val Loss: 0.3329\n",
      "Epoch [39/50], Train Loss: 0.1090, Val Loss: 0.3306\n",
      "Epoch [40/50], Train Loss: 0.1078, Val Loss: 0.3283\n",
      "Epoch [41/50], Train Loss: 0.1067, Val Loss: 0.3261\n",
      "Epoch [42/50], Train Loss: 0.1057, Val Loss: 0.3239\n",
      "Epoch [43/50], Train Loss: 0.1046, Val Loss: 0.3217\n",
      "Epoch [44/50], Train Loss: 0.1035, Val Loss: 0.3195\n",
      "Epoch [45/50], Train Loss: 0.1025, Val Loss: 0.3174\n",
      "Epoch [46/50], Train Loss: 0.1015, Val Loss: 0.3153\n",
      "Epoch [47/50], Train Loss: 0.1005, Val Loss: 0.3132\n",
      "Epoch [48/50], Train Loss: 0.0995, Val Loss: 0.3111\n",
      "Epoch [49/50], Train Loss: 0.0985, Val Loss: 0.3091\n",
      "Epoch [50/50], Train Loss: 0.0975, Val Loss: 0.3071\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=32, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1060, Val Loss: 0.2758\n",
      "Epoch [2/50], Train Loss: 0.1046, Val Loss: 0.2740\n",
      "Epoch [3/50], Train Loss: 0.1031, Val Loss: 0.2722\n",
      "Epoch [4/50], Train Loss: 0.1024, Val Loss: 0.2704\n",
      "Epoch [5/50], Train Loss: 0.1015, Val Loss: 0.2687\n",
      "Epoch [6/50], Train Loss: 0.1008, Val Loss: 0.2669\n",
      "Epoch [7/50], Train Loss: 0.0994, Val Loss: 0.2652\n",
      "Epoch [8/50], Train Loss: 0.0984, Val Loss: 0.2636\n",
      "Epoch [9/50], Train Loss: 0.0975, Val Loss: 0.2619\n",
      "Epoch [10/50], Train Loss: 0.0966, Val Loss: 0.2603\n",
      "Epoch [11/50], Train Loss: 0.0958, Val Loss: 0.2587\n",
      "Epoch [12/50], Train Loss: 0.0948, Val Loss: 0.2571\n",
      "Epoch [13/50], Train Loss: 0.0940, Val Loss: 0.2555\n",
      "Epoch [14/50], Train Loss: 0.0932, Val Loss: 0.2539\n",
      "Epoch [15/50], Train Loss: 0.0921, Val Loss: 0.2524\n",
      "Epoch [16/50], Train Loss: 0.0911, Val Loss: 0.2509\n",
      "Epoch [17/50], Train Loss: 0.0906, Val Loss: 0.2494\n",
      "Epoch [18/50], Train Loss: 0.0895, Val Loss: 0.2479\n",
      "Epoch [19/50], Train Loss: 0.0890, Val Loss: 0.2464\n",
      "Epoch [20/50], Train Loss: 0.0880, Val Loss: 0.2450\n",
      "Epoch [21/50], Train Loss: 0.0874, Val Loss: 0.2435\n",
      "Epoch [22/50], Train Loss: 0.0867, Val Loss: 0.2421\n",
      "Epoch [23/50], Train Loss: 0.0859, Val Loss: 0.2407\n",
      "Epoch [24/50], Train Loss: 0.0852, Val Loss: 0.2393\n",
      "Epoch [25/50], Train Loss: 0.0844, Val Loss: 0.2380\n",
      "Epoch [26/50], Train Loss: 0.0837, Val Loss: 0.2366\n",
      "Epoch [27/50], Train Loss: 0.0832, Val Loss: 0.2353\n",
      "Epoch [28/50], Train Loss: 0.0825, Val Loss: 0.2340\n",
      "Epoch [29/50], Train Loss: 0.0813, Val Loss: 0.2327\n",
      "Epoch [30/50], Train Loss: 0.0808, Val Loss: 0.2314\n",
      "Epoch [31/50], Train Loss: 0.0800, Val Loss: 0.2301\n",
      "Epoch [32/50], Train Loss: 0.0798, Val Loss: 0.2289\n",
      "Epoch [33/50], Train Loss: 0.0792, Val Loss: 0.2276\n",
      "Epoch [34/50], Train Loss: 0.0783, Val Loss: 0.2264\n",
      "Epoch [35/50], Train Loss: 0.0776, Val Loss: 0.2252\n",
      "Epoch [36/50], Train Loss: 0.0769, Val Loss: 0.2240\n",
      "Epoch [37/50], Train Loss: 0.0764, Val Loss: 0.2228\n",
      "Epoch [38/50], Train Loss: 0.0762, Val Loss: 0.2216\n",
      "Epoch [39/50], Train Loss: 0.0752, Val Loss: 0.2205\n",
      "Epoch [40/50], Train Loss: 0.0747, Val Loss: 0.2193\n",
      "Epoch [41/50], Train Loss: 0.0744, Val Loss: 0.2182\n",
      "Epoch [42/50], Train Loss: 0.0732, Val Loss: 0.2171\n",
      "Epoch [43/50], Train Loss: 0.0733, Val Loss: 0.2160\n",
      "Epoch [44/50], Train Loss: 0.0725, Val Loss: 0.2149\n",
      "Epoch [45/50], Train Loss: 0.0717, Val Loss: 0.2138\n",
      "Epoch [46/50], Train Loss: 0.0714, Val Loss: 0.2128\n",
      "Epoch [47/50], Train Loss: 0.0709, Val Loss: 0.2117\n",
      "Epoch [48/50], Train Loss: 0.0704, Val Loss: 0.2107\n",
      "Epoch [49/50], Train Loss: 0.0697, Val Loss: 0.2096\n",
      "Epoch [50/50], Train Loss: 0.0693, Val Loss: 0.2086\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=32, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1641, Val Loss: 0.3761\n",
      "Epoch [2/50], Train Loss: 0.1616, Val Loss: 0.3732\n",
      "Epoch [3/50], Train Loss: 0.1606, Val Loss: 0.3702\n",
      "Epoch [4/50], Train Loss: 0.1585, Val Loss: 0.3674\n",
      "Epoch [5/50], Train Loss: 0.1559, Val Loss: 0.3645\n",
      "Epoch [6/50], Train Loss: 0.1543, Val Loss: 0.3617\n",
      "Epoch [7/50], Train Loss: 0.1525, Val Loss: 0.3589\n",
      "Epoch [8/50], Train Loss: 0.1513, Val Loss: 0.3562\n",
      "Epoch [9/50], Train Loss: 0.1483, Val Loss: 0.3535\n",
      "Epoch [10/50], Train Loss: 0.1465, Val Loss: 0.3509\n",
      "Epoch [11/50], Train Loss: 0.1457, Val Loss: 0.3483\n",
      "Epoch [12/50], Train Loss: 0.1437, Val Loss: 0.3457\n",
      "Epoch [13/50], Train Loss: 0.1425, Val Loss: 0.3431\n",
      "Epoch [14/50], Train Loss: 0.1408, Val Loss: 0.3406\n",
      "Epoch [15/50], Train Loss: 0.1394, Val Loss: 0.3381\n",
      "Epoch [16/50], Train Loss: 0.1367, Val Loss: 0.3356\n",
      "Epoch [17/50], Train Loss: 0.1358, Val Loss: 0.3332\n",
      "Epoch [18/50], Train Loss: 0.1343, Val Loss: 0.3308\n",
      "Epoch [19/50], Train Loss: 0.1325, Val Loss: 0.3285\n",
      "Epoch [20/50], Train Loss: 0.1321, Val Loss: 0.3261\n",
      "Epoch [21/50], Train Loss: 0.1301, Val Loss: 0.3238\n",
      "Epoch [22/50], Train Loss: 0.1285, Val Loss: 0.3215\n",
      "Epoch [23/50], Train Loss: 0.1268, Val Loss: 0.3193\n",
      "Epoch [24/50], Train Loss: 0.1262, Val Loss: 0.3171\n",
      "Epoch [25/50], Train Loss: 0.1252, Val Loss: 0.3149\n",
      "Epoch [26/50], Train Loss: 0.1236, Val Loss: 0.3127\n",
      "Epoch [27/50], Train Loss: 0.1223, Val Loss: 0.3106\n",
      "Epoch [28/50], Train Loss: 0.1210, Val Loss: 0.3085\n",
      "Epoch [29/50], Train Loss: 0.1192, Val Loss: 0.3064\n",
      "Epoch [30/50], Train Loss: 0.1195, Val Loss: 0.3043\n",
      "Epoch [31/50], Train Loss: 0.1168, Val Loss: 0.3023\n",
      "Epoch [32/50], Train Loss: 0.1162, Val Loss: 0.3003\n",
      "Epoch [33/50], Train Loss: 0.1143, Val Loss: 0.2983\n",
      "Epoch [34/50], Train Loss: 0.1138, Val Loss: 0.2963\n",
      "Epoch [35/50], Train Loss: 0.1121, Val Loss: 0.2944\n",
      "Epoch [36/50], Train Loss: 0.1117, Val Loss: 0.2925\n",
      "Epoch [37/50], Train Loss: 0.1106, Val Loss: 0.2906\n",
      "Epoch [38/50], Train Loss: 0.1093, Val Loss: 0.2887\n",
      "Epoch [39/50], Train Loss: 0.1080, Val Loss: 0.2868\n",
      "Epoch [40/50], Train Loss: 0.1079, Val Loss: 0.2850\n",
      "Epoch [41/50], Train Loss: 0.1058, Val Loss: 0.2832\n",
      "Epoch [42/50], Train Loss: 0.1052, Val Loss: 0.2814\n",
      "Epoch [43/50], Train Loss: 0.1039, Val Loss: 0.2797\n",
      "Epoch [44/50], Train Loss: 0.1034, Val Loss: 0.2779\n",
      "Epoch [45/50], Train Loss: 0.1015, Val Loss: 0.2762\n",
      "Epoch [46/50], Train Loss: 0.1010, Val Loss: 0.2745\n",
      "Epoch [47/50], Train Loss: 0.1008, Val Loss: 0.2728\n",
      "Epoch [48/50], Train Loss: 0.0990, Val Loss: 0.2712\n",
      "Epoch [49/50], Train Loss: 0.0989, Val Loss: 0.2695\n",
      "Epoch [50/50], Train Loss: 0.0986, Val Loss: 0.2679\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=64, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1278, Val Loss: 0.3623\n",
      "Epoch [2/50], Train Loss: 0.1262, Val Loss: 0.3591\n",
      "Epoch [3/50], Train Loss: 0.1246, Val Loss: 0.3560\n",
      "Epoch [4/50], Train Loss: 0.1231, Val Loss: 0.3529\n",
      "Epoch [5/50], Train Loss: 0.1215, Val Loss: 0.3499\n",
      "Epoch [6/50], Train Loss: 0.1201, Val Loss: 0.3469\n",
      "Epoch [7/50], Train Loss: 0.1186, Val Loss: 0.3440\n",
      "Epoch [8/50], Train Loss: 0.1171, Val Loss: 0.3411\n",
      "Epoch [9/50], Train Loss: 0.1157, Val Loss: 0.3382\n",
      "Epoch [10/50], Train Loss: 0.1143, Val Loss: 0.3354\n",
      "Epoch [11/50], Train Loss: 0.1130, Val Loss: 0.3326\n",
      "Epoch [12/50], Train Loss: 0.1116, Val Loss: 0.3298\n",
      "Epoch [13/50], Train Loss: 0.1103, Val Loss: 0.3271\n",
      "Epoch [14/50], Train Loss: 0.1090, Val Loss: 0.3244\n",
      "Epoch [15/50], Train Loss: 0.1077, Val Loss: 0.3218\n",
      "Epoch [16/50], Train Loss: 0.1065, Val Loss: 0.3192\n",
      "Epoch [17/50], Train Loss: 0.1053, Val Loss: 0.3166\n",
      "Epoch [18/50], Train Loss: 0.1041, Val Loss: 0.3141\n",
      "Epoch [19/50], Train Loss: 0.1029, Val Loss: 0.3116\n",
      "Epoch [20/50], Train Loss: 0.1017, Val Loss: 0.3091\n",
      "Epoch [21/50], Train Loss: 0.1006, Val Loss: 0.3066\n",
      "Epoch [22/50], Train Loss: 0.0994, Val Loss: 0.3042\n",
      "Epoch [23/50], Train Loss: 0.0983, Val Loss: 0.3018\n",
      "Epoch [24/50], Train Loss: 0.0972, Val Loss: 0.2995\n",
      "Epoch [25/50], Train Loss: 0.0962, Val Loss: 0.2972\n",
      "Epoch [26/50], Train Loss: 0.0951, Val Loss: 0.2949\n",
      "Epoch [27/50], Train Loss: 0.0941, Val Loss: 0.2926\n",
      "Epoch [28/50], Train Loss: 0.0931, Val Loss: 0.2904\n",
      "Epoch [29/50], Train Loss: 0.0921, Val Loss: 0.2882\n",
      "Epoch [30/50], Train Loss: 0.0911, Val Loss: 0.2861\n",
      "Epoch [31/50], Train Loss: 0.0901, Val Loss: 0.2839\n",
      "Epoch [32/50], Train Loss: 0.0892, Val Loss: 0.2818\n",
      "Epoch [33/50], Train Loss: 0.0883, Val Loss: 0.2797\n",
      "Epoch [34/50], Train Loss: 0.0873, Val Loss: 0.2776\n",
      "Epoch [35/50], Train Loss: 0.0864, Val Loss: 0.2756\n",
      "Epoch [36/50], Train Loss: 0.0856, Val Loss: 0.2736\n",
      "Epoch [37/50], Train Loss: 0.0847, Val Loss: 0.2716\n",
      "Epoch [38/50], Train Loss: 0.0838, Val Loss: 0.2696\n",
      "Epoch [39/50], Train Loss: 0.0830, Val Loss: 0.2677\n",
      "Epoch [40/50], Train Loss: 0.0822, Val Loss: 0.2658\n",
      "Epoch [41/50], Train Loss: 0.0814, Val Loss: 0.2639\n",
      "Epoch [42/50], Train Loss: 0.0806, Val Loss: 0.2620\n",
      "Epoch [43/50], Train Loss: 0.0798, Val Loss: 0.2602\n",
      "Epoch [44/50], Train Loss: 0.0790, Val Loss: 0.2584\n",
      "Epoch [45/50], Train Loss: 0.0783, Val Loss: 0.2566\n",
      "Epoch [46/50], Train Loss: 0.0775, Val Loss: 0.2548\n",
      "Epoch [47/50], Train Loss: 0.0768, Val Loss: 0.2531\n",
      "Epoch [48/50], Train Loss: 0.0761, Val Loss: 0.2513\n",
      "Epoch [49/50], Train Loss: 0.0754, Val Loss: 0.2496\n",
      "Epoch [50/50], Train Loss: 0.0747, Val Loss: 0.2479\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=64, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1067, Val Loss: 0.3123\n",
      "Epoch [2/50], Train Loss: 0.1065, Val Loss: 0.3099\n",
      "Epoch [3/50], Train Loss: 0.1046, Val Loss: 0.3075\n",
      "Epoch [4/50], Train Loss: 0.1039, Val Loss: 0.3052\n",
      "Epoch [5/50], Train Loss: 0.1027, Val Loss: 0.3029\n",
      "Epoch [6/50], Train Loss: 0.1018, Val Loss: 0.3006\n",
      "Epoch [7/50], Train Loss: 0.1003, Val Loss: 0.2984\n",
      "Epoch [8/50], Train Loss: 0.0994, Val Loss: 0.2962\n",
      "Epoch [9/50], Train Loss: 0.0983, Val Loss: 0.2940\n",
      "Epoch [10/50], Train Loss: 0.0971, Val Loss: 0.2918\n",
      "Epoch [11/50], Train Loss: 0.0959, Val Loss: 0.2897\n",
      "Epoch [12/50], Train Loss: 0.0954, Val Loss: 0.2876\n",
      "Epoch [13/50], Train Loss: 0.0947, Val Loss: 0.2855\n",
      "Epoch [14/50], Train Loss: 0.0935, Val Loss: 0.2834\n",
      "Epoch [15/50], Train Loss: 0.0924, Val Loss: 0.2814\n",
      "Epoch [16/50], Train Loss: 0.0919, Val Loss: 0.2794\n",
      "Epoch [17/50], Train Loss: 0.0905, Val Loss: 0.2774\n",
      "Epoch [18/50], Train Loss: 0.0896, Val Loss: 0.2754\n",
      "Epoch [19/50], Train Loss: 0.0891, Val Loss: 0.2735\n",
      "Epoch [20/50], Train Loss: 0.0880, Val Loss: 0.2716\n",
      "Epoch [21/50], Train Loss: 0.0871, Val Loss: 0.2697\n",
      "Epoch [22/50], Train Loss: 0.0864, Val Loss: 0.2678\n",
      "Epoch [23/50], Train Loss: 0.0854, Val Loss: 0.2660\n",
      "Epoch [24/50], Train Loss: 0.0848, Val Loss: 0.2641\n",
      "Epoch [25/50], Train Loss: 0.0840, Val Loss: 0.2623\n",
      "Epoch [26/50], Train Loss: 0.0828, Val Loss: 0.2606\n",
      "Epoch [27/50], Train Loss: 0.0824, Val Loss: 0.2588\n",
      "Epoch [28/50], Train Loss: 0.0819, Val Loss: 0.2570\n",
      "Epoch [29/50], Train Loss: 0.0811, Val Loss: 0.2553\n",
      "Epoch [30/50], Train Loss: 0.0801, Val Loss: 0.2536\n",
      "Epoch [31/50], Train Loss: 0.0795, Val Loss: 0.2519\n",
      "Epoch [32/50], Train Loss: 0.0789, Val Loss: 0.2503\n",
      "Epoch [33/50], Train Loss: 0.0779, Val Loss: 0.2486\n",
      "Epoch [34/50], Train Loss: 0.0777, Val Loss: 0.2470\n",
      "Epoch [35/50], Train Loss: 0.0766, Val Loss: 0.2454\n",
      "Epoch [36/50], Train Loss: 0.0760, Val Loss: 0.2438\n",
      "Epoch [37/50], Train Loss: 0.0753, Val Loss: 0.2422\n",
      "Epoch [38/50], Train Loss: 0.0746, Val Loss: 0.2407\n",
      "Epoch [39/50], Train Loss: 0.0741, Val Loss: 0.2392\n",
      "Epoch [40/50], Train Loss: 0.0737, Val Loss: 0.2376\n",
      "Epoch [41/50], Train Loss: 0.0729, Val Loss: 0.2362\n",
      "Epoch [42/50], Train Loss: 0.0724, Val Loss: 0.2347\n",
      "Epoch [43/50], Train Loss: 0.0717, Val Loss: 0.2332\n",
      "Epoch [44/50], Train Loss: 0.0708, Val Loss: 0.2318\n",
      "Epoch [45/50], Train Loss: 0.0707, Val Loss: 0.2303\n",
      "Epoch [46/50], Train Loss: 0.0701, Val Loss: 0.2289\n",
      "Epoch [47/50], Train Loss: 0.0696, Val Loss: 0.2275\n",
      "Epoch [48/50], Train Loss: 0.0689, Val Loss: 0.2261\n",
      "Epoch [49/50], Train Loss: 0.0683, Val Loss: 0.2248\n",
      "Epoch [50/50], Train Loss: 0.0677, Val Loss: 0.2234\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=64, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1491, Val Loss: 0.3649\n",
      "Epoch [2/50], Train Loss: 0.1480, Val Loss: 0.3618\n",
      "Epoch [3/50], Train Loss: 0.1453, Val Loss: 0.3587\n",
      "Epoch [4/50], Train Loss: 0.1435, Val Loss: 0.3557\n",
      "Epoch [5/50], Train Loss: 0.1423, Val Loss: 0.3528\n",
      "Epoch [6/50], Train Loss: 0.1403, Val Loss: 0.3498\n",
      "Epoch [7/50], Train Loss: 0.1389, Val Loss: 0.3469\n",
      "Epoch [8/50], Train Loss: 0.1374, Val Loss: 0.3441\n",
      "Epoch [9/50], Train Loss: 0.1354, Val Loss: 0.3413\n",
      "Epoch [10/50], Train Loss: 0.1334, Val Loss: 0.3385\n",
      "Epoch [11/50], Train Loss: 0.1318, Val Loss: 0.3358\n",
      "Epoch [12/50], Train Loss: 0.1315, Val Loss: 0.3330\n",
      "Epoch [13/50], Train Loss: 0.1296, Val Loss: 0.3303\n",
      "Epoch [14/50], Train Loss: 0.1279, Val Loss: 0.3277\n",
      "Epoch [15/50], Train Loss: 0.1268, Val Loss: 0.3251\n",
      "Epoch [16/50], Train Loss: 0.1255, Val Loss: 0.3225\n",
      "Epoch [17/50], Train Loss: 0.1233, Val Loss: 0.3199\n",
      "Epoch [18/50], Train Loss: 0.1220, Val Loss: 0.3174\n",
      "Epoch [19/50], Train Loss: 0.1210, Val Loss: 0.3149\n",
      "Epoch [20/50], Train Loss: 0.1196, Val Loss: 0.3124\n",
      "Epoch [21/50], Train Loss: 0.1185, Val Loss: 0.3100\n",
      "Epoch [22/50], Train Loss: 0.1170, Val Loss: 0.3076\n",
      "Epoch [23/50], Train Loss: 0.1155, Val Loss: 0.3052\n",
      "Epoch [24/50], Train Loss: 0.1138, Val Loss: 0.3029\n",
      "Epoch [25/50], Train Loss: 0.1127, Val Loss: 0.3006\n",
      "Epoch [26/50], Train Loss: 0.1116, Val Loss: 0.2983\n",
      "Epoch [27/50], Train Loss: 0.1104, Val Loss: 0.2960\n",
      "Epoch [28/50], Train Loss: 0.1095, Val Loss: 0.2938\n",
      "Epoch [29/50], Train Loss: 0.1092, Val Loss: 0.2916\n",
      "Epoch [30/50], Train Loss: 0.1075, Val Loss: 0.2894\n",
      "Epoch [31/50], Train Loss: 0.1060, Val Loss: 0.2872\n",
      "Epoch [32/50], Train Loss: 0.1049, Val Loss: 0.2851\n",
      "Epoch [33/50], Train Loss: 0.1042, Val Loss: 0.2830\n",
      "Epoch [34/50], Train Loss: 0.1023, Val Loss: 0.2809\n",
      "Epoch [35/50], Train Loss: 0.1020, Val Loss: 0.2789\n",
      "Epoch [36/50], Train Loss: 0.1007, Val Loss: 0.2768\n",
      "Epoch [37/50], Train Loss: 0.1001, Val Loss: 0.2748\n",
      "Epoch [38/50], Train Loss: 0.0986, Val Loss: 0.2728\n",
      "Epoch [39/50], Train Loss: 0.0976, Val Loss: 0.2709\n",
      "Epoch [40/50], Train Loss: 0.0968, Val Loss: 0.2689\n",
      "Epoch [41/50], Train Loss: 0.0956, Val Loss: 0.2670\n",
      "Epoch [42/50], Train Loss: 0.0941, Val Loss: 0.2651\n",
      "Epoch [43/50], Train Loss: 0.0933, Val Loss: 0.2632\n",
      "Epoch [44/50], Train Loss: 0.0930, Val Loss: 0.2614\n",
      "Epoch [45/50], Train Loss: 0.0922, Val Loss: 0.2595\n",
      "Epoch [46/50], Train Loss: 0.0912, Val Loss: 0.2577\n",
      "Epoch [47/50], Train Loss: 0.0904, Val Loss: 0.2559\n",
      "Epoch [48/50], Train Loss: 0.0890, Val Loss: 0.2542\n",
      "Epoch [49/50], Train Loss: 0.0884, Val Loss: 0.2524\n",
      "Epoch [50/50], Train Loss: 0.0880, Val Loss: 0.2507\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=64, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1605, Val Loss: 0.4169\n",
      "Epoch [2/50], Train Loss: 0.1585, Val Loss: 0.4133\n",
      "Epoch [3/50], Train Loss: 0.1565, Val Loss: 0.4098\n",
      "Epoch [4/50], Train Loss: 0.1545, Val Loss: 0.4064\n",
      "Epoch [5/50], Train Loss: 0.1526, Val Loss: 0.4030\n",
      "Epoch [6/50], Train Loss: 0.1507, Val Loss: 0.3996\n",
      "Epoch [7/50], Train Loss: 0.1488, Val Loss: 0.3963\n",
      "Epoch [8/50], Train Loss: 0.1470, Val Loss: 0.3931\n",
      "Epoch [9/50], Train Loss: 0.1452, Val Loss: 0.3898\n",
      "Epoch [10/50], Train Loss: 0.1434, Val Loss: 0.3866\n",
      "Epoch [11/50], Train Loss: 0.1417, Val Loss: 0.3835\n",
      "Epoch [12/50], Train Loss: 0.1400, Val Loss: 0.3804\n",
      "Epoch [13/50], Train Loss: 0.1383, Val Loss: 0.3773\n",
      "Epoch [14/50], Train Loss: 0.1366, Val Loss: 0.3743\n",
      "Epoch [15/50], Train Loss: 0.1350, Val Loss: 0.3713\n",
      "Epoch [16/50], Train Loss: 0.1334, Val Loss: 0.3684\n",
      "Epoch [17/50], Train Loss: 0.1318, Val Loss: 0.3655\n",
      "Epoch [18/50], Train Loss: 0.1302, Val Loss: 0.3626\n",
      "Epoch [19/50], Train Loss: 0.1287, Val Loss: 0.3597\n",
      "Epoch [20/50], Train Loss: 0.1272, Val Loss: 0.3569\n",
      "Epoch [21/50], Train Loss: 0.1257, Val Loss: 0.3542\n",
      "Epoch [22/50], Train Loss: 0.1243, Val Loss: 0.3514\n",
      "Epoch [23/50], Train Loss: 0.1228, Val Loss: 0.3487\n",
      "Epoch [24/50], Train Loss: 0.1214, Val Loss: 0.3461\n",
      "Epoch [25/50], Train Loss: 0.1201, Val Loss: 0.3434\n",
      "Epoch [26/50], Train Loss: 0.1187, Val Loss: 0.3408\n",
      "Epoch [27/50], Train Loss: 0.1174, Val Loss: 0.3383\n",
      "Epoch [28/50], Train Loss: 0.1160, Val Loss: 0.3357\n",
      "Epoch [29/50], Train Loss: 0.1147, Val Loss: 0.3332\n",
      "Epoch [30/50], Train Loss: 0.1135, Val Loss: 0.3307\n",
      "Epoch [31/50], Train Loss: 0.1122, Val Loss: 0.3283\n",
      "Epoch [32/50], Train Loss: 0.1110, Val Loss: 0.3259\n",
      "Epoch [33/50], Train Loss: 0.1098, Val Loss: 0.3235\n",
      "Epoch [34/50], Train Loss: 0.1086, Val Loss: 0.3211\n",
      "Epoch [35/50], Train Loss: 0.1074, Val Loss: 0.3188\n",
      "Epoch [36/50], Train Loss: 0.1062, Val Loss: 0.3165\n",
      "Epoch [37/50], Train Loss: 0.1051, Val Loss: 0.3142\n",
      "Epoch [38/50], Train Loss: 0.1040, Val Loss: 0.3120\n",
      "Epoch [39/50], Train Loss: 0.1029, Val Loss: 0.3098\n",
      "Epoch [40/50], Train Loss: 0.1018, Val Loss: 0.3076\n",
      "Epoch [41/50], Train Loss: 0.1007, Val Loss: 0.3054\n",
      "Epoch [42/50], Train Loss: 0.0997, Val Loss: 0.3033\n",
      "Epoch [43/50], Train Loss: 0.0986, Val Loss: 0.3011\n",
      "Epoch [44/50], Train Loss: 0.0976, Val Loss: 0.2991\n",
      "Epoch [45/50], Train Loss: 0.0966, Val Loss: 0.2970\n",
      "Epoch [46/50], Train Loss: 0.0956, Val Loss: 0.2949\n",
      "Epoch [47/50], Train Loss: 0.0947, Val Loss: 0.2929\n",
      "Epoch [48/50], Train Loss: 0.0937, Val Loss: 0.2909\n",
      "Epoch [49/50], Train Loss: 0.0928, Val Loss: 0.2890\n",
      "Epoch [50/50], Train Loss: 0.0919, Val Loss: 0.2870\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=64, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1106, Val Loss: 0.3225\n",
      "Epoch [2/50], Train Loss: 0.1098, Val Loss: 0.3203\n",
      "Epoch [3/50], Train Loss: 0.1082, Val Loss: 0.3182\n",
      "Epoch [4/50], Train Loss: 0.1073, Val Loss: 0.3161\n",
      "Epoch [5/50], Train Loss: 0.1067, Val Loss: 0.3140\n",
      "Epoch [6/50], Train Loss: 0.1052, Val Loss: 0.3119\n",
      "Epoch [7/50], Train Loss: 0.1044, Val Loss: 0.3099\n",
      "Epoch [8/50], Train Loss: 0.1031, Val Loss: 0.3079\n",
      "Epoch [9/50], Train Loss: 0.1024, Val Loss: 0.3059\n",
      "Epoch [10/50], Train Loss: 0.1013, Val Loss: 0.3039\n",
      "Epoch [11/50], Train Loss: 0.1004, Val Loss: 0.3019\n",
      "Epoch [12/50], Train Loss: 0.0993, Val Loss: 0.3000\n",
      "Epoch [13/50], Train Loss: 0.0985, Val Loss: 0.2981\n",
      "Epoch [14/50], Train Loss: 0.0971, Val Loss: 0.2962\n",
      "Epoch [15/50], Train Loss: 0.0963, Val Loss: 0.2944\n",
      "Epoch [16/50], Train Loss: 0.0953, Val Loss: 0.2925\n",
      "Epoch [17/50], Train Loss: 0.0946, Val Loss: 0.2907\n",
      "Epoch [18/50], Train Loss: 0.0938, Val Loss: 0.2889\n",
      "Epoch [19/50], Train Loss: 0.0928, Val Loss: 0.2871\n",
      "Epoch [20/50], Train Loss: 0.0921, Val Loss: 0.2854\n",
      "Epoch [21/50], Train Loss: 0.0910, Val Loss: 0.2836\n",
      "Epoch [22/50], Train Loss: 0.0903, Val Loss: 0.2819\n",
      "Epoch [23/50], Train Loss: 0.0892, Val Loss: 0.2802\n",
      "Epoch [24/50], Train Loss: 0.0888, Val Loss: 0.2785\n",
      "Epoch [25/50], Train Loss: 0.0880, Val Loss: 0.2768\n",
      "Epoch [26/50], Train Loss: 0.0873, Val Loss: 0.2752\n",
      "Epoch [27/50], Train Loss: 0.0861, Val Loss: 0.2736\n",
      "Epoch [28/50], Train Loss: 0.0855, Val Loss: 0.2720\n",
      "Epoch [29/50], Train Loss: 0.0849, Val Loss: 0.2704\n",
      "Epoch [30/50], Train Loss: 0.0839, Val Loss: 0.2688\n",
      "Epoch [31/50], Train Loss: 0.0835, Val Loss: 0.2672\n",
      "Epoch [32/50], Train Loss: 0.0828, Val Loss: 0.2657\n",
      "Epoch [33/50], Train Loss: 0.0821, Val Loss: 0.2642\n",
      "Epoch [34/50], Train Loss: 0.0811, Val Loss: 0.2626\n",
      "Epoch [35/50], Train Loss: 0.0806, Val Loss: 0.2612\n",
      "Epoch [36/50], Train Loss: 0.0798, Val Loss: 0.2597\n",
      "Epoch [37/50], Train Loss: 0.0792, Val Loss: 0.2582\n",
      "Epoch [38/50], Train Loss: 0.0785, Val Loss: 0.2568\n",
      "Epoch [39/50], Train Loss: 0.0780, Val Loss: 0.2553\n",
      "Epoch [40/50], Train Loss: 0.0776, Val Loss: 0.2539\n",
      "Epoch [41/50], Train Loss: 0.0768, Val Loss: 0.2525\n",
      "Epoch [42/50], Train Loss: 0.0760, Val Loss: 0.2511\n",
      "Epoch [43/50], Train Loss: 0.0756, Val Loss: 0.2498\n",
      "Epoch [44/50], Train Loss: 0.0750, Val Loss: 0.2484\n",
      "Epoch [45/50], Train Loss: 0.0744, Val Loss: 0.2471\n",
      "Epoch [46/50], Train Loss: 0.0739, Val Loss: 0.2457\n",
      "Epoch [47/50], Train Loss: 0.0733, Val Loss: 0.2444\n",
      "Epoch [48/50], Train Loss: 0.0726, Val Loss: 0.2431\n",
      "Epoch [49/50], Train Loss: 0.0722, Val Loss: 0.2418\n",
      "Epoch [50/50], Train Loss: 0.0714, Val Loss: 0.2406\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=64, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1456, Val Loss: 0.3566\n",
      "Epoch [2/50], Train Loss: 0.1437, Val Loss: 0.3539\n",
      "Epoch [3/50], Train Loss: 0.1422, Val Loss: 0.3511\n",
      "Epoch [4/50], Train Loss: 0.1403, Val Loss: 0.3485\n",
      "Epoch [5/50], Train Loss: 0.1394, Val Loss: 0.3458\n",
      "Epoch [6/50], Train Loss: 0.1372, Val Loss: 0.3432\n",
      "Epoch [7/50], Train Loss: 0.1361, Val Loss: 0.3406\n",
      "Epoch [8/50], Train Loss: 0.1341, Val Loss: 0.3380\n",
      "Epoch [9/50], Train Loss: 0.1326, Val Loss: 0.3355\n",
      "Epoch [10/50], Train Loss: 0.1312, Val Loss: 0.3330\n",
      "Epoch [11/50], Train Loss: 0.1296, Val Loss: 0.3306\n",
      "Epoch [12/50], Train Loss: 0.1293, Val Loss: 0.3281\n",
      "Epoch [13/50], Train Loss: 0.1262, Val Loss: 0.3257\n",
      "Epoch [14/50], Train Loss: 0.1263, Val Loss: 0.3234\n",
      "Epoch [15/50], Train Loss: 0.1244, Val Loss: 0.3210\n",
      "Epoch [16/50], Train Loss: 0.1223, Val Loss: 0.3187\n",
      "Epoch [17/50], Train Loss: 0.1207, Val Loss: 0.3164\n",
      "Epoch [18/50], Train Loss: 0.1205, Val Loss: 0.3142\n",
      "Epoch [19/50], Train Loss: 0.1194, Val Loss: 0.3120\n",
      "Epoch [20/50], Train Loss: 0.1175, Val Loss: 0.3098\n",
      "Epoch [21/50], Train Loss: 0.1159, Val Loss: 0.3076\n",
      "Epoch [22/50], Train Loss: 0.1164, Val Loss: 0.3055\n",
      "Epoch [23/50], Train Loss: 0.1145, Val Loss: 0.3034\n",
      "Epoch [24/50], Train Loss: 0.1129, Val Loss: 0.3013\n",
      "Epoch [25/50], Train Loss: 0.1113, Val Loss: 0.2992\n",
      "Epoch [26/50], Train Loss: 0.1107, Val Loss: 0.2972\n",
      "Epoch [27/50], Train Loss: 0.1093, Val Loss: 0.2952\n",
      "Epoch [28/50], Train Loss: 0.1083, Val Loss: 0.2932\n",
      "Epoch [29/50], Train Loss: 0.1065, Val Loss: 0.2912\n",
      "Epoch [30/50], Train Loss: 0.1054, Val Loss: 0.2893\n",
      "Epoch [31/50], Train Loss: 0.1049, Val Loss: 0.2874\n",
      "Epoch [32/50], Train Loss: 0.1040, Val Loss: 0.2855\n",
      "Epoch [33/50], Train Loss: 0.1024, Val Loss: 0.2836\n",
      "Epoch [34/50], Train Loss: 0.1015, Val Loss: 0.2818\n",
      "Epoch [35/50], Train Loss: 0.1006, Val Loss: 0.2800\n",
      "Epoch [36/50], Train Loss: 0.0997, Val Loss: 0.2782\n",
      "Epoch [37/50], Train Loss: 0.0990, Val Loss: 0.2764\n",
      "Epoch [38/50], Train Loss: 0.0980, Val Loss: 0.2746\n",
      "Epoch [39/50], Train Loss: 0.0973, Val Loss: 0.2729\n",
      "Epoch [40/50], Train Loss: 0.0961, Val Loss: 0.2712\n",
      "Epoch [41/50], Train Loss: 0.0949, Val Loss: 0.2695\n",
      "Epoch [42/50], Train Loss: 0.0938, Val Loss: 0.2678\n",
      "Epoch [43/50], Train Loss: 0.0933, Val Loss: 0.2661\n",
      "Epoch [44/50], Train Loss: 0.0922, Val Loss: 0.2645\n",
      "Epoch [45/50], Train Loss: 0.0918, Val Loss: 0.2629\n",
      "Epoch [46/50], Train Loss: 0.0913, Val Loss: 0.2613\n",
      "Epoch [47/50], Train Loss: 0.0895, Val Loss: 0.2597\n",
      "Epoch [48/50], Train Loss: 0.0885, Val Loss: 0.2582\n",
      "Epoch [49/50], Train Loss: 0.0889, Val Loss: 0.2566\n",
      "Epoch [50/50], Train Loss: 0.0871, Val Loss: 0.2551\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=64, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1686, Val Loss: 0.4262\n",
      "Epoch [2/50], Train Loss: 0.1666, Val Loss: 0.4228\n",
      "Epoch [3/50], Train Loss: 0.1646, Val Loss: 0.4194\n",
      "Epoch [4/50], Train Loss: 0.1626, Val Loss: 0.4161\n",
      "Epoch [5/50], Train Loss: 0.1607, Val Loss: 0.4128\n",
      "Epoch [6/50], Train Loss: 0.1588, Val Loss: 0.4095\n",
      "Epoch [7/50], Train Loss: 0.1570, Val Loss: 0.4063\n",
      "Epoch [8/50], Train Loss: 0.1551, Val Loss: 0.4032\n",
      "Epoch [9/50], Train Loss: 0.1533, Val Loss: 0.4000\n",
      "Epoch [10/50], Train Loss: 0.1516, Val Loss: 0.3969\n",
      "Epoch [11/50], Train Loss: 0.1498, Val Loss: 0.3939\n",
      "Epoch [12/50], Train Loss: 0.1481, Val Loss: 0.3908\n",
      "Epoch [13/50], Train Loss: 0.1464, Val Loss: 0.3879\n",
      "Epoch [14/50], Train Loss: 0.1447, Val Loss: 0.3849\n",
      "Epoch [15/50], Train Loss: 0.1431, Val Loss: 0.3820\n",
      "Epoch [16/50], Train Loss: 0.1415, Val Loss: 0.3791\n",
      "Epoch [17/50], Train Loss: 0.1399, Val Loss: 0.3762\n",
      "Epoch [18/50], Train Loss: 0.1383, Val Loss: 0.3734\n",
      "Epoch [19/50], Train Loss: 0.1368, Val Loss: 0.3706\n",
      "Epoch [20/50], Train Loss: 0.1352, Val Loss: 0.3679\n",
      "Epoch [21/50], Train Loss: 0.1337, Val Loss: 0.3652\n",
      "Epoch [22/50], Train Loss: 0.1322, Val Loss: 0.3625\n",
      "Epoch [23/50], Train Loss: 0.1308, Val Loss: 0.3598\n",
      "Epoch [24/50], Train Loss: 0.1294, Val Loss: 0.3572\n",
      "Epoch [25/50], Train Loss: 0.1279, Val Loss: 0.3546\n",
      "Epoch [26/50], Train Loss: 0.1265, Val Loss: 0.3520\n",
      "Epoch [27/50], Train Loss: 0.1252, Val Loss: 0.3495\n",
      "Epoch [28/50], Train Loss: 0.1238, Val Loss: 0.3469\n",
      "Epoch [29/50], Train Loss: 0.1225, Val Loss: 0.3445\n",
      "Epoch [30/50], Train Loss: 0.1212, Val Loss: 0.3420\n",
      "Epoch [31/50], Train Loss: 0.1199, Val Loss: 0.3396\n",
      "Epoch [32/50], Train Loss: 0.1186, Val Loss: 0.3372\n",
      "Epoch [33/50], Train Loss: 0.1174, Val Loss: 0.3348\n",
      "Epoch [34/50], Train Loss: 0.1161, Val Loss: 0.3325\n",
      "Epoch [35/50], Train Loss: 0.1149, Val Loss: 0.3301\n",
      "Epoch [36/50], Train Loss: 0.1137, Val Loss: 0.3278\n",
      "Epoch [37/50], Train Loss: 0.1125, Val Loss: 0.3256\n",
      "Epoch [38/50], Train Loss: 0.1114, Val Loss: 0.3233\n",
      "Epoch [39/50], Train Loss: 0.1102, Val Loss: 0.3211\n",
      "Epoch [40/50], Train Loss: 0.1091, Val Loss: 0.3189\n",
      "Epoch [41/50], Train Loss: 0.1080, Val Loss: 0.3167\n",
      "Epoch [42/50], Train Loss: 0.1069, Val Loss: 0.3146\n",
      "Epoch [43/50], Train Loss: 0.1058, Val Loss: 0.3125\n",
      "Epoch [44/50], Train Loss: 0.1048, Val Loss: 0.3104\n",
      "Epoch [45/50], Train Loss: 0.1037, Val Loss: 0.3083\n",
      "Epoch [46/50], Train Loss: 0.1027, Val Loss: 0.3063\n",
      "Epoch [47/50], Train Loss: 0.1017, Val Loss: 0.3042\n",
      "Epoch [48/50], Train Loss: 0.1007, Val Loss: 0.3022\n",
      "Epoch [49/50], Train Loss: 0.0997, Val Loss: 0.3002\n",
      "Epoch [50/50], Train Loss: 0.0987, Val Loss: 0.2983\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=64, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1665, Val Loss: 0.4255\n",
      "Epoch [2/50], Train Loss: 0.1643, Val Loss: 0.4223\n",
      "Epoch [3/50], Train Loss: 0.1627, Val Loss: 0.4190\n",
      "Epoch [4/50], Train Loss: 0.1607, Val Loss: 0.4159\n",
      "Epoch [5/50], Train Loss: 0.1594, Val Loss: 0.4127\n",
      "Epoch [6/50], Train Loss: 0.1572, Val Loss: 0.4096\n",
      "Epoch [7/50], Train Loss: 0.1553, Val Loss: 0.4065\n",
      "Epoch [8/50], Train Loss: 0.1541, Val Loss: 0.4035\n",
      "Epoch [9/50], Train Loss: 0.1517, Val Loss: 0.4005\n",
      "Epoch [10/50], Train Loss: 0.1503, Val Loss: 0.3975\n",
      "Epoch [11/50], Train Loss: 0.1487, Val Loss: 0.3946\n",
      "Epoch [12/50], Train Loss: 0.1471, Val Loss: 0.3917\n",
      "Epoch [13/50], Train Loss: 0.1454, Val Loss: 0.3888\n",
      "Epoch [14/50], Train Loss: 0.1436, Val Loss: 0.3860\n",
      "Epoch [15/50], Train Loss: 0.1421, Val Loss: 0.3832\n",
      "Epoch [16/50], Train Loss: 0.1406, Val Loss: 0.3804\n",
      "Epoch [17/50], Train Loss: 0.1392, Val Loss: 0.3777\n",
      "Epoch [18/50], Train Loss: 0.1375, Val Loss: 0.3749\n",
      "Epoch [19/50], Train Loss: 0.1362, Val Loss: 0.3723\n",
      "Epoch [20/50], Train Loss: 0.1346, Val Loss: 0.3696\n",
      "Epoch [21/50], Train Loss: 0.1330, Val Loss: 0.3670\n",
      "Epoch [22/50], Train Loss: 0.1317, Val Loss: 0.3644\n",
      "Epoch [23/50], Train Loss: 0.1303, Val Loss: 0.3619\n",
      "Epoch [24/50], Train Loss: 0.1288, Val Loss: 0.3593\n",
      "Epoch [25/50], Train Loss: 0.1276, Val Loss: 0.3568\n",
      "Epoch [26/50], Train Loss: 0.1266, Val Loss: 0.3543\n",
      "Epoch [27/50], Train Loss: 0.1249, Val Loss: 0.3519\n",
      "Epoch [28/50], Train Loss: 0.1236, Val Loss: 0.3495\n",
      "Epoch [29/50], Train Loss: 0.1222, Val Loss: 0.3471\n",
      "Epoch [30/50], Train Loss: 0.1211, Val Loss: 0.3447\n",
      "Epoch [31/50], Train Loss: 0.1198, Val Loss: 0.3423\n",
      "Epoch [32/50], Train Loss: 0.1186, Val Loss: 0.3400\n",
      "Epoch [33/50], Train Loss: 0.1173, Val Loss: 0.3377\n",
      "Epoch [34/50], Train Loss: 0.1162, Val Loss: 0.3355\n",
      "Epoch [35/50], Train Loss: 0.1150, Val Loss: 0.3332\n",
      "Epoch [36/50], Train Loss: 0.1139, Val Loss: 0.3310\n",
      "Epoch [37/50], Train Loss: 0.1127, Val Loss: 0.3288\n",
      "Epoch [38/50], Train Loss: 0.1118, Val Loss: 0.3266\n",
      "Epoch [39/50], Train Loss: 0.1105, Val Loss: 0.3245\n",
      "Epoch [40/50], Train Loss: 0.1094, Val Loss: 0.3224\n",
      "Epoch [41/50], Train Loss: 0.1085, Val Loss: 0.3203\n",
      "Epoch [42/50], Train Loss: 0.1076, Val Loss: 0.3182\n",
      "Epoch [43/50], Train Loss: 0.1062, Val Loss: 0.3161\n",
      "Epoch [44/50], Train Loss: 0.1052, Val Loss: 0.3141\n",
      "Epoch [45/50], Train Loss: 0.1037, Val Loss: 0.3121\n",
      "Epoch [46/50], Train Loss: 0.1032, Val Loss: 0.3101\n",
      "Epoch [47/50], Train Loss: 0.1021, Val Loss: 0.3081\n",
      "Epoch [48/50], Train Loss: 0.1013, Val Loss: 0.3062\n",
      "Epoch [49/50], Train Loss: 0.1003, Val Loss: 0.3043\n",
      "Epoch [50/50], Train Loss: 0.0995, Val Loss: 0.3024\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=64, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1467, Val Loss: 0.3813\n",
      "Epoch [2/50], Train Loss: 0.1445, Val Loss: 0.3783\n",
      "Epoch [3/50], Train Loss: 0.1430, Val Loss: 0.3754\n",
      "Epoch [4/50], Train Loss: 0.1411, Val Loss: 0.3726\n",
      "Epoch [5/50], Train Loss: 0.1388, Val Loss: 0.3697\n",
      "Epoch [6/50], Train Loss: 0.1378, Val Loss: 0.3669\n",
      "Epoch [7/50], Train Loss: 0.1369, Val Loss: 0.3642\n",
      "Epoch [8/50], Train Loss: 0.1341, Val Loss: 0.3614\n",
      "Epoch [9/50], Train Loss: 0.1331, Val Loss: 0.3588\n",
      "Epoch [10/50], Train Loss: 0.1323, Val Loss: 0.3561\n",
      "Epoch [11/50], Train Loss: 0.1298, Val Loss: 0.3535\n",
      "Epoch [12/50], Train Loss: 0.1295, Val Loss: 0.3509\n",
      "Epoch [13/50], Train Loss: 0.1271, Val Loss: 0.3483\n",
      "Epoch [14/50], Train Loss: 0.1264, Val Loss: 0.3458\n",
      "Epoch [15/50], Train Loss: 0.1255, Val Loss: 0.3432\n",
      "Epoch [16/50], Train Loss: 0.1239, Val Loss: 0.3408\n",
      "Epoch [17/50], Train Loss: 0.1218, Val Loss: 0.3383\n",
      "Epoch [18/50], Train Loss: 0.1215, Val Loss: 0.3359\n",
      "Epoch [19/50], Train Loss: 0.1201, Val Loss: 0.3335\n",
      "Epoch [20/50], Train Loss: 0.1187, Val Loss: 0.3311\n",
      "Epoch [21/50], Train Loss: 0.1172, Val Loss: 0.3288\n",
      "Epoch [22/50], Train Loss: 0.1154, Val Loss: 0.3265\n",
      "Epoch [23/50], Train Loss: 0.1151, Val Loss: 0.3242\n",
      "Epoch [24/50], Train Loss: 0.1140, Val Loss: 0.3219\n",
      "Epoch [25/50], Train Loss: 0.1128, Val Loss: 0.3197\n",
      "Epoch [26/50], Train Loss: 0.1102, Val Loss: 0.3175\n",
      "Epoch [27/50], Train Loss: 0.1105, Val Loss: 0.3153\n",
      "Epoch [28/50], Train Loss: 0.1086, Val Loss: 0.3132\n",
      "Epoch [29/50], Train Loss: 0.1078, Val Loss: 0.3111\n",
      "Epoch [30/50], Train Loss: 0.1071, Val Loss: 0.3090\n",
      "Epoch [31/50], Train Loss: 0.1060, Val Loss: 0.3069\n",
      "Epoch [32/50], Train Loss: 0.1049, Val Loss: 0.3048\n",
      "Epoch [33/50], Train Loss: 0.1038, Val Loss: 0.3028\n",
      "Epoch [34/50], Train Loss: 0.1025, Val Loss: 0.3008\n",
      "Epoch [35/50], Train Loss: 0.1016, Val Loss: 0.2988\n",
      "Epoch [36/50], Train Loss: 0.1001, Val Loss: 0.2968\n",
      "Epoch [37/50], Train Loss: 0.0993, Val Loss: 0.2949\n",
      "Epoch [38/50], Train Loss: 0.0990, Val Loss: 0.2930\n",
      "Epoch [39/50], Train Loss: 0.0983, Val Loss: 0.2911\n",
      "Epoch [40/50], Train Loss: 0.0970, Val Loss: 0.2892\n",
      "Epoch [41/50], Train Loss: 0.0957, Val Loss: 0.2874\n",
      "Epoch [42/50], Train Loss: 0.0954, Val Loss: 0.2855\n",
      "Epoch [43/50], Train Loss: 0.0937, Val Loss: 0.2837\n",
      "Epoch [44/50], Train Loss: 0.0927, Val Loss: 0.2820\n",
      "Epoch [45/50], Train Loss: 0.0922, Val Loss: 0.2802\n",
      "Epoch [46/50], Train Loss: 0.0908, Val Loss: 0.2785\n",
      "Epoch [47/50], Train Loss: 0.0909, Val Loss: 0.2767\n",
      "Epoch [48/50], Train Loss: 0.0901, Val Loss: 0.2750\n",
      "Epoch [49/50], Train Loss: 0.0889, Val Loss: 0.2733\n",
      "Epoch [50/50], Train Loss: 0.0879, Val Loss: 0.2717\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=16, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0998, Val Loss: 0.2739\n",
      "Epoch [2/50], Train Loss: 0.0685, Val Loss: 0.1971\n",
      "Epoch [3/50], Train Loss: 0.0454, Val Loss: 0.1280\n",
      "Epoch [4/50], Train Loss: 0.0329, Val Loss: 0.0835\n",
      "Epoch [5/50], Train Loss: 0.0288, Val Loss: 0.0621\n",
      "Epoch [6/50], Train Loss: 0.0254, Val Loss: 0.0486\n",
      "Epoch [7/50], Train Loss: 0.0225, Val Loss: 0.0382\n",
      "Epoch [8/50], Train Loss: 0.0199, Val Loss: 0.0297\n",
      "Epoch [9/50], Train Loss: 0.0177, Val Loss: 0.0230\n",
      "Epoch [10/50], Train Loss: 0.0160, Val Loss: 0.0183\n",
      "Epoch [11/50], Train Loss: 0.0148, Val Loss: 0.0155\n",
      "Epoch [12/50], Train Loss: 0.0140, Val Loss: 0.0141\n",
      "Epoch [13/50], Train Loss: 0.0135, Val Loss: 0.0133\n",
      "Epoch [14/50], Train Loss: 0.0131, Val Loss: 0.0128\n",
      "Epoch [15/50], Train Loss: 0.0126, Val Loss: 0.0125\n",
      "Epoch [16/50], Train Loss: 0.0121, Val Loss: 0.0123\n",
      "Epoch [17/50], Train Loss: 0.0116, Val Loss: 0.0124\n",
      "Epoch [18/50], Train Loss: 0.0111, Val Loss: 0.0127\n",
      "Epoch [19/50], Train Loss: 0.0105, Val Loss: 0.0132\n",
      "Epoch [20/50], Train Loss: 0.0098, Val Loss: 0.0138\n",
      "Epoch [21/50], Train Loss: 0.0092, Val Loss: 0.0146\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=16, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0616, Val Loss: 0.1961\n",
      "Epoch [2/50], Train Loss: 0.0474, Val Loss: 0.1563\n",
      "Epoch [3/50], Train Loss: 0.0400, Val Loss: 0.1235\n",
      "Epoch [4/50], Train Loss: 0.0349, Val Loss: 0.0973\n",
      "Epoch [5/50], Train Loss: 0.0330, Val Loss: 0.0788\n",
      "Epoch [6/50], Train Loss: 0.0307, Val Loss: 0.0643\n",
      "Epoch [7/50], Train Loss: 0.0281, Val Loss: 0.0509\n",
      "Epoch [8/50], Train Loss: 0.0266, Val Loss: 0.0389\n",
      "Epoch [9/50], Train Loss: 0.0238, Val Loss: 0.0285\n",
      "Epoch [10/50], Train Loss: 0.0205, Val Loss: 0.0220\n",
      "Epoch [11/50], Train Loss: 0.0178, Val Loss: 0.0182\n",
      "Epoch [12/50], Train Loss: 0.0163, Val Loss: 0.0154\n",
      "Epoch [13/50], Train Loss: 0.0143, Val Loss: 0.0145\n",
      "Epoch [14/50], Train Loss: 0.0124, Val Loss: 0.0160\n",
      "Epoch [15/50], Train Loss: 0.0106, Val Loss: 0.0182\n",
      "Epoch [16/50], Train Loss: 0.0091, Val Loss: 0.0212\n",
      "Epoch [17/50], Train Loss: 0.0089, Val Loss: 0.0183\n",
      "Epoch [18/50], Train Loss: 0.0084, Val Loss: 0.0194\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=16, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.2193, Val Loss: 0.4758\n",
      "Epoch [2/50], Train Loss: 0.1765, Val Loss: 0.3926\n",
      "Epoch [3/50], Train Loss: 0.1314, Val Loss: 0.2797\n",
      "Epoch [4/50], Train Loss: 0.0852, Val Loss: 0.1477\n",
      "Epoch [5/50], Train Loss: 0.0633, Val Loss: 0.0955\n",
      "Epoch [6/50], Train Loss: 0.0603, Val Loss: 0.0761\n",
      "Epoch [7/50], Train Loss: 0.0560, Val Loss: 0.0655\n",
      "Epoch [8/50], Train Loss: 0.0487, Val Loss: 0.0512\n",
      "Epoch [9/50], Train Loss: 0.0471, Val Loss: 0.0454\n",
      "Epoch [10/50], Train Loss: 0.0464, Val Loss: 0.0384\n",
      "Epoch [11/50], Train Loss: 0.0435, Val Loss: 0.0328\n",
      "Epoch [12/50], Train Loss: 0.0415, Val Loss: 0.0342\n",
      "Epoch [13/50], Train Loss: 0.0396, Val Loss: 0.0276\n",
      "Epoch [14/50], Train Loss: 0.0384, Val Loss: 0.0240\n",
      "Epoch [15/50], Train Loss: 0.0362, Val Loss: 0.0263\n",
      "Epoch [16/50], Train Loss: 0.0355, Val Loss: 0.0222\n",
      "Epoch [17/50], Train Loss: 0.0344, Val Loss: 0.0187\n",
      "Epoch [18/50], Train Loss: 0.0345, Val Loss: 0.0184\n",
      "Epoch [19/50], Train Loss: 0.0314, Val Loss: 0.0167\n",
      "Epoch [20/50], Train Loss: 0.0321, Val Loss: 0.0163\n",
      "Epoch [21/50], Train Loss: 0.0304, Val Loss: 0.0146\n",
      "Epoch [22/50], Train Loss: 0.0308, Val Loss: 0.0132\n",
      "Epoch [23/50], Train Loss: 0.0288, Val Loss: 0.0107\n",
      "Epoch [24/50], Train Loss: 0.0276, Val Loss: 0.0097\n",
      "Epoch [25/50], Train Loss: 0.0260, Val Loss: 0.0107\n",
      "Epoch [26/50], Train Loss: 0.0248, Val Loss: 0.0062\n",
      "Epoch [27/50], Train Loss: 0.0248, Val Loss: 0.0136\n",
      "Epoch [28/50], Train Loss: 0.0244, Val Loss: 0.0056\n",
      "Epoch [29/50], Train Loss: 0.0229, Val Loss: 0.0062\n",
      "Epoch [30/50], Train Loss: 0.0221, Val Loss: 0.0088\n",
      "Epoch [31/50], Train Loss: 0.0205, Val Loss: 0.0098\n",
      "Epoch [32/50], Train Loss: 0.0199, Val Loss: 0.0118\n",
      "Epoch [33/50], Train Loss: 0.0193, Val Loss: 0.0119\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=16, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1077, Val Loss: 0.2544\n",
      "Epoch [2/50], Train Loss: 0.0754, Val Loss: 0.2000\n",
      "Epoch [3/50], Train Loss: 0.0536, Val Loss: 0.1474\n",
      "Epoch [4/50], Train Loss: 0.0416, Val Loss: 0.1110\n",
      "Epoch [5/50], Train Loss: 0.0380, Val Loss: 0.0961\n",
      "Epoch [6/50], Train Loss: 0.0361, Val Loss: 0.0888\n",
      "Epoch [7/50], Train Loss: 0.0344, Val Loss: 0.0836\n",
      "Epoch [8/50], Train Loss: 0.0323, Val Loss: 0.0788\n",
      "Epoch [9/50], Train Loss: 0.0293, Val Loss: 0.0742\n",
      "Epoch [10/50], Train Loss: 0.0254, Val Loss: 0.0700\n",
      "Epoch [11/50], Train Loss: 0.0226, Val Loss: 0.0705\n",
      "Epoch [12/50], Train Loss: 0.0198, Val Loss: 0.0610\n",
      "Epoch [13/50], Train Loss: 0.0180, Val Loss: 0.0512\n",
      "Epoch [14/50], Train Loss: 0.0161, Val Loss: 0.0433\n",
      "Epoch [15/50], Train Loss: 0.0138, Val Loss: 0.0357\n",
      "Epoch [16/50], Train Loss: 0.0115, Val Loss: 0.0297\n",
      "Epoch [17/50], Train Loss: 0.0094, Val Loss: 0.0267\n",
      "Epoch [18/50], Train Loss: 0.0077, Val Loss: 0.0249\n",
      "Epoch [19/50], Train Loss: 0.0065, Val Loss: 0.0240\n",
      "Epoch [20/50], Train Loss: 0.0057, Val Loss: 0.0236\n",
      "Epoch [21/50], Train Loss: 0.0051, Val Loss: 0.0233\n",
      "Epoch [22/50], Train Loss: 0.0047, Val Loss: 0.0231\n",
      "Epoch [23/50], Train Loss: 0.0044, Val Loss: 0.0228\n",
      "Epoch [24/50], Train Loss: 0.0041, Val Loss: 0.0224\n",
      "Epoch [25/50], Train Loss: 0.0039, Val Loss: 0.0222\n",
      "Epoch [26/50], Train Loss: 0.0037, Val Loss: 0.0219\n",
      "Epoch [27/50], Train Loss: 0.0036, Val Loss: 0.0213\n",
      "Epoch [28/50], Train Loss: 0.0035, Val Loss: 0.0211\n",
      "Epoch [29/50], Train Loss: 0.0034, Val Loss: 0.0210\n",
      "Epoch [30/50], Train Loss: 0.0033, Val Loss: 0.0203\n",
      "Epoch [31/50], Train Loss: 0.0032, Val Loss: 0.0198\n",
      "Epoch [32/50], Train Loss: 0.0032, Val Loss: 0.0200\n",
      "Epoch [33/50], Train Loss: 0.0031, Val Loss: 0.0195\n",
      "Epoch [34/50], Train Loss: 0.0031, Val Loss: 0.0186\n",
      "Epoch [35/50], Train Loss: 0.0031, Val Loss: 0.0189\n",
      "Epoch [36/50], Train Loss: 0.0030, Val Loss: 0.0190\n",
      "Epoch [37/50], Train Loss: 0.0029, Val Loss: 0.0176\n",
      "Epoch [38/50], Train Loss: 0.0029, Val Loss: 0.0173\n",
      "Epoch [39/50], Train Loss: 0.0030, Val Loss: 0.0186\n",
      "Epoch [40/50], Train Loss: 0.0029, Val Loss: 0.0173\n",
      "Epoch [41/50], Train Loss: 0.0029, Val Loss: 0.0156\n",
      "Epoch [42/50], Train Loss: 0.0030, Val Loss: 0.0176\n",
      "Epoch [43/50], Train Loss: 0.0029, Val Loss: 0.0179\n",
      "Epoch [44/50], Train Loss: 0.0028, Val Loss: 0.0146\n",
      "Epoch [45/50], Train Loss: 0.0029, Val Loss: 0.0151\n",
      "Epoch [46/50], Train Loss: 0.0032, Val Loss: 0.0195\n",
      "Epoch [47/50], Train Loss: 0.0028, Val Loss: 0.0148\n",
      "Epoch [48/50], Train Loss: 0.0031, Val Loss: 0.0123\n",
      "Epoch [49/50], Train Loss: 0.0035, Val Loss: 0.0203\n",
      "Epoch [50/50], Train Loss: 0.0034, Val Loss: 0.0160\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=16, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1087, Val Loss: 0.2603\n",
      "Epoch [2/50], Train Loss: 0.0743, Val Loss: 0.1770\n",
      "Epoch [3/50], Train Loss: 0.0435, Val Loss: 0.0950\n",
      "Epoch [4/50], Train Loss: 0.0381, Val Loss: 0.0742\n",
      "Epoch [5/50], Train Loss: 0.0363, Val Loss: 0.0658\n",
      "Epoch [6/50], Train Loss: 0.0344, Val Loss: 0.0558\n",
      "Epoch [7/50], Train Loss: 0.0316, Val Loss: 0.0436\n",
      "Epoch [8/50], Train Loss: 0.0271, Val Loss: 0.0334\n",
      "Epoch [9/50], Train Loss: 0.0218, Val Loss: 0.0249\n",
      "Epoch [10/50], Train Loss: 0.0175, Val Loss: 0.0242\n",
      "Epoch [11/50], Train Loss: 0.0146, Val Loss: 0.0205\n",
      "Epoch [12/50], Train Loss: 0.0134, Val Loss: 0.0218\n",
      "Epoch [13/50], Train Loss: 0.0132, Val Loss: 0.0166\n",
      "Epoch [14/50], Train Loss: 0.0135, Val Loss: 0.0220\n",
      "Epoch [15/50], Train Loss: 0.0117, Val Loss: 0.0152\n",
      "Epoch [16/50], Train Loss: 0.0115, Val Loss: 0.0219\n",
      "Epoch [17/50], Train Loss: 0.0104, Val Loss: 0.0137\n",
      "Epoch [18/50], Train Loss: 0.0104, Val Loss: 0.0175\n",
      "Epoch [19/50], Train Loss: 0.0104, Val Loss: 0.0186\n",
      "Epoch [20/50], Train Loss: 0.0099, Val Loss: 0.0155\n",
      "Epoch [21/50], Train Loss: 0.0102, Val Loss: 0.0153\n",
      "Epoch [22/50], Train Loss: 0.0097, Val Loss: 0.0205\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=16, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1061, Val Loss: 0.2245\n",
      "Epoch [2/50], Train Loss: 0.0826, Val Loss: 0.1767\n",
      "Epoch [3/50], Train Loss: 0.0665, Val Loss: 0.1229\n",
      "Epoch [4/50], Train Loss: 0.0551, Val Loss: 0.0850\n",
      "Epoch [5/50], Train Loss: 0.0466, Val Loss: 0.0686\n",
      "Epoch [6/50], Train Loss: 0.0444, Val Loss: 0.0578\n",
      "Epoch [7/50], Train Loss: 0.0383, Val Loss: 0.0468\n",
      "Epoch [8/50], Train Loss: 0.0366, Val Loss: 0.0418\n",
      "Epoch [9/50], Train Loss: 0.0331, Val Loss: 0.0375\n",
      "Epoch [10/50], Train Loss: 0.0309, Val Loss: 0.0339\n",
      "Epoch [11/50], Train Loss: 0.0296, Val Loss: 0.0336\n",
      "Epoch [12/50], Train Loss: 0.0276, Val Loss: 0.0367\n",
      "Epoch [13/50], Train Loss: 0.0260, Val Loss: 0.0358\n",
      "Epoch [14/50], Train Loss: 0.0239, Val Loss: 0.0342\n",
      "Epoch [15/50], Train Loss: 0.0204, Val Loss: 0.0357\n",
      "Epoch [16/50], Train Loss: 0.0196, Val Loss: 0.0324\n",
      "Epoch [17/50], Train Loss: 0.0187, Val Loss: 0.0353\n",
      "Epoch [18/50], Train Loss: 0.0174, Val Loss: 0.0343\n",
      "Epoch [19/50], Train Loss: 0.0182, Val Loss: 0.0300\n",
      "Epoch [20/50], Train Loss: 0.0169, Val Loss: 0.0289\n",
      "Epoch [21/50], Train Loss: 0.0173, Val Loss: 0.0297\n",
      "Epoch [22/50], Train Loss: 0.0170, Val Loss: 0.0302\n",
      "Epoch [23/50], Train Loss: 0.0174, Val Loss: 0.0298\n",
      "Epoch [24/50], Train Loss: 0.0161, Val Loss: 0.0279\n",
      "Epoch [25/50], Train Loss: 0.0164, Val Loss: 0.0253\n",
      "Epoch [26/50], Train Loss: 0.0152, Val Loss: 0.0276\n",
      "Epoch [27/50], Train Loss: 0.0145, Val Loss: 0.0243\n",
      "Epoch [28/50], Train Loss: 0.0146, Val Loss: 0.0244\n",
      "Epoch [29/50], Train Loss: 0.0140, Val Loss: 0.0270\n",
      "Epoch [30/50], Train Loss: 0.0134, Val Loss: 0.0267\n",
      "Epoch [31/50], Train Loss: 0.0138, Val Loss: 0.0240\n",
      "Epoch [32/50], Train Loss: 0.0125, Val Loss: 0.0269\n",
      "Epoch [33/50], Train Loss: 0.0128, Val Loss: 0.0250\n",
      "Epoch [34/50], Train Loss: 0.0129, Val Loss: 0.0256\n",
      "Epoch [35/50], Train Loss: 0.0127, Val Loss: 0.0254\n",
      "Epoch [36/50], Train Loss: 0.0127, Val Loss: 0.0259\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=16, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1754, Val Loss: 0.3652\n",
      "Epoch [2/50], Train Loss: 0.1101, Val Loss: 0.2158\n",
      "Epoch [3/50], Train Loss: 0.0472, Val Loss: 0.0815\n",
      "Epoch [4/50], Train Loss: 0.0431, Val Loss: 0.0870\n",
      "Epoch [5/50], Train Loss: 0.0399, Val Loss: 0.0888\n",
      "Epoch [6/50], Train Loss: 0.0377, Val Loss: 0.0835\n",
      "Epoch [7/50], Train Loss: 0.0358, Val Loss: 0.0767\n",
      "Epoch [8/50], Train Loss: 0.0331, Val Loss: 0.0676\n",
      "Epoch [9/50], Train Loss: 0.0294, Val Loss: 0.0583\n",
      "Epoch [10/50], Train Loss: 0.0258, Val Loss: 0.0510\n",
      "Epoch [11/50], Train Loss: 0.0238, Val Loss: 0.0460\n",
      "Epoch [12/50], Train Loss: 0.0226, Val Loss: 0.0421\n",
      "Epoch [13/50], Train Loss: 0.0217, Val Loss: 0.0382\n",
      "Epoch [14/50], Train Loss: 0.0209, Val Loss: 0.0345\n",
      "Epoch [15/50], Train Loss: 0.0204, Val Loss: 0.0319\n",
      "Epoch [16/50], Train Loss: 0.0199, Val Loss: 0.0309\n",
      "Epoch [17/50], Train Loss: 0.0194, Val Loss: 0.0317\n",
      "Epoch [18/50], Train Loss: 0.0188, Val Loss: 0.0328\n",
      "Epoch [19/50], Train Loss: 0.0181, Val Loss: 0.0323\n",
      "Epoch [20/50], Train Loss: 0.0174, Val Loss: 0.0309\n",
      "Epoch [21/50], Train Loss: 0.0165, Val Loss: 0.0320\n",
      "Epoch [22/50], Train Loss: 0.0149, Val Loss: 0.0415\n",
      "Epoch [23/50], Train Loss: 0.0120, Val Loss: 0.0553\n",
      "Epoch [24/50], Train Loss: 0.0085, Val Loss: 0.0463\n",
      "Epoch [25/50], Train Loss: 0.0067, Val Loss: 0.0505\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=16, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1645, Val Loss: 0.3769\n",
      "Epoch [2/50], Train Loss: 0.1089, Val Loss: 0.2520\n",
      "Epoch [3/50], Train Loss: 0.0535, Val Loss: 0.1076\n",
      "Epoch [4/50], Train Loss: 0.0524, Val Loss: 0.1014\n",
      "Epoch [5/50], Train Loss: 0.0495, Val Loss: 0.0918\n",
      "Epoch [6/50], Train Loss: 0.0483, Val Loss: 0.0896\n",
      "Epoch [7/50], Train Loss: 0.0457, Val Loss: 0.0783\n",
      "Epoch [8/50], Train Loss: 0.0434, Val Loss: 0.0653\n",
      "Epoch [9/50], Train Loss: 0.0409, Val Loss: 0.0507\n",
      "Epoch [10/50], Train Loss: 0.0364, Val Loss: 0.0422\n",
      "Epoch [11/50], Train Loss: 0.0343, Val Loss: 0.0417\n",
      "Epoch [12/50], Train Loss: 0.0319, Val Loss: 0.0341\n",
      "Epoch [13/50], Train Loss: 0.0312, Val Loss: 0.0295\n",
      "Epoch [14/50], Train Loss: 0.0300, Val Loss: 0.0311\n",
      "Epoch [15/50], Train Loss: 0.0277, Val Loss: 0.0316\n",
      "Epoch [16/50], Train Loss: 0.0269, Val Loss: 0.0292\n",
      "Epoch [17/50], Train Loss: 0.0237, Val Loss: 0.0324\n",
      "Epoch [18/50], Train Loss: 0.0212, Val Loss: 0.0312\n",
      "Epoch [19/50], Train Loss: 0.0199, Val Loss: 0.0290\n",
      "Epoch [20/50], Train Loss: 0.0180, Val Loss: 0.0430\n",
      "Epoch [21/50], Train Loss: 0.0172, Val Loss: 0.0378\n",
      "Epoch [22/50], Train Loss: 0.0175, Val Loss: 0.0264\n",
      "Epoch [23/50], Train Loss: 0.0154, Val Loss: 0.0440\n",
      "Epoch [24/50], Train Loss: 0.0146, Val Loss: 0.0367\n",
      "Epoch [25/50], Train Loss: 0.0150, Val Loss: 0.0380\n",
      "Epoch [26/50], Train Loss: 0.0158, Val Loss: 0.0378\n",
      "Epoch [27/50], Train Loss: 0.0144, Val Loss: 0.0331\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=16, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1051, Val Loss: 0.2856\n",
      "Epoch [2/50], Train Loss: 0.0764, Val Loss: 0.2019\n",
      "Epoch [3/50], Train Loss: 0.0600, Val Loss: 0.1268\n",
      "Epoch [4/50], Train Loss: 0.0608, Val Loss: 0.1143\n",
      "Epoch [5/50], Train Loss: 0.0573, Val Loss: 0.1036\n",
      "Epoch [6/50], Train Loss: 0.0540, Val Loss: 0.0882\n",
      "Epoch [7/50], Train Loss: 0.0478, Val Loss: 0.0586\n",
      "Epoch [8/50], Train Loss: 0.0421, Val Loss: 0.0385\n",
      "Epoch [9/50], Train Loss: 0.0373, Val Loss: 0.0443\n",
      "Epoch [10/50], Train Loss: 0.0362, Val Loss: 0.0370\n",
      "Epoch [11/50], Train Loss: 0.0355, Val Loss: 0.0357\n",
      "Epoch [12/50], Train Loss: 0.0335, Val Loss: 0.0413\n",
      "Epoch [13/50], Train Loss: 0.0325, Val Loss: 0.0389\n",
      "Epoch [14/50], Train Loss: 0.0328, Val Loss: 0.0354\n",
      "Epoch [15/50], Train Loss: 0.0310, Val Loss: 0.0409\n",
      "Epoch [16/50], Train Loss: 0.0319, Val Loss: 0.0359\n",
      "Epoch [17/50], Train Loss: 0.0297, Val Loss: 0.0373\n",
      "Epoch [18/50], Train Loss: 0.0294, Val Loss: 0.0366\n",
      "Epoch [19/50], Train Loss: 0.0296, Val Loss: 0.0330\n",
      "Epoch [20/50], Train Loss: 0.0290, Val Loss: 0.0347\n",
      "Epoch [21/50], Train Loss: 0.0274, Val Loss: 0.0328\n",
      "Epoch [22/50], Train Loss: 0.0268, Val Loss: 0.0284\n",
      "Epoch [23/50], Train Loss: 0.0265, Val Loss: 0.0296\n",
      "Epoch [24/50], Train Loss: 0.0248, Val Loss: 0.0244\n",
      "Epoch [25/50], Train Loss: 0.0248, Val Loss: 0.0238\n",
      "Epoch [26/50], Train Loss: 0.0217, Val Loss: 0.0280\n",
      "Epoch [27/50], Train Loss: 0.0204, Val Loss: 0.0221\n",
      "Epoch [28/50], Train Loss: 0.0222, Val Loss: 0.0389\n",
      "Epoch [29/50], Train Loss: 0.0200, Val Loss: 0.0330\n",
      "Epoch [30/50], Train Loss: 0.0192, Val Loss: 0.0363\n",
      "Epoch [31/50], Train Loss: 0.0204, Val Loss: 0.0367\n",
      "Epoch [32/50], Train Loss: 0.0210, Val Loss: 0.0340\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=32, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1500, Val Loss: 0.3159\n",
      "Epoch [2/50], Train Loss: 0.0846, Val Loss: 0.1464\n",
      "Epoch [3/50], Train Loss: 0.0274, Val Loss: 0.0347\n",
      "Epoch [4/50], Train Loss: 0.0264, Val Loss: 0.0311\n",
      "Epoch [5/50], Train Loss: 0.0211, Val Loss: 0.0192\n",
      "Epoch [6/50], Train Loss: 0.0162, Val Loss: 0.0097\n",
      "Epoch [7/50], Train Loss: 0.0092, Val Loss: 0.0114\n",
      "Epoch [8/50], Train Loss: 0.0053, Val Loss: 0.0113\n",
      "Epoch [9/50], Train Loss: 0.0056, Val Loss: 0.0087\n",
      "Epoch [10/50], Train Loss: 0.0038, Val Loss: 0.0128\n",
      "Epoch [11/50], Train Loss: 0.0034, Val Loss: 0.0112\n",
      "Epoch [12/50], Train Loss: 0.0073, Val Loss: 0.0076\n",
      "Epoch [13/50], Train Loss: 0.0052, Val Loss: 0.0176\n",
      "Epoch [14/50], Train Loss: 0.0086, Val Loss: 0.0066\n",
      "Epoch [15/50], Train Loss: 0.0052, Val Loss: 0.0139\n",
      "Epoch [16/50], Train Loss: 0.0041, Val Loss: 0.0102\n",
      "Epoch [17/50], Train Loss: 0.0038, Val Loss: 0.0101\n",
      "Epoch [18/50], Train Loss: 0.0033, Val Loss: 0.0115\n",
      "Epoch [19/50], Train Loss: 0.0031, Val Loss: 0.0095\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=32, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1261, Val Loss: 0.2980\n",
      "Epoch [2/50], Train Loss: 0.0805, Val Loss: 0.1834\n",
      "Epoch [3/50], Train Loss: 0.0399, Val Loss: 0.0766\n",
      "Epoch [4/50], Train Loss: 0.0375, Val Loss: 0.0688\n",
      "Epoch [5/50], Train Loss: 0.0341, Val Loss: 0.0558\n",
      "Epoch [6/50], Train Loss: 0.0305, Val Loss: 0.0462\n",
      "Epoch [7/50], Train Loss: 0.0274, Val Loss: 0.0373\n",
      "Epoch [8/50], Train Loss: 0.0229, Val Loss: 0.0254\n",
      "Epoch [9/50], Train Loss: 0.0189, Val Loss: 0.0156\n",
      "Epoch [10/50], Train Loss: 0.0135, Val Loss: 0.0125\n",
      "Epoch [11/50], Train Loss: 0.0108, Val Loss: 0.0123\n",
      "Epoch [12/50], Train Loss: 0.0100, Val Loss: 0.0074\n",
      "Epoch [13/50], Train Loss: 0.0095, Val Loss: 0.0095\n",
      "Epoch [14/50], Train Loss: 0.0150, Val Loss: 0.0069\n",
      "Epoch [15/50], Train Loss: 0.0105, Val Loss: 0.0084\n",
      "Epoch [16/50], Train Loss: 0.0089, Val Loss: 0.0080\n",
      "Epoch [17/50], Train Loss: 0.0083, Val Loss: 0.0092\n",
      "Epoch [18/50], Train Loss: 0.0080, Val Loss: 0.0070\n",
      "Epoch [19/50], Train Loss: 0.0080, Val Loss: 0.0070\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=32, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0855, Val Loss: 0.2374\n",
      "Epoch [2/50], Train Loss: 0.0587, Val Loss: 0.1728\n",
      "Epoch [3/50], Train Loss: 0.0438, Val Loss: 0.1164\n",
      "Epoch [4/50], Train Loss: 0.0368, Val Loss: 0.0811\n",
      "Epoch [5/50], Train Loss: 0.0341, Val Loss: 0.0646\n",
      "Epoch [6/50], Train Loss: 0.0303, Val Loss: 0.0517\n",
      "Epoch [7/50], Train Loss: 0.0264, Val Loss: 0.0386\n",
      "Epoch [8/50], Train Loss: 0.0217, Val Loss: 0.0304\n",
      "Epoch [9/50], Train Loss: 0.0189, Val Loss: 0.0221\n",
      "Epoch [10/50], Train Loss: 0.0171, Val Loss: 0.0156\n",
      "Epoch [11/50], Train Loss: 0.0151, Val Loss: 0.0167\n",
      "Epoch [12/50], Train Loss: 0.0140, Val Loss: 0.0151\n",
      "Epoch [13/50], Train Loss: 0.0128, Val Loss: 0.0140\n",
      "Epoch [14/50], Train Loss: 0.0122, Val Loss: 0.0216\n",
      "Epoch [15/50], Train Loss: 0.0121, Val Loss: 0.0065\n",
      "Epoch [16/50], Train Loss: 0.0108, Val Loss: 0.0160\n",
      "Epoch [17/50], Train Loss: 0.0105, Val Loss: 0.0099\n",
      "Epoch [18/50], Train Loss: 0.0106, Val Loss: 0.0087\n",
      "Epoch [19/50], Train Loss: 0.0101, Val Loss: 0.0124\n",
      "Epoch [20/50], Train Loss: 0.0102, Val Loss: 0.0119\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=32, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1088, Val Loss: 0.2847\n",
      "Epoch [2/50], Train Loss: 0.0536, Val Loss: 0.1151\n",
      "Epoch [3/50], Train Loss: 0.0354, Val Loss: 0.0506\n",
      "Epoch [4/50], Train Loss: 0.0377, Val Loss: 0.0453\n",
      "Epoch [5/50], Train Loss: 0.0312, Val Loss: 0.0246\n",
      "Epoch [6/50], Train Loss: 0.0272, Val Loss: 0.0107\n",
      "Epoch [7/50], Train Loss: 0.0220, Val Loss: 0.0059\n",
      "Epoch [8/50], Train Loss: 0.0154, Val Loss: 0.0084\n",
      "Epoch [9/50], Train Loss: 0.0120, Val Loss: 0.0093\n",
      "Epoch [10/50], Train Loss: 0.0100, Val Loss: 0.0432\n",
      "Epoch [11/50], Train Loss: 0.0081, Val Loss: 0.0159\n",
      "Epoch [12/50], Train Loss: 0.0057, Val Loss: 0.0143\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=32, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0940, Val Loss: 0.2321\n",
      "Epoch [2/50], Train Loss: 0.0469, Val Loss: 0.0970\n",
      "Epoch [3/50], Train Loss: 0.0403, Val Loss: 0.0706\n",
      "Epoch [4/50], Train Loss: 0.0392, Val Loss: 0.0620\n",
      "Epoch [5/50], Train Loss: 0.0343, Val Loss: 0.0412\n",
      "Epoch [6/50], Train Loss: 0.0304, Val Loss: 0.0197\n",
      "Epoch [7/50], Train Loss: 0.0257, Val Loss: 0.0085\n",
      "Epoch [8/50], Train Loss: 0.0221, Val Loss: 0.0062\n",
      "Epoch [9/50], Train Loss: 0.0147, Val Loss: 0.0161\n",
      "Epoch [10/50], Train Loss: 0.0124, Val Loss: 0.0184\n",
      "Epoch [11/50], Train Loss: 0.0122, Val Loss: 0.0146\n",
      "Epoch [12/50], Train Loss: 0.0097, Val Loss: 0.0229\n",
      "Epoch [13/50], Train Loss: 0.0094, Val Loss: 0.0141\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=32, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1451, Val Loss: 0.2820\n",
      "Epoch [2/50], Train Loss: 0.0688, Val Loss: 0.0781\n",
      "Epoch [3/50], Train Loss: 0.0550, Val Loss: 0.0713\n",
      "Epoch [4/50], Train Loss: 0.0498, Val Loss: 0.0582\n",
      "Epoch [5/50], Train Loss: 0.0460, Val Loss: 0.0456\n",
      "Epoch [6/50], Train Loss: 0.0411, Val Loss: 0.0347\n",
      "Epoch [7/50], Train Loss: 0.0372, Val Loss: 0.0267\n",
      "Epoch [8/50], Train Loss: 0.0335, Val Loss: 0.0212\n",
      "Epoch [9/50], Train Loss: 0.0270, Val Loss: 0.0192\n",
      "Epoch [10/50], Train Loss: 0.0250, Val Loss: 0.0254\n",
      "Epoch [11/50], Train Loss: 0.0211, Val Loss: 0.0170\n",
      "Epoch [12/50], Train Loss: 0.0204, Val Loss: 0.0093\n",
      "Epoch [13/50], Train Loss: 0.0216, Val Loss: 0.0283\n",
      "Epoch [14/50], Train Loss: 0.0194, Val Loss: 0.0194\n",
      "Epoch [15/50], Train Loss: 0.0233, Val Loss: 0.0104\n",
      "Epoch [16/50], Train Loss: 0.0197, Val Loss: 0.0235\n",
      "Epoch [17/50], Train Loss: 0.0218, Val Loss: 0.0153\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=32, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1499, Val Loss: 0.3098\n",
      "Epoch [2/50], Train Loss: 0.0459, Val Loss: 0.0587\n",
      "Epoch [3/50], Train Loss: 0.0511, Val Loss: 0.0847\n",
      "Epoch [4/50], Train Loss: 0.0365, Val Loss: 0.0561\n",
      "Epoch [5/50], Train Loss: 0.0372, Val Loss: 0.0453\n",
      "Epoch [6/50], Train Loss: 0.0303, Val Loss: 0.0318\n",
      "Epoch [7/50], Train Loss: 0.0233, Val Loss: 0.0251\n",
      "Epoch [8/50], Train Loss: 0.0210, Val Loss: 0.0235\n",
      "Epoch [9/50], Train Loss: 0.0205, Val Loss: 0.0247\n",
      "Epoch [10/50], Train Loss: 0.0204, Val Loss: 0.0321\n",
      "Epoch [11/50], Train Loss: 0.0200, Val Loss: 0.0490\n",
      "Epoch [12/50], Train Loss: 0.0208, Val Loss: 0.0297\n",
      "Epoch [13/50], Train Loss: 0.0196, Val Loss: 0.0234\n",
      "Epoch [14/50], Train Loss: 0.0168, Val Loss: 0.0242\n",
      "Epoch [15/50], Train Loss: 0.0139, Val Loss: 0.0415\n",
      "Epoch [16/50], Train Loss: 0.0082, Val Loss: 0.0430\n",
      "Epoch [17/50], Train Loss: 0.0161, Val Loss: 0.0326\n",
      "Epoch [18/50], Train Loss: 0.0058, Val Loss: 0.0451\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=32, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0957, Val Loss: 0.2294\n",
      "Epoch [2/50], Train Loss: 0.0434, Val Loss: 0.0758\n",
      "Epoch [3/50], Train Loss: 0.0504, Val Loss: 0.0967\n",
      "Epoch [4/50], Train Loss: 0.0416, Val Loss: 0.0738\n",
      "Epoch [5/50], Train Loss: 0.0423, Val Loss: 0.0667\n",
      "Epoch [6/50], Train Loss: 0.0375, Val Loss: 0.0477\n",
      "Epoch [7/50], Train Loss: 0.0340, Val Loss: 0.0306\n",
      "Epoch [8/50], Train Loss: 0.0283, Val Loss: 0.0259\n",
      "Epoch [9/50], Train Loss: 0.0258, Val Loss: 0.0221\n",
      "Epoch [10/50], Train Loss: 0.0237, Val Loss: 0.0300\n",
      "Epoch [11/50], Train Loss: 0.0226, Val Loss: 0.0346\n",
      "Epoch [12/50], Train Loss: 0.0210, Val Loss: 0.0565\n",
      "Epoch [13/50], Train Loss: 0.0162, Val Loss: 0.0331\n",
      "Epoch [14/50], Train Loss: 0.0133, Val Loss: 0.0476\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=32, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0901, Val Loss: 0.2338\n",
      "Epoch [2/50], Train Loss: 0.0504, Val Loss: 0.0945\n",
      "Epoch [3/50], Train Loss: 0.0532, Val Loss: 0.1052\n",
      "Epoch [4/50], Train Loss: 0.0482, Val Loss: 0.0869\n",
      "Epoch [5/50], Train Loss: 0.0445, Val Loss: 0.0657\n",
      "Epoch [6/50], Train Loss: 0.0430, Val Loss: 0.0401\n",
      "Epoch [7/50], Train Loss: 0.0368, Val Loss: 0.0242\n",
      "Epoch [8/50], Train Loss: 0.0339, Val Loss: 0.0195\n",
      "Epoch [9/50], Train Loss: 0.0287, Val Loss: 0.0208\n",
      "Epoch [10/50], Train Loss: 0.0240, Val Loss: 0.0721\n",
      "Epoch [11/50], Train Loss: 0.0232, Val Loss: 0.0373\n",
      "Epoch [12/50], Train Loss: 0.0222, Val Loss: 0.0591\n",
      "Epoch [13/50], Train Loss: 0.0193, Val Loss: 0.0577\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=64, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0683, Val Loss: 0.1690\n",
      "Epoch [2/50], Train Loss: 0.0344, Val Loss: 0.0689\n",
      "Epoch [3/50], Train Loss: 0.0319, Val Loss: 0.0439\n",
      "Epoch [4/50], Train Loss: 0.0277, Val Loss: 0.0164\n",
      "Epoch [5/50], Train Loss: 0.0228, Val Loss: 0.0089\n",
      "Epoch [6/50], Train Loss: 0.0169, Val Loss: 0.0291\n",
      "Epoch [7/50], Train Loss: 0.0128, Val Loss: 0.0175\n",
      "Epoch [8/50], Train Loss: 0.0108, Val Loss: 0.0202\n",
      "Epoch [9/50], Train Loss: 0.0073, Val Loss: 0.0198\n",
      "Epoch [10/50], Train Loss: 0.0151, Val Loss: 0.0045\n",
      "Epoch [11/50], Train Loss: 0.0058, Val Loss: 0.0164\n",
      "Epoch [12/50], Train Loss: 0.0080, Val Loss: 0.0121\n",
      "Epoch [13/50], Train Loss: 0.0045, Val Loss: 0.0181\n",
      "Epoch [14/50], Train Loss: 0.0037, Val Loss: 0.0145\n",
      "Epoch [15/50], Train Loss: 0.0033, Val Loss: 0.0161\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=64, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0895, Val Loss: 0.2055\n",
      "Epoch [2/50], Train Loss: 0.0298, Val Loss: 0.0317\n",
      "Epoch [3/50], Train Loss: 0.0421, Val Loss: 0.0523\n",
      "Epoch [4/50], Train Loss: 0.0283, Val Loss: 0.0210\n",
      "Epoch [5/50], Train Loss: 0.0259, Val Loss: 0.0103\n",
      "Epoch [6/50], Train Loss: 0.0219, Val Loss: 0.0063\n",
      "Epoch [7/50], Train Loss: 0.0178, Val Loss: 0.0131\n",
      "Epoch [8/50], Train Loss: 0.0159, Val Loss: 0.0081\n",
      "Epoch [9/50], Train Loss: 0.0124, Val Loss: 0.0053\n",
      "Epoch [10/50], Train Loss: 0.0119, Val Loss: 0.0097\n",
      "Epoch [11/50], Train Loss: 0.0077, Val Loss: 0.0152\n",
      "Epoch [12/50], Train Loss: 0.0087, Val Loss: 0.0129\n",
      "Epoch [13/50], Train Loss: 0.0070, Val Loss: 0.0088\n",
      "Epoch [14/50], Train Loss: 0.0060, Val Loss: 0.0153\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=64, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0955, Val Loss: 0.2163\n",
      "Epoch [2/50], Train Loss: 0.0389, Val Loss: 0.0472\n",
      "Epoch [3/50], Train Loss: 0.0401, Val Loss: 0.0409\n",
      "Epoch [4/50], Train Loss: 0.0324, Val Loss: 0.0228\n",
      "Epoch [5/50], Train Loss: 0.0277, Val Loss: 0.0093\n",
      "Epoch [6/50], Train Loss: 0.0238, Val Loss: 0.0081\n",
      "Epoch [7/50], Train Loss: 0.0185, Val Loss: 0.0110\n",
      "Epoch [8/50], Train Loss: 0.0150, Val Loss: 0.0270\n",
      "Epoch [9/50], Train Loss: 0.0136, Val Loss: 0.0082\n",
      "Epoch [10/50], Train Loss: 0.0133, Val Loss: 0.0232\n",
      "Epoch [11/50], Train Loss: 0.0119, Val Loss: 0.0134\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=64, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0877, Val Loss: 0.1770\n",
      "Epoch [2/50], Train Loss: 0.0324, Val Loss: 0.0401\n",
      "Epoch [3/50], Train Loss: 0.0474, Val Loss: 0.0616\n",
      "Epoch [4/50], Train Loss: 0.0327, Val Loss: 0.0302\n",
      "Epoch [5/50], Train Loss: 0.0293, Val Loss: 0.0093\n",
      "Epoch [6/50], Train Loss: 0.0229, Val Loss: 0.0047\n",
      "Epoch [7/50], Train Loss: 0.0173, Val Loss: 0.0346\n",
      "Epoch [8/50], Train Loss: 0.0076, Val Loss: 0.0120\n",
      "Epoch [9/50], Train Loss: 0.0118, Val Loss: 0.0074\n",
      "Epoch [10/50], Train Loss: 0.0074, Val Loss: 0.0404\n",
      "Epoch [11/50], Train Loss: 0.0060, Val Loss: 0.0124\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=64, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0778, Val Loss: 0.1619\n",
      "Epoch [2/50], Train Loss: 0.0338, Val Loss: 0.0356\n",
      "Epoch [3/50], Train Loss: 0.0439, Val Loss: 0.0397\n",
      "Epoch [4/50], Train Loss: 0.0263, Val Loss: 0.0174\n",
      "Epoch [5/50], Train Loss: 0.0190, Val Loss: 0.0381\n",
      "Epoch [6/50], Train Loss: 0.0162, Val Loss: 0.0403\n",
      "Epoch [7/50], Train Loss: 0.0102, Val Loss: 0.0426\n",
      "Epoch [8/50], Train Loss: 0.0096, Val Loss: 0.0502\n",
      "Epoch [9/50], Train Loss: 0.0191, Val Loss: 0.0357\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=64, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0838, Val Loss: 0.1583\n",
      "Epoch [2/50], Train Loss: 0.0426, Val Loss: 0.0444\n",
      "Epoch [3/50], Train Loss: 0.0504, Val Loss: 0.0496\n",
      "Epoch [4/50], Train Loss: 0.0357, Val Loss: 0.0146\n",
      "Epoch [5/50], Train Loss: 0.0293, Val Loss: 0.0161\n",
      "Epoch [6/50], Train Loss: 0.0271, Val Loss: 0.0106\n",
      "Epoch [7/50], Train Loss: 0.0252, Val Loss: 0.0080\n",
      "Epoch [8/50], Train Loss: 0.0236, Val Loss: 0.0566\n",
      "Epoch [9/50], Train Loss: 0.0188, Val Loss: 0.0095\n",
      "Epoch [10/50], Train Loss: 0.0175, Val Loss: 0.0130\n",
      "Epoch [11/50], Train Loss: 0.0130, Val Loss: 0.0155\n",
      "Epoch [12/50], Train Loss: 0.0117, Val Loss: 0.0077\n",
      "Epoch [13/50], Train Loss: 0.0113, Val Loss: 0.0165\n",
      "Epoch [14/50], Train Loss: 0.0154, Val Loss: 0.0172\n",
      "Epoch [15/50], Train Loss: 0.0115, Val Loss: 0.0090\n",
      "Epoch [16/50], Train Loss: 0.0108, Val Loss: 0.0133\n",
      "Epoch [17/50], Train Loss: 0.0105, Val Loss: 0.0145\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=64, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0726, Val Loss: 0.1075\n",
      "Epoch [2/50], Train Loss: 0.0466, Val Loss: 0.0573\n",
      "Epoch [3/50], Train Loss: 0.0476, Val Loss: 0.0655\n",
      "Epoch [4/50], Train Loss: 0.0378, Val Loss: 0.0272\n",
      "Epoch [5/50], Train Loss: 0.0345, Val Loss: 0.0113\n",
      "Epoch [6/50], Train Loss: 0.0277, Val Loss: 0.0058\n",
      "Epoch [7/50], Train Loss: 0.0255, Val Loss: 0.0090\n",
      "Epoch [8/50], Train Loss: 0.0235, Val Loss: 0.0126\n",
      "Epoch [9/50], Train Loss: 0.0219, Val Loss: 0.0197\n",
      "Epoch [10/50], Train Loss: 0.0219, Val Loss: 0.0607\n",
      "Epoch [11/50], Train Loss: 0.0203, Val Loss: 0.0242\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=64, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0736, Val Loss: 0.0447\n",
      "Epoch [2/50], Train Loss: 0.0664, Val Loss: 0.1023\n",
      "Epoch [3/50], Train Loss: 0.0382, Val Loss: 0.0366\n",
      "Epoch [4/50], Train Loss: 0.0417, Val Loss: 0.0137\n",
      "Epoch [5/50], Train Loss: 0.0332, Val Loss: 0.0103\n",
      "Epoch [6/50], Train Loss: 0.0286, Val Loss: 0.0132\n",
      "Epoch [7/50], Train Loss: 0.0275, Val Loss: 0.0224\n",
      "Epoch [8/50], Train Loss: 0.0265, Val Loss: 0.0686\n",
      "Epoch [9/50], Train Loss: 0.0254, Val Loss: 0.0167\n",
      "Epoch [10/50], Train Loss: 0.0258, Val Loss: 0.0121\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=64, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1106, Val Loss: 0.1346\n",
      "Epoch [2/50], Train Loss: 0.0545, Val Loss: 0.0727\n",
      "Epoch [3/50], Train Loss: 0.0543, Val Loss: 0.0509\n",
      "Epoch [4/50], Train Loss: 0.0509, Val Loss: 0.0380\n",
      "Epoch [5/50], Train Loss: 0.0417, Val Loss: 0.0162\n",
      "Epoch [6/50], Train Loss: 0.0339, Val Loss: 0.0143\n",
      "Epoch [7/50], Train Loss: 0.0328, Val Loss: 0.0200\n",
      "Epoch [8/50], Train Loss: 0.0280, Val Loss: 0.0198\n",
      "Epoch [9/50], Train Loss: 0.0260, Val Loss: 0.0474\n",
      "Epoch [10/50], Train Loss: 0.0192, Val Loss: 0.0319\n",
      "Epoch [11/50], Train Loss: 0.0248, Val Loss: 0.0050\n",
      "Epoch [12/50], Train Loss: 0.0258, Val Loss: 0.0684\n",
      "Epoch [13/50], Train Loss: 0.0244, Val Loss: 0.0494\n",
      "Epoch [14/50], Train Loss: 0.0201, Val Loss: 0.0371\n",
      "Epoch [15/50], Train Loss: 0.0156, Val Loss: 0.0347\n",
      "Epoch [16/50], Train Loss: 0.0123, Val Loss: 0.0189\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=16, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0796, Val Loss: 0.2384\n",
      "Epoch [2/50], Train Loss: 0.0740, Val Loss: 0.2273\n",
      "Epoch [3/50], Train Loss: 0.0692, Val Loss: 0.2168\n",
      "Epoch [4/50], Train Loss: 0.0647, Val Loss: 0.2068\n",
      "Epoch [5/50], Train Loss: 0.0606, Val Loss: 0.1972\n",
      "Epoch [6/50], Train Loss: 0.0568, Val Loss: 0.1879\n",
      "Epoch [7/50], Train Loss: 0.0533, Val Loss: 0.1788\n",
      "Epoch [8/50], Train Loss: 0.0500, Val Loss: 0.1700\n",
      "Epoch [9/50], Train Loss: 0.0469, Val Loss: 0.1614\n",
      "Epoch [10/50], Train Loss: 0.0441, Val Loss: 0.1530\n",
      "Epoch [11/50], Train Loss: 0.0416, Val Loss: 0.1449\n",
      "Epoch [12/50], Train Loss: 0.0394, Val Loss: 0.1371\n",
      "Epoch [13/50], Train Loss: 0.0375, Val Loss: 0.1297\n",
      "Epoch [14/50], Train Loss: 0.0359, Val Loss: 0.1228\n",
      "Epoch [15/50], Train Loss: 0.0345, Val Loss: 0.1164\n",
      "Epoch [16/50], Train Loss: 0.0333, Val Loss: 0.1104\n",
      "Epoch [17/50], Train Loss: 0.0324, Val Loss: 0.1049\n",
      "Epoch [18/50], Train Loss: 0.0315, Val Loss: 0.0999\n",
      "Epoch [19/50], Train Loss: 0.0308, Val Loss: 0.0953\n",
      "Epoch [20/50], Train Loss: 0.0301, Val Loss: 0.0910\n",
      "Epoch [21/50], Train Loss: 0.0295, Val Loss: 0.0872\n",
      "Epoch [22/50], Train Loss: 0.0290, Val Loss: 0.0836\n",
      "Epoch [23/50], Train Loss: 0.0285, Val Loss: 0.0803\n",
      "Epoch [24/50], Train Loss: 0.0280, Val Loss: 0.0772\n",
      "Epoch [25/50], Train Loss: 0.0275, Val Loss: 0.0743\n",
      "Epoch [26/50], Train Loss: 0.0271, Val Loss: 0.0717\n",
      "Epoch [27/50], Train Loss: 0.0266, Val Loss: 0.0691\n",
      "Epoch [28/50], Train Loss: 0.0262, Val Loss: 0.0668\n",
      "Epoch [29/50], Train Loss: 0.0258, Val Loss: 0.0645\n",
      "Epoch [30/50], Train Loss: 0.0254, Val Loss: 0.0624\n",
      "Epoch [31/50], Train Loss: 0.0249, Val Loss: 0.0603\n",
      "Epoch [32/50], Train Loss: 0.0245, Val Loss: 0.0583\n",
      "Epoch [33/50], Train Loss: 0.0241, Val Loss: 0.0564\n",
      "Epoch [34/50], Train Loss: 0.0236, Val Loss: 0.0546\n",
      "Epoch [35/50], Train Loss: 0.0232, Val Loss: 0.0528\n",
      "Epoch [36/50], Train Loss: 0.0227, Val Loss: 0.0511\n",
      "Epoch [37/50], Train Loss: 0.0223, Val Loss: 0.0494\n",
      "Epoch [38/50], Train Loss: 0.0218, Val Loss: 0.0477\n",
      "Epoch [39/50], Train Loss: 0.0213, Val Loss: 0.0461\n",
      "Epoch [40/50], Train Loss: 0.0208, Val Loss: 0.0446\n",
      "Epoch [41/50], Train Loss: 0.0204, Val Loss: 0.0430\n",
      "Epoch [42/50], Train Loss: 0.0199, Val Loss: 0.0415\n",
      "Epoch [43/50], Train Loss: 0.0194, Val Loss: 0.0400\n",
      "Epoch [44/50], Train Loss: 0.0188, Val Loss: 0.0386\n",
      "Epoch [45/50], Train Loss: 0.0183, Val Loss: 0.0372\n",
      "Epoch [46/50], Train Loss: 0.0178, Val Loss: 0.0357\n",
      "Epoch [47/50], Train Loss: 0.0173, Val Loss: 0.0343\n",
      "Epoch [48/50], Train Loss: 0.0167, Val Loss: 0.0329\n",
      "Epoch [49/50], Train Loss: 0.0162, Val Loss: 0.0315\n",
      "Epoch [50/50], Train Loss: 0.0157, Val Loss: 0.0301\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=16, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1300, Val Loss: 0.3498\n",
      "Epoch [2/50], Train Loss: 0.1235, Val Loss: 0.3335\n",
      "Epoch [3/50], Train Loss: 0.1170, Val Loss: 0.3186\n",
      "Epoch [4/50], Train Loss: 0.1103, Val Loss: 0.3047\n",
      "Epoch [5/50], Train Loss: 0.1055, Val Loss: 0.2916\n",
      "Epoch [6/50], Train Loss: 0.0995, Val Loss: 0.2790\n",
      "Epoch [7/50], Train Loss: 0.0951, Val Loss: 0.2669\n",
      "Epoch [8/50], Train Loss: 0.0894, Val Loss: 0.2552\n",
      "Epoch [9/50], Train Loss: 0.0855, Val Loss: 0.2438\n",
      "Epoch [10/50], Train Loss: 0.0809, Val Loss: 0.2327\n",
      "Epoch [11/50], Train Loss: 0.0775, Val Loss: 0.2221\n",
      "Epoch [12/50], Train Loss: 0.0732, Val Loss: 0.2118\n",
      "Epoch [13/50], Train Loss: 0.0688, Val Loss: 0.2016\n",
      "Epoch [14/50], Train Loss: 0.0658, Val Loss: 0.1917\n",
      "Epoch [15/50], Train Loss: 0.0623, Val Loss: 0.1820\n",
      "Epoch [16/50], Train Loss: 0.0584, Val Loss: 0.1727\n",
      "Epoch [17/50], Train Loss: 0.0562, Val Loss: 0.1638\n",
      "Epoch [18/50], Train Loss: 0.0525, Val Loss: 0.1552\n",
      "Epoch [19/50], Train Loss: 0.0508, Val Loss: 0.1472\n",
      "Epoch [20/50], Train Loss: 0.0487, Val Loss: 0.1394\n",
      "Epoch [21/50], Train Loss: 0.0467, Val Loss: 0.1324\n",
      "Epoch [22/50], Train Loss: 0.0452, Val Loss: 0.1259\n",
      "Epoch [23/50], Train Loss: 0.0436, Val Loss: 0.1200\n",
      "Epoch [24/50], Train Loss: 0.0423, Val Loss: 0.1149\n",
      "Epoch [25/50], Train Loss: 0.0408, Val Loss: 0.1103\n",
      "Epoch [26/50], Train Loss: 0.0394, Val Loss: 0.1060\n",
      "Epoch [27/50], Train Loss: 0.0394, Val Loss: 0.1022\n",
      "Epoch [28/50], Train Loss: 0.0378, Val Loss: 0.0986\n",
      "Epoch [29/50], Train Loss: 0.0366, Val Loss: 0.0953\n",
      "Epoch [30/50], Train Loss: 0.0367, Val Loss: 0.0920\n",
      "Epoch [31/50], Train Loss: 0.0358, Val Loss: 0.0889\n",
      "Epoch [32/50], Train Loss: 0.0343, Val Loss: 0.0862\n",
      "Epoch [33/50], Train Loss: 0.0347, Val Loss: 0.0833\n",
      "Epoch [34/50], Train Loss: 0.0337, Val Loss: 0.0805\n",
      "Epoch [35/50], Train Loss: 0.0332, Val Loss: 0.0780\n",
      "Epoch [36/50], Train Loss: 0.0322, Val Loss: 0.0750\n",
      "Epoch [37/50], Train Loss: 0.0315, Val Loss: 0.0722\n",
      "Epoch [38/50], Train Loss: 0.0320, Val Loss: 0.0694\n",
      "Epoch [39/50], Train Loss: 0.0299, Val Loss: 0.0669\n",
      "Epoch [40/50], Train Loss: 0.0305, Val Loss: 0.0643\n",
      "Epoch [41/50], Train Loss: 0.0287, Val Loss: 0.0615\n",
      "Epoch [42/50], Train Loss: 0.0278, Val Loss: 0.0586\n",
      "Epoch [43/50], Train Loss: 0.0276, Val Loss: 0.0558\n",
      "Epoch [44/50], Train Loss: 0.0266, Val Loss: 0.0536\n",
      "Epoch [45/50], Train Loss: 0.0254, Val Loss: 0.0514\n",
      "Epoch [46/50], Train Loss: 0.0246, Val Loss: 0.0492\n",
      "Epoch [47/50], Train Loss: 0.0242, Val Loss: 0.0473\n",
      "Epoch [48/50], Train Loss: 0.0232, Val Loss: 0.0458\n",
      "Epoch [49/50], Train Loss: 0.0234, Val Loss: 0.0444\n",
      "Epoch [50/50], Train Loss: 0.0226, Val Loss: 0.0429\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=16, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1216, Val Loss: 0.3240\n",
      "Epoch [2/50], Train Loss: 0.1148, Val Loss: 0.3132\n",
      "Epoch [3/50], Train Loss: 0.1091, Val Loss: 0.3029\n",
      "Epoch [4/50], Train Loss: 0.1060, Val Loss: 0.2929\n",
      "Epoch [5/50], Train Loss: 0.1004, Val Loss: 0.2829\n",
      "Epoch [6/50], Train Loss: 0.0975, Val Loss: 0.2728\n",
      "Epoch [7/50], Train Loss: 0.0931, Val Loss: 0.2628\n",
      "Epoch [8/50], Train Loss: 0.0881, Val Loss: 0.2527\n",
      "Epoch [9/50], Train Loss: 0.0834, Val Loss: 0.2421\n",
      "Epoch [10/50], Train Loss: 0.0798, Val Loss: 0.2313\n",
      "Epoch [11/50], Train Loss: 0.0766, Val Loss: 0.2203\n",
      "Epoch [12/50], Train Loss: 0.0717, Val Loss: 0.2093\n",
      "Epoch [13/50], Train Loss: 0.0676, Val Loss: 0.1979\n",
      "Epoch [14/50], Train Loss: 0.0632, Val Loss: 0.1864\n",
      "Epoch [15/50], Train Loss: 0.0612, Val Loss: 0.1753\n",
      "Epoch [16/50], Train Loss: 0.0585, Val Loss: 0.1648\n",
      "Epoch [17/50], Train Loss: 0.0562, Val Loss: 0.1547\n",
      "Epoch [18/50], Train Loss: 0.0529, Val Loss: 0.1452\n",
      "Epoch [19/50], Train Loss: 0.0509, Val Loss: 0.1359\n",
      "Epoch [20/50], Train Loss: 0.0506, Val Loss: 0.1282\n",
      "Epoch [21/50], Train Loss: 0.0471, Val Loss: 0.1216\n",
      "Epoch [22/50], Train Loss: 0.0458, Val Loss: 0.1157\n",
      "Epoch [23/50], Train Loss: 0.0449, Val Loss: 0.1106\n",
      "Epoch [24/50], Train Loss: 0.0447, Val Loss: 0.1063\n",
      "Epoch [25/50], Train Loss: 0.0435, Val Loss: 0.1030\n",
      "Epoch [26/50], Train Loss: 0.0427, Val Loss: 0.0997\n",
      "Epoch [27/50], Train Loss: 0.0416, Val Loss: 0.0969\n",
      "Epoch [28/50], Train Loss: 0.0417, Val Loss: 0.0942\n",
      "Epoch [29/50], Train Loss: 0.0407, Val Loss: 0.0913\n",
      "Epoch [30/50], Train Loss: 0.0393, Val Loss: 0.0891\n",
      "Epoch [31/50], Train Loss: 0.0397, Val Loss: 0.0870\n",
      "Epoch [32/50], Train Loss: 0.0384, Val Loss: 0.0849\n",
      "Epoch [33/50], Train Loss: 0.0388, Val Loss: 0.0834\n",
      "Epoch [34/50], Train Loss: 0.0382, Val Loss: 0.0817\n",
      "Epoch [35/50], Train Loss: 0.0372, Val Loss: 0.0801\n",
      "Epoch [36/50], Train Loss: 0.0375, Val Loss: 0.0779\n",
      "Epoch [37/50], Train Loss: 0.0363, Val Loss: 0.0769\n",
      "Epoch [38/50], Train Loss: 0.0370, Val Loss: 0.0757\n",
      "Epoch [39/50], Train Loss: 0.0348, Val Loss: 0.0743\n",
      "Epoch [40/50], Train Loss: 0.0355, Val Loss: 0.0726\n",
      "Epoch [41/50], Train Loss: 0.0351, Val Loss: 0.0706\n",
      "Epoch [42/50], Train Loss: 0.0333, Val Loss: 0.0688\n",
      "Epoch [43/50], Train Loss: 0.0346, Val Loss: 0.0691\n",
      "Epoch [44/50], Train Loss: 0.0350, Val Loss: 0.0682\n",
      "Epoch [45/50], Train Loss: 0.0330, Val Loss: 0.0666\n",
      "Epoch [46/50], Train Loss: 0.0332, Val Loss: 0.0659\n",
      "Epoch [47/50], Train Loss: 0.0331, Val Loss: 0.0641\n",
      "Epoch [48/50], Train Loss: 0.0330, Val Loss: 0.0629\n",
      "Epoch [49/50], Train Loss: 0.0338, Val Loss: 0.0626\n",
      "Epoch [50/50], Train Loss: 0.0318, Val Loss: 0.0615\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=16, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1287, Val Loss: 0.3304\n",
      "Epoch [2/50], Train Loss: 0.1211, Val Loss: 0.3171\n",
      "Epoch [3/50], Train Loss: 0.1139, Val Loss: 0.3033\n",
      "Epoch [4/50], Train Loss: 0.1065, Val Loss: 0.2886\n",
      "Epoch [5/50], Train Loss: 0.0986, Val Loss: 0.2724\n",
      "Epoch [6/50], Train Loss: 0.0901, Val Loss: 0.2541\n",
      "Epoch [7/50], Train Loss: 0.0810, Val Loss: 0.2333\n",
      "Epoch [8/50], Train Loss: 0.0712, Val Loss: 0.2098\n",
      "Epoch [9/50], Train Loss: 0.0609, Val Loss: 0.1840\n",
      "Epoch [10/50], Train Loss: 0.0512, Val Loss: 0.1577\n",
      "Epoch [11/50], Train Loss: 0.0433, Val Loss: 0.1345\n",
      "Epoch [12/50], Train Loss: 0.0382, Val Loss: 0.1171\n",
      "Epoch [13/50], Train Loss: 0.0355, Val Loss: 0.1056\n",
      "Epoch [14/50], Train Loss: 0.0342, Val Loss: 0.0983\n",
      "Epoch [15/50], Train Loss: 0.0333, Val Loss: 0.0932\n",
      "Epoch [16/50], Train Loss: 0.0327, Val Loss: 0.0893\n",
      "Epoch [17/50], Train Loss: 0.0322, Val Loss: 0.0862\n",
      "Epoch [18/50], Train Loss: 0.0317, Val Loss: 0.0834\n",
      "Epoch [19/50], Train Loss: 0.0312, Val Loss: 0.0810\n",
      "Epoch [20/50], Train Loss: 0.0307, Val Loss: 0.0787\n",
      "Epoch [21/50], Train Loss: 0.0303, Val Loss: 0.0766\n",
      "Epoch [22/50], Train Loss: 0.0297, Val Loss: 0.0745\n",
      "Epoch [23/50], Train Loss: 0.0292, Val Loss: 0.0725\n",
      "Epoch [24/50], Train Loss: 0.0286, Val Loss: 0.0706\n",
      "Epoch [25/50], Train Loss: 0.0280, Val Loss: 0.0688\n",
      "Epoch [26/50], Train Loss: 0.0273, Val Loss: 0.0670\n",
      "Epoch [27/50], Train Loss: 0.0267, Val Loss: 0.0653\n",
      "Epoch [28/50], Train Loss: 0.0259, Val Loss: 0.0637\n",
      "Epoch [29/50], Train Loss: 0.0252, Val Loss: 0.0621\n",
      "Epoch [30/50], Train Loss: 0.0244, Val Loss: 0.0605\n",
      "Epoch [31/50], Train Loss: 0.0235, Val Loss: 0.0589\n",
      "Epoch [32/50], Train Loss: 0.0226, Val Loss: 0.0573\n",
      "Epoch [33/50], Train Loss: 0.0216, Val Loss: 0.0557\n",
      "Epoch [34/50], Train Loss: 0.0206, Val Loss: 0.0540\n",
      "Epoch [35/50], Train Loss: 0.0195, Val Loss: 0.0523\n",
      "Epoch [36/50], Train Loss: 0.0184, Val Loss: 0.0505\n",
      "Epoch [37/50], Train Loss: 0.0173, Val Loss: 0.0487\n",
      "Epoch [38/50], Train Loss: 0.0162, Val Loss: 0.0470\n",
      "Epoch [39/50], Train Loss: 0.0151, Val Loss: 0.0453\n",
      "Epoch [40/50], Train Loss: 0.0141, Val Loss: 0.0438\n",
      "Epoch [41/50], Train Loss: 0.0133, Val Loss: 0.0428\n",
      "Epoch [42/50], Train Loss: 0.0131, Val Loss: 0.0410\n",
      "Epoch [43/50], Train Loss: 0.0141, Val Loss: 0.0407\n",
      "Epoch [44/50], Train Loss: 0.0116, Val Loss: 0.0401\n",
      "Epoch [45/50], Train Loss: 0.0113, Val Loss: 0.0399\n",
      "Epoch [46/50], Train Loss: 0.0108, Val Loss: 0.0389\n",
      "Epoch [47/50], Train Loss: 0.0107, Val Loss: 0.0386\n",
      "Epoch [48/50], Train Loss: 0.0098, Val Loss: 0.0381\n",
      "Epoch [49/50], Train Loss: 0.0095, Val Loss: 0.0379\n",
      "Epoch [50/50], Train Loss: 0.0089, Val Loss: 0.0375\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=16, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1196, Val Loss: 0.3118\n",
      "Epoch [2/50], Train Loss: 0.1130, Val Loss: 0.2989\n",
      "Epoch [3/50], Train Loss: 0.1060, Val Loss: 0.2854\n",
      "Epoch [4/50], Train Loss: 0.0996, Val Loss: 0.2708\n",
      "Epoch [5/50], Train Loss: 0.0923, Val Loss: 0.2548\n",
      "Epoch [6/50], Train Loss: 0.0850, Val Loss: 0.2370\n",
      "Epoch [7/50], Train Loss: 0.0767, Val Loss: 0.2172\n",
      "Epoch [8/50], Train Loss: 0.0693, Val Loss: 0.1958\n",
      "Epoch [9/50], Train Loss: 0.0616, Val Loss: 0.1735\n",
      "Epoch [10/50], Train Loss: 0.0543, Val Loss: 0.1517\n",
      "Epoch [11/50], Train Loss: 0.0493, Val Loss: 0.1322\n",
      "Epoch [12/50], Train Loss: 0.0453, Val Loss: 0.1168\n",
      "Epoch [13/50], Train Loss: 0.0425, Val Loss: 0.1054\n",
      "Epoch [14/50], Train Loss: 0.0401, Val Loss: 0.0968\n",
      "Epoch [15/50], Train Loss: 0.0392, Val Loss: 0.0905\n",
      "Epoch [16/50], Train Loss: 0.0375, Val Loss: 0.0848\n",
      "Epoch [17/50], Train Loss: 0.0361, Val Loss: 0.0800\n",
      "Epoch [18/50], Train Loss: 0.0354, Val Loss: 0.0751\n",
      "Epoch [19/50], Train Loss: 0.0340, Val Loss: 0.0711\n",
      "Epoch [20/50], Train Loss: 0.0339, Val Loss: 0.0678\n",
      "Epoch [21/50], Train Loss: 0.0321, Val Loss: 0.0646\n",
      "Epoch [22/50], Train Loss: 0.0316, Val Loss: 0.0607\n",
      "Epoch [23/50], Train Loss: 0.0311, Val Loss: 0.0568\n",
      "Epoch [24/50], Train Loss: 0.0303, Val Loss: 0.0530\n",
      "Epoch [25/50], Train Loss: 0.0299, Val Loss: 0.0502\n",
      "Epoch [26/50], Train Loss: 0.0291, Val Loss: 0.0471\n",
      "Epoch [27/50], Train Loss: 0.0279, Val Loss: 0.0439\n",
      "Epoch [28/50], Train Loss: 0.0275, Val Loss: 0.0411\n",
      "Epoch [29/50], Train Loss: 0.0267, Val Loss: 0.0381\n",
      "Epoch [30/50], Train Loss: 0.0273, Val Loss: 0.0339\n",
      "Epoch [31/50], Train Loss: 0.0262, Val Loss: 0.0311\n",
      "Epoch [32/50], Train Loss: 0.0247, Val Loss: 0.0286\n",
      "Epoch [33/50], Train Loss: 0.0250, Val Loss: 0.0263\n",
      "Epoch [34/50], Train Loss: 0.0243, Val Loss: 0.0234\n",
      "Epoch [35/50], Train Loss: 0.0235, Val Loss: 0.0212\n",
      "Epoch [36/50], Train Loss: 0.0229, Val Loss: 0.0186\n",
      "Epoch [37/50], Train Loss: 0.0221, Val Loss: 0.0165\n",
      "Epoch [38/50], Train Loss: 0.0209, Val Loss: 0.0144\n",
      "Epoch [39/50], Train Loss: 0.0205, Val Loss: 0.0129\n",
      "Epoch [40/50], Train Loss: 0.0194, Val Loss: 0.0125\n",
      "Epoch [41/50], Train Loss: 0.0189, Val Loss: 0.0129\n",
      "Epoch [42/50], Train Loss: 0.0176, Val Loss: 0.0119\n",
      "Epoch [43/50], Train Loss: 0.0167, Val Loss: 0.0119\n",
      "Epoch [44/50], Train Loss: 0.0157, Val Loss: 0.0116\n",
      "Epoch [45/50], Train Loss: 0.0163, Val Loss: 0.0120\n",
      "Epoch [46/50], Train Loss: 0.0154, Val Loss: 0.0120\n",
      "Epoch [47/50], Train Loss: 0.0138, Val Loss: 0.0123\n",
      "Epoch [48/50], Train Loss: 0.0146, Val Loss: 0.0121\n",
      "Epoch [49/50], Train Loss: 0.0148, Val Loss: 0.0119\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=16, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1269, Val Loss: 0.3248\n",
      "Epoch [2/50], Train Loss: 0.1218, Val Loss: 0.3137\n",
      "Epoch [3/50], Train Loss: 0.1165, Val Loss: 0.3023\n",
      "Epoch [4/50], Train Loss: 0.1111, Val Loss: 0.2900\n",
      "Epoch [5/50], Train Loss: 0.1033, Val Loss: 0.2767\n",
      "Epoch [6/50], Train Loss: 0.0982, Val Loss: 0.2615\n",
      "Epoch [7/50], Train Loss: 0.0925, Val Loss: 0.2445\n",
      "Epoch [8/50], Train Loss: 0.0865, Val Loss: 0.2255\n",
      "Epoch [9/50], Train Loss: 0.0777, Val Loss: 0.2037\n",
      "Epoch [10/50], Train Loss: 0.0726, Val Loss: 0.1806\n",
      "Epoch [11/50], Train Loss: 0.0673, Val Loss: 0.1578\n",
      "Epoch [12/50], Train Loss: 0.0647, Val Loss: 0.1409\n",
      "Epoch [13/50], Train Loss: 0.0615, Val Loss: 0.1268\n",
      "Epoch [14/50], Train Loss: 0.0593, Val Loss: 0.1163\n",
      "Epoch [15/50], Train Loss: 0.0584, Val Loss: 0.1091\n",
      "Epoch [16/50], Train Loss: 0.0558, Val Loss: 0.1036\n",
      "Epoch [17/50], Train Loss: 0.0553, Val Loss: 0.0989\n",
      "Epoch [18/50], Train Loss: 0.0542, Val Loss: 0.0945\n",
      "Epoch [19/50], Train Loss: 0.0553, Val Loss: 0.0892\n",
      "Epoch [20/50], Train Loss: 0.0527, Val Loss: 0.0852\n",
      "Epoch [21/50], Train Loss: 0.0504, Val Loss: 0.0819\n",
      "Epoch [22/50], Train Loss: 0.0515, Val Loss: 0.0783\n",
      "Epoch [23/50], Train Loss: 0.0518, Val Loss: 0.0749\n",
      "Epoch [24/50], Train Loss: 0.0479, Val Loss: 0.0725\n",
      "Epoch [25/50], Train Loss: 0.0476, Val Loss: 0.0680\n",
      "Epoch [26/50], Train Loss: 0.0488, Val Loss: 0.0669\n",
      "Epoch [27/50], Train Loss: 0.0487, Val Loss: 0.0623\n",
      "Epoch [28/50], Train Loss: 0.0481, Val Loss: 0.0586\n",
      "Epoch [29/50], Train Loss: 0.0456, Val Loss: 0.0564\n",
      "Epoch [30/50], Train Loss: 0.0434, Val Loss: 0.0545\n",
      "Epoch [31/50], Train Loss: 0.0445, Val Loss: 0.0517\n",
      "Epoch [32/50], Train Loss: 0.0444, Val Loss: 0.0487\n",
      "Epoch [33/50], Train Loss: 0.0432, Val Loss: 0.0455\n",
      "Epoch [34/50], Train Loss: 0.0413, Val Loss: 0.0423\n",
      "Epoch [35/50], Train Loss: 0.0428, Val Loss: 0.0417\n",
      "Epoch [36/50], Train Loss: 0.0402, Val Loss: 0.0421\n",
      "Epoch [37/50], Train Loss: 0.0393, Val Loss: 0.0417\n",
      "Epoch [38/50], Train Loss: 0.0395, Val Loss: 0.0417\n",
      "Epoch [39/50], Train Loss: 0.0383, Val Loss: 0.0412\n",
      "Epoch [40/50], Train Loss: 0.0357, Val Loss: 0.0421\n",
      "Epoch [41/50], Train Loss: 0.0354, Val Loss: 0.0445\n",
      "Epoch [42/50], Train Loss: 0.0341, Val Loss: 0.0479\n",
      "Epoch [43/50], Train Loss: 0.0355, Val Loss: 0.0495\n",
      "Epoch [44/50], Train Loss: 0.0351, Val Loss: 0.0500\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=16, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1489, Val Loss: 0.3724\n",
      "Epoch [2/50], Train Loss: 0.1394, Val Loss: 0.3562\n",
      "Epoch [3/50], Train Loss: 0.1308, Val Loss: 0.3402\n",
      "Epoch [4/50], Train Loss: 0.1221, Val Loss: 0.3233\n",
      "Epoch [5/50], Train Loss: 0.1127, Val Loss: 0.3045\n",
      "Epoch [6/50], Train Loss: 0.1020, Val Loss: 0.2823\n",
      "Epoch [7/50], Train Loss: 0.0897, Val Loss: 0.2547\n",
      "Epoch [8/50], Train Loss: 0.0755, Val Loss: 0.2207\n",
      "Epoch [9/50], Train Loss: 0.0617, Val Loss: 0.1854\n",
      "Epoch [10/50], Train Loss: 0.0514, Val Loss: 0.1572\n",
      "Epoch [11/50], Train Loss: 0.0455, Val Loss: 0.1382\n",
      "Epoch [12/50], Train Loss: 0.0422, Val Loss: 0.1260\n",
      "Epoch [13/50], Train Loss: 0.0403, Val Loss: 0.1183\n",
      "Epoch [14/50], Train Loss: 0.0392, Val Loss: 0.1134\n",
      "Epoch [15/50], Train Loss: 0.0385, Val Loss: 0.1102\n",
      "Epoch [16/50], Train Loss: 0.0381, Val Loss: 0.1081\n",
      "Epoch [17/50], Train Loss: 0.0377, Val Loss: 0.1066\n",
      "Epoch [18/50], Train Loss: 0.0374, Val Loss: 0.1055\n",
      "Epoch [19/50], Train Loss: 0.0372, Val Loss: 0.1046\n",
      "Epoch [20/50], Train Loss: 0.0370, Val Loss: 0.1037\n",
      "Epoch [21/50], Train Loss: 0.0367, Val Loss: 0.1028\n",
      "Epoch [22/50], Train Loss: 0.0364, Val Loss: 0.1019\n",
      "Epoch [23/50], Train Loss: 0.0361, Val Loss: 0.1008\n",
      "Epoch [24/50], Train Loss: 0.0357, Val Loss: 0.0996\n",
      "Epoch [25/50], Train Loss: 0.0353, Val Loss: 0.0982\n",
      "Epoch [26/50], Train Loss: 0.0347, Val Loss: 0.0965\n",
      "Epoch [27/50], Train Loss: 0.0342, Val Loss: 0.0945\n",
      "Epoch [28/50], Train Loss: 0.0335, Val Loss: 0.0923\n",
      "Epoch [29/50], Train Loss: 0.0327, Val Loss: 0.0898\n",
      "Epoch [30/50], Train Loss: 0.0319, Val Loss: 0.0871\n",
      "Epoch [31/50], Train Loss: 0.0310, Val Loss: 0.0842\n",
      "Epoch [32/50], Train Loss: 0.0300, Val Loss: 0.0811\n",
      "Epoch [33/50], Train Loss: 0.0290, Val Loss: 0.0779\n",
      "Epoch [34/50], Train Loss: 0.0280, Val Loss: 0.0745\n",
      "Epoch [35/50], Train Loss: 0.0270, Val Loss: 0.0712\n",
      "Epoch [36/50], Train Loss: 0.0261, Val Loss: 0.0678\n",
      "Epoch [37/50], Train Loss: 0.0253, Val Loss: 0.0647\n",
      "Epoch [38/50], Train Loss: 0.0245, Val Loss: 0.0617\n",
      "Epoch [39/50], Train Loss: 0.0239, Val Loss: 0.0589\n",
      "Epoch [40/50], Train Loss: 0.0233, Val Loss: 0.0564\n",
      "Epoch [41/50], Train Loss: 0.0228, Val Loss: 0.0541\n",
      "Epoch [42/50], Train Loss: 0.0224, Val Loss: 0.0520\n",
      "Epoch [43/50], Train Loss: 0.0220, Val Loss: 0.0501\n",
      "Epoch [44/50], Train Loss: 0.0216, Val Loss: 0.0483\n",
      "Epoch [45/50], Train Loss: 0.0213, Val Loss: 0.0467\n",
      "Epoch [46/50], Train Loss: 0.0210, Val Loss: 0.0453\n",
      "Epoch [47/50], Train Loss: 0.0208, Val Loss: 0.0440\n",
      "Epoch [48/50], Train Loss: 0.0205, Val Loss: 0.0429\n",
      "Epoch [49/50], Train Loss: 0.0203, Val Loss: 0.0419\n",
      "Epoch [50/50], Train Loss: 0.0201, Val Loss: 0.0410\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=16, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1862, Val Loss: 0.4753\n",
      "Epoch [2/50], Train Loss: 0.1794, Val Loss: 0.4636\n",
      "Epoch [3/50], Train Loss: 0.1726, Val Loss: 0.4516\n",
      "Epoch [4/50], Train Loss: 0.1661, Val Loss: 0.4383\n",
      "Epoch [5/50], Train Loss: 0.1583, Val Loss: 0.4228\n",
      "Epoch [6/50], Train Loss: 0.1492, Val Loss: 0.4028\n",
      "Epoch [7/50], Train Loss: 0.1370, Val Loss: 0.3745\n",
      "Epoch [8/50], Train Loss: 0.1213, Val Loss: 0.3318\n",
      "Epoch [9/50], Train Loss: 0.1002, Val Loss: 0.2733\n",
      "Epoch [10/50], Train Loss: 0.0777, Val Loss: 0.2143\n",
      "Epoch [11/50], Train Loss: 0.0647, Val Loss: 0.1729\n",
      "Epoch [12/50], Train Loss: 0.0602, Val Loss: 0.1495\n",
      "Epoch [13/50], Train Loss: 0.0564, Val Loss: 0.1363\n",
      "Epoch [14/50], Train Loss: 0.0545, Val Loss: 0.1283\n",
      "Epoch [15/50], Train Loss: 0.0536, Val Loss: 0.1228\n",
      "Epoch [16/50], Train Loss: 0.0529, Val Loss: 0.1192\n",
      "Epoch [17/50], Train Loss: 0.0518, Val Loss: 0.1151\n",
      "Epoch [18/50], Train Loss: 0.0504, Val Loss: 0.1117\n",
      "Epoch [19/50], Train Loss: 0.0483, Val Loss: 0.1087\n",
      "Epoch [20/50], Train Loss: 0.0478, Val Loss: 0.1052\n",
      "Epoch [21/50], Train Loss: 0.0491, Val Loss: 0.1026\n",
      "Epoch [22/50], Train Loss: 0.0463, Val Loss: 0.0989\n",
      "Epoch [23/50], Train Loss: 0.0453, Val Loss: 0.0957\n",
      "Epoch [24/50], Train Loss: 0.0439, Val Loss: 0.0919\n",
      "Epoch [25/50], Train Loss: 0.0445, Val Loss: 0.0884\n",
      "Epoch [26/50], Train Loss: 0.0433, Val Loss: 0.0838\n",
      "Epoch [27/50], Train Loss: 0.0406, Val Loss: 0.0801\n",
      "Epoch [28/50], Train Loss: 0.0431, Val Loss: 0.0784\n",
      "Epoch [29/50], Train Loss: 0.0409, Val Loss: 0.0743\n",
      "Epoch [30/50], Train Loss: 0.0393, Val Loss: 0.0705\n",
      "Epoch [31/50], Train Loss: 0.0389, Val Loss: 0.0669\n",
      "Epoch [32/50], Train Loss: 0.0373, Val Loss: 0.0639\n",
      "Epoch [33/50], Train Loss: 0.0373, Val Loss: 0.0603\n",
      "Epoch [34/50], Train Loss: 0.0359, Val Loss: 0.0570\n",
      "Epoch [35/50], Train Loss: 0.0350, Val Loss: 0.0544\n",
      "Epoch [36/50], Train Loss: 0.0347, Val Loss: 0.0512\n",
      "Epoch [37/50], Train Loss: 0.0337, Val Loss: 0.0482\n",
      "Epoch [38/50], Train Loss: 0.0338, Val Loss: 0.0454\n",
      "Epoch [39/50], Train Loss: 0.0331, Val Loss: 0.0432\n",
      "Epoch [40/50], Train Loss: 0.0321, Val Loss: 0.0412\n",
      "Epoch [41/50], Train Loss: 0.0318, Val Loss: 0.0388\n",
      "Epoch [42/50], Train Loss: 0.0332, Val Loss: 0.0365\n",
      "Epoch [43/50], Train Loss: 0.0307, Val Loss: 0.0353\n",
      "Epoch [44/50], Train Loss: 0.0317, Val Loss: 0.0333\n",
      "Epoch [45/50], Train Loss: 0.0307, Val Loss: 0.0322\n",
      "Epoch [46/50], Train Loss: 0.0298, Val Loss: 0.0306\n",
      "Epoch [47/50], Train Loss: 0.0308, Val Loss: 0.0302\n",
      "Epoch [48/50], Train Loss: 0.0304, Val Loss: 0.0284\n",
      "Epoch [49/50], Train Loss: 0.0297, Val Loss: 0.0276\n",
      "Epoch [50/50], Train Loss: 0.0296, Val Loss: 0.0262\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=16, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1357, Val Loss: 0.3177\n",
      "Epoch [2/50], Train Loss: 0.1298, Val Loss: 0.3109\n",
      "Epoch [3/50], Train Loss: 0.1240, Val Loss: 0.3046\n",
      "Epoch [4/50], Train Loss: 0.1203, Val Loss: 0.2988\n",
      "Epoch [5/50], Train Loss: 0.1153, Val Loss: 0.2930\n",
      "Epoch [6/50], Train Loss: 0.1117, Val Loss: 0.2873\n",
      "Epoch [7/50], Train Loss: 0.1074, Val Loss: 0.2813\n",
      "Epoch [8/50], Train Loss: 0.1033, Val Loss: 0.2752\n",
      "Epoch [9/50], Train Loss: 0.0994, Val Loss: 0.2686\n",
      "Epoch [10/50], Train Loss: 0.0951, Val Loss: 0.2613\n",
      "Epoch [11/50], Train Loss: 0.0909, Val Loss: 0.2533\n",
      "Epoch [12/50], Train Loss: 0.0880, Val Loss: 0.2444\n",
      "Epoch [13/50], Train Loss: 0.0822, Val Loss: 0.2341\n",
      "Epoch [14/50], Train Loss: 0.0772, Val Loss: 0.2216\n",
      "Epoch [15/50], Train Loss: 0.0728, Val Loss: 0.2076\n",
      "Epoch [16/50], Train Loss: 0.0680, Val Loss: 0.1945\n",
      "Epoch [17/50], Train Loss: 0.0683, Val Loss: 0.1840\n",
      "Epoch [18/50], Train Loss: 0.0641, Val Loss: 0.1738\n",
      "Epoch [19/50], Train Loss: 0.0611, Val Loss: 0.1650\n",
      "Epoch [20/50], Train Loss: 0.0614, Val Loss: 0.1583\n",
      "Epoch [21/50], Train Loss: 0.0591, Val Loss: 0.1531\n",
      "Epoch [22/50], Train Loss: 0.0572, Val Loss: 0.1484\n",
      "Epoch [23/50], Train Loss: 0.0563, Val Loss: 0.1428\n",
      "Epoch [24/50], Train Loss: 0.0585, Val Loss: 0.1410\n",
      "Epoch [25/50], Train Loss: 0.0557, Val Loss: 0.1384\n",
      "Epoch [26/50], Train Loss: 0.0554, Val Loss: 0.1361\n",
      "Epoch [27/50], Train Loss: 0.0552, Val Loss: 0.1324\n",
      "Epoch [28/50], Train Loss: 0.0552, Val Loss: 0.1309\n",
      "Epoch [29/50], Train Loss: 0.0558, Val Loss: 0.1287\n",
      "Epoch [30/50], Train Loss: 0.0532, Val Loss: 0.1265\n",
      "Epoch [31/50], Train Loss: 0.0521, Val Loss: 0.1242\n",
      "Epoch [32/50], Train Loss: 0.0521, Val Loss: 0.1225\n",
      "Epoch [33/50], Train Loss: 0.0507, Val Loss: 0.1198\n",
      "Epoch [34/50], Train Loss: 0.0510, Val Loss: 0.1190\n",
      "Epoch [35/50], Train Loss: 0.0499, Val Loss: 0.1172\n",
      "Epoch [36/50], Train Loss: 0.0507, Val Loss: 0.1160\n",
      "Epoch [37/50], Train Loss: 0.0493, Val Loss: 0.1156\n",
      "Epoch [38/50], Train Loss: 0.0483, Val Loss: 0.1124\n",
      "Epoch [39/50], Train Loss: 0.0492, Val Loss: 0.1100\n",
      "Epoch [40/50], Train Loss: 0.0473, Val Loss: 0.1063\n",
      "Epoch [41/50], Train Loss: 0.0467, Val Loss: 0.1029\n",
      "Epoch [42/50], Train Loss: 0.0449, Val Loss: 0.0990\n",
      "Epoch [43/50], Train Loss: 0.0451, Val Loss: 0.0973\n",
      "Epoch [44/50], Train Loss: 0.0461, Val Loss: 0.0943\n",
      "Epoch [45/50], Train Loss: 0.0436, Val Loss: 0.0901\n",
      "Epoch [46/50], Train Loss: 0.0445, Val Loss: 0.0859\n",
      "Epoch [47/50], Train Loss: 0.0432, Val Loss: 0.0830\n",
      "Epoch [48/50], Train Loss: 0.0430, Val Loss: 0.0810\n",
      "Epoch [49/50], Train Loss: 0.0414, Val Loss: 0.0760\n",
      "Epoch [50/50], Train Loss: 0.0407, Val Loss: 0.0726\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=32, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1294, Val Loss: 0.3239\n",
      "Epoch [2/50], Train Loss: 0.1189, Val Loss: 0.3028\n",
      "Epoch [3/50], Train Loss: 0.1088, Val Loss: 0.2816\n",
      "Epoch [4/50], Train Loss: 0.0987, Val Loss: 0.2594\n",
      "Epoch [5/50], Train Loss: 0.0883, Val Loss: 0.2356\n",
      "Epoch [6/50], Train Loss: 0.0776, Val Loss: 0.2098\n",
      "Epoch [7/50], Train Loss: 0.0665, Val Loss: 0.1817\n",
      "Epoch [8/50], Train Loss: 0.0555, Val Loss: 0.1518\n",
      "Epoch [9/50], Train Loss: 0.0455, Val Loss: 0.1223\n",
      "Epoch [10/50], Train Loss: 0.0379, Val Loss: 0.0975\n",
      "Epoch [11/50], Train Loss: 0.0337, Val Loss: 0.0808\n",
      "Epoch [12/50], Train Loss: 0.0317, Val Loss: 0.0712\n",
      "Epoch [13/50], Train Loss: 0.0305, Val Loss: 0.0652\n",
      "Epoch [14/50], Train Loss: 0.0294, Val Loss: 0.0608\n",
      "Epoch [15/50], Train Loss: 0.0286, Val Loss: 0.0572\n",
      "Epoch [16/50], Train Loss: 0.0279, Val Loss: 0.0542\n",
      "Epoch [17/50], Train Loss: 0.0272, Val Loss: 0.0516\n",
      "Epoch [18/50], Train Loss: 0.0267, Val Loss: 0.0492\n",
      "Epoch [19/50], Train Loss: 0.0261, Val Loss: 0.0470\n",
      "Epoch [20/50], Train Loss: 0.0256, Val Loss: 0.0449\n",
      "Epoch [21/50], Train Loss: 0.0251, Val Loss: 0.0429\n",
      "Epoch [22/50], Train Loss: 0.0246, Val Loss: 0.0410\n",
      "Epoch [23/50], Train Loss: 0.0241, Val Loss: 0.0391\n",
      "Epoch [24/50], Train Loss: 0.0236, Val Loss: 0.0373\n",
      "Epoch [25/50], Train Loss: 0.0231, Val Loss: 0.0355\n",
      "Epoch [26/50], Train Loss: 0.0226, Val Loss: 0.0337\n",
      "Epoch [27/50], Train Loss: 0.0221, Val Loss: 0.0321\n",
      "Epoch [28/50], Train Loss: 0.0217, Val Loss: 0.0305\n",
      "Epoch [29/50], Train Loss: 0.0212, Val Loss: 0.0291\n",
      "Epoch [30/50], Train Loss: 0.0208, Val Loss: 0.0277\n",
      "Epoch [31/50], Train Loss: 0.0204, Val Loss: 0.0265\n",
      "Epoch [32/50], Train Loss: 0.0200, Val Loss: 0.0253\n",
      "Epoch [33/50], Train Loss: 0.0196, Val Loss: 0.0243\n",
      "Epoch [34/50], Train Loss: 0.0193, Val Loss: 0.0234\n",
      "Epoch [35/50], Train Loss: 0.0189, Val Loss: 0.0225\n",
      "Epoch [36/50], Train Loss: 0.0186, Val Loss: 0.0217\n",
      "Epoch [37/50], Train Loss: 0.0182, Val Loss: 0.0209\n",
      "Epoch [38/50], Train Loss: 0.0179, Val Loss: 0.0201\n",
      "Epoch [39/50], Train Loss: 0.0175, Val Loss: 0.0192\n",
      "Epoch [40/50], Train Loss: 0.0171, Val Loss: 0.0182\n",
      "Epoch [41/50], Train Loss: 0.0167, Val Loss: 0.0170\n",
      "Epoch [42/50], Train Loss: 0.0162, Val Loss: 0.0157\n",
      "Epoch [43/50], Train Loss: 0.0156, Val Loss: 0.0142\n",
      "Epoch [44/50], Train Loss: 0.0149, Val Loss: 0.0124\n",
      "Epoch [45/50], Train Loss: 0.0141, Val Loss: 0.0104\n",
      "Epoch [46/50], Train Loss: 0.0131, Val Loss: 0.0084\n",
      "Epoch [47/50], Train Loss: 0.0120, Val Loss: 0.0067\n",
      "Epoch [48/50], Train Loss: 0.0108, Val Loss: 0.0056\n",
      "Epoch [49/50], Train Loss: 0.0097, Val Loss: 0.0050\n",
      "Epoch [50/50], Train Loss: 0.0087, Val Loss: 0.0049\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=32, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1073, Val Loss: 0.3058\n",
      "Epoch [2/50], Train Loss: 0.0985, Val Loss: 0.2863\n",
      "Epoch [3/50], Train Loss: 0.0909, Val Loss: 0.2684\n",
      "Epoch [4/50], Train Loss: 0.0831, Val Loss: 0.2511\n",
      "Epoch [5/50], Train Loss: 0.0768, Val Loss: 0.2337\n",
      "Epoch [6/50], Train Loss: 0.0698, Val Loss: 0.2158\n",
      "Epoch [7/50], Train Loss: 0.0630, Val Loss: 0.1974\n",
      "Epoch [8/50], Train Loss: 0.0569, Val Loss: 0.1784\n",
      "Epoch [9/50], Train Loss: 0.0517, Val Loss: 0.1592\n",
      "Epoch [10/50], Train Loss: 0.0462, Val Loss: 0.1404\n",
      "Epoch [11/50], Train Loss: 0.0422, Val Loss: 0.1233\n",
      "Epoch [12/50], Train Loss: 0.0383, Val Loss: 0.1087\n",
      "Epoch [13/50], Train Loss: 0.0363, Val Loss: 0.0968\n",
      "Epoch [14/50], Train Loss: 0.0342, Val Loss: 0.0872\n",
      "Epoch [15/50], Train Loss: 0.0328, Val Loss: 0.0801\n",
      "Epoch [16/50], Train Loss: 0.0319, Val Loss: 0.0741\n",
      "Epoch [17/50], Train Loss: 0.0310, Val Loss: 0.0689\n",
      "Epoch [18/50], Train Loss: 0.0288, Val Loss: 0.0640\n",
      "Epoch [19/50], Train Loss: 0.0282, Val Loss: 0.0594\n",
      "Epoch [20/50], Train Loss: 0.0274, Val Loss: 0.0553\n",
      "Epoch [21/50], Train Loss: 0.0272, Val Loss: 0.0511\n",
      "Epoch [22/50], Train Loss: 0.0259, Val Loss: 0.0469\n",
      "Epoch [23/50], Train Loss: 0.0250, Val Loss: 0.0433\n",
      "Epoch [24/50], Train Loss: 0.0247, Val Loss: 0.0394\n",
      "Epoch [25/50], Train Loss: 0.0226, Val Loss: 0.0348\n",
      "Epoch [26/50], Train Loss: 0.0224, Val Loss: 0.0306\n",
      "Epoch [27/50], Train Loss: 0.0203, Val Loss: 0.0254\n",
      "Epoch [28/50], Train Loss: 0.0196, Val Loss: 0.0212\n",
      "Epoch [29/50], Train Loss: 0.0188, Val Loss: 0.0181\n",
      "Epoch [30/50], Train Loss: 0.0172, Val Loss: 0.0158\n",
      "Epoch [31/50], Train Loss: 0.0163, Val Loss: 0.0146\n",
      "Epoch [32/50], Train Loss: 0.0155, Val Loss: 0.0143\n",
      "Epoch [33/50], Train Loss: 0.0140, Val Loss: 0.0145\n",
      "Epoch [34/50], Train Loss: 0.0134, Val Loss: 0.0147\n",
      "Epoch [35/50], Train Loss: 0.0126, Val Loss: 0.0147\n",
      "Epoch [36/50], Train Loss: 0.0117, Val Loss: 0.0145\n",
      "Epoch [37/50], Train Loss: 0.0110, Val Loss: 0.0147\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=32, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1462, Val Loss: 0.4037\n",
      "Epoch [2/50], Train Loss: 0.1350, Val Loss: 0.3820\n",
      "Epoch [3/50], Train Loss: 0.1274, Val Loss: 0.3616\n",
      "Epoch [4/50], Train Loss: 0.1177, Val Loss: 0.3410\n",
      "Epoch [5/50], Train Loss: 0.1090, Val Loss: 0.3195\n",
      "Epoch [6/50], Train Loss: 0.0997, Val Loss: 0.2966\n",
      "Epoch [7/50], Train Loss: 0.0904, Val Loss: 0.2714\n",
      "Epoch [8/50], Train Loss: 0.0831, Val Loss: 0.2433\n",
      "Epoch [9/50], Train Loss: 0.0731, Val Loss: 0.2116\n",
      "Epoch [10/50], Train Loss: 0.0628, Val Loss: 0.1756\n",
      "Epoch [11/50], Train Loss: 0.0525, Val Loss: 0.1383\n",
      "Epoch [12/50], Train Loss: 0.0467, Val Loss: 0.1059\n",
      "Epoch [13/50], Train Loss: 0.0437, Val Loss: 0.0839\n",
      "Epoch [14/50], Train Loss: 0.0412, Val Loss: 0.0722\n",
      "Epoch [15/50], Train Loss: 0.0402, Val Loss: 0.0654\n",
      "Epoch [16/50], Train Loss: 0.0384, Val Loss: 0.0585\n",
      "Epoch [17/50], Train Loss: 0.0360, Val Loss: 0.0549\n",
      "Epoch [18/50], Train Loss: 0.0354, Val Loss: 0.0502\n",
      "Epoch [19/50], Train Loss: 0.0350, Val Loss: 0.0457\n",
      "Epoch [20/50], Train Loss: 0.0326, Val Loss: 0.0402\n",
      "Epoch [21/50], Train Loss: 0.0323, Val Loss: 0.0378\n",
      "Epoch [22/50], Train Loss: 0.0321, Val Loss: 0.0351\n",
      "Epoch [23/50], Train Loss: 0.0310, Val Loss: 0.0305\n",
      "Epoch [24/50], Train Loss: 0.0295, Val Loss: 0.0278\n",
      "Epoch [25/50], Train Loss: 0.0289, Val Loss: 0.0256\n",
      "Epoch [26/50], Train Loss: 0.0299, Val Loss: 0.0229\n",
      "Epoch [27/50], Train Loss: 0.0287, Val Loss: 0.0197\n",
      "Epoch [28/50], Train Loss: 0.0271, Val Loss: 0.0194\n",
      "Epoch [29/50], Train Loss: 0.0269, Val Loss: 0.0169\n",
      "Epoch [30/50], Train Loss: 0.0261, Val Loss: 0.0151\n",
      "Epoch [31/50], Train Loss: 0.0269, Val Loss: 0.0142\n",
      "Epoch [32/50], Train Loss: 0.0245, Val Loss: 0.0117\n",
      "Epoch [33/50], Train Loss: 0.0242, Val Loss: 0.0101\n",
      "Epoch [34/50], Train Loss: 0.0233, Val Loss: 0.0094\n",
      "Epoch [35/50], Train Loss: 0.0227, Val Loss: 0.0085\n",
      "Epoch [36/50], Train Loss: 0.0218, Val Loss: 0.0074\n",
      "Epoch [37/50], Train Loss: 0.0225, Val Loss: 0.0061\n",
      "Epoch [38/50], Train Loss: 0.0218, Val Loss: 0.0063\n",
      "Epoch [39/50], Train Loss: 0.0199, Val Loss: 0.0049\n",
      "Epoch [40/50], Train Loss: 0.0194, Val Loss: 0.0045\n",
      "Epoch [41/50], Train Loss: 0.0200, Val Loss: 0.0041\n",
      "Epoch [42/50], Train Loss: 0.0192, Val Loss: 0.0039\n",
      "Epoch [43/50], Train Loss: 0.0180, Val Loss: 0.0042\n",
      "Epoch [44/50], Train Loss: 0.0181, Val Loss: 0.0040\n",
      "Epoch [45/50], Train Loss: 0.0171, Val Loss: 0.0036\n",
      "Epoch [46/50], Train Loss: 0.0177, Val Loss: 0.0052\n",
      "Epoch [47/50], Train Loss: 0.0167, Val Loss: 0.0050\n",
      "Epoch [48/50], Train Loss: 0.0166, Val Loss: 0.0041\n",
      "Epoch [49/50], Train Loss: 0.0166, Val Loss: 0.0048\n",
      "Epoch [50/50], Train Loss: 0.0163, Val Loss: 0.0041\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=32, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1303, Val Loss: 0.3696\n",
      "Epoch [2/50], Train Loss: 0.1179, Val Loss: 0.3466\n",
      "Epoch [3/50], Train Loss: 0.1063, Val Loss: 0.3226\n",
      "Epoch [4/50], Train Loss: 0.0944, Val Loss: 0.2954\n",
      "Epoch [5/50], Train Loss: 0.0814, Val Loss: 0.2625\n",
      "Epoch [6/50], Train Loss: 0.0670, Val Loss: 0.2209\n",
      "Epoch [7/50], Train Loss: 0.0518, Val Loss: 0.1699\n",
      "Epoch [8/50], Train Loss: 0.0397, Val Loss: 0.1212\n",
      "Epoch [9/50], Train Loss: 0.0345, Val Loss: 0.0916\n",
      "Epoch [10/50], Train Loss: 0.0331, Val Loss: 0.0774\n",
      "Epoch [11/50], Train Loss: 0.0320, Val Loss: 0.0690\n",
      "Epoch [12/50], Train Loss: 0.0309, Val Loss: 0.0625\n",
      "Epoch [13/50], Train Loss: 0.0298, Val Loss: 0.0568\n",
      "Epoch [14/50], Train Loss: 0.0287, Val Loss: 0.0516\n",
      "Epoch [15/50], Train Loss: 0.0276, Val Loss: 0.0466\n",
      "Epoch [16/50], Train Loss: 0.0265, Val Loss: 0.0418\n",
      "Epoch [17/50], Train Loss: 0.0253, Val Loss: 0.0373\n",
      "Epoch [18/50], Train Loss: 0.0240, Val Loss: 0.0330\n",
      "Epoch [19/50], Train Loss: 0.0228, Val Loss: 0.0290\n",
      "Epoch [20/50], Train Loss: 0.0216, Val Loss: 0.0253\n",
      "Epoch [21/50], Train Loss: 0.0204, Val Loss: 0.0220\n",
      "Epoch [22/50], Train Loss: 0.0192, Val Loss: 0.0191\n",
      "Epoch [23/50], Train Loss: 0.0181, Val Loss: 0.0167\n",
      "Epoch [24/50], Train Loss: 0.0171, Val Loss: 0.0147\n",
      "Epoch [25/50], Train Loss: 0.0161, Val Loss: 0.0133\n",
      "Epoch [26/50], Train Loss: 0.0151, Val Loss: 0.0125\n",
      "Epoch [27/50], Train Loss: 0.0138, Val Loss: 0.0125\n",
      "Epoch [28/50], Train Loss: 0.0123, Val Loss: 0.0143\n",
      "Epoch [29/50], Train Loss: 0.0102, Val Loss: 0.0192\n",
      "Epoch [30/50], Train Loss: 0.0079, Val Loss: 0.0265\n",
      "Epoch [31/50], Train Loss: 0.0064, Val Loss: 0.0328\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=32, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1409, Val Loss: 0.3302\n",
      "Epoch [2/50], Train Loss: 0.1320, Val Loss: 0.3138\n",
      "Epoch [3/50], Train Loss: 0.1236, Val Loss: 0.2965\n",
      "Epoch [4/50], Train Loss: 0.1143, Val Loss: 0.2756\n",
      "Epoch [5/50], Train Loss: 0.1029, Val Loss: 0.2477\n",
      "Epoch [6/50], Train Loss: 0.0886, Val Loss: 0.2068\n",
      "Epoch [7/50], Train Loss: 0.0692, Val Loss: 0.1452\n",
      "Epoch [8/50], Train Loss: 0.0481, Val Loss: 0.0789\n",
      "Epoch [9/50], Train Loss: 0.0403, Val Loss: 0.0603\n",
      "Epoch [10/50], Train Loss: 0.0403, Val Loss: 0.0631\n",
      "Epoch [11/50], Train Loss: 0.0378, Val Loss: 0.0623\n",
      "Epoch [12/50], Train Loss: 0.0369, Val Loss: 0.0608\n",
      "Epoch [13/50], Train Loss: 0.0366, Val Loss: 0.0592\n",
      "Epoch [14/50], Train Loss: 0.0350, Val Loss: 0.0585\n",
      "Epoch [15/50], Train Loss: 0.0344, Val Loss: 0.0565\n",
      "Epoch [16/50], Train Loss: 0.0335, Val Loss: 0.0532\n",
      "Epoch [17/50], Train Loss: 0.0324, Val Loss: 0.0517\n",
      "Epoch [18/50], Train Loss: 0.0323, Val Loss: 0.0487\n",
      "Epoch [19/50], Train Loss: 0.0316, Val Loss: 0.0474\n",
      "Epoch [20/50], Train Loss: 0.0306, Val Loss: 0.0448\n",
      "Epoch [21/50], Train Loss: 0.0298, Val Loss: 0.0422\n",
      "Epoch [22/50], Train Loss: 0.0299, Val Loss: 0.0406\n",
      "Epoch [23/50], Train Loss: 0.0288, Val Loss: 0.0392\n",
      "Epoch [24/50], Train Loss: 0.0284, Val Loss: 0.0373\n",
      "Epoch [25/50], Train Loss: 0.0279, Val Loss: 0.0359\n",
      "Epoch [26/50], Train Loss: 0.0272, Val Loss: 0.0346\n",
      "Epoch [27/50], Train Loss: 0.0266, Val Loss: 0.0330\n",
      "Epoch [28/50], Train Loss: 0.0261, Val Loss: 0.0314\n",
      "Epoch [29/50], Train Loss: 0.0255, Val Loss: 0.0301\n",
      "Epoch [30/50], Train Loss: 0.0259, Val Loss: 0.0277\n",
      "Epoch [31/50], Train Loss: 0.0257, Val Loss: 0.0274\n",
      "Epoch [32/50], Train Loss: 0.0248, Val Loss: 0.0262\n",
      "Epoch [33/50], Train Loss: 0.0243, Val Loss: 0.0255\n",
      "Epoch [34/50], Train Loss: 0.0242, Val Loss: 0.0240\n",
      "Epoch [35/50], Train Loss: 0.0231, Val Loss: 0.0231\n",
      "Epoch [36/50], Train Loss: 0.0226, Val Loss: 0.0219\n",
      "Epoch [37/50], Train Loss: 0.0223, Val Loss: 0.0206\n",
      "Epoch [38/50], Train Loss: 0.0217, Val Loss: 0.0192\n",
      "Epoch [39/50], Train Loss: 0.0216, Val Loss: 0.0172\n",
      "Epoch [40/50], Train Loss: 0.0199, Val Loss: 0.0149\n",
      "Epoch [41/50], Train Loss: 0.0195, Val Loss: 0.0134\n",
      "Epoch [42/50], Train Loss: 0.0182, Val Loss: 0.0105\n",
      "Epoch [43/50], Train Loss: 0.0167, Val Loss: 0.0081\n",
      "Epoch [44/50], Train Loss: 0.0157, Val Loss: 0.0067\n",
      "Epoch [45/50], Train Loss: 0.0139, Val Loss: 0.0057\n",
      "Epoch [46/50], Train Loss: 0.0135, Val Loss: 0.0064\n",
      "Epoch [47/50], Train Loss: 0.0124, Val Loss: 0.0080\n",
      "Epoch [48/50], Train Loss: 0.0114, Val Loss: 0.0111\n",
      "Epoch [49/50], Train Loss: 0.0116, Val Loss: 0.0111\n",
      "Epoch [50/50], Train Loss: 0.0109, Val Loss: 0.0114\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=32, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1294, Val Loss: 0.3615\n",
      "Epoch [2/50], Train Loss: 0.1182, Val Loss: 0.3400\n",
      "Epoch [3/50], Train Loss: 0.1093, Val Loss: 0.3167\n",
      "Epoch [4/50], Train Loss: 0.0990, Val Loss: 0.2890\n",
      "Epoch [5/50], Train Loss: 0.0877, Val Loss: 0.2534\n",
      "Epoch [6/50], Train Loss: 0.0739, Val Loss: 0.2068\n",
      "Epoch [7/50], Train Loss: 0.0604, Val Loss: 0.1535\n",
      "Epoch [8/50], Train Loss: 0.0538, Val Loss: 0.1122\n",
      "Epoch [9/50], Train Loss: 0.0503, Val Loss: 0.0903\n",
      "Epoch [10/50], Train Loss: 0.0508, Val Loss: 0.0808\n",
      "Epoch [11/50], Train Loss: 0.0475, Val Loss: 0.0743\n",
      "Epoch [12/50], Train Loss: 0.0440, Val Loss: 0.0665\n",
      "Epoch [13/50], Train Loss: 0.0435, Val Loss: 0.0598\n",
      "Epoch [14/50], Train Loss: 0.0426, Val Loss: 0.0545\n",
      "Epoch [15/50], Train Loss: 0.0399, Val Loss: 0.0492\n",
      "Epoch [16/50], Train Loss: 0.0407, Val Loss: 0.0425\n",
      "Epoch [17/50], Train Loss: 0.0376, Val Loss: 0.0364\n",
      "Epoch [18/50], Train Loss: 0.0376, Val Loss: 0.0301\n",
      "Epoch [19/50], Train Loss: 0.0355, Val Loss: 0.0255\n",
      "Epoch [20/50], Train Loss: 0.0327, Val Loss: 0.0212\n",
      "Epoch [21/50], Train Loss: 0.0322, Val Loss: 0.0202\n",
      "Epoch [22/50], Train Loss: 0.0305, Val Loss: 0.0205\n",
      "Epoch [23/50], Train Loss: 0.0301, Val Loss: 0.0235\n",
      "Epoch [24/50], Train Loss: 0.0280, Val Loss: 0.0262\n",
      "Epoch [25/50], Train Loss: 0.0274, Val Loss: 0.0292\n",
      "Epoch [26/50], Train Loss: 0.0267, Val Loss: 0.0327\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=32, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1264, Val Loss: 0.3453\n",
      "Epoch [2/50], Train Loss: 0.1161, Val Loss: 0.3274\n",
      "Epoch [3/50], Train Loss: 0.1061, Val Loss: 0.3077\n",
      "Epoch [4/50], Train Loss: 0.0949, Val Loss: 0.2834\n",
      "Epoch [5/50], Train Loss: 0.0814, Val Loss: 0.2502\n",
      "Epoch [6/50], Train Loss: 0.0642, Val Loss: 0.2012\n",
      "Epoch [7/50], Train Loss: 0.0456, Val Loss: 0.1380\n",
      "Epoch [8/50], Train Loss: 0.0369, Val Loss: 0.0988\n",
      "Epoch [9/50], Train Loss: 0.0362, Val Loss: 0.0864\n",
      "Epoch [10/50], Train Loss: 0.0352, Val Loss: 0.0795\n",
      "Epoch [11/50], Train Loss: 0.0343, Val Loss: 0.0740\n",
      "Epoch [12/50], Train Loss: 0.0332, Val Loss: 0.0684\n",
      "Epoch [13/50], Train Loss: 0.0320, Val Loss: 0.0620\n",
      "Epoch [14/50], Train Loss: 0.0307, Val Loss: 0.0545\n",
      "Epoch [15/50], Train Loss: 0.0293, Val Loss: 0.0459\n",
      "Epoch [16/50], Train Loss: 0.0277, Val Loss: 0.0369\n",
      "Epoch [17/50], Train Loss: 0.0261, Val Loss: 0.0290\n",
      "Epoch [18/50], Train Loss: 0.0247, Val Loss: 0.0236\n",
      "Epoch [19/50], Train Loss: 0.0235, Val Loss: 0.0204\n",
      "Epoch [20/50], Train Loss: 0.0225, Val Loss: 0.0186\n",
      "Epoch [21/50], Train Loss: 0.0217, Val Loss: 0.0178\n",
      "Epoch [22/50], Train Loss: 0.0211, Val Loss: 0.0175\n",
      "Epoch [23/50], Train Loss: 0.0206, Val Loss: 0.0177\n",
      "Epoch [24/50], Train Loss: 0.0201, Val Loss: 0.0181\n",
      "Epoch [25/50], Train Loss: 0.0196, Val Loss: 0.0187\n",
      "Epoch [26/50], Train Loss: 0.0192, Val Loss: 0.0194\n",
      "Epoch [27/50], Train Loss: 0.0188, Val Loss: 0.0201\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=32, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1153, Val Loss: 0.3233\n",
      "Epoch [2/50], Train Loss: 0.1046, Val Loss: 0.3039\n",
      "Epoch [3/50], Train Loss: 0.0941, Val Loss: 0.2826\n",
      "Epoch [4/50], Train Loss: 0.0821, Val Loss: 0.2551\n",
      "Epoch [5/50], Train Loss: 0.0682, Val Loss: 0.2156\n",
      "Epoch [6/50], Train Loss: 0.0521, Val Loss: 0.1667\n",
      "Epoch [7/50], Train Loss: 0.0436, Val Loss: 0.1312\n",
      "Epoch [8/50], Train Loss: 0.0407, Val Loss: 0.1146\n",
      "Epoch [9/50], Train Loss: 0.0399, Val Loss: 0.1042\n",
      "Epoch [10/50], Train Loss: 0.0385, Val Loss: 0.0976\n",
      "Epoch [11/50], Train Loss: 0.0385, Val Loss: 0.0927\n",
      "Epoch [12/50], Train Loss: 0.0377, Val Loss: 0.0873\n",
      "Epoch [13/50], Train Loss: 0.0364, Val Loss: 0.0824\n",
      "Epoch [14/50], Train Loss: 0.0362, Val Loss: 0.0772\n",
      "Epoch [15/50], Train Loss: 0.0343, Val Loss: 0.0717\n",
      "Epoch [16/50], Train Loss: 0.0342, Val Loss: 0.0669\n",
      "Epoch [17/50], Train Loss: 0.0328, Val Loss: 0.0611\n",
      "Epoch [18/50], Train Loss: 0.0312, Val Loss: 0.0556\n",
      "Epoch [19/50], Train Loss: 0.0303, Val Loss: 0.0491\n",
      "Epoch [20/50], Train Loss: 0.0287, Val Loss: 0.0450\n",
      "Epoch [21/50], Train Loss: 0.0280, Val Loss: 0.0403\n",
      "Epoch [22/50], Train Loss: 0.0272, Val Loss: 0.0360\n",
      "Epoch [23/50], Train Loss: 0.0266, Val Loss: 0.0339\n",
      "Epoch [24/50], Train Loss: 0.0262, Val Loss: 0.0308\n",
      "Epoch [25/50], Train Loss: 0.0251, Val Loss: 0.0292\n",
      "Epoch [26/50], Train Loss: 0.0253, Val Loss: 0.0271\n",
      "Epoch [27/50], Train Loss: 0.0248, Val Loss: 0.0265\n",
      "Epoch [28/50], Train Loss: 0.0240, Val Loss: 0.0255\n",
      "Epoch [29/50], Train Loss: 0.0238, Val Loss: 0.0245\n",
      "Epoch [30/50], Train Loss: 0.0226, Val Loss: 0.0229\n",
      "Epoch [31/50], Train Loss: 0.0231, Val Loss: 0.0236\n",
      "Epoch [32/50], Train Loss: 0.0218, Val Loss: 0.0233\n",
      "Epoch [33/50], Train Loss: 0.0217, Val Loss: 0.0225\n",
      "Epoch [34/50], Train Loss: 0.0208, Val Loss: 0.0231\n",
      "Epoch [35/50], Train Loss: 0.0202, Val Loss: 0.0223\n",
      "Epoch [36/50], Train Loss: 0.0188, Val Loss: 0.0224\n",
      "Epoch [37/50], Train Loss: 0.0174, Val Loss: 0.0225\n",
      "Epoch [38/50], Train Loss: 0.0147, Val Loss: 0.0234\n",
      "Epoch [39/50], Train Loss: 0.0131, Val Loss: 0.0227\n",
      "Epoch [40/50], Train Loss: 0.0112, Val Loss: 0.0243\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=32, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1541, Val Loss: 0.3788\n",
      "Epoch [2/50], Train Loss: 0.1403, Val Loss: 0.3494\n",
      "Epoch [3/50], Train Loss: 0.1251, Val Loss: 0.3155\n",
      "Epoch [4/50], Train Loss: 0.1043, Val Loss: 0.2716\n",
      "Epoch [5/50], Train Loss: 0.0813, Val Loss: 0.2120\n",
      "Epoch [6/50], Train Loss: 0.0654, Val Loss: 0.1480\n",
      "Epoch [7/50], Train Loss: 0.0543, Val Loss: 0.1177\n",
      "Epoch [8/50], Train Loss: 0.0545, Val Loss: 0.1123\n",
      "Epoch [9/50], Train Loss: 0.0556, Val Loss: 0.1040\n",
      "Epoch [10/50], Train Loss: 0.0532, Val Loss: 0.0976\n",
      "Epoch [11/50], Train Loss: 0.0520, Val Loss: 0.0932\n",
      "Epoch [12/50], Train Loss: 0.0500, Val Loss: 0.0877\n",
      "Epoch [13/50], Train Loss: 0.0498, Val Loss: 0.0821\n",
      "Epoch [14/50], Train Loss: 0.0499, Val Loss: 0.0767\n",
      "Epoch [15/50], Train Loss: 0.0493, Val Loss: 0.0727\n",
      "Epoch [16/50], Train Loss: 0.0454, Val Loss: 0.0644\n",
      "Epoch [17/50], Train Loss: 0.0467, Val Loss: 0.0594\n",
      "Epoch [18/50], Train Loss: 0.0449, Val Loss: 0.0536\n",
      "Epoch [19/50], Train Loss: 0.0426, Val Loss: 0.0476\n",
      "Epoch [20/50], Train Loss: 0.0420, Val Loss: 0.0426\n",
      "Epoch [21/50], Train Loss: 0.0413, Val Loss: 0.0360\n",
      "Epoch [22/50], Train Loss: 0.0396, Val Loss: 0.0324\n",
      "Epoch [23/50], Train Loss: 0.0401, Val Loss: 0.0310\n",
      "Epoch [24/50], Train Loss: 0.0390, Val Loss: 0.0298\n",
      "Epoch [25/50], Train Loss: 0.0372, Val Loss: 0.0255\n",
      "Epoch [26/50], Train Loss: 0.0374, Val Loss: 0.0245\n",
      "Epoch [27/50], Train Loss: 0.0366, Val Loss: 0.0252\n",
      "Epoch [28/50], Train Loss: 0.0362, Val Loss: 0.0230\n",
      "Epoch [29/50], Train Loss: 0.0350, Val Loss: 0.0222\n",
      "Epoch [30/50], Train Loss: 0.0343, Val Loss: 0.0219\n",
      "Epoch [31/50], Train Loss: 0.0353, Val Loss: 0.0206\n",
      "Epoch [32/50], Train Loss: 0.0357, Val Loss: 0.0213\n",
      "Epoch [33/50], Train Loss: 0.0333, Val Loss: 0.0185\n",
      "Epoch [34/50], Train Loss: 0.0325, Val Loss: 0.0195\n",
      "Epoch [35/50], Train Loss: 0.0325, Val Loss: 0.0187\n",
      "Epoch [36/50], Train Loss: 0.0300, Val Loss: 0.0170\n",
      "Epoch [37/50], Train Loss: 0.0297, Val Loss: 0.0174\n",
      "Epoch [38/50], Train Loss: 0.0296, Val Loss: 0.0159\n",
      "Epoch [39/50], Train Loss: 0.0286, Val Loss: 0.0174\n",
      "Epoch [40/50], Train Loss: 0.0272, Val Loss: 0.0188\n",
      "Epoch [41/50], Train Loss: 0.0257, Val Loss: 0.0213\n",
      "Epoch [42/50], Train Loss: 0.0248, Val Loss: 0.0246\n",
      "Epoch [43/50], Train Loss: 0.0234, Val Loss: 0.0266\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=64, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1408, Val Loss: 0.3594\n",
      "Epoch [2/50], Train Loss: 0.1237, Val Loss: 0.3250\n",
      "Epoch [3/50], Train Loss: 0.1066, Val Loss: 0.2861\n",
      "Epoch [4/50], Train Loss: 0.0873, Val Loss: 0.2366\n",
      "Epoch [5/50], Train Loss: 0.0639, Val Loss: 0.1678\n",
      "Epoch [6/50], Train Loss: 0.0374, Val Loss: 0.0821\n",
      "Epoch [7/50], Train Loss: 0.0263, Val Loss: 0.0496\n",
      "Epoch [8/50], Train Loss: 0.0255, Val Loss: 0.0436\n",
      "Epoch [9/50], Train Loss: 0.0242, Val Loss: 0.0381\n",
      "Epoch [10/50], Train Loss: 0.0233, Val Loss: 0.0345\n",
      "Epoch [11/50], Train Loss: 0.0224, Val Loss: 0.0313\n",
      "Epoch [12/50], Train Loss: 0.0216, Val Loss: 0.0286\n",
      "Epoch [13/50], Train Loss: 0.0209, Val Loss: 0.0263\n",
      "Epoch [14/50], Train Loss: 0.0202, Val Loss: 0.0243\n",
      "Epoch [15/50], Train Loss: 0.0196, Val Loss: 0.0225\n",
      "Epoch [16/50], Train Loss: 0.0191, Val Loss: 0.0210\n",
      "Epoch [17/50], Train Loss: 0.0186, Val Loss: 0.0198\n",
      "Epoch [18/50], Train Loss: 0.0181, Val Loss: 0.0186\n",
      "Epoch [19/50], Train Loss: 0.0176, Val Loss: 0.0176\n",
      "Epoch [20/50], Train Loss: 0.0172, Val Loss: 0.0166\n",
      "Epoch [21/50], Train Loss: 0.0167, Val Loss: 0.0155\n",
      "Epoch [22/50], Train Loss: 0.0162, Val Loss: 0.0144\n",
      "Epoch [23/50], Train Loss: 0.0156, Val Loss: 0.0132\n",
      "Epoch [24/50], Train Loss: 0.0150, Val Loss: 0.0117\n",
      "Epoch [25/50], Train Loss: 0.0142, Val Loss: 0.0099\n",
      "Epoch [26/50], Train Loss: 0.0133, Val Loss: 0.0077\n",
      "Epoch [27/50], Train Loss: 0.0120, Val Loss: 0.0054\n",
      "Epoch [28/50], Train Loss: 0.0103, Val Loss: 0.0045\n",
      "Epoch [29/50], Train Loss: 0.0085, Val Loss: 0.0064\n",
      "Epoch [30/50], Train Loss: 0.0073, Val Loss: 0.0091\n",
      "Epoch [31/50], Train Loss: 0.0065, Val Loss: 0.0108\n",
      "Epoch [32/50], Train Loss: 0.0058, Val Loss: 0.0118\n",
      "Epoch [33/50], Train Loss: 0.0053, Val Loss: 0.0126\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=64, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1122, Val Loss: 0.3145\n",
      "Epoch [2/50], Train Loss: 0.0963, Val Loss: 0.2802\n",
      "Epoch [3/50], Train Loss: 0.0815, Val Loss: 0.2432\n",
      "Epoch [4/50], Train Loss: 0.0661, Val Loss: 0.2006\n",
      "Epoch [5/50], Train Loss: 0.0508, Val Loss: 0.1509\n",
      "Epoch [6/50], Train Loss: 0.0379, Val Loss: 0.1005\n",
      "Epoch [7/50], Train Loss: 0.0312, Val Loss: 0.0680\n",
      "Epoch [8/50], Train Loss: 0.0294, Val Loss: 0.0532\n",
      "Epoch [9/50], Train Loss: 0.0288, Val Loss: 0.0450\n",
      "Epoch [10/50], Train Loss: 0.0269, Val Loss: 0.0377\n",
      "Epoch [11/50], Train Loss: 0.0256, Val Loss: 0.0319\n",
      "Epoch [12/50], Train Loss: 0.0250, Val Loss: 0.0267\n",
      "Epoch [13/50], Train Loss: 0.0246, Val Loss: 0.0240\n",
      "Epoch [14/50], Train Loss: 0.0234, Val Loss: 0.0201\n",
      "Epoch [15/50], Train Loss: 0.0225, Val Loss: 0.0167\n",
      "Epoch [16/50], Train Loss: 0.0219, Val Loss: 0.0148\n",
      "Epoch [17/50], Train Loss: 0.0209, Val Loss: 0.0129\n",
      "Epoch [18/50], Train Loss: 0.0202, Val Loss: 0.0119\n",
      "Epoch [19/50], Train Loss: 0.0196, Val Loss: 0.0114\n",
      "Epoch [20/50], Train Loss: 0.0187, Val Loss: 0.0107\n",
      "Epoch [21/50], Train Loss: 0.0182, Val Loss: 0.0107\n",
      "Epoch [22/50], Train Loss: 0.0177, Val Loss: 0.0099\n",
      "Epoch [23/50], Train Loss: 0.0175, Val Loss: 0.0095\n",
      "Epoch [24/50], Train Loss: 0.0164, Val Loss: 0.0112\n",
      "Epoch [25/50], Train Loss: 0.0159, Val Loss: 0.0092\n",
      "Epoch [26/50], Train Loss: 0.0153, Val Loss: 0.0092\n",
      "Epoch [27/50], Train Loss: 0.0150, Val Loss: 0.0108\n",
      "Epoch [28/50], Train Loss: 0.0141, Val Loss: 0.0092\n",
      "Epoch [29/50], Train Loss: 0.0135, Val Loss: 0.0118\n",
      "Epoch [30/50], Train Loss: 0.0126, Val Loss: 0.0130\n",
      "Epoch [31/50], Train Loss: 0.0114, Val Loss: 0.0157\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=64, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1203, Val Loss: 0.3360\n",
      "Epoch [2/50], Train Loss: 0.1068, Val Loss: 0.3052\n",
      "Epoch [3/50], Train Loss: 0.0932, Val Loss: 0.2740\n",
      "Epoch [4/50], Train Loss: 0.0811, Val Loss: 0.2394\n",
      "Epoch [5/50], Train Loss: 0.0679, Val Loss: 0.1984\n",
      "Epoch [6/50], Train Loss: 0.0540, Val Loss: 0.1477\n",
      "Epoch [7/50], Train Loss: 0.0422, Val Loss: 0.0949\n",
      "Epoch [8/50], Train Loss: 0.0400, Val Loss: 0.0715\n",
      "Epoch [9/50], Train Loss: 0.0373, Val Loss: 0.0628\n",
      "Epoch [10/50], Train Loss: 0.0377, Val Loss: 0.0554\n",
      "Epoch [11/50], Train Loss: 0.0362, Val Loss: 0.0485\n",
      "Epoch [12/50], Train Loss: 0.0350, Val Loss: 0.0448\n",
      "Epoch [13/50], Train Loss: 0.0331, Val Loss: 0.0385\n",
      "Epoch [14/50], Train Loss: 0.0320, Val Loss: 0.0332\n",
      "Epoch [15/50], Train Loss: 0.0315, Val Loss: 0.0268\n",
      "Epoch [16/50], Train Loss: 0.0304, Val Loss: 0.0218\n",
      "Epoch [17/50], Train Loss: 0.0290, Val Loss: 0.0166\n",
      "Epoch [18/50], Train Loss: 0.0276, Val Loss: 0.0123\n",
      "Epoch [19/50], Train Loss: 0.0269, Val Loss: 0.0091\n",
      "Epoch [20/50], Train Loss: 0.0261, Val Loss: 0.0077\n",
      "Epoch [21/50], Train Loss: 0.0235, Val Loss: 0.0075\n",
      "Epoch [22/50], Train Loss: 0.0239, Val Loss: 0.0065\n",
      "Epoch [23/50], Train Loss: 0.0214, Val Loss: 0.0101\n",
      "Epoch [24/50], Train Loss: 0.0207, Val Loss: 0.0119\n",
      "Epoch [25/50], Train Loss: 0.0190, Val Loss: 0.0165\n",
      "Epoch [26/50], Train Loss: 0.0183, Val Loss: 0.0217\n",
      "Epoch [27/50], Train Loss: 0.0167, Val Loss: 0.0219\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=64, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1342, Val Loss: 0.3471\n",
      "Epoch [2/50], Train Loss: 0.1134, Val Loss: 0.3051\n",
      "Epoch [3/50], Train Loss: 0.0883, Val Loss: 0.2398\n",
      "Epoch [4/50], Train Loss: 0.0515, Val Loss: 0.1216\n",
      "Epoch [5/50], Train Loss: 0.0290, Val Loss: 0.0598\n",
      "Epoch [6/50], Train Loss: 0.0311, Val Loss: 0.0523\n",
      "Epoch [7/50], Train Loss: 0.0290, Val Loss: 0.0460\n",
      "Epoch [8/50], Train Loss: 0.0274, Val Loss: 0.0406\n",
      "Epoch [9/50], Train Loss: 0.0259, Val Loss: 0.0352\n",
      "Epoch [10/50], Train Loss: 0.0245, Val Loss: 0.0303\n",
      "Epoch [11/50], Train Loss: 0.0233, Val Loss: 0.0263\n",
      "Epoch [12/50], Train Loss: 0.0221, Val Loss: 0.0231\n",
      "Epoch [13/50], Train Loss: 0.0212, Val Loss: 0.0209\n",
      "Epoch [14/50], Train Loss: 0.0205, Val Loss: 0.0193\n",
      "Epoch [15/50], Train Loss: 0.0199, Val Loss: 0.0180\n",
      "Epoch [16/50], Train Loss: 0.0193, Val Loss: 0.0168\n",
      "Epoch [17/50], Train Loss: 0.0187, Val Loss: 0.0156\n",
      "Epoch [18/50], Train Loss: 0.0182, Val Loss: 0.0143\n",
      "Epoch [19/50], Train Loss: 0.0176, Val Loss: 0.0130\n",
      "Epoch [20/50], Train Loss: 0.0170, Val Loss: 0.0117\n",
      "Epoch [21/50], Train Loss: 0.0164, Val Loss: 0.0104\n",
      "Epoch [22/50], Train Loss: 0.0156, Val Loss: 0.0092\n",
      "Epoch [23/50], Train Loss: 0.0147, Val Loss: 0.0081\n",
      "Epoch [24/50], Train Loss: 0.0136, Val Loss: 0.0074\n",
      "Epoch [25/50], Train Loss: 0.0122, Val Loss: 0.0082\n",
      "Epoch [26/50], Train Loss: 0.0104, Val Loss: 0.0122\n",
      "Epoch [27/50], Train Loss: 0.0085, Val Loss: 0.0181\n",
      "Epoch [28/50], Train Loss: 0.0069, Val Loss: 0.0218\n",
      "Epoch [29/50], Train Loss: 0.0058, Val Loss: 0.0223\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=64, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1333, Val Loss: 0.3486\n",
      "Epoch [2/50], Train Loss: 0.1138, Val Loss: 0.3121\n",
      "Epoch [3/50], Train Loss: 0.0948, Val Loss: 0.2655\n",
      "Epoch [4/50], Train Loss: 0.0700, Val Loss: 0.1914\n",
      "Epoch [5/50], Train Loss: 0.0405, Val Loss: 0.0857\n",
      "Epoch [6/50], Train Loss: 0.0349, Val Loss: 0.0625\n",
      "Epoch [7/50], Train Loss: 0.0352, Val Loss: 0.0588\n",
      "Epoch [8/50], Train Loss: 0.0339, Val Loss: 0.0524\n",
      "Epoch [9/50], Train Loss: 0.0319, Val Loss: 0.0466\n",
      "Epoch [10/50], Train Loss: 0.0310, Val Loss: 0.0407\n",
      "Epoch [11/50], Train Loss: 0.0298, Val Loss: 0.0341\n",
      "Epoch [12/50], Train Loss: 0.0285, Val Loss: 0.0278\n",
      "Epoch [13/50], Train Loss: 0.0267, Val Loss: 0.0216\n",
      "Epoch [14/50], Train Loss: 0.0254, Val Loss: 0.0160\n",
      "Epoch [15/50], Train Loss: 0.0238, Val Loss: 0.0100\n",
      "Epoch [16/50], Train Loss: 0.0214, Val Loss: 0.0065\n",
      "Epoch [17/50], Train Loss: 0.0190, Val Loss: 0.0090\n",
      "Epoch [18/50], Train Loss: 0.0158, Val Loss: 0.0289\n",
      "Epoch [19/50], Train Loss: 0.0124, Val Loss: 0.0368\n",
      "Epoch [20/50], Train Loss: 0.0112, Val Loss: 0.0456\n",
      "Epoch [21/50], Train Loss: 0.0102, Val Loss: 0.0537\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=64, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1420, Val Loss: 0.3528\n",
      "Epoch [2/50], Train Loss: 0.1180, Val Loss: 0.3024\n",
      "Epoch [3/50], Train Loss: 0.0910, Val Loss: 0.2325\n",
      "Epoch [4/50], Train Loss: 0.0606, Val Loss: 0.1311\n",
      "Epoch [5/50], Train Loss: 0.0435, Val Loss: 0.0742\n",
      "Epoch [6/50], Train Loss: 0.0413, Val Loss: 0.0679\n",
      "Epoch [7/50], Train Loss: 0.0408, Val Loss: 0.0598\n",
      "Epoch [8/50], Train Loss: 0.0409, Val Loss: 0.0518\n",
      "Epoch [9/50], Train Loss: 0.0379, Val Loss: 0.0440\n",
      "Epoch [10/50], Train Loss: 0.0366, Val Loss: 0.0384\n",
      "Epoch [11/50], Train Loss: 0.0340, Val Loss: 0.0319\n",
      "Epoch [12/50], Train Loss: 0.0335, Val Loss: 0.0282\n",
      "Epoch [13/50], Train Loss: 0.0329, Val Loss: 0.0244\n",
      "Epoch [14/50], Train Loss: 0.0304, Val Loss: 0.0219\n",
      "Epoch [15/50], Train Loss: 0.0300, Val Loss: 0.0206\n",
      "Epoch [16/50], Train Loss: 0.0291, Val Loss: 0.0188\n",
      "Epoch [17/50], Train Loss: 0.0284, Val Loss: 0.0178\n",
      "Epoch [18/50], Train Loss: 0.0294, Val Loss: 0.0171\n",
      "Epoch [19/50], Train Loss: 0.0276, Val Loss: 0.0154\n",
      "Epoch [20/50], Train Loss: 0.0268, Val Loss: 0.0144\n",
      "Epoch [21/50], Train Loss: 0.0266, Val Loss: 0.0124\n",
      "Epoch [22/50], Train Loss: 0.0254, Val Loss: 0.0107\n",
      "Epoch [23/50], Train Loss: 0.0243, Val Loss: 0.0083\n",
      "Epoch [24/50], Train Loss: 0.0229, Val Loss: 0.0058\n",
      "Epoch [25/50], Train Loss: 0.0217, Val Loss: 0.0042\n",
      "Epoch [26/50], Train Loss: 0.0199, Val Loss: 0.0031\n",
      "Epoch [27/50], Train Loss: 0.0183, Val Loss: 0.0044\n",
      "Epoch [28/50], Train Loss: 0.0183, Val Loss: 0.0059\n",
      "Epoch [29/50], Train Loss: 0.0159, Val Loss: 0.0061\n",
      "Epoch [30/50], Train Loss: 0.0155, Val Loss: 0.0097\n",
      "Epoch [31/50], Train Loss: 0.0166, Val Loss: 0.0073\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=64, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1198, Val Loss: 0.3050\n",
      "Epoch [2/50], Train Loss: 0.0992, Val Loss: 0.2615\n",
      "Epoch [3/50], Train Loss: 0.0720, Val Loss: 0.1835\n",
      "Epoch [4/50], Train Loss: 0.0401, Val Loss: 0.0839\n",
      "Epoch [5/50], Train Loss: 0.0372, Val Loss: 0.0741\n",
      "Epoch [6/50], Train Loss: 0.0352, Val Loss: 0.0672\n",
      "Epoch [7/50], Train Loss: 0.0341, Val Loss: 0.0623\n",
      "Epoch [8/50], Train Loss: 0.0327, Val Loss: 0.0568\n",
      "Epoch [9/50], Train Loss: 0.0314, Val Loss: 0.0508\n",
      "Epoch [10/50], Train Loss: 0.0299, Val Loss: 0.0444\n",
      "Epoch [11/50], Train Loss: 0.0284, Val Loss: 0.0379\n",
      "Epoch [12/50], Train Loss: 0.0269, Val Loss: 0.0319\n",
      "Epoch [13/50], Train Loss: 0.0253, Val Loss: 0.0268\n",
      "Epoch [14/50], Train Loss: 0.0240, Val Loss: 0.0235\n",
      "Epoch [15/50], Train Loss: 0.0230, Val Loss: 0.0221\n",
      "Epoch [16/50], Train Loss: 0.0223, Val Loss: 0.0221\n",
      "Epoch [17/50], Train Loss: 0.0217, Val Loss: 0.0222\n",
      "Epoch [18/50], Train Loss: 0.0212, Val Loss: 0.0218\n",
      "Epoch [19/50], Train Loss: 0.0207, Val Loss: 0.0210\n",
      "Epoch [20/50], Train Loss: 0.0201, Val Loss: 0.0201\n",
      "Epoch [21/50], Train Loss: 0.0194, Val Loss: 0.0191\n",
      "Epoch [22/50], Train Loss: 0.0184, Val Loss: 0.0184\n",
      "Epoch [23/50], Train Loss: 0.0168, Val Loss: 0.0188\n",
      "Epoch [24/50], Train Loss: 0.0136, Val Loss: 0.0235\n",
      "Epoch [25/50], Train Loss: 0.0079, Val Loss: 0.0306\n",
      "Epoch [26/50], Train Loss: 0.0060, Val Loss: 0.0316\n",
      "Epoch [27/50], Train Loss: 0.0054, Val Loss: 0.0285\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=64, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1305, Val Loss: 0.3439\n",
      "Epoch [2/50], Train Loss: 0.1040, Val Loss: 0.2860\n",
      "Epoch [3/50], Train Loss: 0.0697, Val Loss: 0.1880\n",
      "Epoch [4/50], Train Loss: 0.0393, Val Loss: 0.0874\n",
      "Epoch [5/50], Train Loss: 0.0423, Val Loss: 0.0878\n",
      "Epoch [6/50], Train Loss: 0.0383, Val Loss: 0.0755\n",
      "Epoch [7/50], Train Loss: 0.0375, Val Loss: 0.0682\n",
      "Epoch [8/50], Train Loss: 0.0354, Val Loss: 0.0596\n",
      "Epoch [9/50], Train Loss: 0.0341, Val Loss: 0.0517\n",
      "Epoch [10/50], Train Loss: 0.0321, Val Loss: 0.0439\n",
      "Epoch [11/50], Train Loss: 0.0301, Val Loss: 0.0362\n",
      "Epoch [12/50], Train Loss: 0.0285, Val Loss: 0.0309\n",
      "Epoch [13/50], Train Loss: 0.0270, Val Loss: 0.0275\n",
      "Epoch [14/50], Train Loss: 0.0260, Val Loss: 0.0262\n",
      "Epoch [15/50], Train Loss: 0.0249, Val Loss: 0.0257\n",
      "Epoch [16/50], Train Loss: 0.0246, Val Loss: 0.0254\n",
      "Epoch [17/50], Train Loss: 0.0243, Val Loss: 0.0249\n",
      "Epoch [18/50], Train Loss: 0.0243, Val Loss: 0.0244\n",
      "Epoch [19/50], Train Loss: 0.0237, Val Loss: 0.0229\n",
      "Epoch [20/50], Train Loss: 0.0240, Val Loss: 0.0235\n",
      "Epoch [21/50], Train Loss: 0.0236, Val Loss: 0.0236\n",
      "Epoch [22/50], Train Loss: 0.0230, Val Loss: 0.0231\n",
      "Epoch [23/50], Train Loss: 0.0227, Val Loss: 0.0230\n",
      "Epoch [24/50], Train Loss: 0.0227, Val Loss: 0.0222\n",
      "Epoch [25/50], Train Loss: 0.0219, Val Loss: 0.0212\n",
      "Epoch [26/50], Train Loss: 0.0217, Val Loss: 0.0209\n",
      "Epoch [27/50], Train Loss: 0.0218, Val Loss: 0.0214\n",
      "Epoch [28/50], Train Loss: 0.0218, Val Loss: 0.0213\n",
      "Epoch [29/50], Train Loss: 0.0208, Val Loss: 0.0204\n",
      "Epoch [30/50], Train Loss: 0.0204, Val Loss: 0.0198\n",
      "Epoch [31/50], Train Loss: 0.0196, Val Loss: 0.0196\n",
      "Epoch [32/50], Train Loss: 0.0194, Val Loss: 0.0220\n",
      "Epoch [33/50], Train Loss: 0.0179, Val Loss: 0.0216\n",
      "Epoch [34/50], Train Loss: 0.0143, Val Loss: 0.0228\n",
      "Epoch [35/50], Train Loss: 0.0107, Val Loss: 0.0236\n",
      "Epoch [36/50], Train Loss: 0.0096, Val Loss: 0.0240\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=64, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1644, Val Loss: 0.3909\n",
      "Epoch [2/50], Train Loss: 0.1388, Val Loss: 0.3445\n",
      "Epoch [3/50], Train Loss: 0.1095, Val Loss: 0.2664\n",
      "Epoch [4/50], Train Loss: 0.0635, Val Loss: 0.0987\n",
      "Epoch [5/50], Train Loss: 0.0491, Val Loss: 0.0737\n",
      "Epoch [6/50], Train Loss: 0.0487, Val Loss: 0.0759\n",
      "Epoch [7/50], Train Loss: 0.0465, Val Loss: 0.0693\n",
      "Epoch [8/50], Train Loss: 0.0444, Val Loss: 0.0596\n",
      "Epoch [9/50], Train Loss: 0.0418, Val Loss: 0.0562\n",
      "Epoch [10/50], Train Loss: 0.0388, Val Loss: 0.0498\n",
      "Epoch [11/50], Train Loss: 0.0385, Val Loss: 0.0420\n",
      "Epoch [12/50], Train Loss: 0.0371, Val Loss: 0.0348\n",
      "Epoch [13/50], Train Loss: 0.0348, Val Loss: 0.0288\n",
      "Epoch [14/50], Train Loss: 0.0336, Val Loss: 0.0227\n",
      "Epoch [15/50], Train Loss: 0.0319, Val Loss: 0.0216\n",
      "Epoch [16/50], Train Loss: 0.0310, Val Loss: 0.0205\n",
      "Epoch [17/50], Train Loss: 0.0311, Val Loss: 0.0225\n",
      "Epoch [18/50], Train Loss: 0.0303, Val Loss: 0.0190\n",
      "Epoch [19/50], Train Loss: 0.0286, Val Loss: 0.0161\n",
      "Epoch [20/50], Train Loss: 0.0305, Val Loss: 0.0170\n",
      "Epoch [21/50], Train Loss: 0.0270, Val Loss: 0.0108\n",
      "Epoch [22/50], Train Loss: 0.0262, Val Loss: 0.0100\n",
      "Epoch [23/50], Train Loss: 0.0240, Val Loss: 0.0062\n",
      "Epoch [24/50], Train Loss: 0.0217, Val Loss: 0.0039\n",
      "Epoch [25/50], Train Loss: 0.0191, Val Loss: 0.0029\n",
      "Epoch [26/50], Train Loss: 0.0174, Val Loss: 0.0045\n",
      "Epoch [27/50], Train Loss: 0.0167, Val Loss: 0.0050\n",
      "Epoch [28/50], Train Loss: 0.0161, Val Loss: 0.0055\n",
      "Epoch [29/50], Train Loss: 0.0159, Val Loss: 0.0046\n",
      "Epoch [30/50], Train Loss: 0.0159, Val Loss: 0.0086\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=16, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1746, Val Loss: 0.4169\n",
      "Epoch [2/50], Train Loss: 0.1741, Val Loss: 0.4162\n",
      "Epoch [3/50], Train Loss: 0.1736, Val Loss: 0.4154\n",
      "Epoch [4/50], Train Loss: 0.1732, Val Loss: 0.4146\n",
      "Epoch [5/50], Train Loss: 0.1727, Val Loss: 0.4139\n",
      "Epoch [6/50], Train Loss: 0.1722, Val Loss: 0.4131\n",
      "Epoch [7/50], Train Loss: 0.1718, Val Loss: 0.4124\n",
      "Epoch [8/50], Train Loss: 0.1713, Val Loss: 0.4116\n",
      "Epoch [9/50], Train Loss: 0.1708, Val Loss: 0.4108\n",
      "Epoch [10/50], Train Loss: 0.1704, Val Loss: 0.4101\n",
      "Epoch [11/50], Train Loss: 0.1699, Val Loss: 0.4093\n",
      "Epoch [12/50], Train Loss: 0.1694, Val Loss: 0.4086\n",
      "Epoch [13/50], Train Loss: 0.1690, Val Loss: 0.4078\n",
      "Epoch [14/50], Train Loss: 0.1685, Val Loss: 0.4071\n",
      "Epoch [15/50], Train Loss: 0.1681, Val Loss: 0.4063\n",
      "Epoch [16/50], Train Loss: 0.1676, Val Loss: 0.4056\n",
      "Epoch [17/50], Train Loss: 0.1672, Val Loss: 0.4049\n",
      "Epoch [18/50], Train Loss: 0.1667, Val Loss: 0.4041\n",
      "Epoch [19/50], Train Loss: 0.1663, Val Loss: 0.4034\n",
      "Epoch [20/50], Train Loss: 0.1658, Val Loss: 0.4027\n",
      "Epoch [21/50], Train Loss: 0.1654, Val Loss: 0.4019\n",
      "Epoch [22/50], Train Loss: 0.1649, Val Loss: 0.4012\n",
      "Epoch [23/50], Train Loss: 0.1645, Val Loss: 0.4005\n",
      "Epoch [24/50], Train Loss: 0.1641, Val Loss: 0.3997\n",
      "Epoch [25/50], Train Loss: 0.1636, Val Loss: 0.3990\n",
      "Epoch [26/50], Train Loss: 0.1632, Val Loss: 0.3983\n",
      "Epoch [27/50], Train Loss: 0.1627, Val Loss: 0.3976\n",
      "Epoch [28/50], Train Loss: 0.1623, Val Loss: 0.3969\n",
      "Epoch [29/50], Train Loss: 0.1619, Val Loss: 0.3961\n",
      "Epoch [30/50], Train Loss: 0.1615, Val Loss: 0.3954\n",
      "Epoch [31/50], Train Loss: 0.1610, Val Loss: 0.3947\n",
      "Epoch [32/50], Train Loss: 0.1606, Val Loss: 0.3940\n",
      "Epoch [33/50], Train Loss: 0.1602, Val Loss: 0.3933\n",
      "Epoch [34/50], Train Loss: 0.1597, Val Loss: 0.3926\n",
      "Epoch [35/50], Train Loss: 0.1593, Val Loss: 0.3919\n",
      "Epoch [36/50], Train Loss: 0.1589, Val Loss: 0.3912\n",
      "Epoch [37/50], Train Loss: 0.1585, Val Loss: 0.3905\n",
      "Epoch [38/50], Train Loss: 0.1581, Val Loss: 0.3898\n",
      "Epoch [39/50], Train Loss: 0.1576, Val Loss: 0.3891\n",
      "Epoch [40/50], Train Loss: 0.1572, Val Loss: 0.3884\n",
      "Epoch [41/50], Train Loss: 0.1568, Val Loss: 0.3877\n",
      "Epoch [42/50], Train Loss: 0.1564, Val Loss: 0.3870\n",
      "Epoch [43/50], Train Loss: 0.1560, Val Loss: 0.3863\n",
      "Epoch [44/50], Train Loss: 0.1556, Val Loss: 0.3857\n",
      "Epoch [45/50], Train Loss: 0.1552, Val Loss: 0.3850\n",
      "Epoch [46/50], Train Loss: 0.1548, Val Loss: 0.3843\n",
      "Epoch [47/50], Train Loss: 0.1543, Val Loss: 0.3836\n",
      "Epoch [48/50], Train Loss: 0.1539, Val Loss: 0.3829\n",
      "Epoch [49/50], Train Loss: 0.1535, Val Loss: 0.3823\n",
      "Epoch [50/50], Train Loss: 0.1531, Val Loss: 0.3816\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=16, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1264, Val Loss: 0.3506\n",
      "Epoch [2/50], Train Loss: 0.1265, Val Loss: 0.3500\n",
      "Epoch [3/50], Train Loss: 0.1257, Val Loss: 0.3494\n",
      "Epoch [4/50], Train Loss: 0.1259, Val Loss: 0.3488\n",
      "Epoch [5/50], Train Loss: 0.1257, Val Loss: 0.3482\n",
      "Epoch [6/50], Train Loss: 0.1250, Val Loss: 0.3477\n",
      "Epoch [7/50], Train Loss: 0.1248, Val Loss: 0.3471\n",
      "Epoch [8/50], Train Loss: 0.1241, Val Loss: 0.3465\n",
      "Epoch [9/50], Train Loss: 0.1249, Val Loss: 0.3459\n",
      "Epoch [10/50], Train Loss: 0.1241, Val Loss: 0.3453\n",
      "Epoch [11/50], Train Loss: 0.1235, Val Loss: 0.3448\n",
      "Epoch [12/50], Train Loss: 0.1234, Val Loss: 0.3442\n",
      "Epoch [13/50], Train Loss: 0.1230, Val Loss: 0.3436\n",
      "Epoch [14/50], Train Loss: 0.1222, Val Loss: 0.3430\n",
      "Epoch [15/50], Train Loss: 0.1220, Val Loss: 0.3425\n",
      "Epoch [16/50], Train Loss: 0.1218, Val Loss: 0.3419\n",
      "Epoch [17/50], Train Loss: 0.1214, Val Loss: 0.3413\n",
      "Epoch [18/50], Train Loss: 0.1222, Val Loss: 0.3408\n",
      "Epoch [19/50], Train Loss: 0.1209, Val Loss: 0.3402\n",
      "Epoch [20/50], Train Loss: 0.1212, Val Loss: 0.3396\n",
      "Epoch [21/50], Train Loss: 0.1204, Val Loss: 0.3391\n",
      "Epoch [22/50], Train Loss: 0.1205, Val Loss: 0.3385\n",
      "Epoch [23/50], Train Loss: 0.1200, Val Loss: 0.3380\n",
      "Epoch [24/50], Train Loss: 0.1202, Val Loss: 0.3374\n",
      "Epoch [25/50], Train Loss: 0.1199, Val Loss: 0.3369\n",
      "Epoch [26/50], Train Loss: 0.1190, Val Loss: 0.3363\n",
      "Epoch [27/50], Train Loss: 0.1189, Val Loss: 0.3358\n",
      "Epoch [28/50], Train Loss: 0.1194, Val Loss: 0.3352\n",
      "Epoch [29/50], Train Loss: 0.1181, Val Loss: 0.3346\n",
      "Epoch [30/50], Train Loss: 0.1188, Val Loss: 0.3341\n",
      "Epoch [31/50], Train Loss: 0.1176, Val Loss: 0.3335\n",
      "Epoch [32/50], Train Loss: 0.1179, Val Loss: 0.3330\n",
      "Epoch [33/50], Train Loss: 0.1175, Val Loss: 0.3325\n",
      "Epoch [34/50], Train Loss: 0.1171, Val Loss: 0.3319\n",
      "Epoch [35/50], Train Loss: 0.1175, Val Loss: 0.3314\n",
      "Epoch [36/50], Train Loss: 0.1164, Val Loss: 0.3308\n",
      "Epoch [37/50], Train Loss: 0.1167, Val Loss: 0.3303\n",
      "Epoch [38/50], Train Loss: 0.1157, Val Loss: 0.3298\n",
      "Epoch [39/50], Train Loss: 0.1165, Val Loss: 0.3292\n",
      "Epoch [40/50], Train Loss: 0.1148, Val Loss: 0.3287\n",
      "Epoch [41/50], Train Loss: 0.1140, Val Loss: 0.3282\n",
      "Epoch [42/50], Train Loss: 0.1149, Val Loss: 0.3276\n",
      "Epoch [43/50], Train Loss: 0.1149, Val Loss: 0.3271\n",
      "Epoch [44/50], Train Loss: 0.1148, Val Loss: 0.3265\n",
      "Epoch [45/50], Train Loss: 0.1137, Val Loss: 0.3260\n",
      "Epoch [46/50], Train Loss: 0.1139, Val Loss: 0.3255\n",
      "Epoch [47/50], Train Loss: 0.1128, Val Loss: 0.3250\n",
      "Epoch [48/50], Train Loss: 0.1132, Val Loss: 0.3245\n",
      "Epoch [49/50], Train Loss: 0.1129, Val Loss: 0.3239\n",
      "Epoch [50/50], Train Loss: 0.1130, Val Loss: 0.3234\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=16, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1234, Val Loss: 0.3073\n",
      "Epoch [2/50], Train Loss: 0.1230, Val Loss: 0.3069\n",
      "Epoch [3/50], Train Loss: 0.1215, Val Loss: 0.3065\n",
      "Epoch [4/50], Train Loss: 0.1232, Val Loss: 0.3060\n",
      "Epoch [5/50], Train Loss: 0.1205, Val Loss: 0.3056\n",
      "Epoch [6/50], Train Loss: 0.1205, Val Loss: 0.3052\n",
      "Epoch [7/50], Train Loss: 0.1204, Val Loss: 0.3048\n",
      "Epoch [8/50], Train Loss: 0.1211, Val Loss: 0.3044\n",
      "Epoch [9/50], Train Loss: 0.1198, Val Loss: 0.3039\n",
      "Epoch [10/50], Train Loss: 0.1200, Val Loss: 0.3035\n",
      "Epoch [11/50], Train Loss: 0.1177, Val Loss: 0.3031\n",
      "Epoch [12/50], Train Loss: 0.1200, Val Loss: 0.3027\n",
      "Epoch [13/50], Train Loss: 0.1188, Val Loss: 0.3023\n",
      "Epoch [14/50], Train Loss: 0.1202, Val Loss: 0.3019\n",
      "Epoch [15/50], Train Loss: 0.1184, Val Loss: 0.3015\n",
      "Epoch [16/50], Train Loss: 0.1184, Val Loss: 0.3010\n",
      "Epoch [17/50], Train Loss: 0.1189, Val Loss: 0.3006\n",
      "Epoch [18/50], Train Loss: 0.1174, Val Loss: 0.3002\n",
      "Epoch [19/50], Train Loss: 0.1176, Val Loss: 0.2998\n",
      "Epoch [20/50], Train Loss: 0.1166, Val Loss: 0.2994\n",
      "Epoch [21/50], Train Loss: 0.1181, Val Loss: 0.2990\n",
      "Epoch [22/50], Train Loss: 0.1170, Val Loss: 0.2986\n",
      "Epoch [23/50], Train Loss: 0.1174, Val Loss: 0.2982\n",
      "Epoch [24/50], Train Loss: 0.1180, Val Loss: 0.2978\n",
      "Epoch [25/50], Train Loss: 0.1148, Val Loss: 0.2974\n",
      "Epoch [26/50], Train Loss: 0.1175, Val Loss: 0.2970\n",
      "Epoch [27/50], Train Loss: 0.1166, Val Loss: 0.2966\n",
      "Epoch [28/50], Train Loss: 0.1166, Val Loss: 0.2962\n",
      "Epoch [29/50], Train Loss: 0.1168, Val Loss: 0.2958\n",
      "Epoch [30/50], Train Loss: 0.1153, Val Loss: 0.2954\n",
      "Epoch [31/50], Train Loss: 0.1151, Val Loss: 0.2950\n",
      "Epoch [32/50], Train Loss: 0.1149, Val Loss: 0.2946\n",
      "Epoch [33/50], Train Loss: 0.1152, Val Loss: 0.2942\n",
      "Epoch [34/50], Train Loss: 0.1151, Val Loss: 0.2938\n",
      "Epoch [35/50], Train Loss: 0.1163, Val Loss: 0.2934\n",
      "Epoch [36/50], Train Loss: 0.1157, Val Loss: 0.2930\n",
      "Epoch [37/50], Train Loss: 0.1153, Val Loss: 0.2926\n",
      "Epoch [38/50], Train Loss: 0.1137, Val Loss: 0.2922\n",
      "Epoch [39/50], Train Loss: 0.1145, Val Loss: 0.2918\n",
      "Epoch [40/50], Train Loss: 0.1126, Val Loss: 0.2914\n",
      "Epoch [41/50], Train Loss: 0.1133, Val Loss: 0.2910\n",
      "Epoch [42/50], Train Loss: 0.1127, Val Loss: 0.2907\n",
      "Epoch [43/50], Train Loss: 0.1131, Val Loss: 0.2903\n",
      "Epoch [44/50], Train Loss: 0.1132, Val Loss: 0.2899\n",
      "Epoch [45/50], Train Loss: 0.1135, Val Loss: 0.2895\n",
      "Epoch [46/50], Train Loss: 0.1115, Val Loss: 0.2891\n",
      "Epoch [47/50], Train Loss: 0.1123, Val Loss: 0.2887\n",
      "Epoch [48/50], Train Loss: 0.1123, Val Loss: 0.2883\n",
      "Epoch [49/50], Train Loss: 0.1126, Val Loss: 0.2880\n",
      "Epoch [50/50], Train Loss: 0.1103, Val Loss: 0.2876\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=16, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1924, Val Loss: 0.4058\n",
      "Epoch [2/50], Train Loss: 0.1919, Val Loss: 0.4052\n",
      "Epoch [3/50], Train Loss: 0.1914, Val Loss: 0.4045\n",
      "Epoch [4/50], Train Loss: 0.1910, Val Loss: 0.4039\n",
      "Epoch [5/50], Train Loss: 0.1905, Val Loss: 0.4032\n",
      "Epoch [6/50], Train Loss: 0.1901, Val Loss: 0.4026\n",
      "Epoch [7/50], Train Loss: 0.1896, Val Loss: 0.4020\n",
      "Epoch [8/50], Train Loss: 0.1892, Val Loss: 0.4013\n",
      "Epoch [9/50], Train Loss: 0.1887, Val Loss: 0.4007\n",
      "Epoch [10/50], Train Loss: 0.1883, Val Loss: 0.4000\n",
      "Epoch [11/50], Train Loss: 0.1878, Val Loss: 0.3994\n",
      "Epoch [12/50], Train Loss: 0.1874, Val Loss: 0.3988\n",
      "Epoch [13/50], Train Loss: 0.1869, Val Loss: 0.3981\n",
      "Epoch [14/50], Train Loss: 0.1865, Val Loss: 0.3975\n",
      "Epoch [15/50], Train Loss: 0.1860, Val Loss: 0.3969\n",
      "Epoch [16/50], Train Loss: 0.1856, Val Loss: 0.3963\n",
      "Epoch [17/50], Train Loss: 0.1851, Val Loss: 0.3956\n",
      "Epoch [18/50], Train Loss: 0.1847, Val Loss: 0.3950\n",
      "Epoch [19/50], Train Loss: 0.1842, Val Loss: 0.3944\n",
      "Epoch [20/50], Train Loss: 0.1838, Val Loss: 0.3938\n",
      "Epoch [21/50], Train Loss: 0.1834, Val Loss: 0.3932\n",
      "Epoch [22/50], Train Loss: 0.1829, Val Loss: 0.3926\n",
      "Epoch [23/50], Train Loss: 0.1825, Val Loss: 0.3919\n",
      "Epoch [24/50], Train Loss: 0.1821, Val Loss: 0.3913\n",
      "Epoch [25/50], Train Loss: 0.1816, Val Loss: 0.3907\n",
      "Epoch [26/50], Train Loss: 0.1812, Val Loss: 0.3901\n",
      "Epoch [27/50], Train Loss: 0.1808, Val Loss: 0.3895\n",
      "Epoch [28/50], Train Loss: 0.1804, Val Loss: 0.3889\n",
      "Epoch [29/50], Train Loss: 0.1799, Val Loss: 0.3883\n",
      "Epoch [30/50], Train Loss: 0.1795, Val Loss: 0.3877\n",
      "Epoch [31/50], Train Loss: 0.1791, Val Loss: 0.3871\n",
      "Epoch [32/50], Train Loss: 0.1787, Val Loss: 0.3865\n",
      "Epoch [33/50], Train Loss: 0.1782, Val Loss: 0.3859\n",
      "Epoch [34/50], Train Loss: 0.1778, Val Loss: 0.3853\n",
      "Epoch [35/50], Train Loss: 0.1774, Val Loss: 0.3847\n",
      "Epoch [36/50], Train Loss: 0.1770, Val Loss: 0.3841\n",
      "Epoch [37/50], Train Loss: 0.1766, Val Loss: 0.3835\n",
      "Epoch [38/50], Train Loss: 0.1762, Val Loss: 0.3829\n",
      "Epoch [39/50], Train Loss: 0.1758, Val Loss: 0.3823\n",
      "Epoch [40/50], Train Loss: 0.1753, Val Loss: 0.3817\n",
      "Epoch [41/50], Train Loss: 0.1749, Val Loss: 0.3811\n",
      "Epoch [42/50], Train Loss: 0.1745, Val Loss: 0.3806\n",
      "Epoch [43/50], Train Loss: 0.1741, Val Loss: 0.3800\n",
      "Epoch [44/50], Train Loss: 0.1737, Val Loss: 0.3794\n",
      "Epoch [45/50], Train Loss: 0.1733, Val Loss: 0.3788\n",
      "Epoch [46/50], Train Loss: 0.1729, Val Loss: 0.3782\n",
      "Epoch [47/50], Train Loss: 0.1725, Val Loss: 0.3777\n",
      "Epoch [48/50], Train Loss: 0.1721, Val Loss: 0.3771\n",
      "Epoch [49/50], Train Loss: 0.1717, Val Loss: 0.3765\n",
      "Epoch [50/50], Train Loss: 0.1713, Val Loss: 0.3760\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=16, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1777, Val Loss: 0.4580\n",
      "Epoch [2/50], Train Loss: 0.1774, Val Loss: 0.4573\n",
      "Epoch [3/50], Train Loss: 0.1763, Val Loss: 0.4566\n",
      "Epoch [4/50], Train Loss: 0.1760, Val Loss: 0.4558\n",
      "Epoch [5/50], Train Loss: 0.1759, Val Loss: 0.4551\n",
      "Epoch [6/50], Train Loss: 0.1758, Val Loss: 0.4544\n",
      "Epoch [7/50], Train Loss: 0.1747, Val Loss: 0.4536\n",
      "Epoch [8/50], Train Loss: 0.1746, Val Loss: 0.4529\n",
      "Epoch [9/50], Train Loss: 0.1741, Val Loss: 0.4522\n",
      "Epoch [10/50], Train Loss: 0.1736, Val Loss: 0.4515\n",
      "Epoch [11/50], Train Loss: 0.1732, Val Loss: 0.4507\n",
      "Epoch [12/50], Train Loss: 0.1727, Val Loss: 0.4500\n",
      "Epoch [13/50], Train Loss: 0.1725, Val Loss: 0.4493\n",
      "Epoch [14/50], Train Loss: 0.1714, Val Loss: 0.4486\n",
      "Epoch [15/50], Train Loss: 0.1718, Val Loss: 0.4479\n",
      "Epoch [16/50], Train Loss: 0.1710, Val Loss: 0.4472\n",
      "Epoch [17/50], Train Loss: 0.1705, Val Loss: 0.4465\n",
      "Epoch [18/50], Train Loss: 0.1708, Val Loss: 0.4458\n",
      "Epoch [19/50], Train Loss: 0.1697, Val Loss: 0.4451\n",
      "Epoch [20/50], Train Loss: 0.1701, Val Loss: 0.4444\n",
      "Epoch [21/50], Train Loss: 0.1690, Val Loss: 0.4437\n",
      "Epoch [22/50], Train Loss: 0.1688, Val Loss: 0.4430\n",
      "Epoch [23/50], Train Loss: 0.1683, Val Loss: 0.4423\n",
      "Epoch [24/50], Train Loss: 0.1683, Val Loss: 0.4416\n",
      "Epoch [25/50], Train Loss: 0.1673, Val Loss: 0.4409\n",
      "Epoch [26/50], Train Loss: 0.1670, Val Loss: 0.4402\n",
      "Epoch [27/50], Train Loss: 0.1669, Val Loss: 0.4395\n",
      "Epoch [28/50], Train Loss: 0.1662, Val Loss: 0.4388\n",
      "Epoch [29/50], Train Loss: 0.1661, Val Loss: 0.4381\n",
      "Epoch [30/50], Train Loss: 0.1654, Val Loss: 0.4374\n",
      "Epoch [31/50], Train Loss: 0.1653, Val Loss: 0.4367\n",
      "Epoch [32/50], Train Loss: 0.1643, Val Loss: 0.4360\n",
      "Epoch [33/50], Train Loss: 0.1649, Val Loss: 0.4354\n",
      "Epoch [34/50], Train Loss: 0.1634, Val Loss: 0.4347\n",
      "Epoch [35/50], Train Loss: 0.1634, Val Loss: 0.4340\n",
      "Epoch [36/50], Train Loss: 0.1631, Val Loss: 0.4333\n",
      "Epoch [37/50], Train Loss: 0.1631, Val Loss: 0.4326\n",
      "Epoch [38/50], Train Loss: 0.1628, Val Loss: 0.4320\n",
      "Epoch [39/50], Train Loss: 0.1622, Val Loss: 0.4313\n",
      "Epoch [40/50], Train Loss: 0.1622, Val Loss: 0.4306\n",
      "Epoch [41/50], Train Loss: 0.1615, Val Loss: 0.4300\n",
      "Epoch [42/50], Train Loss: 0.1610, Val Loss: 0.4293\n",
      "Epoch [43/50], Train Loss: 0.1606, Val Loss: 0.4286\n",
      "Epoch [44/50], Train Loss: 0.1604, Val Loss: 0.4280\n",
      "Epoch [45/50], Train Loss: 0.1596, Val Loss: 0.4273\n",
      "Epoch [46/50], Train Loss: 0.1599, Val Loss: 0.4266\n",
      "Epoch [47/50], Train Loss: 0.1595, Val Loss: 0.4260\n",
      "Epoch [48/50], Train Loss: 0.1590, Val Loss: 0.4253\n",
      "Epoch [49/50], Train Loss: 0.1584, Val Loss: 0.4247\n",
      "Epoch [50/50], Train Loss: 0.1580, Val Loss: 0.4240\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=16, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1815, Val Loss: 0.4397\n",
      "Epoch [2/50], Train Loss: 0.1821, Val Loss: 0.4390\n",
      "Epoch [3/50], Train Loss: 0.1820, Val Loss: 0.4383\n",
      "Epoch [4/50], Train Loss: 0.1820, Val Loss: 0.4376\n",
      "Epoch [5/50], Train Loss: 0.1817, Val Loss: 0.4369\n",
      "Epoch [6/50], Train Loss: 0.1817, Val Loss: 0.4361\n",
      "Epoch [7/50], Train Loss: 0.1791, Val Loss: 0.4354\n",
      "Epoch [8/50], Train Loss: 0.1784, Val Loss: 0.4347\n",
      "Epoch [9/50], Train Loss: 0.1792, Val Loss: 0.4340\n",
      "Epoch [10/50], Train Loss: 0.1785, Val Loss: 0.4333\n",
      "Epoch [11/50], Train Loss: 0.1794, Val Loss: 0.4326\n",
      "Epoch [12/50], Train Loss: 0.1785, Val Loss: 0.4319\n",
      "Epoch [13/50], Train Loss: 0.1761, Val Loss: 0.4312\n",
      "Epoch [14/50], Train Loss: 0.1764, Val Loss: 0.4305\n",
      "Epoch [15/50], Train Loss: 0.1762, Val Loss: 0.4298\n",
      "Epoch [16/50], Train Loss: 0.1758, Val Loss: 0.4291\n",
      "Epoch [17/50], Train Loss: 0.1755, Val Loss: 0.4284\n",
      "Epoch [18/50], Train Loss: 0.1760, Val Loss: 0.4277\n",
      "Epoch [19/50], Train Loss: 0.1740, Val Loss: 0.4270\n",
      "Epoch [20/50], Train Loss: 0.1752, Val Loss: 0.4263\n",
      "Epoch [21/50], Train Loss: 0.1734, Val Loss: 0.4256\n",
      "Epoch [22/50], Train Loss: 0.1740, Val Loss: 0.4249\n",
      "Epoch [23/50], Train Loss: 0.1722, Val Loss: 0.4242\n",
      "Epoch [24/50], Train Loss: 0.1719, Val Loss: 0.4236\n",
      "Epoch [25/50], Train Loss: 0.1717, Val Loss: 0.4229\n",
      "Epoch [26/50], Train Loss: 0.1717, Val Loss: 0.4222\n",
      "Epoch [27/50], Train Loss: 0.1712, Val Loss: 0.4215\n",
      "Epoch [28/50], Train Loss: 0.1726, Val Loss: 0.4208\n",
      "Epoch [29/50], Train Loss: 0.1714, Val Loss: 0.4201\n",
      "Epoch [30/50], Train Loss: 0.1707, Val Loss: 0.4195\n",
      "Epoch [31/50], Train Loss: 0.1702, Val Loss: 0.4188\n",
      "Epoch [32/50], Train Loss: 0.1706, Val Loss: 0.4181\n",
      "Epoch [33/50], Train Loss: 0.1699, Val Loss: 0.4175\n",
      "Epoch [34/50], Train Loss: 0.1685, Val Loss: 0.4168\n",
      "Epoch [35/50], Train Loss: 0.1678, Val Loss: 0.4161\n",
      "Epoch [36/50], Train Loss: 0.1672, Val Loss: 0.4155\n",
      "Epoch [37/50], Train Loss: 0.1674, Val Loss: 0.4148\n",
      "Epoch [38/50], Train Loss: 0.1669, Val Loss: 0.4142\n",
      "Epoch [39/50], Train Loss: 0.1645, Val Loss: 0.4135\n",
      "Epoch [40/50], Train Loss: 0.1658, Val Loss: 0.4129\n",
      "Epoch [41/50], Train Loss: 0.1664, Val Loss: 0.4122\n",
      "Epoch [42/50], Train Loss: 0.1645, Val Loss: 0.4116\n",
      "Epoch [43/50], Train Loss: 0.1652, Val Loss: 0.4109\n",
      "Epoch [44/50], Train Loss: 0.1647, Val Loss: 0.4102\n",
      "Epoch [45/50], Train Loss: 0.1638, Val Loss: 0.4096\n",
      "Epoch [46/50], Train Loss: 0.1632, Val Loss: 0.4090\n",
      "Epoch [47/50], Train Loss: 0.1634, Val Loss: 0.4083\n",
      "Epoch [48/50], Train Loss: 0.1624, Val Loss: 0.4077\n",
      "Epoch [49/50], Train Loss: 0.1624, Val Loss: 0.4070\n",
      "Epoch [50/50], Train Loss: 0.1637, Val Loss: 0.4064\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=16, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0942, Val Loss: 0.2707\n",
      "Epoch [2/50], Train Loss: 0.0940, Val Loss: 0.2704\n",
      "Epoch [3/50], Train Loss: 0.0939, Val Loss: 0.2702\n",
      "Epoch [4/50], Train Loss: 0.0937, Val Loss: 0.2699\n",
      "Epoch [5/50], Train Loss: 0.0935, Val Loss: 0.2696\n",
      "Epoch [6/50], Train Loss: 0.0934, Val Loss: 0.2693\n",
      "Epoch [7/50], Train Loss: 0.0932, Val Loss: 0.2690\n",
      "Epoch [8/50], Train Loss: 0.0931, Val Loss: 0.2687\n",
      "Epoch [9/50], Train Loss: 0.0929, Val Loss: 0.2684\n",
      "Epoch [10/50], Train Loss: 0.0928, Val Loss: 0.2681\n",
      "Epoch [11/50], Train Loss: 0.0926, Val Loss: 0.2678\n",
      "Epoch [12/50], Train Loss: 0.0925, Val Loss: 0.2675\n",
      "Epoch [13/50], Train Loss: 0.0923, Val Loss: 0.2673\n",
      "Epoch [14/50], Train Loss: 0.0922, Val Loss: 0.2670\n",
      "Epoch [15/50], Train Loss: 0.0920, Val Loss: 0.2667\n",
      "Epoch [16/50], Train Loss: 0.0918, Val Loss: 0.2664\n",
      "Epoch [17/50], Train Loss: 0.0917, Val Loss: 0.2661\n",
      "Epoch [18/50], Train Loss: 0.0915, Val Loss: 0.2658\n",
      "Epoch [19/50], Train Loss: 0.0914, Val Loss: 0.2655\n",
      "Epoch [20/50], Train Loss: 0.0912, Val Loss: 0.2653\n",
      "Epoch [21/50], Train Loss: 0.0911, Val Loss: 0.2650\n",
      "Epoch [22/50], Train Loss: 0.0909, Val Loss: 0.2647\n",
      "Epoch [23/50], Train Loss: 0.0908, Val Loss: 0.2644\n",
      "Epoch [24/50], Train Loss: 0.0906, Val Loss: 0.2641\n",
      "Epoch [25/50], Train Loss: 0.0905, Val Loss: 0.2638\n",
      "Epoch [26/50], Train Loss: 0.0904, Val Loss: 0.2636\n",
      "Epoch [27/50], Train Loss: 0.0902, Val Loss: 0.2633\n",
      "Epoch [28/50], Train Loss: 0.0901, Val Loss: 0.2630\n",
      "Epoch [29/50], Train Loss: 0.0899, Val Loss: 0.2627\n",
      "Epoch [30/50], Train Loss: 0.0898, Val Loss: 0.2625\n",
      "Epoch [31/50], Train Loss: 0.0896, Val Loss: 0.2622\n",
      "Epoch [32/50], Train Loss: 0.0895, Val Loss: 0.2619\n",
      "Epoch [33/50], Train Loss: 0.0893, Val Loss: 0.2616\n",
      "Epoch [34/50], Train Loss: 0.0892, Val Loss: 0.2614\n",
      "Epoch [35/50], Train Loss: 0.0890, Val Loss: 0.2611\n",
      "Epoch [36/50], Train Loss: 0.0889, Val Loss: 0.2608\n",
      "Epoch [37/50], Train Loss: 0.0888, Val Loss: 0.2605\n",
      "Epoch [38/50], Train Loss: 0.0886, Val Loss: 0.2603\n",
      "Epoch [39/50], Train Loss: 0.0885, Val Loss: 0.2600\n",
      "Epoch [40/50], Train Loss: 0.0883, Val Loss: 0.2597\n",
      "Epoch [41/50], Train Loss: 0.0882, Val Loss: 0.2594\n",
      "Epoch [42/50], Train Loss: 0.0880, Val Loss: 0.2592\n",
      "Epoch [43/50], Train Loss: 0.0879, Val Loss: 0.2589\n",
      "Epoch [44/50], Train Loss: 0.0878, Val Loss: 0.2586\n",
      "Epoch [45/50], Train Loss: 0.0876, Val Loss: 0.2584\n",
      "Epoch [46/50], Train Loss: 0.0875, Val Loss: 0.2581\n",
      "Epoch [47/50], Train Loss: 0.0873, Val Loss: 0.2578\n",
      "Epoch [48/50], Train Loss: 0.0872, Val Loss: 0.2576\n",
      "Epoch [49/50], Train Loss: 0.0871, Val Loss: 0.2573\n",
      "Epoch [50/50], Train Loss: 0.0869, Val Loss: 0.2570\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=16, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.2037, Val Loss: 0.4930\n",
      "Epoch [2/50], Train Loss: 0.2039, Val Loss: 0.4921\n",
      "Epoch [3/50], Train Loss: 0.2034, Val Loss: 0.4912\n",
      "Epoch [4/50], Train Loss: 0.2021, Val Loss: 0.4903\n",
      "Epoch [5/50], Train Loss: 0.2031, Val Loss: 0.4894\n",
      "Epoch [6/50], Train Loss: 0.2008, Val Loss: 0.4885\n",
      "Epoch [7/50], Train Loss: 0.2014, Val Loss: 0.4876\n",
      "Epoch [8/50], Train Loss: 0.1999, Val Loss: 0.4867\n",
      "Epoch [9/50], Train Loss: 0.1998, Val Loss: 0.4858\n",
      "Epoch [10/50], Train Loss: 0.1987, Val Loss: 0.4849\n",
      "Epoch [11/50], Train Loss: 0.1994, Val Loss: 0.4840\n",
      "Epoch [12/50], Train Loss: 0.1982, Val Loss: 0.4831\n",
      "Epoch [13/50], Train Loss: 0.1978, Val Loss: 0.4822\n",
      "Epoch [14/50], Train Loss: 0.1968, Val Loss: 0.4814\n",
      "Epoch [15/50], Train Loss: 0.1964, Val Loss: 0.4805\n",
      "Epoch [16/50], Train Loss: 0.1961, Val Loss: 0.4796\n",
      "Epoch [17/50], Train Loss: 0.1948, Val Loss: 0.4787\n",
      "Epoch [18/50], Train Loss: 0.1948, Val Loss: 0.4779\n",
      "Epoch [19/50], Train Loss: 0.1932, Val Loss: 0.4770\n",
      "Epoch [20/50], Train Loss: 0.1937, Val Loss: 0.4761\n",
      "Epoch [21/50], Train Loss: 0.1933, Val Loss: 0.4753\n",
      "Epoch [22/50], Train Loss: 0.1930, Val Loss: 0.4744\n",
      "Epoch [23/50], Train Loss: 0.1921, Val Loss: 0.4735\n",
      "Epoch [24/50], Train Loss: 0.1919, Val Loss: 0.4727\n",
      "Epoch [25/50], Train Loss: 0.1914, Val Loss: 0.4718\n",
      "Epoch [26/50], Train Loss: 0.1906, Val Loss: 0.4710\n",
      "Epoch [27/50], Train Loss: 0.1907, Val Loss: 0.4701\n",
      "Epoch [28/50], Train Loss: 0.1894, Val Loss: 0.4693\n",
      "Epoch [29/50], Train Loss: 0.1889, Val Loss: 0.4684\n",
      "Epoch [30/50], Train Loss: 0.1881, Val Loss: 0.4676\n",
      "Epoch [31/50], Train Loss: 0.1885, Val Loss: 0.4667\n",
      "Epoch [32/50], Train Loss: 0.1875, Val Loss: 0.4659\n",
      "Epoch [33/50], Train Loss: 0.1870, Val Loss: 0.4650\n",
      "Epoch [34/50], Train Loss: 0.1872, Val Loss: 0.4642\n",
      "Epoch [35/50], Train Loss: 0.1863, Val Loss: 0.4634\n",
      "Epoch [36/50], Train Loss: 0.1853, Val Loss: 0.4625\n",
      "Epoch [37/50], Train Loss: 0.1847, Val Loss: 0.4617\n",
      "Epoch [38/50], Train Loss: 0.1842, Val Loss: 0.4609\n",
      "Epoch [39/50], Train Loss: 0.1842, Val Loss: 0.4600\n",
      "Epoch [40/50], Train Loss: 0.1837, Val Loss: 0.4592\n",
      "Epoch [41/50], Train Loss: 0.1831, Val Loss: 0.4584\n",
      "Epoch [42/50], Train Loss: 0.1828, Val Loss: 0.4576\n",
      "Epoch [43/50], Train Loss: 0.1829, Val Loss: 0.4567\n",
      "Epoch [44/50], Train Loss: 0.1816, Val Loss: 0.4559\n",
      "Epoch [45/50], Train Loss: 0.1801, Val Loss: 0.4551\n",
      "Epoch [46/50], Train Loss: 0.1801, Val Loss: 0.4543\n",
      "Epoch [47/50], Train Loss: 0.1807, Val Loss: 0.4535\n",
      "Epoch [48/50], Train Loss: 0.1797, Val Loss: 0.4526\n",
      "Epoch [49/50], Train Loss: 0.1789, Val Loss: 0.4518\n",
      "Epoch [50/50], Train Loss: 0.1787, Val Loss: 0.4510\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=16, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.2290, Val Loss: 0.5033\n",
      "Epoch [2/50], Train Loss: 0.2310, Val Loss: 0.5023\n",
      "Epoch [3/50], Train Loss: 0.2295, Val Loss: 0.5014\n",
      "Epoch [4/50], Train Loss: 0.2287, Val Loss: 0.5005\n",
      "Epoch [5/50], Train Loss: 0.2288, Val Loss: 0.4996\n",
      "Epoch [6/50], Train Loss: 0.2291, Val Loss: 0.4986\n",
      "Epoch [7/50], Train Loss: 0.2275, Val Loss: 0.4977\n",
      "Epoch [8/50], Train Loss: 0.2258, Val Loss: 0.4968\n",
      "Epoch [9/50], Train Loss: 0.2258, Val Loss: 0.4959\n",
      "Epoch [10/50], Train Loss: 0.2266, Val Loss: 0.4950\n",
      "Epoch [11/50], Train Loss: 0.2231, Val Loss: 0.4941\n",
      "Epoch [12/50], Train Loss: 0.2244, Val Loss: 0.4932\n",
      "Epoch [13/50], Train Loss: 0.2236, Val Loss: 0.4923\n",
      "Epoch [14/50], Train Loss: 0.2235, Val Loss: 0.4914\n",
      "Epoch [15/50], Train Loss: 0.2229, Val Loss: 0.4905\n",
      "Epoch [16/50], Train Loss: 0.2225, Val Loss: 0.4896\n",
      "Epoch [17/50], Train Loss: 0.2212, Val Loss: 0.4887\n",
      "Epoch [18/50], Train Loss: 0.2202, Val Loss: 0.4878\n",
      "Epoch [19/50], Train Loss: 0.2204, Val Loss: 0.4869\n",
      "Epoch [20/50], Train Loss: 0.2193, Val Loss: 0.4860\n",
      "Epoch [21/50], Train Loss: 0.2188, Val Loss: 0.4851\n",
      "Epoch [22/50], Train Loss: 0.2183, Val Loss: 0.4842\n",
      "Epoch [23/50], Train Loss: 0.2180, Val Loss: 0.4833\n",
      "Epoch [24/50], Train Loss: 0.2177, Val Loss: 0.4825\n",
      "Epoch [25/50], Train Loss: 0.2171, Val Loss: 0.4816\n",
      "Epoch [26/50], Train Loss: 0.2160, Val Loss: 0.4807\n",
      "Epoch [27/50], Train Loss: 0.2154, Val Loss: 0.4798\n",
      "Epoch [28/50], Train Loss: 0.2165, Val Loss: 0.4790\n",
      "Epoch [29/50], Train Loss: 0.2134, Val Loss: 0.4781\n",
      "Epoch [30/50], Train Loss: 0.2126, Val Loss: 0.4773\n",
      "Epoch [31/50], Train Loss: 0.2124, Val Loss: 0.4764\n",
      "Epoch [32/50], Train Loss: 0.2121, Val Loss: 0.4755\n",
      "Epoch [33/50], Train Loss: 0.2130, Val Loss: 0.4747\n",
      "Epoch [34/50], Train Loss: 0.2117, Val Loss: 0.4738\n",
      "Epoch [35/50], Train Loss: 0.2113, Val Loss: 0.4730\n",
      "Epoch [36/50], Train Loss: 0.2110, Val Loss: 0.4721\n",
      "Epoch [37/50], Train Loss: 0.2105, Val Loss: 0.4713\n",
      "Epoch [38/50], Train Loss: 0.2085, Val Loss: 0.4704\n",
      "Epoch [39/50], Train Loss: 0.2078, Val Loss: 0.4696\n",
      "Epoch [40/50], Train Loss: 0.2078, Val Loss: 0.4688\n",
      "Epoch [41/50], Train Loss: 0.2065, Val Loss: 0.4679\n",
      "Epoch [42/50], Train Loss: 0.2065, Val Loss: 0.4671\n",
      "Epoch [43/50], Train Loss: 0.2056, Val Loss: 0.4663\n",
      "Epoch [44/50], Train Loss: 0.2054, Val Loss: 0.4654\n",
      "Epoch [45/50], Train Loss: 0.2054, Val Loss: 0.4646\n",
      "Epoch [46/50], Train Loss: 0.2045, Val Loss: 0.4638\n",
      "Epoch [47/50], Train Loss: 0.2034, Val Loss: 0.4629\n",
      "Epoch [48/50], Train Loss: 0.2040, Val Loss: 0.4621\n",
      "Epoch [49/50], Train Loss: 0.2054, Val Loss: 0.4613\n",
      "Epoch [50/50], Train Loss: 0.2032, Val Loss: 0.4605\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=32, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1143, Val Loss: 0.3492\n",
      "Epoch [2/50], Train Loss: 0.1140, Val Loss: 0.3487\n",
      "Epoch [3/50], Train Loss: 0.1138, Val Loss: 0.3482\n",
      "Epoch [4/50], Train Loss: 0.1136, Val Loss: 0.3477\n",
      "Epoch [5/50], Train Loss: 0.1133, Val Loss: 0.3473\n",
      "Epoch [6/50], Train Loss: 0.1131, Val Loss: 0.3468\n",
      "Epoch [7/50], Train Loss: 0.1129, Val Loss: 0.3463\n",
      "Epoch [8/50], Train Loss: 0.1126, Val Loss: 0.3458\n",
      "Epoch [9/50], Train Loss: 0.1124, Val Loss: 0.3453\n",
      "Epoch [10/50], Train Loss: 0.1122, Val Loss: 0.3448\n",
      "Epoch [11/50], Train Loss: 0.1119, Val Loss: 0.3443\n",
      "Epoch [12/50], Train Loss: 0.1117, Val Loss: 0.3438\n",
      "Epoch [13/50], Train Loss: 0.1115, Val Loss: 0.3433\n",
      "Epoch [14/50], Train Loss: 0.1113, Val Loss: 0.3429\n",
      "Epoch [15/50], Train Loss: 0.1110, Val Loss: 0.3424\n",
      "Epoch [16/50], Train Loss: 0.1108, Val Loss: 0.3419\n",
      "Epoch [17/50], Train Loss: 0.1106, Val Loss: 0.3414\n",
      "Epoch [18/50], Train Loss: 0.1104, Val Loss: 0.3409\n",
      "Epoch [19/50], Train Loss: 0.1101, Val Loss: 0.3404\n",
      "Epoch [20/50], Train Loss: 0.1099, Val Loss: 0.3400\n",
      "Epoch [21/50], Train Loss: 0.1097, Val Loss: 0.3395\n",
      "Epoch [22/50], Train Loss: 0.1095, Val Loss: 0.3390\n",
      "Epoch [23/50], Train Loss: 0.1093, Val Loss: 0.3385\n",
      "Epoch [24/50], Train Loss: 0.1090, Val Loss: 0.3381\n",
      "Epoch [25/50], Train Loss: 0.1088, Val Loss: 0.3376\n",
      "Epoch [26/50], Train Loss: 0.1086, Val Loss: 0.3371\n",
      "Epoch [27/50], Train Loss: 0.1084, Val Loss: 0.3367\n",
      "Epoch [28/50], Train Loss: 0.1082, Val Loss: 0.3362\n",
      "Epoch [29/50], Train Loss: 0.1080, Val Loss: 0.3357\n",
      "Epoch [30/50], Train Loss: 0.1077, Val Loss: 0.3352\n",
      "Epoch [31/50], Train Loss: 0.1075, Val Loss: 0.3348\n",
      "Epoch [32/50], Train Loss: 0.1073, Val Loss: 0.3343\n",
      "Epoch [33/50], Train Loss: 0.1071, Val Loss: 0.3339\n",
      "Epoch [34/50], Train Loss: 0.1069, Val Loss: 0.3334\n",
      "Epoch [35/50], Train Loss: 0.1067, Val Loss: 0.3329\n",
      "Epoch [36/50], Train Loss: 0.1065, Val Loss: 0.3325\n",
      "Epoch [37/50], Train Loss: 0.1062, Val Loss: 0.3320\n",
      "Epoch [38/50], Train Loss: 0.1060, Val Loss: 0.3315\n",
      "Epoch [39/50], Train Loss: 0.1058, Val Loss: 0.3311\n",
      "Epoch [40/50], Train Loss: 0.1056, Val Loss: 0.3306\n",
      "Epoch [41/50], Train Loss: 0.1054, Val Loss: 0.3302\n",
      "Epoch [42/50], Train Loss: 0.1052, Val Loss: 0.3297\n",
      "Epoch [43/50], Train Loss: 0.1050, Val Loss: 0.3293\n",
      "Epoch [44/50], Train Loss: 0.1048, Val Loss: 0.3288\n",
      "Epoch [45/50], Train Loss: 0.1046, Val Loss: 0.3283\n",
      "Epoch [46/50], Train Loss: 0.1044, Val Loss: 0.3279\n",
      "Epoch [47/50], Train Loss: 0.1042, Val Loss: 0.3275\n",
      "Epoch [48/50], Train Loss: 0.1040, Val Loss: 0.3270\n",
      "Epoch [49/50], Train Loss: 0.1038, Val Loss: 0.3265\n",
      "Epoch [50/50], Train Loss: 0.1036, Val Loss: 0.3261\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=32, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1726, Val Loss: 0.4119\n",
      "Epoch [2/50], Train Loss: 0.1719, Val Loss: 0.4112\n",
      "Epoch [3/50], Train Loss: 0.1722, Val Loss: 0.4104\n",
      "Epoch [4/50], Train Loss: 0.1716, Val Loss: 0.4097\n",
      "Epoch [5/50], Train Loss: 0.1716, Val Loss: 0.4089\n",
      "Epoch [6/50], Train Loss: 0.1712, Val Loss: 0.4081\n",
      "Epoch [7/50], Train Loss: 0.1696, Val Loss: 0.4074\n",
      "Epoch [8/50], Train Loss: 0.1701, Val Loss: 0.4066\n",
      "Epoch [9/50], Train Loss: 0.1694, Val Loss: 0.4059\n",
      "Epoch [10/50], Train Loss: 0.1688, Val Loss: 0.4051\n",
      "Epoch [11/50], Train Loss: 0.1686, Val Loss: 0.4044\n",
      "Epoch [12/50], Train Loss: 0.1679, Val Loss: 0.4036\n",
      "Epoch [13/50], Train Loss: 0.1677, Val Loss: 0.4029\n",
      "Epoch [14/50], Train Loss: 0.1667, Val Loss: 0.4022\n",
      "Epoch [15/50], Train Loss: 0.1670, Val Loss: 0.4014\n",
      "Epoch [16/50], Train Loss: 0.1666, Val Loss: 0.4007\n",
      "Epoch [17/50], Train Loss: 0.1651, Val Loss: 0.4000\n",
      "Epoch [18/50], Train Loss: 0.1648, Val Loss: 0.3992\n",
      "Epoch [19/50], Train Loss: 0.1649, Val Loss: 0.3985\n",
      "Epoch [20/50], Train Loss: 0.1647, Val Loss: 0.3978\n",
      "Epoch [21/50], Train Loss: 0.1646, Val Loss: 0.3971\n",
      "Epoch [22/50], Train Loss: 0.1647, Val Loss: 0.3963\n",
      "Epoch [23/50], Train Loss: 0.1637, Val Loss: 0.3956\n",
      "Epoch [24/50], Train Loss: 0.1636, Val Loss: 0.3949\n",
      "Epoch [25/50], Train Loss: 0.1622, Val Loss: 0.3942\n",
      "Epoch [26/50], Train Loss: 0.1628, Val Loss: 0.3935\n",
      "Epoch [27/50], Train Loss: 0.1620, Val Loss: 0.3928\n",
      "Epoch [28/50], Train Loss: 0.1616, Val Loss: 0.3920\n",
      "Epoch [29/50], Train Loss: 0.1600, Val Loss: 0.3913\n",
      "Epoch [30/50], Train Loss: 0.1599, Val Loss: 0.3906\n",
      "Epoch [31/50], Train Loss: 0.1602, Val Loss: 0.3899\n",
      "Epoch [32/50], Train Loss: 0.1601, Val Loss: 0.3892\n",
      "Epoch [33/50], Train Loss: 0.1589, Val Loss: 0.3885\n",
      "Epoch [34/50], Train Loss: 0.1593, Val Loss: 0.3878\n",
      "Epoch [35/50], Train Loss: 0.1583, Val Loss: 0.3871\n",
      "Epoch [36/50], Train Loss: 0.1579, Val Loss: 0.3864\n",
      "Epoch [37/50], Train Loss: 0.1578, Val Loss: 0.3857\n",
      "Epoch [38/50], Train Loss: 0.1577, Val Loss: 0.3850\n",
      "Epoch [39/50], Train Loss: 0.1565, Val Loss: 0.3844\n",
      "Epoch [40/50], Train Loss: 0.1560, Val Loss: 0.3837\n",
      "Epoch [41/50], Train Loss: 0.1556, Val Loss: 0.3830\n",
      "Epoch [42/50], Train Loss: 0.1559, Val Loss: 0.3823\n",
      "Epoch [43/50], Train Loss: 0.1556, Val Loss: 0.3816\n",
      "Epoch [44/50], Train Loss: 0.1552, Val Loss: 0.3809\n",
      "Epoch [45/50], Train Loss: 0.1546, Val Loss: 0.3803\n",
      "Epoch [46/50], Train Loss: 0.1540, Val Loss: 0.3796\n",
      "Epoch [47/50], Train Loss: 0.1543, Val Loss: 0.3789\n",
      "Epoch [48/50], Train Loss: 0.1532, Val Loss: 0.3782\n",
      "Epoch [49/50], Train Loss: 0.1526, Val Loss: 0.3776\n",
      "Epoch [50/50], Train Loss: 0.1519, Val Loss: 0.3769\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=32, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1615, Val Loss: 0.4076\n",
      "Epoch [2/50], Train Loss: 0.1611, Val Loss: 0.4069\n",
      "Epoch [3/50], Train Loss: 0.1611, Val Loss: 0.4062\n",
      "Epoch [4/50], Train Loss: 0.1605, Val Loss: 0.4055\n",
      "Epoch [5/50], Train Loss: 0.1602, Val Loss: 0.4048\n",
      "Epoch [6/50], Train Loss: 0.1599, Val Loss: 0.4041\n",
      "Epoch [7/50], Train Loss: 0.1600, Val Loss: 0.4035\n",
      "Epoch [8/50], Train Loss: 0.1594, Val Loss: 0.4028\n",
      "Epoch [9/50], Train Loss: 0.1583, Val Loss: 0.4021\n",
      "Epoch [10/50], Train Loss: 0.1586, Val Loss: 0.4014\n",
      "Epoch [11/50], Train Loss: 0.1582, Val Loss: 0.4007\n",
      "Epoch [12/50], Train Loss: 0.1577, Val Loss: 0.4001\n",
      "Epoch [13/50], Train Loss: 0.1574, Val Loss: 0.3994\n",
      "Epoch [14/50], Train Loss: 0.1569, Val Loss: 0.3987\n",
      "Epoch [15/50], Train Loss: 0.1572, Val Loss: 0.3981\n",
      "Epoch [16/50], Train Loss: 0.1564, Val Loss: 0.3974\n",
      "Epoch [17/50], Train Loss: 0.1567, Val Loss: 0.3968\n",
      "Epoch [18/50], Train Loss: 0.1553, Val Loss: 0.3961\n",
      "Epoch [19/50], Train Loss: 0.1555, Val Loss: 0.3954\n",
      "Epoch [20/50], Train Loss: 0.1542, Val Loss: 0.3948\n",
      "Epoch [21/50], Train Loss: 0.1538, Val Loss: 0.3941\n",
      "Epoch [22/50], Train Loss: 0.1543, Val Loss: 0.3935\n",
      "Epoch [23/50], Train Loss: 0.1528, Val Loss: 0.3928\n",
      "Epoch [24/50], Train Loss: 0.1523, Val Loss: 0.3922\n",
      "Epoch [25/50], Train Loss: 0.1523, Val Loss: 0.3915\n",
      "Epoch [26/50], Train Loss: 0.1519, Val Loss: 0.3909\n",
      "Epoch [27/50], Train Loss: 0.1518, Val Loss: 0.3902\n",
      "Epoch [28/50], Train Loss: 0.1512, Val Loss: 0.3896\n",
      "Epoch [29/50], Train Loss: 0.1513, Val Loss: 0.3889\n",
      "Epoch [30/50], Train Loss: 0.1517, Val Loss: 0.3883\n",
      "Epoch [31/50], Train Loss: 0.1512, Val Loss: 0.3877\n",
      "Epoch [32/50], Train Loss: 0.1493, Val Loss: 0.3870\n",
      "Epoch [33/50], Train Loss: 0.1498, Val Loss: 0.3864\n",
      "Epoch [34/50], Train Loss: 0.1503, Val Loss: 0.3858\n",
      "Epoch [35/50], Train Loss: 0.1487, Val Loss: 0.3851\n",
      "Epoch [36/50], Train Loss: 0.1486, Val Loss: 0.3845\n",
      "Epoch [37/50], Train Loss: 0.1483, Val Loss: 0.3839\n",
      "Epoch [38/50], Train Loss: 0.1490, Val Loss: 0.3832\n",
      "Epoch [39/50], Train Loss: 0.1473, Val Loss: 0.3826\n",
      "Epoch [40/50], Train Loss: 0.1468, Val Loss: 0.3820\n",
      "Epoch [41/50], Train Loss: 0.1477, Val Loss: 0.3814\n",
      "Epoch [42/50], Train Loss: 0.1461, Val Loss: 0.3808\n",
      "Epoch [43/50], Train Loss: 0.1463, Val Loss: 0.3801\n",
      "Epoch [44/50], Train Loss: 0.1457, Val Loss: 0.3795\n",
      "Epoch [45/50], Train Loss: 0.1453, Val Loss: 0.3789\n",
      "Epoch [46/50], Train Loss: 0.1453, Val Loss: 0.3783\n",
      "Epoch [47/50], Train Loss: 0.1446, Val Loss: 0.3777\n",
      "Epoch [48/50], Train Loss: 0.1448, Val Loss: 0.3771\n",
      "Epoch [49/50], Train Loss: 0.1450, Val Loss: 0.3765\n",
      "Epoch [50/50], Train Loss: 0.1436, Val Loss: 0.3759\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=32, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1691, Val Loss: 0.4558\n",
      "Epoch [2/50], Train Loss: 0.1686, Val Loss: 0.4550\n",
      "Epoch [3/50], Train Loss: 0.1682, Val Loss: 0.4542\n",
      "Epoch [4/50], Train Loss: 0.1677, Val Loss: 0.4534\n",
      "Epoch [5/50], Train Loss: 0.1673, Val Loss: 0.4526\n",
      "Epoch [6/50], Train Loss: 0.1669, Val Loss: 0.4518\n",
      "Epoch [7/50], Train Loss: 0.1664, Val Loss: 0.4510\n",
      "Epoch [8/50], Train Loss: 0.1660, Val Loss: 0.4502\n",
      "Epoch [9/50], Train Loss: 0.1656, Val Loss: 0.4495\n",
      "Epoch [10/50], Train Loss: 0.1651, Val Loss: 0.4487\n",
      "Epoch [11/50], Train Loss: 0.1647, Val Loss: 0.4479\n",
      "Epoch [12/50], Train Loss: 0.1643, Val Loss: 0.4471\n",
      "Epoch [13/50], Train Loss: 0.1639, Val Loss: 0.4463\n",
      "Epoch [14/50], Train Loss: 0.1634, Val Loss: 0.4455\n",
      "Epoch [15/50], Train Loss: 0.1630, Val Loss: 0.4448\n",
      "Epoch [16/50], Train Loss: 0.1626, Val Loss: 0.4440\n",
      "Epoch [17/50], Train Loss: 0.1622, Val Loss: 0.4432\n",
      "Epoch [18/50], Train Loss: 0.1618, Val Loss: 0.4425\n",
      "Epoch [19/50], Train Loss: 0.1613, Val Loss: 0.4417\n",
      "Epoch [20/50], Train Loss: 0.1609, Val Loss: 0.4409\n",
      "Epoch [21/50], Train Loss: 0.1605, Val Loss: 0.4402\n",
      "Epoch [22/50], Train Loss: 0.1601, Val Loss: 0.4394\n",
      "Epoch [23/50], Train Loss: 0.1597, Val Loss: 0.4387\n",
      "Epoch [24/50], Train Loss: 0.1593, Val Loss: 0.4379\n",
      "Epoch [25/50], Train Loss: 0.1589, Val Loss: 0.4371\n",
      "Epoch [26/50], Train Loss: 0.1585, Val Loss: 0.4364\n",
      "Epoch [27/50], Train Loss: 0.1581, Val Loss: 0.4357\n",
      "Epoch [28/50], Train Loss: 0.1577, Val Loss: 0.4349\n",
      "Epoch [29/50], Train Loss: 0.1573, Val Loss: 0.4341\n",
      "Epoch [30/50], Train Loss: 0.1569, Val Loss: 0.4334\n",
      "Epoch [31/50], Train Loss: 0.1565, Val Loss: 0.4327\n",
      "Epoch [32/50], Train Loss: 0.1561, Val Loss: 0.4319\n",
      "Epoch [33/50], Train Loss: 0.1557, Val Loss: 0.4312\n",
      "Epoch [34/50], Train Loss: 0.1553, Val Loss: 0.4304\n",
      "Epoch [35/50], Train Loss: 0.1549, Val Loss: 0.4297\n",
      "Epoch [36/50], Train Loss: 0.1545, Val Loss: 0.4290\n",
      "Epoch [37/50], Train Loss: 0.1541, Val Loss: 0.4283\n",
      "Epoch [38/50], Train Loss: 0.1537, Val Loss: 0.4275\n",
      "Epoch [39/50], Train Loss: 0.1533, Val Loss: 0.4268\n",
      "Epoch [40/50], Train Loss: 0.1529, Val Loss: 0.4261\n",
      "Epoch [41/50], Train Loss: 0.1525, Val Loss: 0.4253\n",
      "Epoch [42/50], Train Loss: 0.1521, Val Loss: 0.4246\n",
      "Epoch [43/50], Train Loss: 0.1518, Val Loss: 0.4239\n",
      "Epoch [44/50], Train Loss: 0.1514, Val Loss: 0.4232\n",
      "Epoch [45/50], Train Loss: 0.1510, Val Loss: 0.4225\n",
      "Epoch [46/50], Train Loss: 0.1506, Val Loss: 0.4218\n",
      "Epoch [47/50], Train Loss: 0.1502, Val Loss: 0.4210\n",
      "Epoch [48/50], Train Loss: 0.1499, Val Loss: 0.4203\n",
      "Epoch [49/50], Train Loss: 0.1495, Val Loss: 0.4196\n",
      "Epoch [50/50], Train Loss: 0.1491, Val Loss: 0.4189\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=32, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1780, Val Loss: 0.4394\n",
      "Epoch [2/50], Train Loss: 0.1773, Val Loss: 0.4387\n",
      "Epoch [3/50], Train Loss: 0.1771, Val Loss: 0.4380\n",
      "Epoch [4/50], Train Loss: 0.1764, Val Loss: 0.4373\n",
      "Epoch [5/50], Train Loss: 0.1764, Val Loss: 0.4366\n",
      "Epoch [6/50], Train Loss: 0.1759, Val Loss: 0.4359\n",
      "Epoch [7/50], Train Loss: 0.1755, Val Loss: 0.4352\n",
      "Epoch [8/50], Train Loss: 0.1752, Val Loss: 0.4345\n",
      "Epoch [9/50], Train Loss: 0.1745, Val Loss: 0.4338\n",
      "Epoch [10/50], Train Loss: 0.1742, Val Loss: 0.4331\n",
      "Epoch [11/50], Train Loss: 0.1739, Val Loss: 0.4324\n",
      "Epoch [12/50], Train Loss: 0.1727, Val Loss: 0.4317\n",
      "Epoch [13/50], Train Loss: 0.1729, Val Loss: 0.4310\n",
      "Epoch [14/50], Train Loss: 0.1726, Val Loss: 0.4303\n",
      "Epoch [15/50], Train Loss: 0.1722, Val Loss: 0.4297\n",
      "Epoch [16/50], Train Loss: 0.1718, Val Loss: 0.4290\n",
      "Epoch [17/50], Train Loss: 0.1711, Val Loss: 0.4283\n",
      "Epoch [18/50], Train Loss: 0.1706, Val Loss: 0.4276\n",
      "Epoch [19/50], Train Loss: 0.1701, Val Loss: 0.4269\n",
      "Epoch [20/50], Train Loss: 0.1699, Val Loss: 0.4262\n",
      "Epoch [21/50], Train Loss: 0.1695, Val Loss: 0.4256\n",
      "Epoch [22/50], Train Loss: 0.1689, Val Loss: 0.4249\n",
      "Epoch [23/50], Train Loss: 0.1687, Val Loss: 0.4242\n",
      "Epoch [24/50], Train Loss: 0.1683, Val Loss: 0.4235\n",
      "Epoch [25/50], Train Loss: 0.1682, Val Loss: 0.4229\n",
      "Epoch [26/50], Train Loss: 0.1673, Val Loss: 0.4222\n",
      "Epoch [27/50], Train Loss: 0.1671, Val Loss: 0.4215\n",
      "Epoch [28/50], Train Loss: 0.1668, Val Loss: 0.4209\n",
      "Epoch [29/50], Train Loss: 0.1660, Val Loss: 0.4202\n",
      "Epoch [30/50], Train Loss: 0.1658, Val Loss: 0.4196\n",
      "Epoch [31/50], Train Loss: 0.1653, Val Loss: 0.4189\n",
      "Epoch [32/50], Train Loss: 0.1652, Val Loss: 0.4182\n",
      "Epoch [33/50], Train Loss: 0.1648, Val Loss: 0.4176\n",
      "Epoch [34/50], Train Loss: 0.1644, Val Loss: 0.4169\n",
      "Epoch [35/50], Train Loss: 0.1633, Val Loss: 0.4163\n",
      "Epoch [36/50], Train Loss: 0.1635, Val Loss: 0.4156\n",
      "Epoch [37/50], Train Loss: 0.1633, Val Loss: 0.4150\n",
      "Epoch [38/50], Train Loss: 0.1628, Val Loss: 0.4143\n",
      "Epoch [39/50], Train Loss: 0.1624, Val Loss: 0.4137\n",
      "Epoch [40/50], Train Loss: 0.1620, Val Loss: 0.4130\n",
      "Epoch [41/50], Train Loss: 0.1615, Val Loss: 0.4124\n",
      "Epoch [42/50], Train Loss: 0.1615, Val Loss: 0.4117\n",
      "Epoch [43/50], Train Loss: 0.1608, Val Loss: 0.4111\n",
      "Epoch [44/50], Train Loss: 0.1598, Val Loss: 0.4104\n",
      "Epoch [45/50], Train Loss: 0.1596, Val Loss: 0.4098\n",
      "Epoch [46/50], Train Loss: 0.1597, Val Loss: 0.4092\n",
      "Epoch [47/50], Train Loss: 0.1595, Val Loss: 0.4085\n",
      "Epoch [48/50], Train Loss: 0.1591, Val Loss: 0.4079\n",
      "Epoch [49/50], Train Loss: 0.1592, Val Loss: 0.4073\n",
      "Epoch [50/50], Train Loss: 0.1583, Val Loss: 0.4066\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=32, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1077, Val Loss: 0.2947\n",
      "Epoch [2/50], Train Loss: 0.1068, Val Loss: 0.2943\n",
      "Epoch [3/50], Train Loss: 0.1077, Val Loss: 0.2939\n",
      "Epoch [4/50], Train Loss: 0.1080, Val Loss: 0.2935\n",
      "Epoch [5/50], Train Loss: 0.1068, Val Loss: 0.2931\n",
      "Epoch [6/50], Train Loss: 0.1070, Val Loss: 0.2927\n",
      "Epoch [7/50], Train Loss: 0.1065, Val Loss: 0.2923\n",
      "Epoch [8/50], Train Loss: 0.1068, Val Loss: 0.2919\n",
      "Epoch [9/50], Train Loss: 0.1055, Val Loss: 0.2916\n",
      "Epoch [10/50], Train Loss: 0.1067, Val Loss: 0.2912\n",
      "Epoch [11/50], Train Loss: 0.1059, Val Loss: 0.2908\n",
      "Epoch [12/50], Train Loss: 0.1062, Val Loss: 0.2904\n",
      "Epoch [13/50], Train Loss: 0.1057, Val Loss: 0.2900\n",
      "Epoch [14/50], Train Loss: 0.1060, Val Loss: 0.2896\n",
      "Epoch [15/50], Train Loss: 0.1042, Val Loss: 0.2892\n",
      "Epoch [16/50], Train Loss: 0.1043, Val Loss: 0.2888\n",
      "Epoch [17/50], Train Loss: 0.1042, Val Loss: 0.2884\n",
      "Epoch [18/50], Train Loss: 0.1051, Val Loss: 0.2880\n",
      "Epoch [19/50], Train Loss: 0.1038, Val Loss: 0.2876\n",
      "Epoch [20/50], Train Loss: 0.1046, Val Loss: 0.2873\n",
      "Epoch [21/50], Train Loss: 0.1037, Val Loss: 0.2869\n",
      "Epoch [22/50], Train Loss: 0.1035, Val Loss: 0.2865\n",
      "Epoch [23/50], Train Loss: 0.1028, Val Loss: 0.2861\n",
      "Epoch [24/50], Train Loss: 0.1025, Val Loss: 0.2857\n",
      "Epoch [25/50], Train Loss: 0.1031, Val Loss: 0.2853\n",
      "Epoch [26/50], Train Loss: 0.1029, Val Loss: 0.2850\n",
      "Epoch [27/50], Train Loss: 0.1029, Val Loss: 0.2846\n",
      "Epoch [28/50], Train Loss: 0.1023, Val Loss: 0.2842\n",
      "Epoch [29/50], Train Loss: 0.1022, Val Loss: 0.2838\n",
      "Epoch [30/50], Train Loss: 0.1024, Val Loss: 0.2835\n",
      "Epoch [31/50], Train Loss: 0.1020, Val Loss: 0.2831\n",
      "Epoch [32/50], Train Loss: 0.1019, Val Loss: 0.2827\n",
      "Epoch [33/50], Train Loss: 0.1020, Val Loss: 0.2823\n",
      "Epoch [34/50], Train Loss: 0.1016, Val Loss: 0.2819\n",
      "Epoch [35/50], Train Loss: 0.1005, Val Loss: 0.2816\n",
      "Epoch [36/50], Train Loss: 0.1021, Val Loss: 0.2812\n",
      "Epoch [37/50], Train Loss: 0.1000, Val Loss: 0.2808\n",
      "Epoch [38/50], Train Loss: 0.1007, Val Loss: 0.2805\n",
      "Epoch [39/50], Train Loss: 0.1007, Val Loss: 0.2801\n",
      "Epoch [40/50], Train Loss: 0.0999, Val Loss: 0.2797\n",
      "Epoch [41/50], Train Loss: 0.0996, Val Loss: 0.2794\n",
      "Epoch [42/50], Train Loss: 0.0998, Val Loss: 0.2790\n",
      "Epoch [43/50], Train Loss: 0.0997, Val Loss: 0.2786\n",
      "Epoch [44/50], Train Loss: 0.0994, Val Loss: 0.2783\n",
      "Epoch [45/50], Train Loss: 0.0997, Val Loss: 0.2779\n",
      "Epoch [46/50], Train Loss: 0.0990, Val Loss: 0.2775\n",
      "Epoch [47/50], Train Loss: 0.0989, Val Loss: 0.2772\n",
      "Epoch [48/50], Train Loss: 0.0990, Val Loss: 0.2768\n",
      "Epoch [49/50], Train Loss: 0.0983, Val Loss: 0.2765\n",
      "Epoch [50/50], Train Loss: 0.0985, Val Loss: 0.2761\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=32, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.2091, Val Loss: 0.4866\n",
      "Epoch [2/50], Train Loss: 0.2085, Val Loss: 0.4856\n",
      "Epoch [3/50], Train Loss: 0.2078, Val Loss: 0.4846\n",
      "Epoch [4/50], Train Loss: 0.2072, Val Loss: 0.4836\n",
      "Epoch [5/50], Train Loss: 0.2066, Val Loss: 0.4826\n",
      "Epoch [6/50], Train Loss: 0.2060, Val Loss: 0.4817\n",
      "Epoch [7/50], Train Loss: 0.2054, Val Loss: 0.4807\n",
      "Epoch [8/50], Train Loss: 0.2047, Val Loss: 0.4797\n",
      "Epoch [9/50], Train Loss: 0.2041, Val Loss: 0.4787\n",
      "Epoch [10/50], Train Loss: 0.2035, Val Loss: 0.4778\n",
      "Epoch [11/50], Train Loss: 0.2029, Val Loss: 0.4768\n",
      "Epoch [12/50], Train Loss: 0.2023, Val Loss: 0.4759\n",
      "Epoch [13/50], Train Loss: 0.2017, Val Loss: 0.4749\n",
      "Epoch [14/50], Train Loss: 0.2011, Val Loss: 0.4739\n",
      "Epoch [15/50], Train Loss: 0.2005, Val Loss: 0.4730\n",
      "Epoch [16/50], Train Loss: 0.1999, Val Loss: 0.4721\n",
      "Epoch [17/50], Train Loss: 0.1993, Val Loss: 0.4711\n",
      "Epoch [18/50], Train Loss: 0.1987, Val Loss: 0.4702\n",
      "Epoch [19/50], Train Loss: 0.1981, Val Loss: 0.4692\n",
      "Epoch [20/50], Train Loss: 0.1975, Val Loss: 0.4683\n",
      "Epoch [21/50], Train Loss: 0.1969, Val Loss: 0.4674\n",
      "Epoch [22/50], Train Loss: 0.1964, Val Loss: 0.4664\n",
      "Epoch [23/50], Train Loss: 0.1958, Val Loss: 0.4655\n",
      "Epoch [24/50], Train Loss: 0.1952, Val Loss: 0.4646\n",
      "Epoch [25/50], Train Loss: 0.1946, Val Loss: 0.4637\n",
      "Epoch [26/50], Train Loss: 0.1941, Val Loss: 0.4628\n",
      "Epoch [27/50], Train Loss: 0.1935, Val Loss: 0.4618\n",
      "Epoch [28/50], Train Loss: 0.1929, Val Loss: 0.4609\n",
      "Epoch [29/50], Train Loss: 0.1923, Val Loss: 0.4600\n",
      "Epoch [30/50], Train Loss: 0.1918, Val Loss: 0.4591\n",
      "Epoch [31/50], Train Loss: 0.1912, Val Loss: 0.4582\n",
      "Epoch [32/50], Train Loss: 0.1907, Val Loss: 0.4573\n",
      "Epoch [33/50], Train Loss: 0.1901, Val Loss: 0.4564\n",
      "Epoch [34/50], Train Loss: 0.1895, Val Loss: 0.4555\n",
      "Epoch [35/50], Train Loss: 0.1890, Val Loss: 0.4546\n",
      "Epoch [36/50], Train Loss: 0.1884, Val Loss: 0.4537\n",
      "Epoch [37/50], Train Loss: 0.1879, Val Loss: 0.4529\n",
      "Epoch [38/50], Train Loss: 0.1873, Val Loss: 0.4520\n",
      "Epoch [39/50], Train Loss: 0.1868, Val Loss: 0.4511\n",
      "Epoch [40/50], Train Loss: 0.1862, Val Loss: 0.4502\n",
      "Epoch [41/50], Train Loss: 0.1857, Val Loss: 0.4493\n",
      "Epoch [42/50], Train Loss: 0.1852, Val Loss: 0.4485\n",
      "Epoch [43/50], Train Loss: 0.1846, Val Loss: 0.4476\n",
      "Epoch [44/50], Train Loss: 0.1841, Val Loss: 0.4467\n",
      "Epoch [45/50], Train Loss: 0.1835, Val Loss: 0.4459\n",
      "Epoch [46/50], Train Loss: 0.1830, Val Loss: 0.4450\n",
      "Epoch [47/50], Train Loss: 0.1825, Val Loss: 0.4441\n",
      "Epoch [48/50], Train Loss: 0.1820, Val Loss: 0.4433\n",
      "Epoch [49/50], Train Loss: 0.1814, Val Loss: 0.4424\n",
      "Epoch [50/50], Train Loss: 0.1809, Val Loss: 0.4416\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=32, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1366, Val Loss: 0.3629\n",
      "Epoch [2/50], Train Loss: 0.1362, Val Loss: 0.3623\n",
      "Epoch [3/50], Train Loss: 0.1359, Val Loss: 0.3618\n",
      "Epoch [4/50], Train Loss: 0.1353, Val Loss: 0.3612\n",
      "Epoch [5/50], Train Loss: 0.1357, Val Loss: 0.3607\n",
      "Epoch [6/50], Train Loss: 0.1348, Val Loss: 0.3602\n",
      "Epoch [7/50], Train Loss: 0.1349, Val Loss: 0.3596\n",
      "Epoch [8/50], Train Loss: 0.1346, Val Loss: 0.3591\n",
      "Epoch [9/50], Train Loss: 0.1344, Val Loss: 0.3586\n",
      "Epoch [10/50], Train Loss: 0.1337, Val Loss: 0.3581\n",
      "Epoch [11/50], Train Loss: 0.1335, Val Loss: 0.3575\n",
      "Epoch [12/50], Train Loss: 0.1333, Val Loss: 0.3570\n",
      "Epoch [13/50], Train Loss: 0.1327, Val Loss: 0.3565\n",
      "Epoch [14/50], Train Loss: 0.1327, Val Loss: 0.3560\n",
      "Epoch [15/50], Train Loss: 0.1326, Val Loss: 0.3555\n",
      "Epoch [16/50], Train Loss: 0.1317, Val Loss: 0.3549\n",
      "Epoch [17/50], Train Loss: 0.1320, Val Loss: 0.3544\n",
      "Epoch [18/50], Train Loss: 0.1315, Val Loss: 0.3539\n",
      "Epoch [19/50], Train Loss: 0.1317, Val Loss: 0.3534\n",
      "Epoch [20/50], Train Loss: 0.1303, Val Loss: 0.3529\n",
      "Epoch [21/50], Train Loss: 0.1304, Val Loss: 0.3524\n",
      "Epoch [22/50], Train Loss: 0.1301, Val Loss: 0.3518\n",
      "Epoch [23/50], Train Loss: 0.1301, Val Loss: 0.3513\n",
      "Epoch [24/50], Train Loss: 0.1299, Val Loss: 0.3508\n",
      "Epoch [25/50], Train Loss: 0.1295, Val Loss: 0.3503\n",
      "Epoch [26/50], Train Loss: 0.1293, Val Loss: 0.3498\n",
      "Epoch [27/50], Train Loss: 0.1289, Val Loss: 0.3493\n",
      "Epoch [28/50], Train Loss: 0.1288, Val Loss: 0.3488\n",
      "Epoch [29/50], Train Loss: 0.1286, Val Loss: 0.3483\n",
      "Epoch [30/50], Train Loss: 0.1282, Val Loss: 0.3478\n",
      "Epoch [31/50], Train Loss: 0.1283, Val Loss: 0.3473\n",
      "Epoch [32/50], Train Loss: 0.1275, Val Loss: 0.3468\n",
      "Epoch [33/50], Train Loss: 0.1275, Val Loss: 0.3463\n",
      "Epoch [34/50], Train Loss: 0.1268, Val Loss: 0.3458\n",
      "Epoch [35/50], Train Loss: 0.1269, Val Loss: 0.3453\n",
      "Epoch [36/50], Train Loss: 0.1260, Val Loss: 0.3448\n",
      "Epoch [37/50], Train Loss: 0.1259, Val Loss: 0.3443\n",
      "Epoch [38/50], Train Loss: 0.1262, Val Loss: 0.3438\n",
      "Epoch [39/50], Train Loss: 0.1255, Val Loss: 0.3433\n",
      "Epoch [40/50], Train Loss: 0.1255, Val Loss: 0.3428\n",
      "Epoch [41/50], Train Loss: 0.1251, Val Loss: 0.3424\n",
      "Epoch [42/50], Train Loss: 0.1248, Val Loss: 0.3419\n",
      "Epoch [43/50], Train Loss: 0.1245, Val Loss: 0.3414\n",
      "Epoch [44/50], Train Loss: 0.1243, Val Loss: 0.3409\n",
      "Epoch [45/50], Train Loss: 0.1242, Val Loss: 0.3404\n",
      "Epoch [46/50], Train Loss: 0.1240, Val Loss: 0.3399\n",
      "Epoch [47/50], Train Loss: 0.1235, Val Loss: 0.3394\n",
      "Epoch [48/50], Train Loss: 0.1232, Val Loss: 0.3390\n",
      "Epoch [49/50], Train Loss: 0.1229, Val Loss: 0.3385\n",
      "Epoch [50/50], Train Loss: 0.1226, Val Loss: 0.3380\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=32, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1227, Val Loss: 0.3475\n",
      "Epoch [2/50], Train Loss: 0.1227, Val Loss: 0.3470\n",
      "Epoch [3/50], Train Loss: 0.1223, Val Loss: 0.3465\n",
      "Epoch [4/50], Train Loss: 0.1219, Val Loss: 0.3460\n",
      "Epoch [5/50], Train Loss: 0.1219, Val Loss: 0.3455\n",
      "Epoch [6/50], Train Loss: 0.1216, Val Loss: 0.3450\n",
      "Epoch [7/50], Train Loss: 0.1223, Val Loss: 0.3446\n",
      "Epoch [8/50], Train Loss: 0.1211, Val Loss: 0.3441\n",
      "Epoch [9/50], Train Loss: 0.1205, Val Loss: 0.3436\n",
      "Epoch [10/50], Train Loss: 0.1201, Val Loss: 0.3431\n",
      "Epoch [11/50], Train Loss: 0.1198, Val Loss: 0.3427\n",
      "Epoch [12/50], Train Loss: 0.1212, Val Loss: 0.3422\n",
      "Epoch [13/50], Train Loss: 0.1198, Val Loss: 0.3417\n",
      "Epoch [14/50], Train Loss: 0.1194, Val Loss: 0.3413\n",
      "Epoch [15/50], Train Loss: 0.1192, Val Loss: 0.3408\n",
      "Epoch [16/50], Train Loss: 0.1192, Val Loss: 0.3403\n",
      "Epoch [17/50], Train Loss: 0.1187, Val Loss: 0.3398\n",
      "Epoch [18/50], Train Loss: 0.1189, Val Loss: 0.3394\n",
      "Epoch [19/50], Train Loss: 0.1183, Val Loss: 0.3389\n",
      "Epoch [20/50], Train Loss: 0.1187, Val Loss: 0.3384\n",
      "Epoch [21/50], Train Loss: 0.1177, Val Loss: 0.3380\n",
      "Epoch [22/50], Train Loss: 0.1175, Val Loss: 0.3375\n",
      "Epoch [23/50], Train Loss: 0.1178, Val Loss: 0.3371\n",
      "Epoch [24/50], Train Loss: 0.1170, Val Loss: 0.3366\n",
      "Epoch [25/50], Train Loss: 0.1169, Val Loss: 0.3361\n",
      "Epoch [26/50], Train Loss: 0.1166, Val Loss: 0.3357\n",
      "Epoch [27/50], Train Loss: 0.1165, Val Loss: 0.3352\n",
      "Epoch [28/50], Train Loss: 0.1159, Val Loss: 0.3348\n",
      "Epoch [29/50], Train Loss: 0.1161, Val Loss: 0.3343\n",
      "Epoch [30/50], Train Loss: 0.1154, Val Loss: 0.3339\n",
      "Epoch [31/50], Train Loss: 0.1152, Val Loss: 0.3334\n",
      "Epoch [32/50], Train Loss: 0.1146, Val Loss: 0.3330\n",
      "Epoch [33/50], Train Loss: 0.1148, Val Loss: 0.3325\n",
      "Epoch [34/50], Train Loss: 0.1146, Val Loss: 0.3321\n",
      "Epoch [35/50], Train Loss: 0.1143, Val Loss: 0.3316\n",
      "Epoch [36/50], Train Loss: 0.1147, Val Loss: 0.3312\n",
      "Epoch [37/50], Train Loss: 0.1133, Val Loss: 0.3307\n",
      "Epoch [38/50], Train Loss: 0.1142, Val Loss: 0.3303\n",
      "Epoch [39/50], Train Loss: 0.1132, Val Loss: 0.3298\n",
      "Epoch [40/50], Train Loss: 0.1130, Val Loss: 0.3294\n",
      "Epoch [41/50], Train Loss: 0.1127, Val Loss: 0.3289\n",
      "Epoch [42/50], Train Loss: 0.1129, Val Loss: 0.3285\n",
      "Epoch [43/50], Train Loss: 0.1123, Val Loss: 0.3281\n",
      "Epoch [44/50], Train Loss: 0.1122, Val Loss: 0.3276\n",
      "Epoch [45/50], Train Loss: 0.1125, Val Loss: 0.3272\n",
      "Epoch [46/50], Train Loss: 0.1127, Val Loss: 0.3267\n",
      "Epoch [47/50], Train Loss: 0.1118, Val Loss: 0.3263\n",
      "Epoch [48/50], Train Loss: 0.1117, Val Loss: 0.3259\n",
      "Epoch [49/50], Train Loss: 0.1115, Val Loss: 0.3254\n",
      "Epoch [50/50], Train Loss: 0.1106, Val Loss: 0.3250\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=64, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1247, Val Loss: 0.3444\n",
      "Epoch [2/50], Train Loss: 0.1244, Val Loss: 0.3439\n",
      "Epoch [3/50], Train Loss: 0.1241, Val Loss: 0.3433\n",
      "Epoch [4/50], Train Loss: 0.1238, Val Loss: 0.3427\n",
      "Epoch [5/50], Train Loss: 0.1235, Val Loss: 0.3421\n",
      "Epoch [6/50], Train Loss: 0.1232, Val Loss: 0.3415\n",
      "Epoch [7/50], Train Loss: 0.1229, Val Loss: 0.3410\n",
      "Epoch [8/50], Train Loss: 0.1226, Val Loss: 0.3404\n",
      "Epoch [9/50], Train Loss: 0.1223, Val Loss: 0.3398\n",
      "Epoch [10/50], Train Loss: 0.1220, Val Loss: 0.3392\n",
      "Epoch [11/50], Train Loss: 0.1217, Val Loss: 0.3387\n",
      "Epoch [12/50], Train Loss: 0.1214, Val Loss: 0.3381\n",
      "Epoch [13/50], Train Loss: 0.1211, Val Loss: 0.3375\n",
      "Epoch [14/50], Train Loss: 0.1208, Val Loss: 0.3370\n",
      "Epoch [15/50], Train Loss: 0.1205, Val Loss: 0.3364\n",
      "Epoch [16/50], Train Loss: 0.1202, Val Loss: 0.3358\n",
      "Epoch [17/50], Train Loss: 0.1199, Val Loss: 0.3353\n",
      "Epoch [18/50], Train Loss: 0.1196, Val Loss: 0.3347\n",
      "Epoch [19/50], Train Loss: 0.1193, Val Loss: 0.3342\n",
      "Epoch [20/50], Train Loss: 0.1191, Val Loss: 0.3336\n",
      "Epoch [21/50], Train Loss: 0.1188, Val Loss: 0.3330\n",
      "Epoch [22/50], Train Loss: 0.1185, Val Loss: 0.3325\n",
      "Epoch [23/50], Train Loss: 0.1182, Val Loss: 0.3319\n",
      "Epoch [24/50], Train Loss: 0.1179, Val Loss: 0.3314\n",
      "Epoch [25/50], Train Loss: 0.1176, Val Loss: 0.3308\n",
      "Epoch [26/50], Train Loss: 0.1173, Val Loss: 0.3303\n",
      "Epoch [27/50], Train Loss: 0.1171, Val Loss: 0.3297\n",
      "Epoch [28/50], Train Loss: 0.1168, Val Loss: 0.3292\n",
      "Epoch [29/50], Train Loss: 0.1165, Val Loss: 0.3286\n",
      "Epoch [30/50], Train Loss: 0.1162, Val Loss: 0.3281\n",
      "Epoch [31/50], Train Loss: 0.1160, Val Loss: 0.3276\n",
      "Epoch [32/50], Train Loss: 0.1157, Val Loss: 0.3270\n",
      "Epoch [33/50], Train Loss: 0.1154, Val Loss: 0.3265\n",
      "Epoch [34/50], Train Loss: 0.1151, Val Loss: 0.3259\n",
      "Epoch [35/50], Train Loss: 0.1149, Val Loss: 0.3254\n",
      "Epoch [36/50], Train Loss: 0.1146, Val Loss: 0.3249\n",
      "Epoch [37/50], Train Loss: 0.1143, Val Loss: 0.3243\n",
      "Epoch [38/50], Train Loss: 0.1140, Val Loss: 0.3238\n",
      "Epoch [39/50], Train Loss: 0.1138, Val Loss: 0.3233\n",
      "Epoch [40/50], Train Loss: 0.1135, Val Loss: 0.3227\n",
      "Epoch [41/50], Train Loss: 0.1132, Val Loss: 0.3222\n",
      "Epoch [42/50], Train Loss: 0.1130, Val Loss: 0.3217\n",
      "Epoch [43/50], Train Loss: 0.1127, Val Loss: 0.3212\n",
      "Epoch [44/50], Train Loss: 0.1124, Val Loss: 0.3206\n",
      "Epoch [45/50], Train Loss: 0.1122, Val Loss: 0.3201\n",
      "Epoch [46/50], Train Loss: 0.1119, Val Loss: 0.3196\n",
      "Epoch [47/50], Train Loss: 0.1116, Val Loss: 0.3191\n",
      "Epoch [48/50], Train Loss: 0.1114, Val Loss: 0.3186\n",
      "Epoch [49/50], Train Loss: 0.1111, Val Loss: 0.3180\n",
      "Epoch [50/50], Train Loss: 0.1109, Val Loss: 0.3175\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=64, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1190, Val Loss: 0.3736\n",
      "Epoch [2/50], Train Loss: 0.1188, Val Loss: 0.3729\n",
      "Epoch [3/50], Train Loss: 0.1181, Val Loss: 0.3723\n",
      "Epoch [4/50], Train Loss: 0.1177, Val Loss: 0.3716\n",
      "Epoch [5/50], Train Loss: 0.1177, Val Loss: 0.3710\n",
      "Epoch [6/50], Train Loss: 0.1169, Val Loss: 0.3704\n",
      "Epoch [7/50], Train Loss: 0.1165, Val Loss: 0.3697\n",
      "Epoch [8/50], Train Loss: 0.1172, Val Loss: 0.3691\n",
      "Epoch [9/50], Train Loss: 0.1161, Val Loss: 0.3685\n",
      "Epoch [10/50], Train Loss: 0.1161, Val Loss: 0.3678\n",
      "Epoch [11/50], Train Loss: 0.1157, Val Loss: 0.3672\n",
      "Epoch [12/50], Train Loss: 0.1158, Val Loss: 0.3666\n",
      "Epoch [13/50], Train Loss: 0.1150, Val Loss: 0.3659\n",
      "Epoch [14/50], Train Loss: 0.1144, Val Loss: 0.3653\n",
      "Epoch [15/50], Train Loss: 0.1148, Val Loss: 0.3647\n",
      "Epoch [16/50], Train Loss: 0.1143, Val Loss: 0.3641\n",
      "Epoch [17/50], Train Loss: 0.1134, Val Loss: 0.3635\n",
      "Epoch [18/50], Train Loss: 0.1136, Val Loss: 0.3628\n",
      "Epoch [19/50], Train Loss: 0.1135, Val Loss: 0.3622\n",
      "Epoch [20/50], Train Loss: 0.1128, Val Loss: 0.3616\n",
      "Epoch [21/50], Train Loss: 0.1127, Val Loss: 0.3610\n",
      "Epoch [22/50], Train Loss: 0.1125, Val Loss: 0.3604\n",
      "Epoch [23/50], Train Loss: 0.1119, Val Loss: 0.3598\n",
      "Epoch [24/50], Train Loss: 0.1116, Val Loss: 0.3591\n",
      "Epoch [25/50], Train Loss: 0.1116, Val Loss: 0.3585\n",
      "Epoch [26/50], Train Loss: 0.1118, Val Loss: 0.3579\n",
      "Epoch [27/50], Train Loss: 0.1114, Val Loss: 0.3573\n",
      "Epoch [28/50], Train Loss: 0.1108, Val Loss: 0.3567\n",
      "Epoch [29/50], Train Loss: 0.1111, Val Loss: 0.3561\n",
      "Epoch [30/50], Train Loss: 0.1107, Val Loss: 0.3555\n",
      "Epoch [31/50], Train Loss: 0.1104, Val Loss: 0.3549\n",
      "Epoch [32/50], Train Loss: 0.1093, Val Loss: 0.3543\n",
      "Epoch [33/50], Train Loss: 0.1093, Val Loss: 0.3537\n",
      "Epoch [34/50], Train Loss: 0.1095, Val Loss: 0.3531\n",
      "Epoch [35/50], Train Loss: 0.1092, Val Loss: 0.3526\n",
      "Epoch [36/50], Train Loss: 0.1088, Val Loss: 0.3520\n",
      "Epoch [37/50], Train Loss: 0.1085, Val Loss: 0.3514\n",
      "Epoch [38/50], Train Loss: 0.1081, Val Loss: 0.3508\n",
      "Epoch [39/50], Train Loss: 0.1078, Val Loss: 0.3502\n",
      "Epoch [40/50], Train Loss: 0.1079, Val Loss: 0.3496\n",
      "Epoch [41/50], Train Loss: 0.1077, Val Loss: 0.3490\n",
      "Epoch [42/50], Train Loss: 0.1074, Val Loss: 0.3485\n",
      "Epoch [43/50], Train Loss: 0.1068, Val Loss: 0.3479\n",
      "Epoch [44/50], Train Loss: 0.1072, Val Loss: 0.3473\n",
      "Epoch [45/50], Train Loss: 0.1067, Val Loss: 0.3467\n",
      "Epoch [46/50], Train Loss: 0.1064, Val Loss: 0.3462\n",
      "Epoch [47/50], Train Loss: 0.1058, Val Loss: 0.3456\n",
      "Epoch [48/50], Train Loss: 0.1063, Val Loss: 0.3450\n",
      "Epoch [49/50], Train Loss: 0.1058, Val Loss: 0.3444\n",
      "Epoch [50/50], Train Loss: 0.1055, Val Loss: 0.3439\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=64, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1880, Val Loss: 0.4599\n",
      "Epoch [2/50], Train Loss: 0.1867, Val Loss: 0.4591\n",
      "Epoch [3/50], Train Loss: 0.1856, Val Loss: 0.4583\n",
      "Epoch [4/50], Train Loss: 0.1861, Val Loss: 0.4574\n",
      "Epoch [5/50], Train Loss: 0.1859, Val Loss: 0.4566\n",
      "Epoch [6/50], Train Loss: 0.1843, Val Loss: 0.4558\n",
      "Epoch [7/50], Train Loss: 0.1848, Val Loss: 0.4550\n",
      "Epoch [8/50], Train Loss: 0.1845, Val Loss: 0.4541\n",
      "Epoch [9/50], Train Loss: 0.1842, Val Loss: 0.4533\n",
      "Epoch [10/50], Train Loss: 0.1839, Val Loss: 0.4525\n",
      "Epoch [11/50], Train Loss: 0.1832, Val Loss: 0.4517\n",
      "Epoch [12/50], Train Loss: 0.1822, Val Loss: 0.4509\n",
      "Epoch [13/50], Train Loss: 0.1822, Val Loss: 0.4500\n",
      "Epoch [14/50], Train Loss: 0.1816, Val Loss: 0.4492\n",
      "Epoch [15/50], Train Loss: 0.1807, Val Loss: 0.4484\n",
      "Epoch [16/50], Train Loss: 0.1801, Val Loss: 0.4476\n",
      "Epoch [17/50], Train Loss: 0.1805, Val Loss: 0.4468\n",
      "Epoch [18/50], Train Loss: 0.1799, Val Loss: 0.4460\n",
      "Epoch [19/50], Train Loss: 0.1789, Val Loss: 0.4452\n",
      "Epoch [20/50], Train Loss: 0.1785, Val Loss: 0.4444\n",
      "Epoch [21/50], Train Loss: 0.1784, Val Loss: 0.4436\n",
      "Epoch [22/50], Train Loss: 0.1787, Val Loss: 0.4428\n",
      "Epoch [23/50], Train Loss: 0.1782, Val Loss: 0.4420\n",
      "Epoch [24/50], Train Loss: 0.1776, Val Loss: 0.4412\n",
      "Epoch [25/50], Train Loss: 0.1759, Val Loss: 0.4404\n",
      "Epoch [26/50], Train Loss: 0.1758, Val Loss: 0.4397\n",
      "Epoch [27/50], Train Loss: 0.1754, Val Loss: 0.4389\n",
      "Epoch [28/50], Train Loss: 0.1751, Val Loss: 0.4381\n",
      "Epoch [29/50], Train Loss: 0.1754, Val Loss: 0.4373\n",
      "Epoch [30/50], Train Loss: 0.1747, Val Loss: 0.4365\n",
      "Epoch [31/50], Train Loss: 0.1735, Val Loss: 0.4358\n",
      "Epoch [32/50], Train Loss: 0.1732, Val Loss: 0.4350\n",
      "Epoch [33/50], Train Loss: 0.1744, Val Loss: 0.4342\n",
      "Epoch [34/50], Train Loss: 0.1727, Val Loss: 0.4335\n",
      "Epoch [35/50], Train Loss: 0.1725, Val Loss: 0.4327\n",
      "Epoch [36/50], Train Loss: 0.1721, Val Loss: 0.4319\n",
      "Epoch [37/50], Train Loss: 0.1707, Val Loss: 0.4312\n",
      "Epoch [38/50], Train Loss: 0.1704, Val Loss: 0.4304\n",
      "Epoch [39/50], Train Loss: 0.1700, Val Loss: 0.4296\n",
      "Epoch [40/50], Train Loss: 0.1701, Val Loss: 0.4289\n",
      "Epoch [41/50], Train Loss: 0.1696, Val Loss: 0.4281\n",
      "Epoch [42/50], Train Loss: 0.1691, Val Loss: 0.4274\n",
      "Epoch [43/50], Train Loss: 0.1687, Val Loss: 0.4266\n",
      "Epoch [44/50], Train Loss: 0.1694, Val Loss: 0.4259\n",
      "Epoch [45/50], Train Loss: 0.1674, Val Loss: 0.4251\n",
      "Epoch [46/50], Train Loss: 0.1680, Val Loss: 0.4244\n",
      "Epoch [47/50], Train Loss: 0.1669, Val Loss: 0.4236\n",
      "Epoch [48/50], Train Loss: 0.1666, Val Loss: 0.4229\n",
      "Epoch [49/50], Train Loss: 0.1663, Val Loss: 0.4222\n",
      "Epoch [50/50], Train Loss: 0.1655, Val Loss: 0.4214\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=64, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1589, Val Loss: 0.4093\n",
      "Epoch [2/50], Train Loss: 0.1585, Val Loss: 0.4087\n",
      "Epoch [3/50], Train Loss: 0.1581, Val Loss: 0.4081\n",
      "Epoch [4/50], Train Loss: 0.1578, Val Loss: 0.4074\n",
      "Epoch [5/50], Train Loss: 0.1574, Val Loss: 0.4068\n",
      "Epoch [6/50], Train Loss: 0.1570, Val Loss: 0.4062\n",
      "Epoch [7/50], Train Loss: 0.1567, Val Loss: 0.4055\n",
      "Epoch [8/50], Train Loss: 0.1563, Val Loss: 0.4049\n",
      "Epoch [9/50], Train Loss: 0.1559, Val Loss: 0.4043\n",
      "Epoch [10/50], Train Loss: 0.1556, Val Loss: 0.4037\n",
      "Epoch [11/50], Train Loss: 0.1552, Val Loss: 0.4030\n",
      "Epoch [12/50], Train Loss: 0.1549, Val Loss: 0.4024\n",
      "Epoch [13/50], Train Loss: 0.1545, Val Loss: 0.4018\n",
      "Epoch [14/50], Train Loss: 0.1541, Val Loss: 0.4012\n",
      "Epoch [15/50], Train Loss: 0.1538, Val Loss: 0.4006\n",
      "Epoch [16/50], Train Loss: 0.1534, Val Loss: 0.4000\n",
      "Epoch [17/50], Train Loss: 0.1531, Val Loss: 0.3993\n",
      "Epoch [18/50], Train Loss: 0.1527, Val Loss: 0.3987\n",
      "Epoch [19/50], Train Loss: 0.1524, Val Loss: 0.3981\n",
      "Epoch [20/50], Train Loss: 0.1520, Val Loss: 0.3975\n",
      "Epoch [21/50], Train Loss: 0.1517, Val Loss: 0.3969\n",
      "Epoch [22/50], Train Loss: 0.1513, Val Loss: 0.3963\n",
      "Epoch [23/50], Train Loss: 0.1510, Val Loss: 0.3957\n",
      "Epoch [24/50], Train Loss: 0.1506, Val Loss: 0.3951\n",
      "Epoch [25/50], Train Loss: 0.1503, Val Loss: 0.3945\n",
      "Epoch [26/50], Train Loss: 0.1499, Val Loss: 0.3939\n",
      "Epoch [27/50], Train Loss: 0.1496, Val Loss: 0.3933\n",
      "Epoch [28/50], Train Loss: 0.1493, Val Loss: 0.3927\n",
      "Epoch [29/50], Train Loss: 0.1489, Val Loss: 0.3921\n",
      "Epoch [30/50], Train Loss: 0.1486, Val Loss: 0.3915\n",
      "Epoch [31/50], Train Loss: 0.1482, Val Loss: 0.3909\n",
      "Epoch [32/50], Train Loss: 0.1479, Val Loss: 0.3903\n",
      "Epoch [33/50], Train Loss: 0.1476, Val Loss: 0.3897\n",
      "Epoch [34/50], Train Loss: 0.1472, Val Loss: 0.3892\n",
      "Epoch [35/50], Train Loss: 0.1469, Val Loss: 0.3886\n",
      "Epoch [36/50], Train Loss: 0.1466, Val Loss: 0.3880\n",
      "Epoch [37/50], Train Loss: 0.1462, Val Loss: 0.3874\n",
      "Epoch [38/50], Train Loss: 0.1459, Val Loss: 0.3868\n",
      "Epoch [39/50], Train Loss: 0.1456, Val Loss: 0.3862\n",
      "Epoch [40/50], Train Loss: 0.1452, Val Loss: 0.3857\n",
      "Epoch [41/50], Train Loss: 0.1449, Val Loss: 0.3851\n",
      "Epoch [42/50], Train Loss: 0.1446, Val Loss: 0.3845\n",
      "Epoch [43/50], Train Loss: 0.1443, Val Loss: 0.3839\n",
      "Epoch [44/50], Train Loss: 0.1439, Val Loss: 0.3833\n",
      "Epoch [45/50], Train Loss: 0.1436, Val Loss: 0.3828\n",
      "Epoch [46/50], Train Loss: 0.1433, Val Loss: 0.3822\n",
      "Epoch [47/50], Train Loss: 0.1430, Val Loss: 0.3816\n",
      "Epoch [48/50], Train Loss: 0.1426, Val Loss: 0.3811\n",
      "Epoch [49/50], Train Loss: 0.1423, Val Loss: 0.3805\n",
      "Epoch [50/50], Train Loss: 0.1420, Val Loss: 0.3799\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=64, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1190, Val Loss: 0.3251\n",
      "Epoch [2/50], Train Loss: 0.1191, Val Loss: 0.3246\n",
      "Epoch [3/50], Train Loss: 0.1188, Val Loss: 0.3242\n",
      "Epoch [4/50], Train Loss: 0.1185, Val Loss: 0.3238\n",
      "Epoch [5/50], Train Loss: 0.1182, Val Loss: 0.3233\n",
      "Epoch [6/50], Train Loss: 0.1177, Val Loss: 0.3229\n",
      "Epoch [7/50], Train Loss: 0.1178, Val Loss: 0.3224\n",
      "Epoch [8/50], Train Loss: 0.1174, Val Loss: 0.3220\n",
      "Epoch [9/50], Train Loss: 0.1172, Val Loss: 0.3216\n",
      "Epoch [10/50], Train Loss: 0.1170, Val Loss: 0.3211\n",
      "Epoch [11/50], Train Loss: 0.1168, Val Loss: 0.3207\n",
      "Epoch [12/50], Train Loss: 0.1163, Val Loss: 0.3203\n",
      "Epoch [13/50], Train Loss: 0.1165, Val Loss: 0.3198\n",
      "Epoch [14/50], Train Loss: 0.1163, Val Loss: 0.3194\n",
      "Epoch [15/50], Train Loss: 0.1158, Val Loss: 0.3190\n",
      "Epoch [16/50], Train Loss: 0.1156, Val Loss: 0.3186\n",
      "Epoch [17/50], Train Loss: 0.1154, Val Loss: 0.3181\n",
      "Epoch [18/50], Train Loss: 0.1151, Val Loss: 0.3177\n",
      "Epoch [19/50], Train Loss: 0.1150, Val Loss: 0.3173\n",
      "Epoch [20/50], Train Loss: 0.1144, Val Loss: 0.3169\n",
      "Epoch [21/50], Train Loss: 0.1144, Val Loss: 0.3164\n",
      "Epoch [22/50], Train Loss: 0.1141, Val Loss: 0.3160\n",
      "Epoch [23/50], Train Loss: 0.1137, Val Loss: 0.3156\n",
      "Epoch [24/50], Train Loss: 0.1138, Val Loss: 0.3152\n",
      "Epoch [25/50], Train Loss: 0.1134, Val Loss: 0.3148\n",
      "Epoch [26/50], Train Loss: 0.1131, Val Loss: 0.3143\n",
      "Epoch [27/50], Train Loss: 0.1131, Val Loss: 0.3139\n",
      "Epoch [28/50], Train Loss: 0.1129, Val Loss: 0.3135\n",
      "Epoch [29/50], Train Loss: 0.1125, Val Loss: 0.3131\n",
      "Epoch [30/50], Train Loss: 0.1120, Val Loss: 0.3127\n",
      "Epoch [31/50], Train Loss: 0.1122, Val Loss: 0.3123\n",
      "Epoch [32/50], Train Loss: 0.1118, Val Loss: 0.3119\n",
      "Epoch [33/50], Train Loss: 0.1118, Val Loss: 0.3115\n",
      "Epoch [34/50], Train Loss: 0.1116, Val Loss: 0.3110\n",
      "Epoch [35/50], Train Loss: 0.1111, Val Loss: 0.3106\n",
      "Epoch [36/50], Train Loss: 0.1108, Val Loss: 0.3102\n",
      "Epoch [37/50], Train Loss: 0.1103, Val Loss: 0.3098\n",
      "Epoch [38/50], Train Loss: 0.1104, Val Loss: 0.3094\n",
      "Epoch [39/50], Train Loss: 0.1104, Val Loss: 0.3090\n",
      "Epoch [40/50], Train Loss: 0.1099, Val Loss: 0.3086\n",
      "Epoch [41/50], Train Loss: 0.1098, Val Loss: 0.3082\n",
      "Epoch [42/50], Train Loss: 0.1096, Val Loss: 0.3078\n",
      "Epoch [43/50], Train Loss: 0.1094, Val Loss: 0.3074\n",
      "Epoch [44/50], Train Loss: 0.1094, Val Loss: 0.3070\n",
      "Epoch [45/50], Train Loss: 0.1088, Val Loss: 0.3066\n",
      "Epoch [46/50], Train Loss: 0.1083, Val Loss: 0.3062\n",
      "Epoch [47/50], Train Loss: 0.1085, Val Loss: 0.3058\n",
      "Epoch [48/50], Train Loss: 0.1081, Val Loss: 0.3054\n",
      "Epoch [49/50], Train Loss: 0.1082, Val Loss: 0.3050\n",
      "Epoch [50/50], Train Loss: 0.1080, Val Loss: 0.3046\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=64, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1315, Val Loss: 0.3604\n",
      "Epoch [2/50], Train Loss: 0.1315, Val Loss: 0.3599\n",
      "Epoch [3/50], Train Loss: 0.1315, Val Loss: 0.3594\n",
      "Epoch [4/50], Train Loss: 0.1311, Val Loss: 0.3588\n",
      "Epoch [5/50], Train Loss: 0.1309, Val Loss: 0.3583\n",
      "Epoch [6/50], Train Loss: 0.1308, Val Loss: 0.3578\n",
      "Epoch [7/50], Train Loss: 0.1306, Val Loss: 0.3573\n",
      "Epoch [8/50], Train Loss: 0.1292, Val Loss: 0.3567\n",
      "Epoch [9/50], Train Loss: 0.1299, Val Loss: 0.3562\n",
      "Epoch [10/50], Train Loss: 0.1298, Val Loss: 0.3557\n",
      "Epoch [11/50], Train Loss: 0.1291, Val Loss: 0.3552\n",
      "Epoch [12/50], Train Loss: 0.1285, Val Loss: 0.3547\n",
      "Epoch [13/50], Train Loss: 0.1285, Val Loss: 0.3542\n",
      "Epoch [14/50], Train Loss: 0.1280, Val Loss: 0.3536\n",
      "Epoch [15/50], Train Loss: 0.1278, Val Loss: 0.3531\n",
      "Epoch [16/50], Train Loss: 0.1283, Val Loss: 0.3526\n",
      "Epoch [17/50], Train Loss: 0.1275, Val Loss: 0.3521\n",
      "Epoch [18/50], Train Loss: 0.1267, Val Loss: 0.3516\n",
      "Epoch [19/50], Train Loss: 0.1260, Val Loss: 0.3511\n",
      "Epoch [20/50], Train Loss: 0.1267, Val Loss: 0.3506\n",
      "Epoch [21/50], Train Loss: 0.1262, Val Loss: 0.3501\n",
      "Epoch [22/50], Train Loss: 0.1261, Val Loss: 0.3496\n",
      "Epoch [23/50], Train Loss: 0.1258, Val Loss: 0.3490\n",
      "Epoch [24/50], Train Loss: 0.1258, Val Loss: 0.3485\n",
      "Epoch [25/50], Train Loss: 0.1258, Val Loss: 0.3480\n",
      "Epoch [26/50], Train Loss: 0.1245, Val Loss: 0.3475\n",
      "Epoch [27/50], Train Loss: 0.1245, Val Loss: 0.3470\n",
      "Epoch [28/50], Train Loss: 0.1248, Val Loss: 0.3465\n",
      "Epoch [29/50], Train Loss: 0.1243, Val Loss: 0.3460\n",
      "Epoch [30/50], Train Loss: 0.1243, Val Loss: 0.3455\n",
      "Epoch [31/50], Train Loss: 0.1241, Val Loss: 0.3450\n",
      "Epoch [32/50], Train Loss: 0.1232, Val Loss: 0.3445\n",
      "Epoch [33/50], Train Loss: 0.1231, Val Loss: 0.3441\n",
      "Epoch [34/50], Train Loss: 0.1231, Val Loss: 0.3436\n",
      "Epoch [35/50], Train Loss: 0.1225, Val Loss: 0.3431\n",
      "Epoch [36/50], Train Loss: 0.1226, Val Loss: 0.3426\n",
      "Epoch [37/50], Train Loss: 0.1220, Val Loss: 0.3421\n",
      "Epoch [38/50], Train Loss: 0.1221, Val Loss: 0.3416\n",
      "Epoch [39/50], Train Loss: 0.1216, Val Loss: 0.3411\n",
      "Epoch [40/50], Train Loss: 0.1210, Val Loss: 0.3406\n",
      "Epoch [41/50], Train Loss: 0.1216, Val Loss: 0.3401\n",
      "Epoch [42/50], Train Loss: 0.1206, Val Loss: 0.3397\n",
      "Epoch [43/50], Train Loss: 0.1209, Val Loss: 0.3392\n",
      "Epoch [44/50], Train Loss: 0.1200, Val Loss: 0.3387\n",
      "Epoch [45/50], Train Loss: 0.1201, Val Loss: 0.3382\n",
      "Epoch [46/50], Train Loss: 0.1199, Val Loss: 0.3377\n",
      "Epoch [47/50], Train Loss: 0.1196, Val Loss: 0.3372\n",
      "Epoch [48/50], Train Loss: 0.1188, Val Loss: 0.3368\n",
      "Epoch [49/50], Train Loss: 0.1192, Val Loss: 0.3363\n",
      "Epoch [50/50], Train Loss: 0.1192, Val Loss: 0.3358\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=64, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1103, Val Loss: 0.3109\n",
      "Epoch [2/50], Train Loss: 0.1101, Val Loss: 0.3105\n",
      "Epoch [3/50], Train Loss: 0.1098, Val Loss: 0.3100\n",
      "Epoch [4/50], Train Loss: 0.1096, Val Loss: 0.3096\n",
      "Epoch [5/50], Train Loss: 0.1093, Val Loss: 0.3091\n",
      "Epoch [6/50], Train Loss: 0.1091, Val Loss: 0.3087\n",
      "Epoch [7/50], Train Loss: 0.1088, Val Loss: 0.3082\n",
      "Epoch [8/50], Train Loss: 0.1086, Val Loss: 0.3078\n",
      "Epoch [9/50], Train Loss: 0.1084, Val Loss: 0.3074\n",
      "Epoch [10/50], Train Loss: 0.1081, Val Loss: 0.3069\n",
      "Epoch [11/50], Train Loss: 0.1079, Val Loss: 0.3065\n",
      "Epoch [12/50], Train Loss: 0.1076, Val Loss: 0.3060\n",
      "Epoch [13/50], Train Loss: 0.1074, Val Loss: 0.3056\n",
      "Epoch [14/50], Train Loss: 0.1072, Val Loss: 0.3051\n",
      "Epoch [15/50], Train Loss: 0.1069, Val Loss: 0.3047\n",
      "Epoch [16/50], Train Loss: 0.1067, Val Loss: 0.3043\n",
      "Epoch [17/50], Train Loss: 0.1065, Val Loss: 0.3038\n",
      "Epoch [18/50], Train Loss: 0.1062, Val Loss: 0.3034\n",
      "Epoch [19/50], Train Loss: 0.1060, Val Loss: 0.3030\n",
      "Epoch [20/50], Train Loss: 0.1058, Val Loss: 0.3025\n",
      "Epoch [21/50], Train Loss: 0.1055, Val Loss: 0.3021\n",
      "Epoch [22/50], Train Loss: 0.1053, Val Loss: 0.3017\n",
      "Epoch [23/50], Train Loss: 0.1051, Val Loss: 0.3012\n",
      "Epoch [24/50], Train Loss: 0.1048, Val Loss: 0.3008\n",
      "Epoch [25/50], Train Loss: 0.1046, Val Loss: 0.3004\n",
      "Epoch [26/50], Train Loss: 0.1044, Val Loss: 0.2999\n",
      "Epoch [27/50], Train Loss: 0.1042, Val Loss: 0.2995\n",
      "Epoch [28/50], Train Loss: 0.1039, Val Loss: 0.2991\n",
      "Epoch [29/50], Train Loss: 0.1037, Val Loss: 0.2987\n",
      "Epoch [30/50], Train Loss: 0.1035, Val Loss: 0.2983\n",
      "Epoch [31/50], Train Loss: 0.1033, Val Loss: 0.2978\n",
      "Epoch [32/50], Train Loss: 0.1030, Val Loss: 0.2974\n",
      "Epoch [33/50], Train Loss: 0.1028, Val Loss: 0.2970\n",
      "Epoch [34/50], Train Loss: 0.1026, Val Loss: 0.2966\n",
      "Epoch [35/50], Train Loss: 0.1024, Val Loss: 0.2962\n",
      "Epoch [36/50], Train Loss: 0.1021, Val Loss: 0.2957\n",
      "Epoch [37/50], Train Loss: 0.1019, Val Loss: 0.2953\n",
      "Epoch [38/50], Train Loss: 0.1017, Val Loss: 0.2949\n",
      "Epoch [39/50], Train Loss: 0.1015, Val Loss: 0.2945\n",
      "Epoch [40/50], Train Loss: 0.1013, Val Loss: 0.2941\n",
      "Epoch [41/50], Train Loss: 0.1011, Val Loss: 0.2937\n",
      "Epoch [42/50], Train Loss: 0.1008, Val Loss: 0.2933\n",
      "Epoch [43/50], Train Loss: 0.1006, Val Loss: 0.2928\n",
      "Epoch [44/50], Train Loss: 0.1004, Val Loss: 0.2924\n",
      "Epoch [45/50], Train Loss: 0.1002, Val Loss: 0.2920\n",
      "Epoch [46/50], Train Loss: 0.1000, Val Loss: 0.2916\n",
      "Epoch [47/50], Train Loss: 0.0998, Val Loss: 0.2912\n",
      "Epoch [48/50], Train Loss: 0.0996, Val Loss: 0.2908\n",
      "Epoch [49/50], Train Loss: 0.0994, Val Loss: 0.2904\n",
      "Epoch [50/50], Train Loss: 0.0991, Val Loss: 0.2900\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=64, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1728, Val Loss: 0.4586\n",
      "Epoch [2/50], Train Loss: 0.1722, Val Loss: 0.4578\n",
      "Epoch [3/50], Train Loss: 0.1718, Val Loss: 0.4571\n",
      "Epoch [4/50], Train Loss: 0.1712, Val Loss: 0.4563\n",
      "Epoch [5/50], Train Loss: 0.1712, Val Loss: 0.4555\n",
      "Epoch [6/50], Train Loss: 0.1703, Val Loss: 0.4548\n",
      "Epoch [7/50], Train Loss: 0.1699, Val Loss: 0.4540\n",
      "Epoch [8/50], Train Loss: 0.1696, Val Loss: 0.4533\n",
      "Epoch [9/50], Train Loss: 0.1696, Val Loss: 0.4525\n",
      "Epoch [10/50], Train Loss: 0.1685, Val Loss: 0.4518\n",
      "Epoch [11/50], Train Loss: 0.1685, Val Loss: 0.4510\n",
      "Epoch [12/50], Train Loss: 0.1680, Val Loss: 0.4503\n",
      "Epoch [13/50], Train Loss: 0.1678, Val Loss: 0.4496\n",
      "Epoch [14/50], Train Loss: 0.1671, Val Loss: 0.4488\n",
      "Epoch [15/50], Train Loss: 0.1668, Val Loss: 0.4481\n",
      "Epoch [16/50], Train Loss: 0.1664, Val Loss: 0.4473\n",
      "Epoch [17/50], Train Loss: 0.1661, Val Loss: 0.4466\n",
      "Epoch [18/50], Train Loss: 0.1653, Val Loss: 0.4459\n",
      "Epoch [19/50], Train Loss: 0.1653, Val Loss: 0.4451\n",
      "Epoch [20/50], Train Loss: 0.1650, Val Loss: 0.4444\n",
      "Epoch [21/50], Train Loss: 0.1643, Val Loss: 0.4437\n",
      "Epoch [22/50], Train Loss: 0.1642, Val Loss: 0.4430\n",
      "Epoch [23/50], Train Loss: 0.1633, Val Loss: 0.4422\n",
      "Epoch [24/50], Train Loss: 0.1627, Val Loss: 0.4415\n",
      "Epoch [25/50], Train Loss: 0.1628, Val Loss: 0.4408\n",
      "Epoch [26/50], Train Loss: 0.1621, Val Loss: 0.4401\n",
      "Epoch [27/50], Train Loss: 0.1617, Val Loss: 0.4394\n",
      "Epoch [28/50], Train Loss: 0.1614, Val Loss: 0.4387\n",
      "Epoch [29/50], Train Loss: 0.1611, Val Loss: 0.4379\n",
      "Epoch [30/50], Train Loss: 0.1611, Val Loss: 0.4372\n",
      "Epoch [31/50], Train Loss: 0.1604, Val Loss: 0.4365\n",
      "Epoch [32/50], Train Loss: 0.1600, Val Loss: 0.4358\n",
      "Epoch [33/50], Train Loss: 0.1597, Val Loss: 0.4351\n",
      "Epoch [34/50], Train Loss: 0.1596, Val Loss: 0.4344\n",
      "Epoch [35/50], Train Loss: 0.1590, Val Loss: 0.4337\n",
      "Epoch [36/50], Train Loss: 0.1585, Val Loss: 0.4330\n",
      "Epoch [37/50], Train Loss: 0.1584, Val Loss: 0.4323\n",
      "Epoch [38/50], Train Loss: 0.1577, Val Loss: 0.4316\n",
      "Epoch [39/50], Train Loss: 0.1570, Val Loss: 0.4309\n",
      "Epoch [40/50], Train Loss: 0.1570, Val Loss: 0.4302\n",
      "Epoch [41/50], Train Loss: 0.1566, Val Loss: 0.4295\n",
      "Epoch [42/50], Train Loss: 0.1558, Val Loss: 0.4288\n",
      "Epoch [43/50], Train Loss: 0.1556, Val Loss: 0.4281\n",
      "Epoch [44/50], Train Loss: 0.1553, Val Loss: 0.4275\n",
      "Epoch [45/50], Train Loss: 0.1547, Val Loss: 0.4268\n",
      "Epoch [46/50], Train Loss: 0.1548, Val Loss: 0.4261\n",
      "Epoch [47/50], Train Loss: 0.1544, Val Loss: 0.4254\n",
      "Epoch [48/50], Train Loss: 0.1538, Val Loss: 0.4247\n",
      "Epoch [49/50], Train Loss: 0.1532, Val Loss: 0.4240\n",
      "Epoch [50/50], Train Loss: 0.1532, Val Loss: 0.4234\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=64, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1758, Val Loss: 0.4328\n",
      "Epoch [2/50], Train Loss: 0.1748, Val Loss: 0.4321\n",
      "Epoch [3/50], Train Loss: 0.1753, Val Loss: 0.4314\n",
      "Epoch [4/50], Train Loss: 0.1749, Val Loss: 0.4306\n",
      "Epoch [5/50], Train Loss: 0.1737, Val Loss: 0.4299\n",
      "Epoch [6/50], Train Loss: 0.1743, Val Loss: 0.4292\n",
      "Epoch [7/50], Train Loss: 0.1736, Val Loss: 0.4284\n",
      "Epoch [8/50], Train Loss: 0.1731, Val Loss: 0.4277\n",
      "Epoch [9/50], Train Loss: 0.1722, Val Loss: 0.4270\n",
      "Epoch [10/50], Train Loss: 0.1717, Val Loss: 0.4263\n",
      "Epoch [11/50], Train Loss: 0.1718, Val Loss: 0.4255\n",
      "Epoch [12/50], Train Loss: 0.1707, Val Loss: 0.4248\n",
      "Epoch [13/50], Train Loss: 0.1711, Val Loss: 0.4241\n",
      "Epoch [14/50], Train Loss: 0.1700, Val Loss: 0.4234\n",
      "Epoch [15/50], Train Loss: 0.1695, Val Loss: 0.4227\n",
      "Epoch [16/50], Train Loss: 0.1702, Val Loss: 0.4220\n",
      "Epoch [17/50], Train Loss: 0.1688, Val Loss: 0.4213\n",
      "Epoch [18/50], Train Loss: 0.1683, Val Loss: 0.4206\n",
      "Epoch [19/50], Train Loss: 0.1677, Val Loss: 0.4199\n",
      "Epoch [20/50], Train Loss: 0.1676, Val Loss: 0.4192\n",
      "Epoch [21/50], Train Loss: 0.1674, Val Loss: 0.4185\n",
      "Epoch [22/50], Train Loss: 0.1671, Val Loss: 0.4178\n",
      "Epoch [23/50], Train Loss: 0.1674, Val Loss: 0.4171\n",
      "Epoch [24/50], Train Loss: 0.1664, Val Loss: 0.4164\n",
      "Epoch [25/50], Train Loss: 0.1658, Val Loss: 0.4157\n",
      "Epoch [26/50], Train Loss: 0.1649, Val Loss: 0.4150\n",
      "Epoch [27/50], Train Loss: 0.1651, Val Loss: 0.4143\n",
      "Epoch [28/50], Train Loss: 0.1643, Val Loss: 0.4136\n",
      "Epoch [29/50], Train Loss: 0.1646, Val Loss: 0.4129\n",
      "Epoch [30/50], Train Loss: 0.1638, Val Loss: 0.4122\n",
      "Epoch [31/50], Train Loss: 0.1629, Val Loss: 0.4115\n",
      "Epoch [32/50], Train Loss: 0.1635, Val Loss: 0.4109\n",
      "Epoch [33/50], Train Loss: 0.1623, Val Loss: 0.4102\n",
      "Epoch [34/50], Train Loss: 0.1622, Val Loss: 0.4095\n",
      "Epoch [35/50], Train Loss: 0.1616, Val Loss: 0.4088\n",
      "Epoch [36/50], Train Loss: 0.1610, Val Loss: 0.4082\n",
      "Epoch [37/50], Train Loss: 0.1611, Val Loss: 0.4075\n",
      "Epoch [38/50], Train Loss: 0.1607, Val Loss: 0.4068\n",
      "Epoch [39/50], Train Loss: 0.1599, Val Loss: 0.4061\n",
      "Epoch [40/50], Train Loss: 0.1602, Val Loss: 0.4055\n",
      "Epoch [41/50], Train Loss: 0.1594, Val Loss: 0.4048\n",
      "Epoch [42/50], Train Loss: 0.1589, Val Loss: 0.4042\n",
      "Epoch [43/50], Train Loss: 0.1585, Val Loss: 0.4035\n",
      "Epoch [44/50], Train Loss: 0.1580, Val Loss: 0.4028\n",
      "Epoch [45/50], Train Loss: 0.1585, Val Loss: 0.4022\n",
      "Epoch [46/50], Train Loss: 0.1573, Val Loss: 0.4015\n",
      "Epoch [47/50], Train Loss: 0.1565, Val Loss: 0.4009\n",
      "Epoch [48/50], Train Loss: 0.1572, Val Loss: 0.4002\n",
      "Epoch [49/50], Train Loss: 0.1562, Val Loss: 0.3996\n",
      "Epoch [50/50], Train Loss: 0.1559, Val Loss: 0.3989\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=16, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1396, Val Loss: 0.3552\n",
      "Epoch [2/50], Train Loss: 0.1310, Val Loss: 0.3365\n",
      "Epoch [3/50], Train Loss: 0.1229, Val Loss: 0.3180\n",
      "Epoch [4/50], Train Loss: 0.1150, Val Loss: 0.2994\n",
      "Epoch [5/50], Train Loss: 0.1070, Val Loss: 0.2802\n",
      "Epoch [6/50], Train Loss: 0.0988, Val Loss: 0.2602\n",
      "Epoch [7/50], Train Loss: 0.0905, Val Loss: 0.2395\n",
      "Epoch [8/50], Train Loss: 0.0821, Val Loss: 0.2181\n",
      "Epoch [9/50], Train Loss: 0.0737, Val Loss: 0.1960\n",
      "Epoch [10/50], Train Loss: 0.0653, Val Loss: 0.1736\n",
      "Epoch [11/50], Train Loss: 0.0572, Val Loss: 0.1517\n",
      "Epoch [12/50], Train Loss: 0.0497, Val Loss: 0.1310\n",
      "Epoch [13/50], Train Loss: 0.0432, Val Loss: 0.1125\n",
      "Epoch [14/50], Train Loss: 0.0378, Val Loss: 0.0968\n",
      "Epoch [15/50], Train Loss: 0.0336, Val Loss: 0.0843\n",
      "Epoch [16/50], Train Loss: 0.0305, Val Loss: 0.0745\n",
      "Epoch [17/50], Train Loss: 0.0283, Val Loss: 0.0669\n",
      "Epoch [18/50], Train Loss: 0.0266, Val Loss: 0.0610\n",
      "Epoch [19/50], Train Loss: 0.0252, Val Loss: 0.0561\n",
      "Epoch [20/50], Train Loss: 0.0240, Val Loss: 0.0520\n",
      "Epoch [21/50], Train Loss: 0.0230, Val Loss: 0.0484\n",
      "Epoch [22/50], Train Loss: 0.0221, Val Loss: 0.0450\n",
      "Epoch [23/50], Train Loss: 0.0213, Val Loss: 0.0420\n",
      "Epoch [24/50], Train Loss: 0.0206, Val Loss: 0.0391\n",
      "Epoch [25/50], Train Loss: 0.0199, Val Loss: 0.0364\n",
      "Epoch [26/50], Train Loss: 0.0193, Val Loss: 0.0338\n",
      "Epoch [27/50], Train Loss: 0.0187, Val Loss: 0.0313\n",
      "Epoch [28/50], Train Loss: 0.0181, Val Loss: 0.0290\n",
      "Epoch [29/50], Train Loss: 0.0175, Val Loss: 0.0268\n",
      "Epoch [30/50], Train Loss: 0.0170, Val Loss: 0.0247\n",
      "Epoch [31/50], Train Loss: 0.0164, Val Loss: 0.0227\n",
      "Epoch [32/50], Train Loss: 0.0159, Val Loss: 0.0209\n",
      "Epoch [33/50], Train Loss: 0.0154, Val Loss: 0.0192\n",
      "Epoch [34/50], Train Loss: 0.0149, Val Loss: 0.0176\n",
      "Epoch [35/50], Train Loss: 0.0144, Val Loss: 0.0162\n",
      "Epoch [36/50], Train Loss: 0.0140, Val Loss: 0.0149\n",
      "Epoch [37/50], Train Loss: 0.0135, Val Loss: 0.0138\n",
      "Epoch [38/50], Train Loss: 0.0130, Val Loss: 0.0129\n",
      "Epoch [39/50], Train Loss: 0.0126, Val Loss: 0.0121\n",
      "Epoch [40/50], Train Loss: 0.0121, Val Loss: 0.0114\n",
      "Epoch [41/50], Train Loss: 0.0117, Val Loss: 0.0109\n",
      "Epoch [42/50], Train Loss: 0.0113, Val Loss: 0.0105\n",
      "Epoch [43/50], Train Loss: 0.0108, Val Loss: 0.0102\n",
      "Epoch [44/50], Train Loss: 0.0104, Val Loss: 0.0099\n",
      "Epoch [45/50], Train Loss: 0.0100, Val Loss: 0.0098\n",
      "Epoch [46/50], Train Loss: 0.0096, Val Loss: 0.0097\n",
      "Epoch [47/50], Train Loss: 0.0092, Val Loss: 0.0096\n",
      "Epoch [48/50], Train Loss: 0.0088, Val Loss: 0.0096\n",
      "Epoch [49/50], Train Loss: 0.0084, Val Loss: 0.0096\n",
      "Epoch [50/50], Train Loss: 0.0080, Val Loss: 0.0097\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=16, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.2146, Val Loss: 0.5342\n",
      "Epoch [2/50], Train Loss: 0.2040, Val Loss: 0.5136\n",
      "Epoch [3/50], Train Loss: 0.1953, Val Loss: 0.4935\n",
      "Epoch [4/50], Train Loss: 0.1853, Val Loss: 0.4737\n",
      "Epoch [5/50], Train Loss: 0.1766, Val Loss: 0.4536\n",
      "Epoch [6/50], Train Loss: 0.1671, Val Loss: 0.4330\n",
      "Epoch [7/50], Train Loss: 0.1582, Val Loss: 0.4118\n",
      "Epoch [8/50], Train Loss: 0.1481, Val Loss: 0.3898\n",
      "Epoch [9/50], Train Loss: 0.1385, Val Loss: 0.3669\n",
      "Epoch [10/50], Train Loss: 0.1278, Val Loss: 0.3429\n",
      "Epoch [11/50], Train Loss: 0.1180, Val Loss: 0.3181\n",
      "Epoch [12/50], Train Loss: 0.1080, Val Loss: 0.2923\n",
      "Epoch [13/50], Train Loss: 0.0976, Val Loss: 0.2659\n",
      "Epoch [14/50], Train Loss: 0.0879, Val Loss: 0.2397\n",
      "Epoch [15/50], Train Loss: 0.0784, Val Loss: 0.2141\n",
      "Epoch [16/50], Train Loss: 0.0686, Val Loss: 0.1900\n",
      "Epoch [17/50], Train Loss: 0.0622, Val Loss: 0.1684\n",
      "Epoch [18/50], Train Loss: 0.0568, Val Loss: 0.1500\n",
      "Epoch [19/50], Train Loss: 0.0529, Val Loss: 0.1347\n",
      "Epoch [20/50], Train Loss: 0.0494, Val Loss: 0.1221\n",
      "Epoch [21/50], Train Loss: 0.0457, Val Loss: 0.1120\n",
      "Epoch [22/50], Train Loss: 0.0441, Val Loss: 0.1033\n",
      "Epoch [23/50], Train Loss: 0.0422, Val Loss: 0.0961\n",
      "Epoch [24/50], Train Loss: 0.0402, Val Loss: 0.0901\n",
      "Epoch [25/50], Train Loss: 0.0396, Val Loss: 0.0847\n",
      "Epoch [26/50], Train Loss: 0.0373, Val Loss: 0.0801\n",
      "Epoch [27/50], Train Loss: 0.0365, Val Loss: 0.0759\n",
      "Epoch [28/50], Train Loss: 0.0359, Val Loss: 0.0722\n",
      "Epoch [29/50], Train Loss: 0.0360, Val Loss: 0.0687\n",
      "Epoch [30/50], Train Loss: 0.0351, Val Loss: 0.0652\n",
      "Epoch [31/50], Train Loss: 0.0334, Val Loss: 0.0624\n",
      "Epoch [32/50], Train Loss: 0.0326, Val Loss: 0.0599\n",
      "Epoch [33/50], Train Loss: 0.0322, Val Loss: 0.0573\n",
      "Epoch [34/50], Train Loss: 0.0329, Val Loss: 0.0548\n",
      "Epoch [35/50], Train Loss: 0.0321, Val Loss: 0.0521\n",
      "Epoch [36/50], Train Loss: 0.0310, Val Loss: 0.0496\n",
      "Epoch [37/50], Train Loss: 0.0310, Val Loss: 0.0476\n",
      "Epoch [38/50], Train Loss: 0.0302, Val Loss: 0.0459\n",
      "Epoch [39/50], Train Loss: 0.0304, Val Loss: 0.0444\n",
      "Epoch [40/50], Train Loss: 0.0301, Val Loss: 0.0429\n",
      "Epoch [41/50], Train Loss: 0.0291, Val Loss: 0.0412\n",
      "Epoch [42/50], Train Loss: 0.0294, Val Loss: 0.0393\n",
      "Epoch [43/50], Train Loss: 0.0280, Val Loss: 0.0378\n",
      "Epoch [44/50], Train Loss: 0.0287, Val Loss: 0.0364\n",
      "Epoch [45/50], Train Loss: 0.0275, Val Loss: 0.0348\n",
      "Epoch [46/50], Train Loss: 0.0270, Val Loss: 0.0334\n",
      "Epoch [47/50], Train Loss: 0.0276, Val Loss: 0.0325\n",
      "Epoch [48/50], Train Loss: 0.0275, Val Loss: 0.0311\n",
      "Epoch [49/50], Train Loss: 0.0277, Val Loss: 0.0298\n",
      "Epoch [50/50], Train Loss: 0.0268, Val Loss: 0.0287\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=16, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.2173, Val Loss: 0.5446\n",
      "Epoch [2/50], Train Loss: 0.2090, Val Loss: 0.5281\n",
      "Epoch [3/50], Train Loss: 0.2013, Val Loss: 0.5127\n",
      "Epoch [4/50], Train Loss: 0.1945, Val Loss: 0.4977\n",
      "Epoch [5/50], Train Loss: 0.1882, Val Loss: 0.4829\n",
      "Epoch [6/50], Train Loss: 0.1800, Val Loss: 0.4680\n",
      "Epoch [7/50], Train Loss: 0.1732, Val Loss: 0.4528\n",
      "Epoch [8/50], Train Loss: 0.1669, Val Loss: 0.4367\n",
      "Epoch [9/50], Train Loss: 0.1587, Val Loss: 0.4199\n",
      "Epoch [10/50], Train Loss: 0.1514, Val Loss: 0.4020\n",
      "Epoch [11/50], Train Loss: 0.1427, Val Loss: 0.3825\n",
      "Epoch [12/50], Train Loss: 0.1351, Val Loss: 0.3618\n",
      "Epoch [13/50], Train Loss: 0.1264, Val Loss: 0.3397\n",
      "Epoch [14/50], Train Loss: 0.1179, Val Loss: 0.3158\n",
      "Epoch [15/50], Train Loss: 0.1097, Val Loss: 0.2906\n",
      "Epoch [16/50], Train Loss: 0.1018, Val Loss: 0.2644\n",
      "Epoch [17/50], Train Loss: 0.0903, Val Loss: 0.2377\n",
      "Epoch [18/50], Train Loss: 0.0831, Val Loss: 0.2110\n",
      "Epoch [19/50], Train Loss: 0.0780, Val Loss: 0.1856\n",
      "Epoch [20/50], Train Loss: 0.0738, Val Loss: 0.1638\n",
      "Epoch [21/50], Train Loss: 0.0677, Val Loss: 0.1453\n",
      "Epoch [22/50], Train Loss: 0.0649, Val Loss: 0.1296\n",
      "Epoch [23/50], Train Loss: 0.0657, Val Loss: 0.1200\n",
      "Epoch [24/50], Train Loss: 0.0611, Val Loss: 0.1135\n",
      "Epoch [25/50], Train Loss: 0.0590, Val Loss: 0.1064\n",
      "Epoch [26/50], Train Loss: 0.0579, Val Loss: 0.0999\n",
      "Epoch [27/50], Train Loss: 0.0570, Val Loss: 0.0945\n",
      "Epoch [28/50], Train Loss: 0.0586, Val Loss: 0.0901\n",
      "Epoch [29/50], Train Loss: 0.0561, Val Loss: 0.0866\n",
      "Epoch [30/50], Train Loss: 0.0547, Val Loss: 0.0845\n",
      "Epoch [31/50], Train Loss: 0.0554, Val Loss: 0.0820\n",
      "Epoch [32/50], Train Loss: 0.0528, Val Loss: 0.0777\n",
      "Epoch [33/50], Train Loss: 0.0503, Val Loss: 0.0749\n",
      "Epoch [34/50], Train Loss: 0.0487, Val Loss: 0.0722\n",
      "Epoch [35/50], Train Loss: 0.0511, Val Loss: 0.0700\n",
      "Epoch [36/50], Train Loss: 0.0515, Val Loss: 0.0674\n",
      "Epoch [37/50], Train Loss: 0.0509, Val Loss: 0.0637\n",
      "Epoch [38/50], Train Loss: 0.0483, Val Loss: 0.0614\n",
      "Epoch [39/50], Train Loss: 0.0494, Val Loss: 0.0590\n",
      "Epoch [40/50], Train Loss: 0.0475, Val Loss: 0.0565\n",
      "Epoch [41/50], Train Loss: 0.0449, Val Loss: 0.0539\n",
      "Epoch [42/50], Train Loss: 0.0461, Val Loss: 0.0507\n",
      "Epoch [43/50], Train Loss: 0.0430, Val Loss: 0.0487\n",
      "Epoch [44/50], Train Loss: 0.0456, Val Loss: 0.0469\n",
      "Epoch [45/50], Train Loss: 0.0436, Val Loss: 0.0434\n",
      "Epoch [46/50], Train Loss: 0.0431, Val Loss: 0.0411\n",
      "Epoch [47/50], Train Loss: 0.0420, Val Loss: 0.0386\n",
      "Epoch [48/50], Train Loss: 0.0414, Val Loss: 0.0343\n",
      "Epoch [49/50], Train Loss: 0.0408, Val Loss: 0.0330\n",
      "Epoch [50/50], Train Loss: 0.0392, Val Loss: 0.0319\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=16, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1290, Val Loss: 0.2852\n",
      "Epoch [2/50], Train Loss: 0.1210, Val Loss: 0.2731\n",
      "Epoch [3/50], Train Loss: 0.1142, Val Loss: 0.2618\n",
      "Epoch [4/50], Train Loss: 0.1080, Val Loss: 0.2511\n",
      "Epoch [5/50], Train Loss: 0.1022, Val Loss: 0.2407\n",
      "Epoch [6/50], Train Loss: 0.0967, Val Loss: 0.2303\n",
      "Epoch [7/50], Train Loss: 0.0914, Val Loss: 0.2197\n",
      "Epoch [8/50], Train Loss: 0.0861, Val Loss: 0.2088\n",
      "Epoch [9/50], Train Loss: 0.0810, Val Loss: 0.1975\n",
      "Epoch [10/50], Train Loss: 0.0757, Val Loss: 0.1854\n",
      "Epoch [11/50], Train Loss: 0.0704, Val Loss: 0.1725\n",
      "Epoch [12/50], Train Loss: 0.0649, Val Loss: 0.1587\n",
      "Epoch [13/50], Train Loss: 0.0594, Val Loss: 0.1439\n",
      "Epoch [14/50], Train Loss: 0.0538, Val Loss: 0.1285\n",
      "Epoch [15/50], Train Loss: 0.0485, Val Loss: 0.1129\n",
      "Epoch [16/50], Train Loss: 0.0437, Val Loss: 0.0982\n",
      "Epoch [17/50], Train Loss: 0.0398, Val Loss: 0.0855\n",
      "Epoch [18/50], Train Loss: 0.0369, Val Loss: 0.0758\n",
      "Epoch [19/50], Train Loss: 0.0349, Val Loss: 0.0693\n",
      "Epoch [20/50], Train Loss: 0.0336, Val Loss: 0.0654\n",
      "Epoch [21/50], Train Loss: 0.0327, Val Loss: 0.0632\n",
      "Epoch [22/50], Train Loss: 0.0319, Val Loss: 0.0618\n",
      "Epoch [23/50], Train Loss: 0.0313, Val Loss: 0.0607\n",
      "Epoch [24/50], Train Loss: 0.0307, Val Loss: 0.0597\n",
      "Epoch [25/50], Train Loss: 0.0301, Val Loss: 0.0586\n",
      "Epoch [26/50], Train Loss: 0.0296, Val Loss: 0.0574\n",
      "Epoch [27/50], Train Loss: 0.0291, Val Loss: 0.0560\n",
      "Epoch [28/50], Train Loss: 0.0286, Val Loss: 0.0545\n",
      "Epoch [29/50], Train Loss: 0.0281, Val Loss: 0.0530\n",
      "Epoch [30/50], Train Loss: 0.0276, Val Loss: 0.0514\n",
      "Epoch [31/50], Train Loss: 0.0271, Val Loss: 0.0497\n",
      "Epoch [32/50], Train Loss: 0.0266, Val Loss: 0.0480\n",
      "Epoch [33/50], Train Loss: 0.0260, Val Loss: 0.0463\n",
      "Epoch [34/50], Train Loss: 0.0255, Val Loss: 0.0446\n",
      "Epoch [35/50], Train Loss: 0.0249, Val Loss: 0.0429\n",
      "Epoch [36/50], Train Loss: 0.0244, Val Loss: 0.0413\n",
      "Epoch [37/50], Train Loss: 0.0238, Val Loss: 0.0397\n",
      "Epoch [38/50], Train Loss: 0.0232, Val Loss: 0.0381\n",
      "Epoch [39/50], Train Loss: 0.0226, Val Loss: 0.0366\n",
      "Epoch [40/50], Train Loss: 0.0220, Val Loss: 0.0352\n",
      "Epoch [41/50], Train Loss: 0.0213, Val Loss: 0.0339\n",
      "Epoch [42/50], Train Loss: 0.0207, Val Loss: 0.0327\n",
      "Epoch [43/50], Train Loss: 0.0200, Val Loss: 0.0316\n",
      "Epoch [44/50], Train Loss: 0.0193, Val Loss: 0.0307\n",
      "Epoch [45/50], Train Loss: 0.0186, Val Loss: 0.0299\n",
      "Epoch [46/50], Train Loss: 0.0179, Val Loss: 0.0293\n",
      "Epoch [47/50], Train Loss: 0.0171, Val Loss: 0.0289\n",
      "Epoch [48/50], Train Loss: 0.0164, Val Loss: 0.0286\n",
      "Epoch [49/50], Train Loss: 0.0156, Val Loss: 0.0285\n",
      "Epoch [50/50], Train Loss: 0.0149, Val Loss: 0.0284\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=16, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1315, Val Loss: 0.3166\n",
      "Epoch [2/50], Train Loss: 0.1216, Val Loss: 0.2989\n",
      "Epoch [3/50], Train Loss: 0.1124, Val Loss: 0.2814\n",
      "Epoch [4/50], Train Loss: 0.1035, Val Loss: 0.2635\n",
      "Epoch [5/50], Train Loss: 0.0936, Val Loss: 0.2448\n",
      "Epoch [6/50], Train Loss: 0.0850, Val Loss: 0.2255\n",
      "Epoch [7/50], Train Loss: 0.0776, Val Loss: 0.2059\n",
      "Epoch [8/50], Train Loss: 0.0683, Val Loss: 0.1864\n",
      "Epoch [9/50], Train Loss: 0.0611, Val Loss: 0.1678\n",
      "Epoch [10/50], Train Loss: 0.0556, Val Loss: 0.1509\n",
      "Epoch [11/50], Train Loss: 0.0513, Val Loss: 0.1361\n",
      "Epoch [12/50], Train Loss: 0.0483, Val Loss: 0.1244\n",
      "Epoch [13/50], Train Loss: 0.0464, Val Loss: 0.1160\n",
      "Epoch [14/50], Train Loss: 0.0460, Val Loss: 0.1101\n",
      "Epoch [15/50], Train Loss: 0.0443, Val Loss: 0.1059\n",
      "Epoch [16/50], Train Loss: 0.0439, Val Loss: 0.1032\n",
      "Epoch [17/50], Train Loss: 0.0440, Val Loss: 0.1013\n",
      "Epoch [18/50], Train Loss: 0.0433, Val Loss: 0.0993\n",
      "Epoch [19/50], Train Loss: 0.0430, Val Loss: 0.0974\n",
      "Epoch [20/50], Train Loss: 0.0431, Val Loss: 0.0953\n",
      "Epoch [21/50], Train Loss: 0.0433, Val Loss: 0.0937\n",
      "Epoch [22/50], Train Loss: 0.0418, Val Loss: 0.0922\n",
      "Epoch [23/50], Train Loss: 0.0415, Val Loss: 0.0905\n",
      "Epoch [24/50], Train Loss: 0.0411, Val Loss: 0.0887\n",
      "Epoch [25/50], Train Loss: 0.0407, Val Loss: 0.0872\n",
      "Epoch [26/50], Train Loss: 0.0419, Val Loss: 0.0854\n",
      "Epoch [27/50], Train Loss: 0.0393, Val Loss: 0.0828\n",
      "Epoch [28/50], Train Loss: 0.0392, Val Loss: 0.0804\n",
      "Epoch [29/50], Train Loss: 0.0388, Val Loss: 0.0785\n",
      "Epoch [30/50], Train Loss: 0.0379, Val Loss: 0.0762\n",
      "Epoch [31/50], Train Loss: 0.0374, Val Loss: 0.0738\n",
      "Epoch [32/50], Train Loss: 0.0375, Val Loss: 0.0713\n",
      "Epoch [33/50], Train Loss: 0.0351, Val Loss: 0.0684\n",
      "Epoch [34/50], Train Loss: 0.0348, Val Loss: 0.0649\n",
      "Epoch [35/50], Train Loss: 0.0354, Val Loss: 0.0617\n",
      "Epoch [36/50], Train Loss: 0.0345, Val Loss: 0.0579\n",
      "Epoch [37/50], Train Loss: 0.0327, Val Loss: 0.0539\n",
      "Epoch [38/50], Train Loss: 0.0325, Val Loss: 0.0505\n",
      "Epoch [39/50], Train Loss: 0.0326, Val Loss: 0.0467\n",
      "Epoch [40/50], Train Loss: 0.0315, Val Loss: 0.0430\n",
      "Epoch [41/50], Train Loss: 0.0311, Val Loss: 0.0392\n",
      "Epoch [42/50], Train Loss: 0.0304, Val Loss: 0.0361\n",
      "Epoch [43/50], Train Loss: 0.0281, Val Loss: 0.0323\n",
      "Epoch [44/50], Train Loss: 0.0292, Val Loss: 0.0289\n",
      "Epoch [45/50], Train Loss: 0.0283, Val Loss: 0.0270\n",
      "Epoch [46/50], Train Loss: 0.0272, Val Loss: 0.0247\n",
      "Epoch [47/50], Train Loss: 0.0270, Val Loss: 0.0235\n",
      "Epoch [48/50], Train Loss: 0.0258, Val Loss: 0.0217\n",
      "Epoch [49/50], Train Loss: 0.0263, Val Loss: 0.0202\n",
      "Epoch [50/50], Train Loss: 0.0252, Val Loss: 0.0196\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=16, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0813, Val Loss: 0.2580\n",
      "Epoch [2/50], Train Loss: 0.0781, Val Loss: 0.2487\n",
      "Epoch [3/50], Train Loss: 0.0742, Val Loss: 0.2403\n",
      "Epoch [4/50], Train Loss: 0.0703, Val Loss: 0.2324\n",
      "Epoch [5/50], Train Loss: 0.0673, Val Loss: 0.2249\n",
      "Epoch [6/50], Train Loss: 0.0640, Val Loss: 0.2174\n",
      "Epoch [7/50], Train Loss: 0.0625, Val Loss: 0.2102\n",
      "Epoch [8/50], Train Loss: 0.0595, Val Loss: 0.2029\n",
      "Epoch [9/50], Train Loss: 0.0564, Val Loss: 0.1959\n",
      "Epoch [10/50], Train Loss: 0.0561, Val Loss: 0.1890\n",
      "Epoch [11/50], Train Loss: 0.0549, Val Loss: 0.1822\n",
      "Epoch [12/50], Train Loss: 0.0520, Val Loss: 0.1754\n",
      "Epoch [13/50], Train Loss: 0.0503, Val Loss: 0.1689\n",
      "Epoch [14/50], Train Loss: 0.0491, Val Loss: 0.1627\n",
      "Epoch [15/50], Train Loss: 0.0477, Val Loss: 0.1567\n",
      "Epoch [16/50], Train Loss: 0.0480, Val Loss: 0.1510\n",
      "Epoch [17/50], Train Loss: 0.0455, Val Loss: 0.1453\n",
      "Epoch [18/50], Train Loss: 0.0458, Val Loss: 0.1401\n",
      "Epoch [19/50], Train Loss: 0.0450, Val Loss: 0.1353\n",
      "Epoch [20/50], Train Loss: 0.0436, Val Loss: 0.1304\n",
      "Epoch [21/50], Train Loss: 0.0432, Val Loss: 0.1263\n",
      "Epoch [22/50], Train Loss: 0.0428, Val Loss: 0.1223\n",
      "Epoch [23/50], Train Loss: 0.0413, Val Loss: 0.1188\n",
      "Epoch [24/50], Train Loss: 0.0418, Val Loss: 0.1157\n",
      "Epoch [25/50], Train Loss: 0.0410, Val Loss: 0.1125\n",
      "Epoch [26/50], Train Loss: 0.0404, Val Loss: 0.1090\n",
      "Epoch [27/50], Train Loss: 0.0396, Val Loss: 0.1058\n",
      "Epoch [28/50], Train Loss: 0.0398, Val Loss: 0.1025\n",
      "Epoch [29/50], Train Loss: 0.0388, Val Loss: 0.0997\n",
      "Epoch [30/50], Train Loss: 0.0384, Val Loss: 0.0967\n",
      "Epoch [31/50], Train Loss: 0.0384, Val Loss: 0.0942\n",
      "Epoch [32/50], Train Loss: 0.0382, Val Loss: 0.0913\n",
      "Epoch [33/50], Train Loss: 0.0386, Val Loss: 0.0891\n",
      "Epoch [34/50], Train Loss: 0.0377, Val Loss: 0.0864\n",
      "Epoch [35/50], Train Loss: 0.0351, Val Loss: 0.0830\n",
      "Epoch [36/50], Train Loss: 0.0350, Val Loss: 0.0802\n",
      "Epoch [37/50], Train Loss: 0.0353, Val Loss: 0.0769\n",
      "Epoch [38/50], Train Loss: 0.0340, Val Loss: 0.0731\n",
      "Epoch [39/50], Train Loss: 0.0337, Val Loss: 0.0695\n",
      "Epoch [40/50], Train Loss: 0.0324, Val Loss: 0.0654\n",
      "Epoch [41/50], Train Loss: 0.0316, Val Loss: 0.0620\n",
      "Epoch [42/50], Train Loss: 0.0313, Val Loss: 0.0591\n",
      "Epoch [43/50], Train Loss: 0.0284, Val Loss: 0.0570\n",
      "Epoch [44/50], Train Loss: 0.0277, Val Loss: 0.0558\n",
      "Epoch [45/50], Train Loss: 0.0275, Val Loss: 0.0543\n",
      "Epoch [46/50], Train Loss: 0.0257, Val Loss: 0.0531\n",
      "Epoch [47/50], Train Loss: 0.0249, Val Loss: 0.0517\n",
      "Epoch [48/50], Train Loss: 0.0259, Val Loss: 0.0501\n",
      "Epoch [49/50], Train Loss: 0.0249, Val Loss: 0.0485\n",
      "Epoch [50/50], Train Loss: 0.0251, Val Loss: 0.0483\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=16, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0702, Val Loss: 0.2076\n",
      "Epoch [2/50], Train Loss: 0.0651, Val Loss: 0.1977\n",
      "Epoch [3/50], Train Loss: 0.0607, Val Loss: 0.1883\n",
      "Epoch [4/50], Train Loss: 0.0567, Val Loss: 0.1792\n",
      "Epoch [5/50], Train Loss: 0.0529, Val Loss: 0.1701\n",
      "Epoch [6/50], Train Loss: 0.0494, Val Loss: 0.1610\n",
      "Epoch [7/50], Train Loss: 0.0463, Val Loss: 0.1521\n",
      "Epoch [8/50], Train Loss: 0.0435, Val Loss: 0.1434\n",
      "Epoch [9/50], Train Loss: 0.0411, Val Loss: 0.1351\n",
      "Epoch [10/50], Train Loss: 0.0393, Val Loss: 0.1275\n",
      "Epoch [11/50], Train Loss: 0.0380, Val Loss: 0.1210\n",
      "Epoch [12/50], Train Loss: 0.0371, Val Loss: 0.1155\n",
      "Epoch [13/50], Train Loss: 0.0365, Val Loss: 0.1110\n",
      "Epoch [14/50], Train Loss: 0.0361, Val Loss: 0.1075\n",
      "Epoch [15/50], Train Loss: 0.0358, Val Loss: 0.1046\n",
      "Epoch [16/50], Train Loss: 0.0356, Val Loss: 0.1021\n",
      "Epoch [17/50], Train Loss: 0.0353, Val Loss: 0.0999\n",
      "Epoch [18/50], Train Loss: 0.0350, Val Loss: 0.0979\n",
      "Epoch [19/50], Train Loss: 0.0347, Val Loss: 0.0958\n",
      "Epoch [20/50], Train Loss: 0.0344, Val Loss: 0.0937\n",
      "Epoch [21/50], Train Loss: 0.0340, Val Loss: 0.0915\n",
      "Epoch [22/50], Train Loss: 0.0336, Val Loss: 0.0892\n",
      "Epoch [23/50], Train Loss: 0.0330, Val Loss: 0.0867\n",
      "Epoch [24/50], Train Loss: 0.0324, Val Loss: 0.0840\n",
      "Epoch [25/50], Train Loss: 0.0317, Val Loss: 0.0813\n",
      "Epoch [26/50], Train Loss: 0.0309, Val Loss: 0.0785\n",
      "Epoch [27/50], Train Loss: 0.0299, Val Loss: 0.0757\n",
      "Epoch [28/50], Train Loss: 0.0287, Val Loss: 0.0732\n",
      "Epoch [29/50], Train Loss: 0.0274, Val Loss: 0.0712\n",
      "Epoch [30/50], Train Loss: 0.0260, Val Loss: 0.0696\n",
      "Epoch [31/50], Train Loss: 0.0245, Val Loss: 0.0684\n",
      "Epoch [32/50], Train Loss: 0.0231, Val Loss: 0.0668\n",
      "Epoch [33/50], Train Loss: 0.0218, Val Loss: 0.0647\n",
      "Epoch [34/50], Train Loss: 0.0206, Val Loss: 0.0620\n",
      "Epoch [35/50], Train Loss: 0.0193, Val Loss: 0.0588\n",
      "Epoch [36/50], Train Loss: 0.0181, Val Loss: 0.0556\n",
      "Epoch [37/50], Train Loss: 0.0169, Val Loss: 0.0524\n",
      "Epoch [38/50], Train Loss: 0.0157, Val Loss: 0.0493\n",
      "Epoch [39/50], Train Loss: 0.0146, Val Loss: 0.0466\n",
      "Epoch [40/50], Train Loss: 0.0135, Val Loss: 0.0443\n",
      "Epoch [41/50], Train Loss: 0.0125, Val Loss: 0.0426\n",
      "Epoch [42/50], Train Loss: 0.0116, Val Loss: 0.0416\n",
      "Epoch [43/50], Train Loss: 0.0107, Val Loss: 0.0411\n",
      "Epoch [44/50], Train Loss: 0.0099, Val Loss: 0.0410\n",
      "Epoch [45/50], Train Loss: 0.0092, Val Loss: 0.0412\n",
      "Epoch [46/50], Train Loss: 0.0086, Val Loss: 0.0415\n",
      "Epoch [47/50], Train Loss: 0.0080, Val Loss: 0.0418\n",
      "Epoch [48/50], Train Loss: 0.0076, Val Loss: 0.0421\n",
      "Epoch [49/50], Train Loss: 0.0071, Val Loss: 0.0424\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=16, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1541, Val Loss: 0.3902\n",
      "Epoch [2/50], Train Loss: 0.1437, Val Loss: 0.3759\n",
      "Epoch [3/50], Train Loss: 0.1359, Val Loss: 0.3622\n",
      "Epoch [4/50], Train Loss: 0.1269, Val Loss: 0.3487\n",
      "Epoch [5/50], Train Loss: 0.1197, Val Loss: 0.3347\n",
      "Epoch [6/50], Train Loss: 0.1119, Val Loss: 0.3201\n",
      "Epoch [7/50], Train Loss: 0.1031, Val Loss: 0.3041\n",
      "Epoch [8/50], Train Loss: 0.0946, Val Loss: 0.2864\n",
      "Epoch [9/50], Train Loss: 0.0852, Val Loss: 0.2661\n",
      "Epoch [10/50], Train Loss: 0.0759, Val Loss: 0.2426\n",
      "Epoch [11/50], Train Loss: 0.0671, Val Loss: 0.2168\n",
      "Epoch [12/50], Train Loss: 0.0593, Val Loss: 0.1913\n",
      "Epoch [13/50], Train Loss: 0.0535, Val Loss: 0.1707\n",
      "Epoch [14/50], Train Loss: 0.0509, Val Loss: 0.1562\n",
      "Epoch [15/50], Train Loss: 0.0489, Val Loss: 0.1460\n",
      "Epoch [16/50], Train Loss: 0.0469, Val Loss: 0.1389\n",
      "Epoch [17/50], Train Loss: 0.0466, Val Loss: 0.1329\n",
      "Epoch [18/50], Train Loss: 0.0450, Val Loss: 0.1280\n",
      "Epoch [19/50], Train Loss: 0.0454, Val Loss: 0.1242\n",
      "Epoch [20/50], Train Loss: 0.0439, Val Loss: 0.1209\n",
      "Epoch [21/50], Train Loss: 0.0444, Val Loss: 0.1176\n",
      "Epoch [22/50], Train Loss: 0.0435, Val Loss: 0.1144\n",
      "Epoch [23/50], Train Loss: 0.0421, Val Loss: 0.1118\n",
      "Epoch [24/50], Train Loss: 0.0432, Val Loss: 0.1090\n",
      "Epoch [25/50], Train Loss: 0.0411, Val Loss: 0.1064\n",
      "Epoch [26/50], Train Loss: 0.0416, Val Loss: 0.1042\n",
      "Epoch [27/50], Train Loss: 0.0405, Val Loss: 0.1015\n",
      "Epoch [28/50], Train Loss: 0.0398, Val Loss: 0.0985\n",
      "Epoch [29/50], Train Loss: 0.0389, Val Loss: 0.0952\n",
      "Epoch [30/50], Train Loss: 0.0410, Val Loss: 0.0930\n",
      "Epoch [31/50], Train Loss: 0.0379, Val Loss: 0.0904\n",
      "Epoch [32/50], Train Loss: 0.0368, Val Loss: 0.0871\n",
      "Epoch [33/50], Train Loss: 0.0359, Val Loss: 0.0837\n",
      "Epoch [34/50], Train Loss: 0.0359, Val Loss: 0.0808\n",
      "Epoch [35/50], Train Loss: 0.0341, Val Loss: 0.0779\n",
      "Epoch [36/50], Train Loss: 0.0340, Val Loss: 0.0743\n",
      "Epoch [37/50], Train Loss: 0.0339, Val Loss: 0.0709\n",
      "Epoch [38/50], Train Loss: 0.0334, Val Loss: 0.0679\n",
      "Epoch [39/50], Train Loss: 0.0316, Val Loss: 0.0653\n",
      "Epoch [40/50], Train Loss: 0.0323, Val Loss: 0.0625\n",
      "Epoch [41/50], Train Loss: 0.0315, Val Loss: 0.0597\n",
      "Epoch [42/50], Train Loss: 0.0297, Val Loss: 0.0577\n",
      "Epoch [43/50], Train Loss: 0.0304, Val Loss: 0.0552\n",
      "Epoch [44/50], Train Loss: 0.0298, Val Loss: 0.0535\n",
      "Epoch [45/50], Train Loss: 0.0288, Val Loss: 0.0517\n",
      "Epoch [46/50], Train Loss: 0.0295, Val Loss: 0.0503\n",
      "Epoch [47/50], Train Loss: 0.0292, Val Loss: 0.0489\n",
      "Epoch [48/50], Train Loss: 0.0290, Val Loss: 0.0477\n",
      "Epoch [49/50], Train Loss: 0.0285, Val Loss: 0.0465\n",
      "Epoch [50/50], Train Loss: 0.0281, Val Loss: 0.0455\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=16, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1373, Val Loss: 0.3594\n",
      "Epoch [2/50], Train Loss: 0.1281, Val Loss: 0.3433\n",
      "Epoch [3/50], Train Loss: 0.1195, Val Loss: 0.3275\n",
      "Epoch [4/50], Train Loss: 0.1133, Val Loss: 0.3114\n",
      "Epoch [5/50], Train Loss: 0.1056, Val Loss: 0.2948\n",
      "Epoch [6/50], Train Loss: 0.1000, Val Loss: 0.2773\n",
      "Epoch [7/50], Train Loss: 0.0921, Val Loss: 0.2587\n",
      "Epoch [8/50], Train Loss: 0.0869, Val Loss: 0.2387\n",
      "Epoch [9/50], Train Loss: 0.0780, Val Loss: 0.2175\n",
      "Epoch [10/50], Train Loss: 0.0725, Val Loss: 0.1954\n",
      "Epoch [11/50], Train Loss: 0.0668, Val Loss: 0.1763\n",
      "Epoch [12/50], Train Loss: 0.0648, Val Loss: 0.1611\n",
      "Epoch [13/50], Train Loss: 0.0622, Val Loss: 0.1491\n",
      "Epoch [14/50], Train Loss: 0.0593, Val Loss: 0.1399\n",
      "Epoch [15/50], Train Loss: 0.0577, Val Loss: 0.1351\n",
      "Epoch [16/50], Train Loss: 0.0556, Val Loss: 0.1295\n",
      "Epoch [17/50], Train Loss: 0.0543, Val Loss: 0.1260\n",
      "Epoch [18/50], Train Loss: 0.0548, Val Loss: 0.1234\n",
      "Epoch [19/50], Train Loss: 0.0566, Val Loss: 0.1225\n",
      "Epoch [20/50], Train Loss: 0.0558, Val Loss: 0.1213\n",
      "Epoch [21/50], Train Loss: 0.0545, Val Loss: 0.1193\n",
      "Epoch [22/50], Train Loss: 0.0512, Val Loss: 0.1161\n",
      "Epoch [23/50], Train Loss: 0.0522, Val Loss: 0.1146\n",
      "Epoch [24/50], Train Loss: 0.0503, Val Loss: 0.1131\n",
      "Epoch [25/50], Train Loss: 0.0499, Val Loss: 0.1106\n",
      "Epoch [26/50], Train Loss: 0.0506, Val Loss: 0.1084\n",
      "Epoch [27/50], Train Loss: 0.0498, Val Loss: 0.1069\n",
      "Epoch [28/50], Train Loss: 0.0488, Val Loss: 0.1037\n",
      "Epoch [29/50], Train Loss: 0.0506, Val Loss: 0.1022\n",
      "Epoch [30/50], Train Loss: 0.0481, Val Loss: 0.0998\n",
      "Epoch [31/50], Train Loss: 0.0470, Val Loss: 0.0977\n",
      "Epoch [32/50], Train Loss: 0.0465, Val Loss: 0.0967\n",
      "Epoch [33/50], Train Loss: 0.0469, Val Loss: 0.0936\n",
      "Epoch [34/50], Train Loss: 0.0469, Val Loss: 0.0914\n",
      "Epoch [35/50], Train Loss: 0.0456, Val Loss: 0.0874\n",
      "Epoch [36/50], Train Loss: 0.0451, Val Loss: 0.0838\n",
      "Epoch [37/50], Train Loss: 0.0440, Val Loss: 0.0810\n",
      "Epoch [38/50], Train Loss: 0.0447, Val Loss: 0.0793\n",
      "Epoch [39/50], Train Loss: 0.0430, Val Loss: 0.0753\n",
      "Epoch [40/50], Train Loss: 0.0414, Val Loss: 0.0705\n",
      "Epoch [41/50], Train Loss: 0.0422, Val Loss: 0.0687\n",
      "Epoch [42/50], Train Loss: 0.0413, Val Loss: 0.0649\n",
      "Epoch [43/50], Train Loss: 0.0409, Val Loss: 0.0612\n",
      "Epoch [44/50], Train Loss: 0.0396, Val Loss: 0.0585\n",
      "Epoch [45/50], Train Loss: 0.0399, Val Loss: 0.0546\n",
      "Epoch [46/50], Train Loss: 0.0377, Val Loss: 0.0515\n",
      "Epoch [47/50], Train Loss: 0.0382, Val Loss: 0.0482\n",
      "Epoch [48/50], Train Loss: 0.0365, Val Loss: 0.0460\n",
      "Epoch [49/50], Train Loss: 0.0357, Val Loss: 0.0439\n",
      "Epoch [50/50], Train Loss: 0.0349, Val Loss: 0.0429\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=32, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1307, Val Loss: 0.3291\n",
      "Epoch [2/50], Train Loss: 0.1207, Val Loss: 0.3091\n",
      "Epoch [3/50], Train Loss: 0.1117, Val Loss: 0.2894\n",
      "Epoch [4/50], Train Loss: 0.1028, Val Loss: 0.2692\n",
      "Epoch [5/50], Train Loss: 0.0939, Val Loss: 0.2478\n",
      "Epoch [6/50], Train Loss: 0.0846, Val Loss: 0.2244\n",
      "Epoch [7/50], Train Loss: 0.0749, Val Loss: 0.1984\n",
      "Epoch [8/50], Train Loss: 0.0646, Val Loss: 0.1695\n",
      "Epoch [9/50], Train Loss: 0.0540, Val Loss: 0.1378\n",
      "Epoch [10/50], Train Loss: 0.0438, Val Loss: 0.1059\n",
      "Epoch [11/50], Train Loss: 0.0356, Val Loss: 0.0793\n",
      "Epoch [12/50], Train Loss: 0.0307, Val Loss: 0.0631\n",
      "Epoch [13/50], Train Loss: 0.0284, Val Loss: 0.0556\n",
      "Epoch [14/50], Train Loss: 0.0269, Val Loss: 0.0517\n",
      "Epoch [15/50], Train Loss: 0.0257, Val Loss: 0.0490\n",
      "Epoch [16/50], Train Loss: 0.0248, Val Loss: 0.0467\n",
      "Epoch [17/50], Train Loss: 0.0240, Val Loss: 0.0447\n",
      "Epoch [18/50], Train Loss: 0.0234, Val Loss: 0.0427\n",
      "Epoch [19/50], Train Loss: 0.0227, Val Loss: 0.0408\n",
      "Epoch [20/50], Train Loss: 0.0221, Val Loss: 0.0390\n",
      "Epoch [21/50], Train Loss: 0.0215, Val Loss: 0.0372\n",
      "Epoch [22/50], Train Loss: 0.0210, Val Loss: 0.0354\n",
      "Epoch [23/50], Train Loss: 0.0204, Val Loss: 0.0337\n",
      "Epoch [24/50], Train Loss: 0.0199, Val Loss: 0.0320\n",
      "Epoch [25/50], Train Loss: 0.0193, Val Loss: 0.0304\n",
      "Epoch [26/50], Train Loss: 0.0188, Val Loss: 0.0288\n",
      "Epoch [27/50], Train Loss: 0.0183, Val Loss: 0.0273\n",
      "Epoch [28/50], Train Loss: 0.0177, Val Loss: 0.0259\n",
      "Epoch [29/50], Train Loss: 0.0172, Val Loss: 0.0245\n",
      "Epoch [30/50], Train Loss: 0.0166, Val Loss: 0.0231\n",
      "Epoch [31/50], Train Loss: 0.0161, Val Loss: 0.0217\n",
      "Epoch [32/50], Train Loss: 0.0155, Val Loss: 0.0204\n",
      "Epoch [33/50], Train Loss: 0.0149, Val Loss: 0.0191\n",
      "Epoch [34/50], Train Loss: 0.0143, Val Loss: 0.0178\n",
      "Epoch [35/50], Train Loss: 0.0137, Val Loss: 0.0165\n",
      "Epoch [36/50], Train Loss: 0.0130, Val Loss: 0.0152\n",
      "Epoch [37/50], Train Loss: 0.0122, Val Loss: 0.0139\n",
      "Epoch [38/50], Train Loss: 0.0115, Val Loss: 0.0128\n",
      "Epoch [39/50], Train Loss: 0.0106, Val Loss: 0.0119\n",
      "Epoch [40/50], Train Loss: 0.0097, Val Loss: 0.0114\n",
      "Epoch [41/50], Train Loss: 0.0087, Val Loss: 0.0115\n",
      "Epoch [42/50], Train Loss: 0.0077, Val Loss: 0.0125\n",
      "Epoch [43/50], Train Loss: 0.0068, Val Loss: 0.0143\n",
      "Epoch [44/50], Train Loss: 0.0060, Val Loss: 0.0167\n",
      "Epoch [45/50], Train Loss: 0.0054, Val Loss: 0.0190\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=32, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1857, Val Loss: 0.4391\n",
      "Epoch [2/50], Train Loss: 0.1724, Val Loss: 0.4136\n",
      "Epoch [3/50], Train Loss: 0.1601, Val Loss: 0.3888\n",
      "Epoch [4/50], Train Loss: 0.1478, Val Loss: 0.3632\n",
      "Epoch [5/50], Train Loss: 0.1354, Val Loss: 0.3360\n",
      "Epoch [6/50], Train Loss: 0.1231, Val Loss: 0.3059\n",
      "Epoch [7/50], Train Loss: 0.1089, Val Loss: 0.2721\n",
      "Epoch [8/50], Train Loss: 0.0935, Val Loss: 0.2327\n",
      "Epoch [9/50], Train Loss: 0.0775, Val Loss: 0.1878\n",
      "Epoch [10/50], Train Loss: 0.0605, Val Loss: 0.1393\n",
      "Epoch [11/50], Train Loss: 0.0479, Val Loss: 0.0957\n",
      "Epoch [12/50], Train Loss: 0.0386, Val Loss: 0.0681\n",
      "Epoch [13/50], Train Loss: 0.0360, Val Loss: 0.0563\n",
      "Epoch [14/50], Train Loss: 0.0352, Val Loss: 0.0507\n",
      "Epoch [15/50], Train Loss: 0.0337, Val Loss: 0.0468\n",
      "Epoch [16/50], Train Loss: 0.0329, Val Loss: 0.0436\n",
      "Epoch [17/50], Train Loss: 0.0325, Val Loss: 0.0421\n",
      "Epoch [18/50], Train Loss: 0.0316, Val Loss: 0.0397\n",
      "Epoch [19/50], Train Loss: 0.0316, Val Loss: 0.0378\n",
      "Epoch [20/50], Train Loss: 0.0302, Val Loss: 0.0359\n",
      "Epoch [21/50], Train Loss: 0.0291, Val Loss: 0.0340\n",
      "Epoch [22/50], Train Loss: 0.0297, Val Loss: 0.0321\n",
      "Epoch [23/50], Train Loss: 0.0286, Val Loss: 0.0308\n",
      "Epoch [24/50], Train Loss: 0.0273, Val Loss: 0.0284\n",
      "Epoch [25/50], Train Loss: 0.0270, Val Loss: 0.0255\n",
      "Epoch [26/50], Train Loss: 0.0264, Val Loss: 0.0240\n",
      "Epoch [27/50], Train Loss: 0.0267, Val Loss: 0.0216\n",
      "Epoch [28/50], Train Loss: 0.0248, Val Loss: 0.0201\n",
      "Epoch [29/50], Train Loss: 0.0246, Val Loss: 0.0179\n",
      "Epoch [30/50], Train Loss: 0.0243, Val Loss: 0.0160\n",
      "Epoch [31/50], Train Loss: 0.0233, Val Loss: 0.0144\n",
      "Epoch [32/50], Train Loss: 0.0232, Val Loss: 0.0127\n",
      "Epoch [33/50], Train Loss: 0.0225, Val Loss: 0.0111\n",
      "Epoch [34/50], Train Loss: 0.0218, Val Loss: 0.0092\n",
      "Epoch [35/50], Train Loss: 0.0211, Val Loss: 0.0076\n",
      "Epoch [36/50], Train Loss: 0.0202, Val Loss: 0.0067\n",
      "Epoch [37/50], Train Loss: 0.0191, Val Loss: 0.0063\n",
      "Epoch [38/50], Train Loss: 0.0184, Val Loss: 0.0058\n",
      "Epoch [39/50], Train Loss: 0.0181, Val Loss: 0.0068\n",
      "Epoch [40/50], Train Loss: 0.0171, Val Loss: 0.0077\n",
      "Epoch [41/50], Train Loss: 0.0154, Val Loss: 0.0099\n",
      "Epoch [42/50], Train Loss: 0.0144, Val Loss: 0.0121\n",
      "Epoch [43/50], Train Loss: 0.0139, Val Loss: 0.0145\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=32, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1452, Val Loss: 0.3781\n",
      "Epoch [2/50], Train Loss: 0.1356, Val Loss: 0.3589\n",
      "Epoch [3/50], Train Loss: 0.1278, Val Loss: 0.3390\n",
      "Epoch [4/50], Train Loss: 0.1179, Val Loss: 0.3178\n",
      "Epoch [5/50], Train Loss: 0.1096, Val Loss: 0.2945\n",
      "Epoch [6/50], Train Loss: 0.1004, Val Loss: 0.2685\n",
      "Epoch [7/50], Train Loss: 0.0898, Val Loss: 0.2388\n",
      "Epoch [8/50], Train Loss: 0.0805, Val Loss: 0.2057\n",
      "Epoch [9/50], Train Loss: 0.0696, Val Loss: 0.1689\n",
      "Epoch [10/50], Train Loss: 0.0611, Val Loss: 0.1321\n",
      "Epoch [11/50], Train Loss: 0.0524, Val Loss: 0.1000\n",
      "Epoch [12/50], Train Loss: 0.0484, Val Loss: 0.0814\n",
      "Epoch [13/50], Train Loss: 0.0464, Val Loss: 0.0715\n",
      "Epoch [14/50], Train Loss: 0.0457, Val Loss: 0.0667\n",
      "Epoch [15/50], Train Loss: 0.0426, Val Loss: 0.0614\n",
      "Epoch [16/50], Train Loss: 0.0416, Val Loss: 0.0605\n",
      "Epoch [17/50], Train Loss: 0.0406, Val Loss: 0.0544\n",
      "Epoch [18/50], Train Loss: 0.0397, Val Loss: 0.0520\n",
      "Epoch [19/50], Train Loss: 0.0382, Val Loss: 0.0503\n",
      "Epoch [20/50], Train Loss: 0.0381, Val Loss: 0.0477\n",
      "Epoch [21/50], Train Loss: 0.0363, Val Loss: 0.0457\n",
      "Epoch [22/50], Train Loss: 0.0347, Val Loss: 0.0420\n",
      "Epoch [23/50], Train Loss: 0.0344, Val Loss: 0.0379\n",
      "Epoch [24/50], Train Loss: 0.0329, Val Loss: 0.0364\n",
      "Epoch [25/50], Train Loss: 0.0337, Val Loss: 0.0352\n",
      "Epoch [26/50], Train Loss: 0.0321, Val Loss: 0.0322\n",
      "Epoch [27/50], Train Loss: 0.0331, Val Loss: 0.0309\n",
      "Epoch [28/50], Train Loss: 0.0319, Val Loss: 0.0290\n",
      "Epoch [29/50], Train Loss: 0.0302, Val Loss: 0.0260\n",
      "Epoch [30/50], Train Loss: 0.0307, Val Loss: 0.0262\n",
      "Epoch [31/50], Train Loss: 0.0302, Val Loss: 0.0259\n",
      "Epoch [32/50], Train Loss: 0.0287, Val Loss: 0.0245\n",
      "Epoch [33/50], Train Loss: 0.0277, Val Loss: 0.0229\n",
      "Epoch [34/50], Train Loss: 0.0290, Val Loss: 0.0227\n",
      "Epoch [35/50], Train Loss: 0.0278, Val Loss: 0.0213\n",
      "Epoch [36/50], Train Loss: 0.0279, Val Loss: 0.0200\n",
      "Epoch [37/50], Train Loss: 0.0267, Val Loss: 0.0202\n",
      "Epoch [38/50], Train Loss: 0.0269, Val Loss: 0.0192\n",
      "Epoch [39/50], Train Loss: 0.0261, Val Loss: 0.0186\n",
      "Epoch [40/50], Train Loss: 0.0271, Val Loss: 0.0186\n",
      "Epoch [41/50], Train Loss: 0.0253, Val Loss: 0.0175\n",
      "Epoch [42/50], Train Loss: 0.0266, Val Loss: 0.0170\n",
      "Epoch [43/50], Train Loss: 0.0262, Val Loss: 0.0163\n",
      "Epoch [44/50], Train Loss: 0.0239, Val Loss: 0.0160\n",
      "Epoch [45/50], Train Loss: 0.0245, Val Loss: 0.0155\n",
      "Epoch [46/50], Train Loss: 0.0237, Val Loss: 0.0149\n",
      "Epoch [47/50], Train Loss: 0.0239, Val Loss: 0.0143\n",
      "Epoch [48/50], Train Loss: 0.0237, Val Loss: 0.0136\n",
      "Epoch [49/50], Train Loss: 0.0227, Val Loss: 0.0131\n",
      "Epoch [50/50], Train Loss: 0.0235, Val Loss: 0.0124\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=32, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1262, Val Loss: 0.3180\n",
      "Epoch [2/50], Train Loss: 0.1167, Val Loss: 0.3015\n",
      "Epoch [3/50], Train Loss: 0.1074, Val Loss: 0.2833\n",
      "Epoch [4/50], Train Loss: 0.0971, Val Loss: 0.2609\n",
      "Epoch [5/50], Train Loss: 0.0844, Val Loss: 0.2299\n",
      "Epoch [6/50], Train Loss: 0.0677, Val Loss: 0.1831\n",
      "Epoch [7/50], Train Loss: 0.0478, Val Loss: 0.1196\n",
      "Epoch [8/50], Train Loss: 0.0357, Val Loss: 0.0794\n",
      "Epoch [9/50], Train Loss: 0.0328, Val Loss: 0.0684\n",
      "Epoch [10/50], Train Loss: 0.0309, Val Loss: 0.0628\n",
      "Epoch [11/50], Train Loss: 0.0298, Val Loss: 0.0595\n",
      "Epoch [12/50], Train Loss: 0.0288, Val Loss: 0.0570\n",
      "Epoch [13/50], Train Loss: 0.0279, Val Loss: 0.0547\n",
      "Epoch [14/50], Train Loss: 0.0271, Val Loss: 0.0525\n",
      "Epoch [15/50], Train Loss: 0.0264, Val Loss: 0.0503\n",
      "Epoch [16/50], Train Loss: 0.0257, Val Loss: 0.0481\n",
      "Epoch [17/50], Train Loss: 0.0250, Val Loss: 0.0460\n",
      "Epoch [18/50], Train Loss: 0.0244, Val Loss: 0.0440\n",
      "Epoch [19/50], Train Loss: 0.0238, Val Loss: 0.0422\n",
      "Epoch [20/50], Train Loss: 0.0233, Val Loss: 0.0404\n",
      "Epoch [21/50], Train Loss: 0.0228, Val Loss: 0.0389\n",
      "Epoch [22/50], Train Loss: 0.0224, Val Loss: 0.0375\n",
      "Epoch [23/50], Train Loss: 0.0220, Val Loss: 0.0364\n",
      "Epoch [24/50], Train Loss: 0.0217, Val Loss: 0.0354\n",
      "Epoch [25/50], Train Loss: 0.0214, Val Loss: 0.0346\n",
      "Epoch [26/50], Train Loss: 0.0211, Val Loss: 0.0339\n",
      "Epoch [27/50], Train Loss: 0.0208, Val Loss: 0.0334\n",
      "Epoch [28/50], Train Loss: 0.0206, Val Loss: 0.0329\n",
      "Epoch [29/50], Train Loss: 0.0204, Val Loss: 0.0325\n",
      "Epoch [30/50], Train Loss: 0.0201, Val Loss: 0.0321\n",
      "Epoch [31/50], Train Loss: 0.0199, Val Loss: 0.0318\n",
      "Epoch [32/50], Train Loss: 0.0197, Val Loss: 0.0314\n",
      "Epoch [33/50], Train Loss: 0.0195, Val Loss: 0.0311\n",
      "Epoch [34/50], Train Loss: 0.0193, Val Loss: 0.0308\n",
      "Epoch [35/50], Train Loss: 0.0191, Val Loss: 0.0305\n",
      "Epoch [36/50], Train Loss: 0.0189, Val Loss: 0.0302\n",
      "Epoch [37/50], Train Loss: 0.0187, Val Loss: 0.0299\n",
      "Epoch [38/50], Train Loss: 0.0184, Val Loss: 0.0296\n",
      "Epoch [39/50], Train Loss: 0.0182, Val Loss: 0.0292\n",
      "Epoch [40/50], Train Loss: 0.0180, Val Loss: 0.0289\n",
      "Epoch [41/50], Train Loss: 0.0177, Val Loss: 0.0286\n",
      "Epoch [42/50], Train Loss: 0.0175, Val Loss: 0.0282\n",
      "Epoch [43/50], Train Loss: 0.0172, Val Loss: 0.0278\n",
      "Epoch [44/50], Train Loss: 0.0170, Val Loss: 0.0274\n",
      "Epoch [45/50], Train Loss: 0.0167, Val Loss: 0.0270\n",
      "Epoch [46/50], Train Loss: 0.0164, Val Loss: 0.0265\n",
      "Epoch [47/50], Train Loss: 0.0160, Val Loss: 0.0260\n",
      "Epoch [48/50], Train Loss: 0.0157, Val Loss: 0.0254\n",
      "Epoch [49/50], Train Loss: 0.0153, Val Loss: 0.0248\n",
      "Epoch [50/50], Train Loss: 0.0148, Val Loss: 0.0241\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=32, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1292, Val Loss: 0.3742\n",
      "Epoch [2/50], Train Loss: 0.1162, Val Loss: 0.3470\n",
      "Epoch [3/50], Train Loss: 0.1043, Val Loss: 0.3196\n",
      "Epoch [4/50], Train Loss: 0.0926, Val Loss: 0.2896\n",
      "Epoch [5/50], Train Loss: 0.0802, Val Loss: 0.2540\n",
      "Epoch [6/50], Train Loss: 0.0657, Val Loss: 0.2101\n",
      "Epoch [7/50], Train Loss: 0.0518, Val Loss: 0.1564\n",
      "Epoch [8/50], Train Loss: 0.0394, Val Loss: 0.1031\n",
      "Epoch [9/50], Train Loss: 0.0346, Val Loss: 0.0709\n",
      "Epoch [10/50], Train Loss: 0.0336, Val Loss: 0.0583\n",
      "Epoch [11/50], Train Loss: 0.0336, Val Loss: 0.0502\n",
      "Epoch [12/50], Train Loss: 0.0320, Val Loss: 0.0415\n",
      "Epoch [13/50], Train Loss: 0.0305, Val Loss: 0.0340\n",
      "Epoch [14/50], Train Loss: 0.0286, Val Loss: 0.0286\n",
      "Epoch [15/50], Train Loss: 0.0277, Val Loss: 0.0228\n",
      "Epoch [16/50], Train Loss: 0.0256, Val Loss: 0.0175\n",
      "Epoch [17/50], Train Loss: 0.0246, Val Loss: 0.0152\n",
      "Epoch [18/50], Train Loss: 0.0237, Val Loss: 0.0124\n",
      "Epoch [19/50], Train Loss: 0.0236, Val Loss: 0.0105\n",
      "Epoch [20/50], Train Loss: 0.0229, Val Loss: 0.0085\n",
      "Epoch [21/50], Train Loss: 0.0228, Val Loss: 0.0077\n",
      "Epoch [22/50], Train Loss: 0.0225, Val Loss: 0.0077\n",
      "Epoch [23/50], Train Loss: 0.0216, Val Loss: 0.0068\n",
      "Epoch [24/50], Train Loss: 0.0215, Val Loss: 0.0069\n",
      "Epoch [25/50], Train Loss: 0.0205, Val Loss: 0.0055\n",
      "Epoch [26/50], Train Loss: 0.0214, Val Loss: 0.0061\n",
      "Epoch [27/50], Train Loss: 0.0209, Val Loss: 0.0050\n",
      "Epoch [28/50], Train Loss: 0.0205, Val Loss: 0.0055\n",
      "Epoch [29/50], Train Loss: 0.0208, Val Loss: 0.0047\n",
      "Epoch [30/50], Train Loss: 0.0199, Val Loss: 0.0056\n",
      "Epoch [31/50], Train Loss: 0.0203, Val Loss: 0.0048\n",
      "Epoch [32/50], Train Loss: 0.0196, Val Loss: 0.0050\n",
      "Epoch [33/50], Train Loss: 0.0190, Val Loss: 0.0048\n",
      "Epoch [34/50], Train Loss: 0.0192, Val Loss: 0.0049\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=32, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1952, Val Loss: 0.4655\n",
      "Epoch [2/50], Train Loss: 0.1793, Val Loss: 0.4390\n",
      "Epoch [3/50], Train Loss: 0.1636, Val Loss: 0.4117\n",
      "Epoch [4/50], Train Loss: 0.1500, Val Loss: 0.3819\n",
      "Epoch [5/50], Train Loss: 0.1334, Val Loss: 0.3471\n",
      "Epoch [6/50], Train Loss: 0.1138, Val Loss: 0.3045\n",
      "Epoch [7/50], Train Loss: 0.0958, Val Loss: 0.2530\n",
      "Epoch [8/50], Train Loss: 0.0764, Val Loss: 0.1969\n",
      "Epoch [9/50], Train Loss: 0.0660, Val Loss: 0.1515\n",
      "Epoch [10/50], Train Loss: 0.0614, Val Loss: 0.1261\n",
      "Epoch [11/50], Train Loss: 0.0601, Val Loss: 0.1148\n",
      "Epoch [12/50], Train Loss: 0.0569, Val Loss: 0.1044\n",
      "Epoch [13/50], Train Loss: 0.0567, Val Loss: 0.0970\n",
      "Epoch [14/50], Train Loss: 0.0511, Val Loss: 0.0907\n",
      "Epoch [15/50], Train Loss: 0.0527, Val Loss: 0.0849\n",
      "Epoch [16/50], Train Loss: 0.0480, Val Loss: 0.0801\n",
      "Epoch [17/50], Train Loss: 0.0496, Val Loss: 0.0732\n",
      "Epoch [18/50], Train Loss: 0.0463, Val Loss: 0.0681\n",
      "Epoch [19/50], Train Loss: 0.0447, Val Loss: 0.0634\n",
      "Epoch [20/50], Train Loss: 0.0436, Val Loss: 0.0561\n",
      "Epoch [21/50], Train Loss: 0.0419, Val Loss: 0.0530\n",
      "Epoch [22/50], Train Loss: 0.0430, Val Loss: 0.0463\n",
      "Epoch [23/50], Train Loss: 0.0407, Val Loss: 0.0415\n",
      "Epoch [24/50], Train Loss: 0.0387, Val Loss: 0.0396\n",
      "Epoch [25/50], Train Loss: 0.0384, Val Loss: 0.0371\n",
      "Epoch [26/50], Train Loss: 0.0357, Val Loss: 0.0328\n",
      "Epoch [27/50], Train Loss: 0.0350, Val Loss: 0.0294\n",
      "Epoch [28/50], Train Loss: 0.0338, Val Loss: 0.0270\n",
      "Epoch [29/50], Train Loss: 0.0337, Val Loss: 0.0250\n",
      "Epoch [30/50], Train Loss: 0.0325, Val Loss: 0.0209\n",
      "Epoch [31/50], Train Loss: 0.0309, Val Loss: 0.0211\n",
      "Epoch [32/50], Train Loss: 0.0300, Val Loss: 0.0178\n",
      "Epoch [33/50], Train Loss: 0.0289, Val Loss: 0.0144\n",
      "Epoch [34/50], Train Loss: 0.0284, Val Loss: 0.0129\n",
      "Epoch [35/50], Train Loss: 0.0279, Val Loss: 0.0101\n",
      "Epoch [36/50], Train Loss: 0.0257, Val Loss: 0.0096\n",
      "Epoch [37/50], Train Loss: 0.0272, Val Loss: 0.0080\n",
      "Epoch [38/50], Train Loss: 0.0252, Val Loss: 0.0069\n",
      "Epoch [39/50], Train Loss: 0.0247, Val Loss: 0.0060\n",
      "Epoch [40/50], Train Loss: 0.0228, Val Loss: 0.0085\n",
      "Epoch [41/50], Train Loss: 0.0228, Val Loss: 0.0075\n",
      "Epoch [42/50], Train Loss: 0.0257, Val Loss: 0.0086\n",
      "Epoch [43/50], Train Loss: 0.0228, Val Loss: 0.0105\n",
      "Epoch [44/50], Train Loss: 0.0226, Val Loss: 0.0087\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=32, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1309, Val Loss: 0.3520\n",
      "Epoch [2/50], Train Loss: 0.1154, Val Loss: 0.3217\n",
      "Epoch [3/50], Train Loss: 0.0998, Val Loss: 0.2874\n",
      "Epoch [4/50], Train Loss: 0.0827, Val Loss: 0.2449\n",
      "Epoch [5/50], Train Loss: 0.0635, Val Loss: 0.1900\n",
      "Epoch [6/50], Train Loss: 0.0449, Val Loss: 0.1265\n",
      "Epoch [7/50], Train Loss: 0.0355, Val Loss: 0.0858\n",
      "Epoch [8/50], Train Loss: 0.0349, Val Loss: 0.0746\n",
      "Epoch [9/50], Train Loss: 0.0339, Val Loss: 0.0693\n",
      "Epoch [10/50], Train Loss: 0.0328, Val Loss: 0.0643\n",
      "Epoch [11/50], Train Loss: 0.0318, Val Loss: 0.0597\n",
      "Epoch [12/50], Train Loss: 0.0308, Val Loss: 0.0553\n",
      "Epoch [13/50], Train Loss: 0.0298, Val Loss: 0.0509\n",
      "Epoch [14/50], Train Loss: 0.0287, Val Loss: 0.0465\n",
      "Epoch [15/50], Train Loss: 0.0276, Val Loss: 0.0422\n",
      "Epoch [16/50], Train Loss: 0.0265, Val Loss: 0.0380\n",
      "Epoch [17/50], Train Loss: 0.0254, Val Loss: 0.0342\n",
      "Epoch [18/50], Train Loss: 0.0243, Val Loss: 0.0308\n",
      "Epoch [19/50], Train Loss: 0.0233, Val Loss: 0.0278\n",
      "Epoch [20/50], Train Loss: 0.0224, Val Loss: 0.0254\n",
      "Epoch [21/50], Train Loss: 0.0217, Val Loss: 0.0235\n",
      "Epoch [22/50], Train Loss: 0.0211, Val Loss: 0.0221\n",
      "Epoch [23/50], Train Loss: 0.0206, Val Loss: 0.0211\n",
      "Epoch [24/50], Train Loss: 0.0202, Val Loss: 0.0205\n",
      "Epoch [25/50], Train Loss: 0.0199, Val Loss: 0.0201\n",
      "Epoch [26/50], Train Loss: 0.0197, Val Loss: 0.0200\n",
      "Epoch [27/50], Train Loss: 0.0195, Val Loss: 0.0200\n",
      "Epoch [28/50], Train Loss: 0.0193, Val Loss: 0.0200\n",
      "Epoch [29/50], Train Loss: 0.0190, Val Loss: 0.0202\n",
      "Epoch [30/50], Train Loss: 0.0188, Val Loss: 0.0204\n",
      "Epoch [31/50], Train Loss: 0.0186, Val Loss: 0.0206\n",
      "Epoch [32/50], Train Loss: 0.0184, Val Loss: 0.0209\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=32, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1196, Val Loss: 0.3271\n",
      "Epoch [2/50], Train Loss: 0.1061, Val Loss: 0.2996\n",
      "Epoch [3/50], Train Loss: 0.0923, Val Loss: 0.2683\n",
      "Epoch [4/50], Train Loss: 0.0770, Val Loss: 0.2288\n",
      "Epoch [5/50], Train Loss: 0.0593, Val Loss: 0.1769\n",
      "Epoch [6/50], Train Loss: 0.0445, Val Loss: 0.1213\n",
      "Epoch [7/50], Train Loss: 0.0405, Val Loss: 0.0950\n",
      "Epoch [8/50], Train Loss: 0.0411, Val Loss: 0.0856\n",
      "Epoch [9/50], Train Loss: 0.0394, Val Loss: 0.0798\n",
      "Epoch [10/50], Train Loss: 0.0385, Val Loss: 0.0738\n",
      "Epoch [11/50], Train Loss: 0.0372, Val Loss: 0.0660\n",
      "Epoch [12/50], Train Loss: 0.0351, Val Loss: 0.0590\n",
      "Epoch [13/50], Train Loss: 0.0347, Val Loss: 0.0525\n",
      "Epoch [14/50], Train Loss: 0.0328, Val Loss: 0.0437\n",
      "Epoch [15/50], Train Loss: 0.0323, Val Loss: 0.0381\n",
      "Epoch [16/50], Train Loss: 0.0299, Val Loss: 0.0325\n",
      "Epoch [17/50], Train Loss: 0.0289, Val Loss: 0.0287\n",
      "Epoch [18/50], Train Loss: 0.0278, Val Loss: 0.0267\n",
      "Epoch [19/50], Train Loss: 0.0275, Val Loss: 0.0251\n",
      "Epoch [20/50], Train Loss: 0.0261, Val Loss: 0.0246\n",
      "Epoch [21/50], Train Loss: 0.0254, Val Loss: 0.0228\n",
      "Epoch [22/50], Train Loss: 0.0257, Val Loss: 0.0233\n",
      "Epoch [23/50], Train Loss: 0.0254, Val Loss: 0.0221\n",
      "Epoch [24/50], Train Loss: 0.0249, Val Loss: 0.0222\n",
      "Epoch [25/50], Train Loss: 0.0246, Val Loss: 0.0217\n",
      "Epoch [26/50], Train Loss: 0.0251, Val Loss: 0.0218\n",
      "Epoch [27/50], Train Loss: 0.0241, Val Loss: 0.0228\n",
      "Epoch [28/50], Train Loss: 0.0238, Val Loss: 0.0220\n",
      "Epoch [29/50], Train Loss: 0.0241, Val Loss: 0.0222\n",
      "Epoch [30/50], Train Loss: 0.0234, Val Loss: 0.0225\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=32, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1207, Val Loss: 0.2925\n",
      "Epoch [2/50], Train Loss: 0.1095, Val Loss: 0.2729\n",
      "Epoch [3/50], Train Loss: 0.0999, Val Loss: 0.2532\n",
      "Epoch [4/50], Train Loss: 0.0894, Val Loss: 0.2309\n",
      "Epoch [5/50], Train Loss: 0.0807, Val Loss: 0.2034\n",
      "Epoch [6/50], Train Loss: 0.0691, Val Loss: 0.1681\n",
      "Epoch [7/50], Train Loss: 0.0552, Val Loss: 0.1260\n",
      "Epoch [8/50], Train Loss: 0.0510, Val Loss: 0.0998\n",
      "Epoch [9/50], Train Loss: 0.0479, Val Loss: 0.0932\n",
      "Epoch [10/50], Train Loss: 0.0467, Val Loss: 0.0894\n",
      "Epoch [11/50], Train Loss: 0.0459, Val Loss: 0.0881\n",
      "Epoch [12/50], Train Loss: 0.0450, Val Loss: 0.0848\n",
      "Epoch [13/50], Train Loss: 0.0434, Val Loss: 0.0833\n",
      "Epoch [14/50], Train Loss: 0.0416, Val Loss: 0.0783\n",
      "Epoch [15/50], Train Loss: 0.0427, Val Loss: 0.0746\n",
      "Epoch [16/50], Train Loss: 0.0417, Val Loss: 0.0692\n",
      "Epoch [17/50], Train Loss: 0.0395, Val Loss: 0.0652\n",
      "Epoch [18/50], Train Loss: 0.0400, Val Loss: 0.0617\n",
      "Epoch [19/50], Train Loss: 0.0392, Val Loss: 0.0599\n",
      "Epoch [20/50], Train Loss: 0.0365, Val Loss: 0.0541\n",
      "Epoch [21/50], Train Loss: 0.0371, Val Loss: 0.0537\n",
      "Epoch [22/50], Train Loss: 0.0364, Val Loss: 0.0487\n",
      "Epoch [23/50], Train Loss: 0.0354, Val Loss: 0.0455\n",
      "Epoch [24/50], Train Loss: 0.0350, Val Loss: 0.0471\n",
      "Epoch [25/50], Train Loss: 0.0358, Val Loss: 0.0423\n",
      "Epoch [26/50], Train Loss: 0.0362, Val Loss: 0.0456\n",
      "Epoch [27/50], Train Loss: 0.0342, Val Loss: 0.0407\n",
      "Epoch [28/50], Train Loss: 0.0351, Val Loss: 0.0409\n",
      "Epoch [29/50], Train Loss: 0.0335, Val Loss: 0.0385\n",
      "Epoch [30/50], Train Loss: 0.0332, Val Loss: 0.0403\n",
      "Epoch [31/50], Train Loss: 0.0333, Val Loss: 0.0385\n",
      "Epoch [32/50], Train Loss: 0.0331, Val Loss: 0.0373\n",
      "Epoch [33/50], Train Loss: 0.0333, Val Loss: 0.0398\n",
      "Epoch [34/50], Train Loss: 0.0327, Val Loss: 0.0362\n",
      "Epoch [35/50], Train Loss: 0.0317, Val Loss: 0.0366\n",
      "Epoch [36/50], Train Loss: 0.0311, Val Loss: 0.0336\n",
      "Epoch [37/50], Train Loss: 0.0311, Val Loss: 0.0334\n",
      "Epoch [38/50], Train Loss: 0.0306, Val Loss: 0.0337\n",
      "Epoch [39/50], Train Loss: 0.0304, Val Loss: 0.0341\n",
      "Epoch [40/50], Train Loss: 0.0289, Val Loss: 0.0316\n",
      "Epoch [41/50], Train Loss: 0.0295, Val Loss: 0.0322\n",
      "Epoch [42/50], Train Loss: 0.0298, Val Loss: 0.0317\n",
      "Epoch [43/50], Train Loss: 0.0287, Val Loss: 0.0317\n",
      "Epoch [44/50], Train Loss: 0.0280, Val Loss: 0.0295\n",
      "Epoch [45/50], Train Loss: 0.0284, Val Loss: 0.0321\n",
      "Epoch [46/50], Train Loss: 0.0286, Val Loss: 0.0300\n",
      "Epoch [47/50], Train Loss: 0.0269, Val Loss: 0.0295\n",
      "Epoch [48/50], Train Loss: 0.0259, Val Loss: 0.0301\n",
      "Epoch [49/50], Train Loss: 0.0257, Val Loss: 0.0291\n",
      "Epoch [50/50], Train Loss: 0.0244, Val Loss: 0.0278\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=64, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1695, Val Loss: 0.4289\n",
      "Epoch [2/50], Train Loss: 0.1522, Val Loss: 0.3940\n",
      "Epoch [3/50], Train Loss: 0.1359, Val Loss: 0.3569\n",
      "Epoch [4/50], Train Loss: 0.1180, Val Loss: 0.3118\n",
      "Epoch [5/50], Train Loss: 0.0957, Val Loss: 0.2477\n",
      "Epoch [6/50], Train Loss: 0.0641, Val Loss: 0.1351\n",
      "Epoch [7/50], Train Loss: 0.0285, Val Loss: 0.0294\n",
      "Epoch [8/50], Train Loss: 0.0280, Val Loss: 0.0276\n",
      "Epoch [9/50], Train Loss: 0.0255, Val Loss: 0.0227\n",
      "Epoch [10/50], Train Loss: 0.0243, Val Loss: 0.0198\n",
      "Epoch [11/50], Train Loss: 0.0232, Val Loss: 0.0174\n",
      "Epoch [12/50], Train Loss: 0.0222, Val Loss: 0.0154\n",
      "Epoch [13/50], Train Loss: 0.0213, Val Loss: 0.0136\n",
      "Epoch [14/50], Train Loss: 0.0204, Val Loss: 0.0121\n",
      "Epoch [15/50], Train Loss: 0.0196, Val Loss: 0.0108\n",
      "Epoch [16/50], Train Loss: 0.0189, Val Loss: 0.0096\n",
      "Epoch [17/50], Train Loss: 0.0182, Val Loss: 0.0085\n",
      "Epoch [18/50], Train Loss: 0.0175, Val Loss: 0.0075\n",
      "Epoch [19/50], Train Loss: 0.0168, Val Loss: 0.0065\n",
      "Epoch [20/50], Train Loss: 0.0161, Val Loss: 0.0055\n",
      "Epoch [21/50], Train Loss: 0.0153, Val Loss: 0.0045\n",
      "Epoch [22/50], Train Loss: 0.0144, Val Loss: 0.0037\n",
      "Epoch [23/50], Train Loss: 0.0133, Val Loss: 0.0038\n",
      "Epoch [24/50], Train Loss: 0.0118, Val Loss: 0.0062\n",
      "Epoch [25/50], Train Loss: 0.0098, Val Loss: 0.0140\n",
      "Epoch [26/50], Train Loss: 0.0076, Val Loss: 0.0268\n",
      "Epoch [27/50], Train Loss: 0.0063, Val Loss: 0.0350\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=64, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1319, Val Loss: 0.3670\n",
      "Epoch [2/50], Train Loss: 0.1185, Val Loss: 0.3379\n",
      "Epoch [3/50], Train Loss: 0.1054, Val Loss: 0.3074\n",
      "Epoch [4/50], Train Loss: 0.0909, Val Loss: 0.2713\n",
      "Epoch [5/50], Train Loss: 0.0743, Val Loss: 0.2228\n",
      "Epoch [6/50], Train Loss: 0.0538, Val Loss: 0.1500\n",
      "Epoch [7/50], Train Loss: 0.0346, Val Loss: 0.0747\n",
      "Epoch [8/50], Train Loss: 0.0328, Val Loss: 0.0616\n",
      "Epoch [9/50], Train Loss: 0.0305, Val Loss: 0.0541\n",
      "Epoch [10/50], Train Loss: 0.0296, Val Loss: 0.0489\n",
      "Epoch [11/50], Train Loss: 0.0281, Val Loss: 0.0455\n",
      "Epoch [12/50], Train Loss: 0.0269, Val Loss: 0.0406\n",
      "Epoch [13/50], Train Loss: 0.0261, Val Loss: 0.0377\n",
      "Epoch [14/50], Train Loss: 0.0253, Val Loss: 0.0342\n",
      "Epoch [15/50], Train Loss: 0.0245, Val Loss: 0.0312\n",
      "Epoch [16/50], Train Loss: 0.0237, Val Loss: 0.0291\n",
      "Epoch [17/50], Train Loss: 0.0236, Val Loss: 0.0275\n",
      "Epoch [18/50], Train Loss: 0.0224, Val Loss: 0.0259\n",
      "Epoch [19/50], Train Loss: 0.0220, Val Loss: 0.0243\n",
      "Epoch [20/50], Train Loss: 0.0211, Val Loss: 0.0236\n",
      "Epoch [21/50], Train Loss: 0.0206, Val Loss: 0.0227\n",
      "Epoch [22/50], Train Loss: 0.0204, Val Loss: 0.0219\n",
      "Epoch [23/50], Train Loss: 0.0193, Val Loss: 0.0209\n",
      "Epoch [24/50], Train Loss: 0.0190, Val Loss: 0.0201\n",
      "Epoch [25/50], Train Loss: 0.0185, Val Loss: 0.0192\n",
      "Epoch [26/50], Train Loss: 0.0179, Val Loss: 0.0179\n",
      "Epoch [27/50], Train Loss: 0.0175, Val Loss: 0.0166\n",
      "Epoch [28/50], Train Loss: 0.0169, Val Loss: 0.0156\n",
      "Epoch [29/50], Train Loss: 0.0170, Val Loss: 0.0146\n",
      "Epoch [30/50], Train Loss: 0.0165, Val Loss: 0.0137\n",
      "Epoch [31/50], Train Loss: 0.0155, Val Loss: 0.0127\n",
      "Epoch [32/50], Train Loss: 0.0152, Val Loss: 0.0114\n",
      "Epoch [33/50], Train Loss: 0.0140, Val Loss: 0.0100\n",
      "Epoch [34/50], Train Loss: 0.0133, Val Loss: 0.0086\n",
      "Epoch [35/50], Train Loss: 0.0119, Val Loss: 0.0082\n",
      "Epoch [36/50], Train Loss: 0.0111, Val Loss: 0.0099\n",
      "Epoch [37/50], Train Loss: 0.0093, Val Loss: 0.0164\n",
      "Epoch [38/50], Train Loss: 0.0084, Val Loss: 0.0305\n",
      "Epoch [39/50], Train Loss: 0.0080, Val Loss: 0.0286\n",
      "Epoch [40/50], Train Loss: 0.0073, Val Loss: 0.0366\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=64, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1003, Val Loss: 0.2881\n",
      "Epoch [2/50], Train Loss: 0.0866, Val Loss: 0.2597\n",
      "Epoch [3/50], Train Loss: 0.0773, Val Loss: 0.2321\n",
      "Epoch [4/50], Train Loss: 0.0670, Val Loss: 0.2034\n",
      "Epoch [5/50], Train Loss: 0.0576, Val Loss: 0.1722\n",
      "Epoch [6/50], Train Loss: 0.0469, Val Loss: 0.1375\n",
      "Epoch [7/50], Train Loss: 0.0392, Val Loss: 0.1006\n",
      "Epoch [8/50], Train Loss: 0.0333, Val Loss: 0.0683\n",
      "Epoch [9/50], Train Loss: 0.0315, Val Loss: 0.0513\n",
      "Epoch [10/50], Train Loss: 0.0299, Val Loss: 0.0438\n",
      "Epoch [11/50], Train Loss: 0.0291, Val Loss: 0.0393\n",
      "Epoch [12/50], Train Loss: 0.0282, Val Loss: 0.0351\n",
      "Epoch [13/50], Train Loss: 0.0268, Val Loss: 0.0287\n",
      "Epoch [14/50], Train Loss: 0.0259, Val Loss: 0.0249\n",
      "Epoch [15/50], Train Loss: 0.0244, Val Loss: 0.0209\n",
      "Epoch [16/50], Train Loss: 0.0233, Val Loss: 0.0162\n",
      "Epoch [17/50], Train Loss: 0.0226, Val Loss: 0.0143\n",
      "Epoch [18/50], Train Loss: 0.0212, Val Loss: 0.0093\n",
      "Epoch [19/50], Train Loss: 0.0196, Val Loss: 0.0075\n",
      "Epoch [20/50], Train Loss: 0.0186, Val Loss: 0.0055\n",
      "Epoch [21/50], Train Loss: 0.0178, Val Loss: 0.0064\n",
      "Epoch [22/50], Train Loss: 0.0159, Val Loss: 0.0065\n",
      "Epoch [23/50], Train Loss: 0.0149, Val Loss: 0.0130\n",
      "Epoch [24/50], Train Loss: 0.0136, Val Loss: 0.0135\n",
      "Epoch [25/50], Train Loss: 0.0129, Val Loss: 0.0205\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=64, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0967, Val Loss: 0.2819\n",
      "Epoch [2/50], Train Loss: 0.0810, Val Loss: 0.2443\n",
      "Epoch [3/50], Train Loss: 0.0641, Val Loss: 0.1932\n",
      "Epoch [4/50], Train Loss: 0.0447, Val Loss: 0.1200\n",
      "Epoch [5/50], Train Loss: 0.0330, Val Loss: 0.0659\n",
      "Epoch [6/50], Train Loss: 0.0331, Val Loss: 0.0570\n",
      "Epoch [7/50], Train Loss: 0.0314, Val Loss: 0.0492\n",
      "Epoch [8/50], Train Loss: 0.0301, Val Loss: 0.0425\n",
      "Epoch [9/50], Train Loss: 0.0288, Val Loss: 0.0363\n",
      "Epoch [10/50], Train Loss: 0.0276, Val Loss: 0.0303\n",
      "Epoch [11/50], Train Loss: 0.0263, Val Loss: 0.0248\n",
      "Epoch [12/50], Train Loss: 0.0250, Val Loss: 0.0200\n",
      "Epoch [13/50], Train Loss: 0.0237, Val Loss: 0.0161\n",
      "Epoch [14/50], Train Loss: 0.0224, Val Loss: 0.0135\n",
      "Epoch [15/50], Train Loss: 0.0213, Val Loss: 0.0118\n",
      "Epoch [16/50], Train Loss: 0.0202, Val Loss: 0.0108\n",
      "Epoch [17/50], Train Loss: 0.0192, Val Loss: 0.0098\n",
      "Epoch [18/50], Train Loss: 0.0181, Val Loss: 0.0086\n",
      "Epoch [19/50], Train Loss: 0.0167, Val Loss: 0.0073\n",
      "Epoch [20/50], Train Loss: 0.0148, Val Loss: 0.0078\n",
      "Epoch [21/50], Train Loss: 0.0119, Val Loss: 0.0163\n",
      "Epoch [22/50], Train Loss: 0.0088, Val Loss: 0.0268\n",
      "Epoch [23/50], Train Loss: 0.0071, Val Loss: 0.0366\n",
      "Epoch [24/50], Train Loss: 0.0066, Val Loss: 0.0386\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=64, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1148, Val Loss: 0.3205\n",
      "Epoch [2/50], Train Loss: 0.0970, Val Loss: 0.2823\n",
      "Epoch [3/50], Train Loss: 0.0787, Val Loss: 0.2312\n",
      "Epoch [4/50], Train Loss: 0.0547, Val Loss: 0.1457\n",
      "Epoch [5/50], Train Loss: 0.0345, Val Loss: 0.0616\n",
      "Epoch [6/50], Train Loss: 0.0369, Val Loss: 0.0593\n",
      "Epoch [7/50], Train Loss: 0.0341, Val Loss: 0.0526\n",
      "Epoch [8/50], Train Loss: 0.0332, Val Loss: 0.0475\n",
      "Epoch [9/50], Train Loss: 0.0318, Val Loss: 0.0418\n",
      "Epoch [10/50], Train Loss: 0.0307, Val Loss: 0.0360\n",
      "Epoch [11/50], Train Loss: 0.0293, Val Loss: 0.0314\n",
      "Epoch [12/50], Train Loss: 0.0278, Val Loss: 0.0270\n",
      "Epoch [13/50], Train Loss: 0.0272, Val Loss: 0.0231\n",
      "Epoch [14/50], Train Loss: 0.0264, Val Loss: 0.0186\n",
      "Epoch [15/50], Train Loss: 0.0253, Val Loss: 0.0170\n",
      "Epoch [16/50], Train Loss: 0.0239, Val Loss: 0.0145\n",
      "Epoch [17/50], Train Loss: 0.0232, Val Loss: 0.0139\n",
      "Epoch [18/50], Train Loss: 0.0223, Val Loss: 0.0135\n",
      "Epoch [19/50], Train Loss: 0.0224, Val Loss: 0.0133\n",
      "Epoch [20/50], Train Loss: 0.0214, Val Loss: 0.0131\n",
      "Epoch [21/50], Train Loss: 0.0203, Val Loss: 0.0126\n",
      "Epoch [22/50], Train Loss: 0.0201, Val Loss: 0.0118\n",
      "Epoch [23/50], Train Loss: 0.0195, Val Loss: 0.0105\n",
      "Epoch [24/50], Train Loss: 0.0185, Val Loss: 0.0094\n",
      "Epoch [25/50], Train Loss: 0.0180, Val Loss: 0.0071\n",
      "Epoch [26/50], Train Loss: 0.0166, Val Loss: 0.0055\n",
      "Epoch [27/50], Train Loss: 0.0146, Val Loss: 0.0077\n",
      "Epoch [28/50], Train Loss: 0.0127, Val Loss: 0.0123\n",
      "Epoch [29/50], Train Loss: 0.0098, Val Loss: 0.0213\n",
      "Epoch [30/50], Train Loss: 0.0089, Val Loss: 0.0259\n",
      "Epoch [31/50], Train Loss: 0.0086, Val Loss: 0.0253\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=64, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1227, Val Loss: 0.3219\n",
      "Epoch [2/50], Train Loss: 0.1037, Val Loss: 0.2809\n",
      "Epoch [3/50], Train Loss: 0.0815, Val Loss: 0.2201\n",
      "Epoch [4/50], Train Loss: 0.0545, Val Loss: 0.1253\n",
      "Epoch [5/50], Train Loss: 0.0434, Val Loss: 0.0804\n",
      "Epoch [6/50], Train Loss: 0.0466, Val Loss: 0.0841\n",
      "Epoch [7/50], Train Loss: 0.0435, Val Loss: 0.0797\n",
      "Epoch [8/50], Train Loss: 0.0415, Val Loss: 0.0730\n",
      "Epoch [9/50], Train Loss: 0.0401, Val Loss: 0.0680\n",
      "Epoch [10/50], Train Loss: 0.0390, Val Loss: 0.0641\n",
      "Epoch [11/50], Train Loss: 0.0373, Val Loss: 0.0573\n",
      "Epoch [12/50], Train Loss: 0.0372, Val Loss: 0.0531\n",
      "Epoch [13/50], Train Loss: 0.0364, Val Loss: 0.0494\n",
      "Epoch [14/50], Train Loss: 0.0349, Val Loss: 0.0437\n",
      "Epoch [15/50], Train Loss: 0.0340, Val Loss: 0.0395\n",
      "Epoch [16/50], Train Loss: 0.0323, Val Loss: 0.0346\n",
      "Epoch [17/50], Train Loss: 0.0311, Val Loss: 0.0288\n",
      "Epoch [18/50], Train Loss: 0.0298, Val Loss: 0.0258\n",
      "Epoch [19/50], Train Loss: 0.0282, Val Loss: 0.0211\n",
      "Epoch [20/50], Train Loss: 0.0263, Val Loss: 0.0183\n",
      "Epoch [21/50], Train Loss: 0.0254, Val Loss: 0.0159\n",
      "Epoch [22/50], Train Loss: 0.0245, Val Loss: 0.0125\n",
      "Epoch [23/50], Train Loss: 0.0226, Val Loss: 0.0077\n",
      "Epoch [24/50], Train Loss: 0.0205, Val Loss: 0.0043\n",
      "Epoch [25/50], Train Loss: 0.0184, Val Loss: 0.0058\n",
      "Epoch [26/50], Train Loss: 0.0167, Val Loss: 0.0096\n",
      "Epoch [27/50], Train Loss: 0.0160, Val Loss: 0.0088\n",
      "Epoch [28/50], Train Loss: 0.0163, Val Loss: 0.0084\n",
      "Epoch [29/50], Train Loss: 0.0147, Val Loss: 0.0120\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=64, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1469, Val Loss: 0.3406\n",
      "Epoch [2/50], Train Loss: 0.1188, Val Loss: 0.2868\n",
      "Epoch [3/50], Train Loss: 0.0857, Val Loss: 0.2008\n",
      "Epoch [4/50], Train Loss: 0.0436, Val Loss: 0.0729\n",
      "Epoch [5/50], Train Loss: 0.0403, Val Loss: 0.0738\n",
      "Epoch [6/50], Train Loss: 0.0375, Val Loss: 0.0731\n",
      "Epoch [7/50], Train Loss: 0.0356, Val Loss: 0.0695\n",
      "Epoch [8/50], Train Loss: 0.0343, Val Loss: 0.0659\n",
      "Epoch [9/50], Train Loss: 0.0330, Val Loss: 0.0614\n",
      "Epoch [10/50], Train Loss: 0.0316, Val Loss: 0.0565\n",
      "Epoch [11/50], Train Loss: 0.0301, Val Loss: 0.0513\n",
      "Epoch [12/50], Train Loss: 0.0285, Val Loss: 0.0463\n",
      "Epoch [13/50], Train Loss: 0.0269, Val Loss: 0.0419\n",
      "Epoch [14/50], Train Loss: 0.0252, Val Loss: 0.0389\n",
      "Epoch [15/50], Train Loss: 0.0238, Val Loss: 0.0374\n",
      "Epoch [16/50], Train Loss: 0.0228, Val Loss: 0.0369\n",
      "Epoch [17/50], Train Loss: 0.0223, Val Loss: 0.0368\n",
      "Epoch [18/50], Train Loss: 0.0219, Val Loss: 0.0365\n",
      "Epoch [19/50], Train Loss: 0.0215, Val Loss: 0.0359\n",
      "Epoch [20/50], Train Loss: 0.0212, Val Loss: 0.0351\n",
      "Epoch [21/50], Train Loss: 0.0210, Val Loss: 0.0344\n",
      "Epoch [22/50], Train Loss: 0.0207, Val Loss: 0.0338\n",
      "Epoch [23/50], Train Loss: 0.0205, Val Loss: 0.0331\n",
      "Epoch [24/50], Train Loss: 0.0202, Val Loss: 0.0324\n",
      "Epoch [25/50], Train Loss: 0.0200, Val Loss: 0.0317\n",
      "Epoch [26/50], Train Loss: 0.0197, Val Loss: 0.0309\n",
      "Epoch [27/50], Train Loss: 0.0195, Val Loss: 0.0300\n",
      "Epoch [28/50], Train Loss: 0.0192, Val Loss: 0.0290\n",
      "Epoch [29/50], Train Loss: 0.0188, Val Loss: 0.0279\n",
      "Epoch [30/50], Train Loss: 0.0185, Val Loss: 0.0265\n",
      "Epoch [31/50], Train Loss: 0.0180, Val Loss: 0.0249\n",
      "Epoch [32/50], Train Loss: 0.0175, Val Loss: 0.0227\n",
      "Epoch [33/50], Train Loss: 0.0168, Val Loss: 0.0200\n",
      "Epoch [34/50], Train Loss: 0.0159, Val Loss: 0.0166\n",
      "Epoch [35/50], Train Loss: 0.0146, Val Loss: 0.0132\n",
      "Epoch [36/50], Train Loss: 0.0125, Val Loss: 0.0128\n",
      "Epoch [37/50], Train Loss: 0.0094, Val Loss: 0.0178\n",
      "Epoch [38/50], Train Loss: 0.0067, Val Loss: 0.0224\n",
      "Epoch [39/50], Train Loss: 0.0049, Val Loss: 0.0241\n",
      "Epoch [40/50], Train Loss: 0.0041, Val Loss: 0.0231\n",
      "Epoch [41/50], Train Loss: 0.0041, Val Loss: 0.0225\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=64, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1045, Val Loss: 0.3031\n",
      "Epoch [2/50], Train Loss: 0.0905, Val Loss: 0.2730\n",
      "Epoch [3/50], Train Loss: 0.0742, Val Loss: 0.2275\n",
      "Epoch [4/50], Train Loss: 0.0516, Val Loss: 0.1370\n",
      "Epoch [5/50], Train Loss: 0.0374, Val Loss: 0.0769\n",
      "Epoch [6/50], Train Loss: 0.0402, Val Loss: 0.0778\n",
      "Epoch [7/50], Train Loss: 0.0371, Val Loss: 0.0689\n",
      "Epoch [8/50], Train Loss: 0.0361, Val Loss: 0.0619\n",
      "Epoch [9/50], Train Loss: 0.0346, Val Loss: 0.0544\n",
      "Epoch [10/50], Train Loss: 0.0327, Val Loss: 0.0461\n",
      "Epoch [11/50], Train Loss: 0.0324, Val Loss: 0.0387\n",
      "Epoch [12/50], Train Loss: 0.0301, Val Loss: 0.0298\n",
      "Epoch [13/50], Train Loss: 0.0282, Val Loss: 0.0230\n",
      "Epoch [14/50], Train Loss: 0.0268, Val Loss: 0.0177\n",
      "Epoch [15/50], Train Loss: 0.0251, Val Loss: 0.0139\n",
      "Epoch [16/50], Train Loss: 0.0245, Val Loss: 0.0126\n",
      "Epoch [17/50], Train Loss: 0.0238, Val Loss: 0.0128\n",
      "Epoch [18/50], Train Loss: 0.0227, Val Loss: 0.0141\n",
      "Epoch [19/50], Train Loss: 0.0224, Val Loss: 0.0141\n",
      "Epoch [20/50], Train Loss: 0.0224, Val Loss: 0.0138\n",
      "Epoch [21/50], Train Loss: 0.0216, Val Loss: 0.0138\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=64, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1001, Val Loss: 0.2822\n",
      "Epoch [2/50], Train Loss: 0.0840, Val Loss: 0.2457\n",
      "Epoch [3/50], Train Loss: 0.0670, Val Loss: 0.1950\n",
      "Epoch [4/50], Train Loss: 0.0501, Val Loss: 0.1286\n",
      "Epoch [5/50], Train Loss: 0.0446, Val Loss: 0.0962\n",
      "Epoch [6/50], Train Loss: 0.0461, Val Loss: 0.0934\n",
      "Epoch [7/50], Train Loss: 0.0444, Val Loss: 0.0873\n",
      "Epoch [8/50], Train Loss: 0.0436, Val Loss: 0.0790\n",
      "Epoch [9/50], Train Loss: 0.0416, Val Loss: 0.0717\n",
      "Epoch [10/50], Train Loss: 0.0392, Val Loss: 0.0616\n",
      "Epoch [11/50], Train Loss: 0.0382, Val Loss: 0.0514\n",
      "Epoch [12/50], Train Loss: 0.0364, Val Loss: 0.0388\n",
      "Epoch [13/50], Train Loss: 0.0354, Val Loss: 0.0257\n",
      "Epoch [14/50], Train Loss: 0.0324, Val Loss: 0.0123\n",
      "Epoch [15/50], Train Loss: 0.0303, Val Loss: 0.0085\n",
      "Epoch [16/50], Train Loss: 0.0270, Val Loss: 0.0066\n",
      "Epoch [17/50], Train Loss: 0.0238, Val Loss: 0.0116\n",
      "Epoch [18/50], Train Loss: 0.0199, Val Loss: 0.0275\n",
      "Epoch [19/50], Train Loss: 0.0165, Val Loss: 0.0302\n",
      "Epoch [20/50], Train Loss: 0.0162, Val Loss: 0.0272\n",
      "Epoch [21/50], Train Loss: 0.0158, Val Loss: 0.0310\n",
      "Early stopping triggered.\n",
      "Best Parameters: (0.0005, 'adam', 64, 1, 0.5), Best Validation Loss: 0.0017\n",
      "Best model loaded.\n"
     ]
    }
   ],
   "source": [
    "#LSTM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from itertools import product\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Transform to PyTorch Tensor\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32).to(device)\n",
    "\n",
    "# LSTM Model with Dropout\n",
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout):\n",
    "        super(MyLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.dropout(out[:, -1, :])  # Dropout on the last hidden state\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Hyperparameter Grid\n",
    "param_grid = {\n",
    "    \"learning_rate\": [0.005, 0.001, 0.0005, 0.0001],\n",
    "    \"optimizer\": [\"adam\", \"sgd\", \"adamw\"],\n",
    "    \"hidden_size\": [16, 32, 64, 128],\n",
    "    \"num_layers\": [1, 2, 3],\n",
    "    \"dropout\": [0.0, 0.2, 0.5]\n",
    "}\n",
    "\n",
    "# Generate all combinations of hyperparameters\n",
    "param_combinations = list(product(*param_grid.values()))\n",
    "best_val_loss = float(\"inf\")\n",
    "best_params = None\n",
    "\n",
    "# Grid Search\n",
    "for params in param_combinations:\n",
    "    learning_rate, optimizer_name, hidden_size, num_layers, dropout = params\n",
    "\n",
    "    print(f\"Testing parameters: lr={learning_rate}, optimizer={optimizer_name}, hidden_size={hidden_size}, num_layers={num_layers}, dropout={dropout}\")\n",
    "\n",
    "    # Initialize model\n",
    "    model = MyLSTM(input_size=5, hidden_size=hidden_size, num_layers=num_layers, output_size=5, dropout=dropout).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Initialize optimizer\n",
    "    if optimizer_name == \"adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    elif optimizer_name == \"sgd\":\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    elif optimizer_name == \"adamw\":\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Early Stopping\n",
    "    patience = 5\n",
    "    early_stop_counter = 0\n",
    "    best_model_val_loss = float(\"inf\")\n",
    "\n",
    "    # Training Loop\n",
    "    for epoch in range(50):  # Max epochs\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for i in range(0, len(X_train), 32):  # Batch size = 32\n",
    "            x_batch = X_train[i:i+32]\n",
    "            y_batch = y_train[i:i+32]\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(x_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss /= len(X_train) / 32  # Average train loss\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val)\n",
    "            val_loss = criterion(val_outputs, y_val).item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/50], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early Stopping Check\n",
    "        if val_loss < best_model_val_loss:\n",
    "            best_model_val_loss = val_loss\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            if early_stop_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    # Update best parameters if current combination is better\n",
    "    if best_model_val_loss < best_val_loss:\n",
    "        best_val_loss = best_model_val_loss\n",
    "        best_params = params\n",
    "        # Save best model\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "print(f\"Best Parameters: {best_params}, Best Validation Loss: {best_val_loss:.4f}\")\n",
    "\n",
    "# Load Best Model\n",
    "model = MyLSTM(\n",
    "    input_size=5,\n",
    "    hidden_size=best_params[2],\n",
    "    num_layers=best_params[3],\n",
    "    output_size=5,\n",
    "    dropout=best_params[4]\n",
    ").to(device)\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "print(\"Best model loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the best parameters is: Best Parameters: (lr = 0.0005, optimizer = 'adam', hidden_size:64, num_layers = 1, best_dropout = 0.5) with the Validation Loss: 0.0017\n",
    "\n",
    "Next step, I will used this best model to assess the model performance on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open - MSE: 0.0013, MAE: 0.0317, R: 0.7000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High - MSE: 0.0024, MAE: 0.0452, R: 0.4148\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low - MSE: 0.0022, MAE: 0.0425, R: 0.4953\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Close - MSE: 0.0614, MAE: 0.2453, R: -23.8826\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Volume - MSE: 0.0016, MAE: 0.0296, R: -1.1215\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Metrics (All Features):\n",
      "Mean Squared Error (MSE): 0.0138\n",
      "Mean Absolute Error (MAE): 0.0789\n",
      "R Score: -4.6788\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "best_lr, best_optimizer_name, best_hidden_size, best_num_layers, best_dropout = best_params\n",
    "model = MyLSTM(\n",
    "    input_size=5,\n",
    "    hidden_size=best_hidden_size,\n",
    "    num_layers=best_num_layers,\n",
    "    output_size=5,\n",
    "    dropout=best_dropout\n",
    ").to(device)\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "\n",
    "# Test\n",
    "model.eval()\n",
    "\n",
    "# Transform to PyTorch Tensor\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test)\n",
    "\n",
    "# Transform prediction and true to NumPy\n",
    "predictions = predictions.cpu().numpy()\n",
    "y_test = y_test.cpu().numpy()\n",
    "\n",
    "features = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "\n",
    "# Calculate metrics\n",
    "for i, feature in enumerate(features):\n",
    "    mse = mean_squared_error(y_test[:, i], predictions[:, i])\n",
    "    mae = mean_absolute_error(y_test[:, i], predictions[:, i])\n",
    "    r2 = r2_score(y_test[:, i], predictions[:, i])\n",
    "    print(f\"{feature} - MSE: {mse:.4f}, MAE: {mae:.4f}, R: {r2:.4f}\")\n",
    "\n",
    "    # Plot Charts\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(y_test[:, i], label=f\"True {feature}\", color=\"blue\")\n",
    "    plt.plot(predictions[:, i], label=f\"Predicted {feature}\", color=\"red\")\n",
    "    plt.title(f\"Predicted vs True {feature}\")\n",
    "    plt.xlabel(\"Sample Index\")\n",
    "    plt.ylabel(f\"Normalized {feature} Value\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Total Metrics\n",
    "mse_total = mean_squared_error(y_test, predictions)\n",
    "mae_total = mean_absolute_error(y_test, predictions)\n",
    "r2_total = r2_score(y_test, predictions)\n",
    "\n",
    "print(\"\\nOverall Metrics (All Features):\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_total:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_total:.4f}\")\n",
    "print(f\"R Score: {r2_total:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These tables indicates that the model have a good performance on predicitng close, high, and open, but did bad in volume and close. Howeverm, I design the LSTM for the next 1 day prediction base on previous 30 days, which means that this model works for those people that want to make a decision to sell or keep the stock. By predicting close, high and open is enough to make such a decision.\n",
    "\n",
    "Next I will also compare to other model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Vanilla RNN ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=16, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0355, Val Loss: 0.0404\n",
      "Epoch [2/50], Train Loss: 0.0379, Val Loss: 0.0197\n",
      "Epoch [3/50], Train Loss: 0.0263, Val Loss: 0.0138\n",
      "Epoch [4/50], Train Loss: 0.0242, Val Loss: 0.0200\n",
      "Epoch [5/50], Train Loss: 0.0130, Val Loss: 0.0035\n",
      "Epoch [6/50], Train Loss: 0.0048, Val Loss: 0.0072\n",
      "Epoch [7/50], Train Loss: 0.0050, Val Loss: 0.0131\n",
      "Epoch [8/50], Train Loss: 0.0032, Val Loss: 0.0060\n",
      "Epoch [9/50], Train Loss: 0.0029, Val Loss: 0.0018\n",
      "Epoch [10/50], Train Loss: 0.0025, Val Loss: 0.0035\n",
      "Epoch [11/50], Train Loss: 0.0025, Val Loss: 0.0059\n",
      "Epoch [12/50], Train Loss: 0.0018, Val Loss: 0.0011\n",
      "Epoch [13/50], Train Loss: 0.0026, Val Loss: 0.0030\n",
      "Epoch [14/50], Train Loss: 0.0022, Val Loss: 0.0046\n",
      "Epoch [15/50], Train Loss: 0.0016, Val Loss: 0.0010\n",
      "Epoch [16/50], Train Loss: 0.0017, Val Loss: 0.0020\n",
      "Epoch [17/50], Train Loss: 0.0017, Val Loss: 0.0022\n",
      "Epoch [18/50], Train Loss: 0.0016, Val Loss: 0.0012\n",
      "Epoch [19/50], Train Loss: 0.0015, Val Loss: 0.0017\n",
      "Epoch [20/50], Train Loss: 0.0017, Val Loss: 0.0018\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=16, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0482, Val Loss: 0.0424\n",
      "Epoch [2/50], Train Loss: 0.0731, Val Loss: 0.0060\n",
      "Epoch [3/50], Train Loss: 0.0397, Val Loss: 0.0069\n",
      "Epoch [4/50], Train Loss: 0.0328, Val Loss: 0.0171\n",
      "Epoch [5/50], Train Loss: 0.0186, Val Loss: 0.0053\n",
      "Epoch [6/50], Train Loss: 0.0124, Val Loss: 0.0024\n",
      "Epoch [7/50], Train Loss: 0.0109, Val Loss: 0.0133\n",
      "Epoch [8/50], Train Loss: 0.0083, Val Loss: 0.0027\n",
      "Epoch [9/50], Train Loss: 0.0074, Val Loss: 0.0078\n",
      "Epoch [10/50], Train Loss: 0.0065, Val Loss: 0.0070\n",
      "Epoch [11/50], Train Loss: 0.0059, Val Loss: 0.0017\n",
      "Epoch [12/50], Train Loss: 0.0058, Val Loss: 0.0011\n",
      "Epoch [13/50], Train Loss: 0.0054, Val Loss: 0.0097\n",
      "Epoch [14/50], Train Loss: 0.0050, Val Loss: 0.0023\n",
      "Epoch [15/50], Train Loss: 0.0060, Val Loss: 0.0077\n",
      "Epoch [16/50], Train Loss: 0.0047, Val Loss: 0.0031\n",
      "Epoch [17/50], Train Loss: 0.0059, Val Loss: 0.0179\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=16, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0879, Val Loss: 0.1105\n",
      "Epoch [2/50], Train Loss: 0.0692, Val Loss: 0.0459\n",
      "Epoch [3/50], Train Loss: 0.0521, Val Loss: 0.0161\n",
      "Epoch [4/50], Train Loss: 0.0357, Val Loss: 0.0056\n",
      "Epoch [5/50], Train Loss: 0.0275, Val Loss: 0.0042\n",
      "Epoch [6/50], Train Loss: 0.0199, Val Loss: 0.0182\n",
      "Epoch [7/50], Train Loss: 0.0166, Val Loss: 0.0230\n",
      "Epoch [8/50], Train Loss: 0.0134, Val Loss: 0.0060\n",
      "Epoch [9/50], Train Loss: 0.0132, Val Loss: 0.0061\n",
      "Epoch [10/50], Train Loss: 0.0107, Val Loss: 0.0107\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=16, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0398, Val Loss: 0.1036\n",
      "Epoch [2/50], Train Loss: 0.0603, Val Loss: 0.0788\n",
      "Epoch [3/50], Train Loss: 0.0649, Val Loss: 0.0550\n",
      "Epoch [4/50], Train Loss: 0.0610, Val Loss: 0.0381\n",
      "Epoch [5/50], Train Loss: 0.0416, Val Loss: 0.0171\n",
      "Epoch [6/50], Train Loss: 0.0370, Val Loss: 0.0130\n",
      "Epoch [7/50], Train Loss: 0.0239, Val Loss: 0.0081\n",
      "Epoch [8/50], Train Loss: 0.0312, Val Loss: 0.0348\n",
      "Epoch [9/50], Train Loss: 0.0270, Val Loss: 0.0091\n",
      "Epoch [10/50], Train Loss: 0.0112, Val Loss: 0.0043\n",
      "Epoch [11/50], Train Loss: 0.0059, Val Loss: 0.0142\n",
      "Epoch [12/50], Train Loss: 0.0059, Val Loss: 0.0045\n",
      "Epoch [13/50], Train Loss: 0.0035, Val Loss: 0.0050\n",
      "Epoch [14/50], Train Loss: 0.0030, Val Loss: 0.0100\n",
      "Epoch [15/50], Train Loss: 0.0028, Val Loss: 0.0027\n",
      "Epoch [16/50], Train Loss: 0.0028, Val Loss: 0.0031\n",
      "Epoch [17/50], Train Loss: 0.0032, Val Loss: 0.0097\n",
      "Epoch [18/50], Train Loss: 0.0027, Val Loss: 0.0038\n",
      "Epoch [19/50], Train Loss: 0.0029, Val Loss: 0.0021\n",
      "Epoch [20/50], Train Loss: 0.0044, Val Loss: 0.0093\n",
      "Epoch [21/50], Train Loss: 0.0059, Val Loss: 0.0060\n",
      "Epoch [22/50], Train Loss: 0.0052, Val Loss: 0.0019\n",
      "Epoch [23/50], Train Loss: 0.0083, Val Loss: 0.0092\n",
      "Epoch [24/50], Train Loss: 0.0122, Val Loss: 0.0084\n",
      "Epoch [25/50], Train Loss: 0.0176, Val Loss: 0.0147\n",
      "Epoch [26/50], Train Loss: 0.0112, Val Loss: 0.0273\n",
      "Epoch [27/50], Train Loss: 0.0110, Val Loss: 0.0065\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=16, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0453, Val Loss: 0.0870\n",
      "Epoch [2/50], Train Loss: 0.0645, Val Loss: 0.0406\n",
      "Epoch [3/50], Train Loss: 0.0479, Val Loss: 0.0155\n",
      "Epoch [4/50], Train Loss: 0.0201, Val Loss: 0.0065\n",
      "Epoch [5/50], Train Loss: 0.0151, Val Loss: 0.0073\n",
      "Epoch [6/50], Train Loss: 0.0154, Val Loss: 0.0193\n",
      "Epoch [7/50], Train Loss: 0.0229, Val Loss: 0.0114\n",
      "Epoch [8/50], Train Loss: 0.0236, Val Loss: 0.0086\n",
      "Epoch [9/50], Train Loss: 0.0175, Val Loss: 0.0667\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=16, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0719, Val Loss: 0.0935\n",
      "Epoch [2/50], Train Loss: 0.0664, Val Loss: 0.0589\n",
      "Epoch [3/50], Train Loss: 0.0451, Val Loss: 0.0273\n",
      "Epoch [4/50], Train Loss: 0.0364, Val Loss: 0.0155\n",
      "Epoch [5/50], Train Loss: 0.0387, Val Loss: 0.0072\n",
      "Epoch [6/50], Train Loss: 0.0337, Val Loss: 0.0047\n",
      "Epoch [7/50], Train Loss: 0.0328, Val Loss: 0.0163\n",
      "Epoch [8/50], Train Loss: 0.0194, Val Loss: 0.0339\n",
      "Epoch [9/50], Train Loss: 0.0161, Val Loss: 0.0053\n",
      "Epoch [10/50], Train Loss: 0.0134, Val Loss: 0.0050\n",
      "Epoch [11/50], Train Loss: 0.0135, Val Loss: 0.0290\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=16, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0295, Val Loss: 0.1212\n",
      "Epoch [2/50], Train Loss: 0.0587, Val Loss: 0.1190\n",
      "Epoch [3/50], Train Loss: 0.0520, Val Loss: 0.0712\n",
      "Epoch [4/50], Train Loss: 0.0244, Val Loss: 0.0809\n",
      "Epoch [5/50], Train Loss: 0.0699, Val Loss: 0.0299\n",
      "Epoch [6/50], Train Loss: 0.0703, Val Loss: 0.0552\n",
      "Epoch [7/50], Train Loss: 0.0656, Val Loss: 0.1398\n",
      "Epoch [8/50], Train Loss: 0.0313, Val Loss: 0.0531\n",
      "Epoch [9/50], Train Loss: 0.0292, Val Loss: 0.0688\n",
      "Epoch [10/50], Train Loss: 0.0309, Val Loss: 0.0874\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=16, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0457, Val Loss: 0.1034\n",
      "Epoch [2/50], Train Loss: 0.0589, Val Loss: 0.0622\n",
      "Epoch [3/50], Train Loss: 0.0502, Val Loss: 0.0305\n",
      "Epoch [4/50], Train Loss: 0.0332, Val Loss: 0.0161\n",
      "Epoch [5/50], Train Loss: 0.0257, Val Loss: 0.0082\n",
      "Epoch [6/50], Train Loss: 0.0253, Val Loss: 0.0093\n",
      "Epoch [7/50], Train Loss: 0.0218, Val Loss: 0.0106\n",
      "Epoch [8/50], Train Loss: 0.0242, Val Loss: 0.0042\n",
      "Epoch [9/50], Train Loss: 0.0221, Val Loss: 0.0135\n",
      "Epoch [10/50], Train Loss: 0.0183, Val Loss: 0.0120\n",
      "Epoch [11/50], Train Loss: 0.0223, Val Loss: 0.0064\n",
      "Epoch [12/50], Train Loss: 0.0123, Val Loss: 0.0109\n",
      "Epoch [13/50], Train Loss: 0.0100, Val Loss: 0.0128\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=16, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1047, Val Loss: 0.1111\n",
      "Epoch [2/50], Train Loss: 0.0784, Val Loss: 0.0887\n",
      "Epoch [3/50], Train Loss: 0.0561, Val Loss: 0.0535\n",
      "Epoch [4/50], Train Loss: 0.0478, Val Loss: 0.0260\n",
      "Epoch [5/50], Train Loss: 0.0518, Val Loss: 0.0239\n",
      "Epoch [6/50], Train Loss: 0.0390, Val Loss: 0.0254\n",
      "Epoch [7/50], Train Loss: 0.0297, Val Loss: 0.0231\n",
      "Epoch [8/50], Train Loss: 0.0263, Val Loss: 0.0172\n",
      "Epoch [9/50], Train Loss: 0.0275, Val Loss: 0.0117\n",
      "Epoch [10/50], Train Loss: 0.0220, Val Loss: 0.0135\n",
      "Epoch [11/50], Train Loss: 0.0160, Val Loss: 0.0029\n",
      "Epoch [12/50], Train Loss: 0.0421, Val Loss: 0.0560\n",
      "Epoch [13/50], Train Loss: 0.0448, Val Loss: 0.0327\n",
      "Epoch [14/50], Train Loss: 0.0285, Val Loss: 0.0304\n",
      "Epoch [15/50], Train Loss: 0.0308, Val Loss: 0.0947\n",
      "Epoch [16/50], Train Loss: 0.0299, Val Loss: 0.0264\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=32, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0191, Val Loss: 0.0789\n",
      "Epoch [2/50], Train Loss: 0.0389, Val Loss: 0.0557\n",
      "Epoch [3/50], Train Loss: 0.0249, Val Loss: 0.0471\n",
      "Epoch [4/50], Train Loss: 0.0284, Val Loss: 0.0121\n",
      "Epoch [5/50], Train Loss: 0.0183, Val Loss: 0.0099\n",
      "Epoch [6/50], Train Loss: 0.0077, Val Loss: 0.0073\n",
      "Epoch [7/50], Train Loss: 0.0041, Val Loss: 0.0022\n",
      "Epoch [8/50], Train Loss: 0.0026, Val Loss: 0.0065\n",
      "Epoch [9/50], Train Loss: 0.0018, Val Loss: 0.0025\n",
      "Epoch [10/50], Train Loss: 0.0017, Val Loss: 0.0019\n",
      "Epoch [11/50], Train Loss: 0.0015, Val Loss: 0.0013\n",
      "Epoch [12/50], Train Loss: 0.0017, Val Loss: 0.0040\n",
      "Epoch [13/50], Train Loss: 0.0014, Val Loss: 0.0016\n",
      "Epoch [14/50], Train Loss: 0.0015, Val Loss: 0.0017\n",
      "Epoch [15/50], Train Loss: 0.0014, Val Loss: 0.0014\n",
      "Epoch [16/50], Train Loss: 0.0017, Val Loss: 0.0037\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=32, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0367, Val Loss: 0.1073\n",
      "Epoch [2/50], Train Loss: 0.0819, Val Loss: 0.1428\n",
      "Epoch [3/50], Train Loss: 0.0382, Val Loss: 0.0236\n",
      "Epoch [4/50], Train Loss: 0.0387, Val Loss: 0.0280\n",
      "Epoch [5/50], Train Loss: 0.0258, Val Loss: 0.0444\n",
      "Epoch [6/50], Train Loss: 0.0099, Val Loss: 0.0077\n",
      "Epoch [7/50], Train Loss: 0.0089, Val Loss: 0.0044\n",
      "Epoch [8/50], Train Loss: 0.0064, Val Loss: 0.0028\n",
      "Epoch [9/50], Train Loss: 0.0053, Val Loss: 0.0026\n",
      "Epoch [10/50], Train Loss: 0.0053, Val Loss: 0.0036\n",
      "Epoch [11/50], Train Loss: 0.0045, Val Loss: 0.0040\n",
      "Epoch [12/50], Train Loss: 0.0046, Val Loss: 0.0067\n",
      "Epoch [13/50], Train Loss: 0.0039, Val Loss: 0.0024\n",
      "Epoch [14/50], Train Loss: 0.0041, Val Loss: 0.0045\n",
      "Epoch [15/50], Train Loss: 0.0039, Val Loss: 0.0026\n",
      "Epoch [16/50], Train Loss: 0.0043, Val Loss: 0.0039\n",
      "Epoch [17/50], Train Loss: 0.0039, Val Loss: 0.0042\n",
      "Epoch [18/50], Train Loss: 0.0036, Val Loss: 0.0023\n",
      "Epoch [19/50], Train Loss: 0.0038, Val Loss: 0.0021\n",
      "Epoch [20/50], Train Loss: 0.0037, Val Loss: 0.0014\n",
      "Epoch [21/50], Train Loss: 0.0036, Val Loss: 0.0033\n",
      "Epoch [22/50], Train Loss: 0.0032, Val Loss: 0.0014\n",
      "Epoch [23/50], Train Loss: 0.0032, Val Loss: 0.0012\n",
      "Epoch [24/50], Train Loss: 0.0033, Val Loss: 0.0028\n",
      "Epoch [25/50], Train Loss: 0.0031, Val Loss: 0.0016\n",
      "Epoch [26/50], Train Loss: 0.0032, Val Loss: 0.0024\n",
      "Epoch [27/50], Train Loss: 0.0031, Val Loss: 0.0021\n",
      "Epoch [28/50], Train Loss: 0.0030, Val Loss: 0.0033\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=32, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0410, Val Loss: 0.0456\n",
      "Epoch [2/50], Train Loss: 0.0454, Val Loss: 0.0310\n",
      "Epoch [3/50], Train Loss: 0.0279, Val Loss: 0.0179\n",
      "Epoch [4/50], Train Loss: 0.0250, Val Loss: 0.0127\n",
      "Epoch [5/50], Train Loss: 0.0238, Val Loss: 0.0107\n",
      "Epoch [6/50], Train Loss: 0.0168, Val Loss: 0.0077\n",
      "Epoch [7/50], Train Loss: 0.0093, Val Loss: 0.0032\n",
      "Epoch [8/50], Train Loss: 0.0084, Val Loss: 0.0060\n",
      "Epoch [9/50], Train Loss: 0.0070, Val Loss: 0.0026\n",
      "Epoch [10/50], Train Loss: 0.0062, Val Loss: 0.0022\n",
      "Epoch [11/50], Train Loss: 0.0077, Val Loss: 0.0063\n",
      "Epoch [12/50], Train Loss: 0.0068, Val Loss: 0.0023\n",
      "Epoch [13/50], Train Loss: 0.0053, Val Loss: 0.0027\n",
      "Epoch [14/50], Train Loss: 0.0052, Val Loss: 0.0051\n",
      "Epoch [15/50], Train Loss: 0.0054, Val Loss: 0.0014\n",
      "Epoch [16/50], Train Loss: 0.0060, Val Loss: 0.0021\n",
      "Epoch [17/50], Train Loss: 0.0050, Val Loss: 0.0024\n",
      "Epoch [18/50], Train Loss: 0.0060, Val Loss: 0.0042\n",
      "Epoch [19/50], Train Loss: 0.0055, Val Loss: 0.0031\n",
      "Epoch [20/50], Train Loss: 0.0059, Val Loss: 0.0074\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=32, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0183, Val Loss: 0.0626\n",
      "Epoch [2/50], Train Loss: 0.0433, Val Loss: 0.0642\n",
      "Epoch [3/50], Train Loss: 0.0421, Val Loss: 0.0264\n",
      "Epoch [4/50], Train Loss: 0.0375, Val Loss: 0.0046\n",
      "Epoch [5/50], Train Loss: 0.0303, Val Loss: 0.0073\n",
      "Epoch [6/50], Train Loss: 0.0251, Val Loss: 0.0071\n",
      "Epoch [7/50], Train Loss: 0.0129, Val Loss: 0.0027\n",
      "Epoch [8/50], Train Loss: 0.0083, Val Loss: 0.0068\n",
      "Epoch [9/50], Train Loss: 0.0269, Val Loss: 0.0040\n",
      "Epoch [10/50], Train Loss: 0.0254, Val Loss: 0.0811\n",
      "Epoch [11/50], Train Loss: 0.0124, Val Loss: 0.0095\n",
      "Epoch [12/50], Train Loss: 0.0177, Val Loss: 0.0041\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=32, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0263, Val Loss: 0.0366\n",
      "Epoch [2/50], Train Loss: 0.0553, Val Loss: 0.0549\n",
      "Epoch [3/50], Train Loss: 0.0392, Val Loss: 0.0164\n",
      "Epoch [4/50], Train Loss: 0.0456, Val Loss: 0.0262\n",
      "Epoch [5/50], Train Loss: 0.0360, Val Loss: 0.0082\n",
      "Epoch [6/50], Train Loss: 0.0303, Val Loss: 0.0117\n",
      "Epoch [7/50], Train Loss: 0.0206, Val Loss: 0.0078\n",
      "Epoch [8/50], Train Loss: 0.0132, Val Loss: 0.0105\n",
      "Epoch [9/50], Train Loss: 0.0106, Val Loss: 0.0074\n",
      "Epoch [10/50], Train Loss: 0.0110, Val Loss: 0.0088\n",
      "Epoch [11/50], Train Loss: 0.0115, Val Loss: 0.0122\n",
      "Epoch [12/50], Train Loss: 0.0239, Val Loss: 0.0109\n",
      "Epoch [13/50], Train Loss: 0.0091, Val Loss: 0.0035\n",
      "Epoch [14/50], Train Loss: 0.0080, Val Loss: 0.0150\n",
      "Epoch [15/50], Train Loss: 0.0073, Val Loss: 0.0030\n",
      "Epoch [16/50], Train Loss: 0.0069, Val Loss: 0.0018\n",
      "Epoch [17/50], Train Loss: 0.0102, Val Loss: 0.0156\n",
      "Epoch [18/50], Train Loss: 0.0077, Val Loss: 0.0068\n",
      "Epoch [19/50], Train Loss: 0.0184, Val Loss: 0.0145\n",
      "Epoch [20/50], Train Loss: 0.0112, Val Loss: 0.0191\n",
      "Epoch [21/50], Train Loss: 0.0217, Val Loss: 0.0102\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=32, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0382, Val Loss: 0.0310\n",
      "Epoch [2/50], Train Loss: 0.0518, Val Loss: 0.0480\n",
      "Epoch [3/50], Train Loss: 0.0431, Val Loss: 0.0211\n",
      "Epoch [4/50], Train Loss: 0.0332, Val Loss: 0.0216\n",
      "Epoch [5/50], Train Loss: 0.0226, Val Loss: 0.0057\n",
      "Epoch [6/50], Train Loss: 0.0271, Val Loss: 0.0060\n",
      "Epoch [7/50], Train Loss: 0.0329, Val Loss: 0.0145\n",
      "Epoch [8/50], Train Loss: 0.0340, Val Loss: 0.0048\n",
      "Epoch [9/50], Train Loss: 0.0193, Val Loss: 0.0030\n",
      "Epoch [10/50], Train Loss: 0.0118, Val Loss: 0.0047\n",
      "Epoch [11/50], Train Loss: 0.0253, Val Loss: 0.0127\n",
      "Epoch [12/50], Train Loss: 0.0142, Val Loss: 0.0188\n",
      "Epoch [13/50], Train Loss: 0.0125, Val Loss: 0.0109\n",
      "Epoch [14/50], Train Loss: 0.0136, Val Loss: 0.0032\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=32, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0233, Val Loss: 0.0619\n",
      "Epoch [2/50], Train Loss: 0.0389, Val Loss: 0.0320\n",
      "Epoch [3/50], Train Loss: 0.0690, Val Loss: 0.1153\n",
      "Epoch [4/50], Train Loss: 0.0441, Val Loss: 0.0850\n",
      "Epoch [5/50], Train Loss: 0.0440, Val Loss: 0.0572\n",
      "Epoch [6/50], Train Loss: 0.0375, Val Loss: 0.0667\n",
      "Epoch [7/50], Train Loss: 0.0249, Val Loss: 0.0134\n",
      "Epoch [8/50], Train Loss: 0.0195, Val Loss: 0.0037\n",
      "Epoch [9/50], Train Loss: 0.0286, Val Loss: 0.0102\n",
      "Epoch [10/50], Train Loss: 0.0320, Val Loss: 0.0284\n",
      "Epoch [11/50], Train Loss: 0.0240, Val Loss: 0.0069\n",
      "Epoch [12/50], Train Loss: 0.0114, Val Loss: 0.0644\n",
      "Epoch [13/50], Train Loss: 0.0288, Val Loss: 0.0413\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=32, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0275, Val Loss: 0.0594\n",
      "Epoch [2/50], Train Loss: 0.0480, Val Loss: 0.1022\n",
      "Epoch [3/50], Train Loss: 0.0502, Val Loss: 0.0985\n",
      "Epoch [4/50], Train Loss: 0.0415, Val Loss: 0.0908\n",
      "Epoch [5/50], Train Loss: 0.0474, Val Loss: 0.0557\n",
      "Epoch [6/50], Train Loss: 0.0371, Val Loss: 0.0379\n",
      "Epoch [7/50], Train Loss: 0.0310, Val Loss: 0.0505\n",
      "Epoch [8/50], Train Loss: 0.0420, Val Loss: 0.0244\n",
      "Epoch [9/50], Train Loss: 0.0446, Val Loss: 0.0580\n",
      "Epoch [10/50], Train Loss: 0.0434, Val Loss: 0.0352\n",
      "Epoch [11/50], Train Loss: 0.0238, Val Loss: 0.0041\n",
      "Epoch [12/50], Train Loss: 0.0191, Val Loss: 0.0100\n",
      "Epoch [13/50], Train Loss: 0.0355, Val Loss: 0.0252\n",
      "Epoch [14/50], Train Loss: 0.0193, Val Loss: 0.0149\n",
      "Epoch [15/50], Train Loss: 0.0103, Val Loss: 0.0201\n",
      "Epoch [16/50], Train Loss: 0.0128, Val Loss: 0.0195\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=32, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0394, Val Loss: 0.0410\n",
      "Epoch [2/50], Train Loss: 0.0572, Val Loss: 0.0613\n",
      "Epoch [3/50], Train Loss: 0.0528, Val Loss: 0.0851\n",
      "Epoch [4/50], Train Loss: 0.0478, Val Loss: 0.0196\n",
      "Epoch [5/50], Train Loss: 0.0350, Val Loss: 0.0090\n",
      "Epoch [6/50], Train Loss: 0.0259, Val Loss: 0.0173\n",
      "Epoch [7/50], Train Loss: 0.0236, Val Loss: 0.0129\n",
      "Epoch [8/50], Train Loss: 0.0301, Val Loss: 0.0200\n",
      "Epoch [9/50], Train Loss: 0.0394, Val Loss: 0.0464\n",
      "Epoch [10/50], Train Loss: 0.0304, Val Loss: 0.0238\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=64, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0164, Val Loss: 0.0224\n",
      "Epoch [2/50], Train Loss: 0.0291, Val Loss: 0.0383\n",
      "Epoch [3/50], Train Loss: 0.0330, Val Loss: 0.0406\n",
      "Epoch [4/50], Train Loss: 0.0360, Val Loss: 0.0231\n",
      "Epoch [5/50], Train Loss: 0.0316, Val Loss: 0.0186\n",
      "Epoch [6/50], Train Loss: 0.0247, Val Loss: 0.0076\n",
      "Epoch [7/50], Train Loss: 0.0214, Val Loss: 0.0142\n",
      "Epoch [8/50], Train Loss: 0.0106, Val Loss: 0.0078\n",
      "Epoch [9/50], Train Loss: 0.0064, Val Loss: 0.0053\n",
      "Epoch [10/50], Train Loss: 0.0044, Val Loss: 0.0049\n",
      "Epoch [11/50], Train Loss: 0.0057, Val Loss: 0.0058\n",
      "Epoch [12/50], Train Loss: 0.0038, Val Loss: 0.0104\n",
      "Epoch [13/50], Train Loss: 0.0027, Val Loss: 0.0032\n",
      "Epoch [14/50], Train Loss: 0.0034, Val Loss: 0.0047\n",
      "Epoch [15/50], Train Loss: 0.0024, Val Loss: 0.0026\n",
      "Epoch [16/50], Train Loss: 0.0022, Val Loss: 0.0050\n",
      "Epoch [17/50], Train Loss: 0.0025, Val Loss: 0.0053\n",
      "Epoch [18/50], Train Loss: 0.0021, Val Loss: 0.0021\n",
      "Epoch [19/50], Train Loss: 0.0022, Val Loss: 0.0037\n",
      "Epoch [20/50], Train Loss: 0.0018, Val Loss: 0.0027\n",
      "Epoch [21/50], Train Loss: 0.0022, Val Loss: 0.0030\n",
      "Epoch [22/50], Train Loss: 0.0018, Val Loss: 0.0012\n",
      "Epoch [23/50], Train Loss: 0.0021, Val Loss: 0.0042\n",
      "Epoch [24/50], Train Loss: 0.0017, Val Loss: 0.0018\n",
      "Epoch [25/50], Train Loss: 0.0021, Val Loss: 0.0027\n",
      "Epoch [26/50], Train Loss: 0.0016, Val Loss: 0.0011\n",
      "Epoch [27/50], Train Loss: 0.0023, Val Loss: 0.0051\n",
      "Epoch [28/50], Train Loss: 0.0015, Val Loss: 0.0014\n",
      "Epoch [29/50], Train Loss: 0.0021, Val Loss: 0.0036\n",
      "Epoch [30/50], Train Loss: 0.0018, Val Loss: 0.0013\n",
      "Epoch [31/50], Train Loss: 0.0025, Val Loss: 0.0070\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=64, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0178, Val Loss: 0.0423\n",
      "Epoch [2/50], Train Loss: 0.0342, Val Loss: 0.0235\n",
      "Epoch [3/50], Train Loss: 0.0281, Val Loss: 0.0213\n",
      "Epoch [4/50], Train Loss: 0.0237, Val Loss: 0.0425\n",
      "Epoch [5/50], Train Loss: 0.0228, Val Loss: 0.0137\n",
      "Epoch [6/50], Train Loss: 0.0297, Val Loss: 0.0201\n",
      "Epoch [7/50], Train Loss: 0.0143, Val Loss: 0.0131\n",
      "Epoch [8/50], Train Loss: 0.0096, Val Loss: 0.0115\n",
      "Epoch [9/50], Train Loss: 0.0052, Val Loss: 0.0013\n",
      "Epoch [10/50], Train Loss: 0.0048, Val Loss: 0.0079\n",
      "Epoch [11/50], Train Loss: 0.0058, Val Loss: 0.0102\n",
      "Epoch [12/50], Train Loss: 0.0035, Val Loss: 0.0032\n",
      "Epoch [13/50], Train Loss: 0.0038, Val Loss: 0.0101\n",
      "Epoch [14/50], Train Loss: 0.0027, Val Loss: 0.0010\n",
      "Epoch [15/50], Train Loss: 0.0027, Val Loss: 0.0039\n",
      "Epoch [16/50], Train Loss: 0.0024, Val Loss: 0.0023\n",
      "Epoch [17/50], Train Loss: 0.0025, Val Loss: 0.0038\n",
      "Epoch [18/50], Train Loss: 0.0027, Val Loss: 0.0043\n",
      "Epoch [19/50], Train Loss: 0.0028, Val Loss: 0.0023\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=64, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0269, Val Loss: 0.0576\n",
      "Epoch [2/50], Train Loss: 0.0310, Val Loss: 0.0203\n",
      "Epoch [3/50], Train Loss: 0.0812, Val Loss: 0.0086\n",
      "Epoch [4/50], Train Loss: 0.0314, Val Loss: 0.0107\n",
      "Epoch [5/50], Train Loss: 0.0224, Val Loss: 0.0210\n",
      "Epoch [6/50], Train Loss: 0.0212, Val Loss: 0.0056\n",
      "Epoch [7/50], Train Loss: 0.0122, Val Loss: 0.0198\n",
      "Epoch [8/50], Train Loss: 0.0070, Val Loss: 0.0025\n",
      "Epoch [9/50], Train Loss: 0.0079, Val Loss: 0.0039\n",
      "Epoch [10/50], Train Loss: 0.0055, Val Loss: 0.0040\n",
      "Epoch [11/50], Train Loss: 0.0053, Val Loss: 0.0063\n",
      "Epoch [12/50], Train Loss: 0.0047, Val Loss: 0.0032\n",
      "Epoch [13/50], Train Loss: 0.0051, Val Loss: 0.0019\n",
      "Epoch [14/50], Train Loss: 0.0052, Val Loss: 0.0034\n",
      "Epoch [15/50], Train Loss: 0.0040, Val Loss: 0.0023\n",
      "Epoch [16/50], Train Loss: 0.0044, Val Loss: 0.0024\n",
      "Epoch [17/50], Train Loss: 0.0043, Val Loss: 0.0026\n",
      "Epoch [18/50], Train Loss: 0.0041, Val Loss: 0.0036\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=64, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0193, Val Loss: 0.0321\n",
      "Epoch [2/50], Train Loss: 0.0426, Val Loss: 0.0363\n",
      "Epoch [3/50], Train Loss: 0.0302, Val Loss: 0.0102\n",
      "Epoch [4/50], Train Loss: 0.0248, Val Loss: 0.0441\n",
      "Epoch [5/50], Train Loss: 0.0360, Val Loss: 0.0048\n",
      "Epoch [6/50], Train Loss: 0.0386, Val Loss: 0.0196\n",
      "Epoch [7/50], Train Loss: 0.0267, Val Loss: 0.0061\n",
      "Epoch [8/50], Train Loss: 0.0298, Val Loss: 0.0045\n",
      "Epoch [9/50], Train Loss: 0.0101, Val Loss: 0.0120\n",
      "Epoch [10/50], Train Loss: 0.0232, Val Loss: 0.0041\n",
      "Epoch [11/50], Train Loss: 0.0056, Val Loss: 0.0044\n",
      "Epoch [12/50], Train Loss: 0.0078, Val Loss: 0.0036\n",
      "Epoch [13/50], Train Loss: 0.0047, Val Loss: 0.0023\n",
      "Epoch [14/50], Train Loss: 0.0104, Val Loss: 0.0061\n",
      "Epoch [15/50], Train Loss: 0.0066, Val Loss: 0.0111\n",
      "Epoch [16/50], Train Loss: 0.0249, Val Loss: 0.0106\n",
      "Epoch [17/50], Train Loss: 0.0187, Val Loss: 0.0080\n",
      "Epoch [18/50], Train Loss: 0.0103, Val Loss: 0.0076\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=64, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0236, Val Loss: 0.0319\n",
      "Epoch [2/50], Train Loss: 0.0331, Val Loss: 0.0437\n",
      "Epoch [3/50], Train Loss: 0.0346, Val Loss: 0.0357\n",
      "Epoch [4/50], Train Loss: 0.0347, Val Loss: 0.0174\n",
      "Epoch [5/50], Train Loss: 0.0296, Val Loss: 0.0068\n",
      "Epoch [6/50], Train Loss: 0.0320, Val Loss: 0.0079\n",
      "Epoch [7/50], Train Loss: 0.0314, Val Loss: 0.0167\n",
      "Epoch [8/50], Train Loss: 0.0237, Val Loss: 0.0183\n",
      "Epoch [9/50], Train Loss: 0.0296, Val Loss: 0.0061\n",
      "Epoch [10/50], Train Loss: 0.0205, Val Loss: 0.0093\n",
      "Epoch [11/50], Train Loss: 0.0108, Val Loss: 0.0037\n",
      "Epoch [12/50], Train Loss: 0.0078, Val Loss: 0.0031\n",
      "Epoch [13/50], Train Loss: 0.0206, Val Loss: 0.0172\n",
      "Epoch [14/50], Train Loss: 0.0235, Val Loss: 0.0391\n",
      "Epoch [15/50], Train Loss: 0.0210, Val Loss: 0.0279\n",
      "Epoch [16/50], Train Loss: 0.0212, Val Loss: 0.0552\n",
      "Epoch [17/50], Train Loss: 0.0136, Val Loss: 0.0170\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=64, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0292, Val Loss: 0.0592\n",
      "Epoch [2/50], Train Loss: 0.0369, Val Loss: 0.0233\n",
      "Epoch [3/50], Train Loss: 0.0995, Val Loss: 0.1236\n",
      "Epoch [4/50], Train Loss: 0.0488, Val Loss: 0.0317\n",
      "Epoch [5/50], Train Loss: 0.0845, Val Loss: 0.0067\n",
      "Epoch [6/50], Train Loss: 0.0749, Val Loss: 0.0530\n",
      "Epoch [7/50], Train Loss: 0.0573, Val Loss: 0.1692\n",
      "Epoch [8/50], Train Loss: 0.0401, Val Loss: 0.1436\n",
      "Epoch [9/50], Train Loss: 0.0418, Val Loss: 0.1787\n",
      "Epoch [10/50], Train Loss: 0.0398, Val Loss: 0.1551\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=64, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0235, Val Loss: 0.0228\n",
      "Epoch [2/50], Train Loss: 0.0373, Val Loss: 0.0535\n",
      "Epoch [3/50], Train Loss: 0.0305, Val Loss: 0.0581\n",
      "Epoch [4/50], Train Loss: 0.0590, Val Loss: 0.0587\n",
      "Epoch [5/50], Train Loss: 0.0247, Val Loss: 0.0589\n",
      "Epoch [6/50], Train Loss: 0.0275, Val Loss: 0.0410\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=64, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0269, Val Loss: 0.0546\n",
      "Epoch [2/50], Train Loss: 0.0345, Val Loss: 0.0607\n",
      "Epoch [3/50], Train Loss: 0.0389, Val Loss: 0.0226\n",
      "Epoch [4/50], Train Loss: 0.1023, Val Loss: 0.0303\n",
      "Epoch [5/50], Train Loss: 0.0711, Val Loss: 0.0453\n",
      "Epoch [6/50], Train Loss: 0.0602, Val Loss: 0.0799\n",
      "Epoch [7/50], Train Loss: 0.0516, Val Loss: 0.1008\n",
      "Epoch [8/50], Train Loss: 0.0501, Val Loss: 0.0883\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=64, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0321, Val Loss: 0.0445\n",
      "Epoch [2/50], Train Loss: 0.0456, Val Loss: 0.0446\n",
      "Epoch [3/50], Train Loss: 0.0493, Val Loss: 0.0401\n",
      "Epoch [4/50], Train Loss: 0.0488, Val Loss: 0.0486\n",
      "Epoch [5/50], Train Loss: 0.0687, Val Loss: 0.0459\n",
      "Epoch [6/50], Train Loss: 0.0517, Val Loss: 0.0556\n",
      "Epoch [7/50], Train Loss: 0.0704, Val Loss: 0.0202\n",
      "Epoch [8/50], Train Loss: 0.0598, Val Loss: 0.1315\n",
      "Epoch [9/50], Train Loss: 0.0391, Val Loss: 0.0765\n",
      "Epoch [10/50], Train Loss: 0.0408, Val Loss: 0.0439\n",
      "Epoch [11/50], Train Loss: 0.0488, Val Loss: 0.0462\n",
      "Epoch [12/50], Train Loss: 0.0554, Val Loss: 0.0485\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=128, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0156, Val Loss: 0.0418\n",
      "Epoch [2/50], Train Loss: 0.0208, Val Loss: 0.0141\n",
      "Epoch [3/50], Train Loss: 0.0192, Val Loss: 0.0242\n",
      "Epoch [4/50], Train Loss: 0.0975, Val Loss: 0.2139\n",
      "Epoch [5/50], Train Loss: 0.0948, Val Loss: 0.0623\n",
      "Epoch [6/50], Train Loss: 0.0565, Val Loss: 0.0087\n",
      "Epoch [7/50], Train Loss: 0.0880, Val Loss: 0.0595\n",
      "Epoch [8/50], Train Loss: 0.0563, Val Loss: 0.0691\n",
      "Epoch [9/50], Train Loss: 0.0504, Val Loss: 0.0682\n",
      "Epoch [10/50], Train Loss: 0.0525, Val Loss: 0.0706\n",
      "Epoch [11/50], Train Loss: 0.0547, Val Loss: 0.0695\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=128, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1098, Val Loss: 0.0758\n",
      "Epoch [2/50], Train Loss: 0.0829, Val Loss: 0.0544\n",
      "Epoch [3/50], Train Loss: 0.0337, Val Loss: 0.0443\n",
      "Epoch [4/50], Train Loss: 0.0391, Val Loss: 0.0192\n",
      "Epoch [5/50], Train Loss: 0.0213, Val Loss: 0.0308\n",
      "Epoch [6/50], Train Loss: 0.0295, Val Loss: 0.0278\n",
      "Epoch [7/50], Train Loss: 0.0149, Val Loss: 0.0263\n",
      "Epoch [8/50], Train Loss: 0.0065, Val Loss: 0.0074\n",
      "Epoch [9/50], Train Loss: 0.0072, Val Loss: 0.0088\n",
      "Epoch [10/50], Train Loss: 0.0045, Val Loss: 0.0101\n",
      "Epoch [11/50], Train Loss: 0.0029, Val Loss: 0.0014\n",
      "Epoch [12/50], Train Loss: 0.0031, Val Loss: 0.0021\n",
      "Epoch [13/50], Train Loss: 0.0027, Val Loss: 0.0025\n",
      "Epoch [14/50], Train Loss: 0.0029, Val Loss: 0.0048\n",
      "Epoch [15/50], Train Loss: 0.0023, Val Loss: 0.0015\n",
      "Epoch [16/50], Train Loss: 0.0025, Val Loss: 0.0018\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=128, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0203, Val Loss: 0.2134\n",
      "Epoch [2/50], Train Loss: 0.1370, Val Loss: 0.0165\n",
      "Epoch [3/50], Train Loss: 0.2644, Val Loss: 0.1347\n",
      "Epoch [4/50], Train Loss: 0.1076, Val Loss: 0.0175\n",
      "Epoch [5/50], Train Loss: 0.1858, Val Loss: 0.1853\n",
      "Epoch [6/50], Train Loss: 0.1017, Val Loss: 0.1082\n",
      "Epoch [7/50], Train Loss: 0.0613, Val Loss: 0.0717\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=128, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0823, Val Loss: 0.2210\n",
      "Epoch [2/50], Train Loss: 0.2800, Val Loss: 0.0228\n",
      "Epoch [3/50], Train Loss: 0.1611, Val Loss: 0.0532\n",
      "Epoch [4/50], Train Loss: 0.1179, Val Loss: 0.0732\n",
      "Epoch [5/50], Train Loss: 0.0596, Val Loss: 0.0570\n",
      "Epoch [6/50], Train Loss: 0.0680, Val Loss: 0.0498\n",
      "Epoch [7/50], Train Loss: 0.0730, Val Loss: 0.0549\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=128, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0466, Val Loss: 0.2310\n",
      "Epoch [2/50], Train Loss: 0.0579, Val Loss: 0.0642\n",
      "Epoch [3/50], Train Loss: 0.0471, Val Loss: 0.0468\n",
      "Epoch [4/50], Train Loss: 0.0363, Val Loss: 0.0668\n",
      "Epoch [5/50], Train Loss: 0.0395, Val Loss: 0.0825\n",
      "Epoch [6/50], Train Loss: 0.0345, Val Loss: 0.0558\n",
      "Epoch [7/50], Train Loss: 0.0422, Val Loss: 0.0727\n",
      "Epoch [8/50], Train Loss: 0.4133, Val Loss: 0.4139\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=128, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0320, Val Loss: 0.0367\n",
      "Epoch [2/50], Train Loss: 0.1927, Val Loss: 0.1093\n",
      "Epoch [3/50], Train Loss: 0.2600, Val Loss: 0.0858\n",
      "Epoch [4/50], Train Loss: 0.1116, Val Loss: 0.0380\n",
      "Epoch [5/50], Train Loss: 0.0869, Val Loss: 0.0603\n",
      "Epoch [6/50], Train Loss: 0.0707, Val Loss: 0.0605\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=128, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.2517, Val Loss: 0.1147\n",
      "Epoch [2/50], Train Loss: 0.3418, Val Loss: 0.2984\n",
      "Epoch [3/50], Train Loss: 0.1513, Val Loss: 0.0534\n",
      "Epoch [4/50], Train Loss: 0.0669, Val Loss: 0.1168\n",
      "Epoch [5/50], Train Loss: 0.0602, Val Loss: 0.1205\n",
      "Epoch [6/50], Train Loss: 0.0571, Val Loss: 0.1015\n",
      "Epoch [7/50], Train Loss: 0.0586, Val Loss: 0.0853\n",
      "Epoch [8/50], Train Loss: 0.0627, Val Loss: 0.0749\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=128, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.2994, Val Loss: 0.0747\n",
      "Epoch [2/50], Train Loss: 0.0916, Val Loss: 0.0452\n",
      "Epoch [3/50], Train Loss: 0.0856, Val Loss: 0.0611\n",
      "Epoch [4/50], Train Loss: 0.0547, Val Loss: 0.0316\n",
      "Epoch [5/50], Train Loss: 0.0743, Val Loss: 0.0513\n",
      "Epoch [6/50], Train Loss: 0.0715, Val Loss: 0.0595\n",
      "Epoch [7/50], Train Loss: 0.0612, Val Loss: 0.0597\n",
      "Epoch [8/50], Train Loss: 0.0594, Val Loss: 0.0624\n",
      "Epoch [9/50], Train Loss: 0.0609, Val Loss: 0.0675\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=128, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0270, Val Loss: 0.0319\n",
      "Epoch [2/50], Train Loss: 0.0429, Val Loss: 0.0450\n",
      "Epoch [3/50], Train Loss: 0.0358, Val Loss: 0.0363\n",
      "Epoch [4/50], Train Loss: 0.0368, Val Loss: 0.0260\n",
      "Epoch [5/50], Train Loss: 0.0552, Val Loss: 0.0284\n",
      "Epoch [6/50], Train Loss: 0.1111, Val Loss: 0.0894\n",
      "Epoch [7/50], Train Loss: 0.0674, Val Loss: 0.0186\n",
      "Epoch [8/50], Train Loss: 0.0599, Val Loss: 0.0180\n",
      "Epoch [9/50], Train Loss: 0.0665, Val Loss: 0.0228\n",
      "Epoch [10/50], Train Loss: 0.0577, Val Loss: 0.0380\n",
      "Epoch [11/50], Train Loss: 0.0514, Val Loss: 0.0478\n",
      "Epoch [12/50], Train Loss: 0.0499, Val Loss: 0.0407\n",
      "Epoch [13/50], Train Loss: 0.0510, Val Loss: 0.0382\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=16, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1608, Val Loss: 0.3616\n",
      "Epoch [2/50], Train Loss: 0.1127, Val Loss: 0.2902\n",
      "Epoch [3/50], Train Loss: 0.0827, Val Loss: 0.2388\n",
      "Epoch [4/50], Train Loss: 0.0636, Val Loss: 0.2013\n",
      "Epoch [5/50], Train Loss: 0.0516, Val Loss: 0.1735\n",
      "Epoch [6/50], Train Loss: 0.0440, Val Loss: 0.1528\n",
      "Epoch [7/50], Train Loss: 0.0392, Val Loss: 0.1373\n",
      "Epoch [8/50], Train Loss: 0.0362, Val Loss: 0.1256\n",
      "Epoch [9/50], Train Loss: 0.0344, Val Loss: 0.1167\n",
      "Epoch [10/50], Train Loss: 0.0332, Val Loss: 0.1098\n",
      "Epoch [11/50], Train Loss: 0.0324, Val Loss: 0.1044\n",
      "Epoch [12/50], Train Loss: 0.0319, Val Loss: 0.1002\n",
      "Epoch [13/50], Train Loss: 0.0315, Val Loss: 0.0968\n",
      "Epoch [14/50], Train Loss: 0.0312, Val Loss: 0.0941\n",
      "Epoch [15/50], Train Loss: 0.0309, Val Loss: 0.0919\n",
      "Epoch [16/50], Train Loss: 0.0307, Val Loss: 0.0900\n",
      "Epoch [17/50], Train Loss: 0.0305, Val Loss: 0.0884\n",
      "Epoch [18/50], Train Loss: 0.0303, Val Loss: 0.0871\n",
      "Epoch [19/50], Train Loss: 0.0301, Val Loss: 0.0859\n",
      "Epoch [20/50], Train Loss: 0.0299, Val Loss: 0.0848\n",
      "Epoch [21/50], Train Loss: 0.0297, Val Loss: 0.0839\n",
      "Epoch [22/50], Train Loss: 0.0295, Val Loss: 0.0830\n",
      "Epoch [23/50], Train Loss: 0.0293, Val Loss: 0.0821\n",
      "Epoch [24/50], Train Loss: 0.0291, Val Loss: 0.0814\n",
      "Epoch [25/50], Train Loss: 0.0289, Val Loss: 0.0806\n",
      "Epoch [26/50], Train Loss: 0.0287, Val Loss: 0.0799\n",
      "Epoch [27/50], Train Loss: 0.0285, Val Loss: 0.0793\n",
      "Epoch [28/50], Train Loss: 0.0284, Val Loss: 0.0786\n",
      "Epoch [29/50], Train Loss: 0.0282, Val Loss: 0.0779\n",
      "Epoch [30/50], Train Loss: 0.0280, Val Loss: 0.0773\n",
      "Epoch [31/50], Train Loss: 0.0278, Val Loss: 0.0767\n",
      "Epoch [32/50], Train Loss: 0.0276, Val Loss: 0.0761\n",
      "Epoch [33/50], Train Loss: 0.0274, Val Loss: 0.0755\n",
      "Epoch [34/50], Train Loss: 0.0272, Val Loss: 0.0749\n",
      "Epoch [35/50], Train Loss: 0.0270, Val Loss: 0.0743\n",
      "Epoch [36/50], Train Loss: 0.0268, Val Loss: 0.0737\n",
      "Epoch [37/50], Train Loss: 0.0266, Val Loss: 0.0731\n",
      "Epoch [38/50], Train Loss: 0.0264, Val Loss: 0.0726\n",
      "Epoch [39/50], Train Loss: 0.0262, Val Loss: 0.0720\n",
      "Epoch [40/50], Train Loss: 0.0261, Val Loss: 0.0714\n",
      "Epoch [41/50], Train Loss: 0.0259, Val Loss: 0.0708\n",
      "Epoch [42/50], Train Loss: 0.0257, Val Loss: 0.0702\n",
      "Epoch [43/50], Train Loss: 0.0255, Val Loss: 0.0697\n",
      "Epoch [44/50], Train Loss: 0.0253, Val Loss: 0.0691\n",
      "Epoch [45/50], Train Loss: 0.0251, Val Loss: 0.0685\n",
      "Epoch [46/50], Train Loss: 0.0249, Val Loss: 0.0680\n",
      "Epoch [47/50], Train Loss: 0.0247, Val Loss: 0.0674\n",
      "Epoch [48/50], Train Loss: 0.0245, Val Loss: 0.0668\n",
      "Epoch [49/50], Train Loss: 0.0243, Val Loss: 0.0663\n",
      "Epoch [50/50], Train Loss: 0.0242, Val Loss: 0.0657\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=16, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1729, Val Loss: 0.4332\n",
      "Epoch [2/50], Train Loss: 0.1235, Val Loss: 0.3462\n",
      "Epoch [3/50], Train Loss: 0.0929, Val Loss: 0.2848\n",
      "Epoch [4/50], Train Loss: 0.0746, Val Loss: 0.2404\n",
      "Epoch [5/50], Train Loss: 0.0621, Val Loss: 0.2066\n",
      "Epoch [6/50], Train Loss: 0.0525, Val Loss: 0.1812\n",
      "Epoch [7/50], Train Loss: 0.0492, Val Loss: 0.1620\n",
      "Epoch [8/50], Train Loss: 0.0462, Val Loss: 0.1474\n",
      "Epoch [9/50], Train Loss: 0.0420, Val Loss: 0.1357\n",
      "Epoch [10/50], Train Loss: 0.0408, Val Loss: 0.1267\n",
      "Epoch [11/50], Train Loss: 0.0399, Val Loss: 0.1196\n",
      "Epoch [12/50], Train Loss: 0.0390, Val Loss: 0.1141\n",
      "Epoch [13/50], Train Loss: 0.0383, Val Loss: 0.1098\n",
      "Epoch [14/50], Train Loss: 0.0381, Val Loss: 0.1062\n",
      "Epoch [15/50], Train Loss: 0.0376, Val Loss: 0.1034\n",
      "Epoch [16/50], Train Loss: 0.0370, Val Loss: 0.1009\n",
      "Epoch [17/50], Train Loss: 0.0372, Val Loss: 0.0984\n",
      "Epoch [18/50], Train Loss: 0.0355, Val Loss: 0.0962\n",
      "Epoch [19/50], Train Loss: 0.0359, Val Loss: 0.0943\n",
      "Epoch [20/50], Train Loss: 0.0350, Val Loss: 0.0930\n",
      "Epoch [21/50], Train Loss: 0.0351, Val Loss: 0.0914\n",
      "Epoch [22/50], Train Loss: 0.0340, Val Loss: 0.0898\n",
      "Epoch [23/50], Train Loss: 0.0338, Val Loss: 0.0883\n",
      "Epoch [24/50], Train Loss: 0.0337, Val Loss: 0.0871\n",
      "Epoch [25/50], Train Loss: 0.0349, Val Loss: 0.0856\n",
      "Epoch [26/50], Train Loss: 0.0336, Val Loss: 0.0842\n",
      "Epoch [27/50], Train Loss: 0.0326, Val Loss: 0.0834\n",
      "Epoch [28/50], Train Loss: 0.0322, Val Loss: 0.0821\n",
      "Epoch [29/50], Train Loss: 0.0315, Val Loss: 0.0807\n",
      "Epoch [30/50], Train Loss: 0.0312, Val Loss: 0.0796\n",
      "Epoch [31/50], Train Loss: 0.0307, Val Loss: 0.0784\n",
      "Epoch [32/50], Train Loss: 0.0311, Val Loss: 0.0772\n",
      "Epoch [33/50], Train Loss: 0.0308, Val Loss: 0.0764\n",
      "Epoch [34/50], Train Loss: 0.0301, Val Loss: 0.0750\n",
      "Epoch [35/50], Train Loss: 0.0297, Val Loss: 0.0740\n",
      "Epoch [36/50], Train Loss: 0.0290, Val Loss: 0.0730\n",
      "Epoch [37/50], Train Loss: 0.0291, Val Loss: 0.0720\n",
      "Epoch [38/50], Train Loss: 0.0284, Val Loss: 0.0706\n",
      "Epoch [39/50], Train Loss: 0.0281, Val Loss: 0.0693\n",
      "Epoch [40/50], Train Loss: 0.0285, Val Loss: 0.0682\n",
      "Epoch [41/50], Train Loss: 0.0279, Val Loss: 0.0669\n",
      "Epoch [42/50], Train Loss: 0.0279, Val Loss: 0.0658\n",
      "Epoch [43/50], Train Loss: 0.0276, Val Loss: 0.0646\n",
      "Epoch [44/50], Train Loss: 0.0274, Val Loss: 0.0636\n",
      "Epoch [45/50], Train Loss: 0.0269, Val Loss: 0.0625\n",
      "Epoch [46/50], Train Loss: 0.0262, Val Loss: 0.0612\n",
      "Epoch [47/50], Train Loss: 0.0252, Val Loss: 0.0600\n",
      "Epoch [48/50], Train Loss: 0.0251, Val Loss: 0.0593\n",
      "Epoch [49/50], Train Loss: 0.0250, Val Loss: 0.0582\n",
      "Epoch [50/50], Train Loss: 0.0241, Val Loss: 0.0574\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=16, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0920, Val Loss: 0.1741\n",
      "Epoch [2/50], Train Loss: 0.0788, Val Loss: 0.1498\n",
      "Epoch [3/50], Train Loss: 0.0686, Val Loss: 0.1310\n",
      "Epoch [4/50], Train Loss: 0.0622, Val Loss: 0.1164\n",
      "Epoch [5/50], Train Loss: 0.0602, Val Loss: 0.1045\n",
      "Epoch [6/50], Train Loss: 0.0549, Val Loss: 0.0955\n",
      "Epoch [7/50], Train Loss: 0.0522, Val Loss: 0.0880\n",
      "Epoch [8/50], Train Loss: 0.0513, Val Loss: 0.0823\n",
      "Epoch [9/50], Train Loss: 0.0470, Val Loss: 0.0776\n",
      "Epoch [10/50], Train Loss: 0.0462, Val Loss: 0.0734\n",
      "Epoch [11/50], Train Loss: 0.0468, Val Loss: 0.0699\n",
      "Epoch [12/50], Train Loss: 0.0448, Val Loss: 0.0669\n",
      "Epoch [13/50], Train Loss: 0.0456, Val Loss: 0.0644\n",
      "Epoch [14/50], Train Loss: 0.0442, Val Loss: 0.0626\n",
      "Epoch [15/50], Train Loss: 0.0435, Val Loss: 0.0609\n",
      "Epoch [16/50], Train Loss: 0.0408, Val Loss: 0.0590\n",
      "Epoch [17/50], Train Loss: 0.0416, Val Loss: 0.0575\n",
      "Epoch [18/50], Train Loss: 0.0416, Val Loss: 0.0557\n",
      "Epoch [19/50], Train Loss: 0.0399, Val Loss: 0.0543\n",
      "Epoch [20/50], Train Loss: 0.0392, Val Loss: 0.0530\n",
      "Epoch [21/50], Train Loss: 0.0392, Val Loss: 0.0518\n",
      "Epoch [22/50], Train Loss: 0.0392, Val Loss: 0.0508\n",
      "Epoch [23/50], Train Loss: 0.0379, Val Loss: 0.0501\n",
      "Epoch [24/50], Train Loss: 0.0378, Val Loss: 0.0495\n",
      "Epoch [25/50], Train Loss: 0.0369, Val Loss: 0.0488\n",
      "Epoch [26/50], Train Loss: 0.0360, Val Loss: 0.0475\n",
      "Epoch [27/50], Train Loss: 0.0364, Val Loss: 0.0468\n",
      "Epoch [28/50], Train Loss: 0.0348, Val Loss: 0.0460\n",
      "Epoch [29/50], Train Loss: 0.0360, Val Loss: 0.0454\n",
      "Epoch [30/50], Train Loss: 0.0361, Val Loss: 0.0449\n",
      "Epoch [31/50], Train Loss: 0.0345, Val Loss: 0.0443\n",
      "Epoch [32/50], Train Loss: 0.0344, Val Loss: 0.0435\n",
      "Epoch [33/50], Train Loss: 0.0339, Val Loss: 0.0423\n",
      "Epoch [34/50], Train Loss: 0.0336, Val Loss: 0.0420\n",
      "Epoch [35/50], Train Loss: 0.0323, Val Loss: 0.0414\n",
      "Epoch [36/50], Train Loss: 0.0329, Val Loss: 0.0405\n",
      "Epoch [37/50], Train Loss: 0.0324, Val Loss: 0.0401\n",
      "Epoch [38/50], Train Loss: 0.0319, Val Loss: 0.0395\n",
      "Epoch [39/50], Train Loss: 0.0306, Val Loss: 0.0387\n",
      "Epoch [40/50], Train Loss: 0.0310, Val Loss: 0.0380\n",
      "Epoch [41/50], Train Loss: 0.0306, Val Loss: 0.0377\n",
      "Epoch [42/50], Train Loss: 0.0297, Val Loss: 0.0372\n",
      "Epoch [43/50], Train Loss: 0.0301, Val Loss: 0.0366\n",
      "Epoch [44/50], Train Loss: 0.0293, Val Loss: 0.0359\n",
      "Epoch [45/50], Train Loss: 0.0289, Val Loss: 0.0357\n",
      "Epoch [46/50], Train Loss: 0.0302, Val Loss: 0.0349\n",
      "Epoch [47/50], Train Loss: 0.0276, Val Loss: 0.0344\n",
      "Epoch [48/50], Train Loss: 0.0279, Val Loss: 0.0337\n",
      "Epoch [49/50], Train Loss: 0.0281, Val Loss: 0.0329\n",
      "Epoch [50/50], Train Loss: 0.0276, Val Loss: 0.0324\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=16, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.2119, Val Loss: 0.3879\n",
      "Epoch [2/50], Train Loss: 0.1405, Val Loss: 0.3026\n",
      "Epoch [3/50], Train Loss: 0.1011, Val Loss: 0.2461\n",
      "Epoch [4/50], Train Loss: 0.0772, Val Loss: 0.2063\n",
      "Epoch [5/50], Train Loss: 0.0621, Val Loss: 0.1772\n",
      "Epoch [6/50], Train Loss: 0.0523, Val Loss: 0.1557\n",
      "Epoch [7/50], Train Loss: 0.0459, Val Loss: 0.1397\n",
      "Epoch [8/50], Train Loss: 0.0418, Val Loss: 0.1276\n",
      "Epoch [9/50], Train Loss: 0.0391, Val Loss: 0.1185\n",
      "Epoch [10/50], Train Loss: 0.0373, Val Loss: 0.1116\n",
      "Epoch [11/50], Train Loss: 0.0361, Val Loss: 0.1062\n",
      "Epoch [12/50], Train Loss: 0.0353, Val Loss: 0.1021\n",
      "Epoch [13/50], Train Loss: 0.0347, Val Loss: 0.0988\n",
      "Epoch [14/50], Train Loss: 0.0343, Val Loss: 0.0962\n",
      "Epoch [15/50], Train Loss: 0.0339, Val Loss: 0.0940\n",
      "Epoch [16/50], Train Loss: 0.0336, Val Loss: 0.0922\n",
      "Epoch [17/50], Train Loss: 0.0333, Val Loss: 0.0907\n",
      "Epoch [18/50], Train Loss: 0.0331, Val Loss: 0.0894\n",
      "Epoch [19/50], Train Loss: 0.0328, Val Loss: 0.0882\n",
      "Epoch [20/50], Train Loss: 0.0326, Val Loss: 0.0871\n",
      "Epoch [21/50], Train Loss: 0.0324, Val Loss: 0.0861\n",
      "Epoch [22/50], Train Loss: 0.0321, Val Loss: 0.0852\n",
      "Epoch [23/50], Train Loss: 0.0319, Val Loss: 0.0843\n",
      "Epoch [24/50], Train Loss: 0.0317, Val Loss: 0.0835\n",
      "Epoch [25/50], Train Loss: 0.0314, Val Loss: 0.0826\n",
      "Epoch [26/50], Train Loss: 0.0312, Val Loss: 0.0818\n",
      "Epoch [27/50], Train Loss: 0.0310, Val Loss: 0.0809\n",
      "Epoch [28/50], Train Loss: 0.0307, Val Loss: 0.0801\n",
      "Epoch [29/50], Train Loss: 0.0305, Val Loss: 0.0793\n",
      "Epoch [30/50], Train Loss: 0.0302, Val Loss: 0.0784\n",
      "Epoch [31/50], Train Loss: 0.0300, Val Loss: 0.0776\n",
      "Epoch [32/50], Train Loss: 0.0297, Val Loss: 0.0767\n",
      "Epoch [33/50], Train Loss: 0.0295, Val Loss: 0.0759\n",
      "Epoch [34/50], Train Loss: 0.0292, Val Loss: 0.0750\n",
      "Epoch [35/50], Train Loss: 0.0290, Val Loss: 0.0741\n",
      "Epoch [36/50], Train Loss: 0.0287, Val Loss: 0.0732\n",
      "Epoch [37/50], Train Loss: 0.0284, Val Loss: 0.0723\n",
      "Epoch [38/50], Train Loss: 0.0281, Val Loss: 0.0714\n",
      "Epoch [39/50], Train Loss: 0.0278, Val Loss: 0.0704\n",
      "Epoch [40/50], Train Loss: 0.0276, Val Loss: 0.0695\n",
      "Epoch [41/50], Train Loss: 0.0273, Val Loss: 0.0685\n",
      "Epoch [42/50], Train Loss: 0.0270, Val Loss: 0.0676\n",
      "Epoch [43/50], Train Loss: 0.0267, Val Loss: 0.0665\n",
      "Epoch [44/50], Train Loss: 0.0264, Val Loss: 0.0655\n",
      "Epoch [45/50], Train Loss: 0.0261, Val Loss: 0.0645\n",
      "Epoch [46/50], Train Loss: 0.0257, Val Loss: 0.0635\n",
      "Epoch [47/50], Train Loss: 0.0254, Val Loss: 0.0624\n",
      "Epoch [48/50], Train Loss: 0.0251, Val Loss: 0.0613\n",
      "Epoch [49/50], Train Loss: 0.0248, Val Loss: 0.0602\n",
      "Epoch [50/50], Train Loss: 0.0244, Val Loss: 0.0591\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=16, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1897, Val Loss: 0.4416\n",
      "Epoch [2/50], Train Loss: 0.1405, Val Loss: 0.3651\n",
      "Epoch [3/50], Train Loss: 0.1079, Val Loss: 0.3051\n",
      "Epoch [4/50], Train Loss: 0.0874, Val Loss: 0.2594\n",
      "Epoch [5/50], Train Loss: 0.0726, Val Loss: 0.2239\n",
      "Epoch [6/50], Train Loss: 0.0629, Val Loss: 0.1965\n",
      "Epoch [7/50], Train Loss: 0.0575, Val Loss: 0.1764\n",
      "Epoch [8/50], Train Loss: 0.0538, Val Loss: 0.1610\n",
      "Epoch [9/50], Train Loss: 0.0508, Val Loss: 0.1489\n",
      "Epoch [10/50], Train Loss: 0.0502, Val Loss: 0.1402\n",
      "Epoch [11/50], Train Loss: 0.0492, Val Loss: 0.1332\n",
      "Epoch [12/50], Train Loss: 0.0490, Val Loss: 0.1286\n",
      "Epoch [13/50], Train Loss: 0.0471, Val Loss: 0.1249\n",
      "Epoch [14/50], Train Loss: 0.0462, Val Loss: 0.1209\n",
      "Epoch [15/50], Train Loss: 0.0473, Val Loss: 0.1184\n",
      "Epoch [16/50], Train Loss: 0.0477, Val Loss: 0.1164\n",
      "Epoch [17/50], Train Loss: 0.0457, Val Loss: 0.1145\n",
      "Epoch [18/50], Train Loss: 0.0452, Val Loss: 0.1128\n",
      "Epoch [19/50], Train Loss: 0.0451, Val Loss: 0.1115\n",
      "Epoch [20/50], Train Loss: 0.0451, Val Loss: 0.1104\n",
      "Epoch [21/50], Train Loss: 0.0447, Val Loss: 0.1094\n",
      "Epoch [22/50], Train Loss: 0.0446, Val Loss: 0.1090\n",
      "Epoch [23/50], Train Loss: 0.0442, Val Loss: 0.1076\n",
      "Epoch [24/50], Train Loss: 0.0441, Val Loss: 0.1071\n",
      "Epoch [25/50], Train Loss: 0.0427, Val Loss: 0.1062\n",
      "Epoch [26/50], Train Loss: 0.0421, Val Loss: 0.1053\n",
      "Epoch [27/50], Train Loss: 0.0427, Val Loss: 0.1045\n",
      "Epoch [28/50], Train Loss: 0.0428, Val Loss: 0.1037\n",
      "Epoch [29/50], Train Loss: 0.0431, Val Loss: 0.1031\n",
      "Epoch [30/50], Train Loss: 0.0418, Val Loss: 0.1020\n",
      "Epoch [31/50], Train Loss: 0.0411, Val Loss: 0.1013\n",
      "Epoch [32/50], Train Loss: 0.0410, Val Loss: 0.1008\n",
      "Epoch [33/50], Train Loss: 0.0408, Val Loss: 0.1002\n",
      "Epoch [34/50], Train Loss: 0.0428, Val Loss: 0.0993\n",
      "Epoch [35/50], Train Loss: 0.0408, Val Loss: 0.0985\n",
      "Epoch [36/50], Train Loss: 0.0402, Val Loss: 0.0982\n",
      "Epoch [37/50], Train Loss: 0.0397, Val Loss: 0.0975\n",
      "Epoch [38/50], Train Loss: 0.0393, Val Loss: 0.0969\n",
      "Epoch [39/50], Train Loss: 0.0386, Val Loss: 0.0962\n",
      "Epoch [40/50], Train Loss: 0.0388, Val Loss: 0.0955\n",
      "Epoch [41/50], Train Loss: 0.0392, Val Loss: 0.0947\n",
      "Epoch [42/50], Train Loss: 0.0380, Val Loss: 0.0943\n",
      "Epoch [43/50], Train Loss: 0.0387, Val Loss: 0.0934\n",
      "Epoch [44/50], Train Loss: 0.0384, Val Loss: 0.0922\n",
      "Epoch [45/50], Train Loss: 0.0385, Val Loss: 0.0918\n",
      "Epoch [46/50], Train Loss: 0.0374, Val Loss: 0.0912\n",
      "Epoch [47/50], Train Loss: 0.0370, Val Loss: 0.0905\n",
      "Epoch [48/50], Train Loss: 0.0367, Val Loss: 0.0897\n",
      "Epoch [49/50], Train Loss: 0.0375, Val Loss: 0.0884\n",
      "Epoch [50/50], Train Loss: 0.0367, Val Loss: 0.0872\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=16, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.2076, Val Loss: 0.3675\n",
      "Epoch [2/50], Train Loss: 0.1580, Val Loss: 0.2841\n",
      "Epoch [3/50], Train Loss: 0.1239, Val Loss: 0.2307\n",
      "Epoch [4/50], Train Loss: 0.1131, Val Loss: 0.1935\n",
      "Epoch [5/50], Train Loss: 0.1042, Val Loss: 0.1677\n",
      "Epoch [6/50], Train Loss: 0.0908, Val Loss: 0.1519\n",
      "Epoch [7/50], Train Loss: 0.0885, Val Loss: 0.1396\n",
      "Epoch [8/50], Train Loss: 0.0825, Val Loss: 0.1309\n",
      "Epoch [9/50], Train Loss: 0.0817, Val Loss: 0.1232\n",
      "Epoch [10/50], Train Loss: 0.0791, Val Loss: 0.1190\n",
      "Epoch [11/50], Train Loss: 0.0729, Val Loss: 0.1153\n",
      "Epoch [12/50], Train Loss: 0.0752, Val Loss: 0.1109\n",
      "Epoch [13/50], Train Loss: 0.0743, Val Loss: 0.1072\n",
      "Epoch [14/50], Train Loss: 0.0698, Val Loss: 0.1048\n",
      "Epoch [15/50], Train Loss: 0.0651, Val Loss: 0.1024\n",
      "Epoch [16/50], Train Loss: 0.0697, Val Loss: 0.1000\n",
      "Epoch [17/50], Train Loss: 0.0672, Val Loss: 0.0961\n",
      "Epoch [18/50], Train Loss: 0.0640, Val Loss: 0.0936\n",
      "Epoch [19/50], Train Loss: 0.0649, Val Loss: 0.0901\n",
      "Epoch [20/50], Train Loss: 0.0628, Val Loss: 0.0881\n",
      "Epoch [21/50], Train Loss: 0.0583, Val Loss: 0.0884\n",
      "Epoch [22/50], Train Loss: 0.0582, Val Loss: 0.0862\n",
      "Epoch [23/50], Train Loss: 0.0578, Val Loss: 0.0828\n",
      "Epoch [24/50], Train Loss: 0.0551, Val Loss: 0.0812\n",
      "Epoch [25/50], Train Loss: 0.0543, Val Loss: 0.0814\n",
      "Epoch [26/50], Train Loss: 0.0558, Val Loss: 0.0798\n",
      "Epoch [27/50], Train Loss: 0.0530, Val Loss: 0.0761\n",
      "Epoch [28/50], Train Loss: 0.0537, Val Loss: 0.0755\n",
      "Epoch [29/50], Train Loss: 0.0557, Val Loss: 0.0747\n",
      "Epoch [30/50], Train Loss: 0.0519, Val Loss: 0.0740\n",
      "Epoch [31/50], Train Loss: 0.0506, Val Loss: 0.0723\n",
      "Epoch [32/50], Train Loss: 0.0501, Val Loss: 0.0712\n",
      "Epoch [33/50], Train Loss: 0.0481, Val Loss: 0.0688\n",
      "Epoch [34/50], Train Loss: 0.0488, Val Loss: 0.0680\n",
      "Epoch [35/50], Train Loss: 0.0481, Val Loss: 0.0665\n",
      "Epoch [36/50], Train Loss: 0.0480, Val Loss: 0.0655\n",
      "Epoch [37/50], Train Loss: 0.0453, Val Loss: 0.0641\n",
      "Epoch [38/50], Train Loss: 0.0445, Val Loss: 0.0630\n",
      "Epoch [39/50], Train Loss: 0.0441, Val Loss: 0.0605\n",
      "Epoch [40/50], Train Loss: 0.0442, Val Loss: 0.0602\n",
      "Epoch [41/50], Train Loss: 0.0447, Val Loss: 0.0586\n",
      "Epoch [42/50], Train Loss: 0.0420, Val Loss: 0.0570\n",
      "Epoch [43/50], Train Loss: 0.0426, Val Loss: 0.0557\n",
      "Epoch [44/50], Train Loss: 0.0423, Val Loss: 0.0552\n",
      "Epoch [45/50], Train Loss: 0.0421, Val Loss: 0.0537\n",
      "Epoch [46/50], Train Loss: 0.0416, Val Loss: 0.0527\n",
      "Epoch [47/50], Train Loss: 0.0411, Val Loss: 0.0514\n",
      "Epoch [48/50], Train Loss: 0.0417, Val Loss: 0.0512\n",
      "Epoch [49/50], Train Loss: 0.0405, Val Loss: 0.0496\n",
      "Epoch [50/50], Train Loss: 0.0395, Val Loss: 0.0480\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=16, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.2287, Val Loss: 0.4524\n",
      "Epoch [2/50], Train Loss: 0.1369, Val Loss: 0.3338\n",
      "Epoch [3/50], Train Loss: 0.0881, Val Loss: 0.2554\n",
      "Epoch [4/50], Train Loss: 0.0618, Val Loss: 0.2029\n",
      "Epoch [5/50], Train Loss: 0.0480, Val Loss: 0.1678\n",
      "Epoch [6/50], Train Loss: 0.0410, Val Loss: 0.1442\n",
      "Epoch [7/50], Train Loss: 0.0377, Val Loss: 0.1284\n",
      "Epoch [8/50], Train Loss: 0.0361, Val Loss: 0.1177\n",
      "Epoch [9/50], Train Loss: 0.0354, Val Loss: 0.1105\n",
      "Epoch [10/50], Train Loss: 0.0350, Val Loss: 0.1054\n",
      "Epoch [11/50], Train Loss: 0.0349, Val Loss: 0.1019\n",
      "Epoch [12/50], Train Loss: 0.0348, Val Loss: 0.0995\n",
      "Epoch [13/50], Train Loss: 0.0347, Val Loss: 0.0977\n",
      "Epoch [14/50], Train Loss: 0.0346, Val Loss: 0.0964\n",
      "Epoch [15/50], Train Loss: 0.0345, Val Loss: 0.0953\n",
      "Epoch [16/50], Train Loss: 0.0344, Val Loss: 0.0945\n",
      "Epoch [17/50], Train Loss: 0.0343, Val Loss: 0.0939\n",
      "Epoch [18/50], Train Loss: 0.0342, Val Loss: 0.0933\n",
      "Epoch [19/50], Train Loss: 0.0340, Val Loss: 0.0928\n",
      "Epoch [20/50], Train Loss: 0.0339, Val Loss: 0.0924\n",
      "Epoch [21/50], Train Loss: 0.0338, Val Loss: 0.0919\n",
      "Epoch [22/50], Train Loss: 0.0337, Val Loss: 0.0915\n",
      "Epoch [23/50], Train Loss: 0.0335, Val Loss: 0.0911\n",
      "Epoch [24/50], Train Loss: 0.0334, Val Loss: 0.0907\n",
      "Epoch [25/50], Train Loss: 0.0333, Val Loss: 0.0903\n",
      "Epoch [26/50], Train Loss: 0.0331, Val Loss: 0.0899\n",
      "Epoch [27/50], Train Loss: 0.0330, Val Loss: 0.0895\n",
      "Epoch [28/50], Train Loss: 0.0328, Val Loss: 0.0891\n",
      "Epoch [29/50], Train Loss: 0.0327, Val Loss: 0.0887\n",
      "Epoch [30/50], Train Loss: 0.0325, Val Loss: 0.0882\n",
      "Epoch [31/50], Train Loss: 0.0324, Val Loss: 0.0878\n",
      "Epoch [32/50], Train Loss: 0.0322, Val Loss: 0.0874\n",
      "Epoch [33/50], Train Loss: 0.0320, Val Loss: 0.0869\n",
      "Epoch [34/50], Train Loss: 0.0319, Val Loss: 0.0865\n",
      "Epoch [35/50], Train Loss: 0.0317, Val Loss: 0.0860\n",
      "Epoch [36/50], Train Loss: 0.0315, Val Loss: 0.0855\n",
      "Epoch [37/50], Train Loss: 0.0313, Val Loss: 0.0850\n",
      "Epoch [38/50], Train Loss: 0.0311, Val Loss: 0.0845\n",
      "Epoch [39/50], Train Loss: 0.0310, Val Loss: 0.0840\n",
      "Epoch [40/50], Train Loss: 0.0308, Val Loss: 0.0835\n",
      "Epoch [41/50], Train Loss: 0.0306, Val Loss: 0.0829\n",
      "Epoch [42/50], Train Loss: 0.0303, Val Loss: 0.0823\n",
      "Epoch [43/50], Train Loss: 0.0301, Val Loss: 0.0818\n",
      "Epoch [44/50], Train Loss: 0.0299, Val Loss: 0.0812\n",
      "Epoch [45/50], Train Loss: 0.0297, Val Loss: 0.0805\n",
      "Epoch [46/50], Train Loss: 0.0295, Val Loss: 0.0799\n",
      "Epoch [47/50], Train Loss: 0.0292, Val Loss: 0.0792\n",
      "Epoch [48/50], Train Loss: 0.0290, Val Loss: 0.0786\n",
      "Epoch [49/50], Train Loss: 0.0287, Val Loss: 0.0779\n",
      "Epoch [50/50], Train Loss: 0.0285, Val Loss: 0.0771\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=16, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1808, Val Loss: 0.3395\n",
      "Epoch [2/50], Train Loss: 0.1315, Val Loss: 0.2684\n",
      "Epoch [3/50], Train Loss: 0.1014, Val Loss: 0.2195\n",
      "Epoch [4/50], Train Loss: 0.0833, Val Loss: 0.1852\n",
      "Epoch [5/50], Train Loss: 0.0714, Val Loss: 0.1609\n",
      "Epoch [6/50], Train Loss: 0.0638, Val Loss: 0.1440\n",
      "Epoch [7/50], Train Loss: 0.0614, Val Loss: 0.1328\n",
      "Epoch [8/50], Train Loss: 0.0573, Val Loss: 0.1255\n",
      "Epoch [9/50], Train Loss: 0.0582, Val Loss: 0.1195\n",
      "Epoch [10/50], Train Loss: 0.0562, Val Loss: 0.1154\n",
      "Epoch [11/50], Train Loss: 0.0575, Val Loss: 0.1127\n",
      "Epoch [12/50], Train Loss: 0.0568, Val Loss: 0.1113\n",
      "Epoch [13/50], Train Loss: 0.0533, Val Loss: 0.1105\n",
      "Epoch [14/50], Train Loss: 0.0539, Val Loss: 0.1098\n",
      "Epoch [15/50], Train Loss: 0.0543, Val Loss: 0.1086\n",
      "Epoch [16/50], Train Loss: 0.0544, Val Loss: 0.1076\n",
      "Epoch [17/50], Train Loss: 0.0544, Val Loss: 0.1076\n",
      "Epoch [18/50], Train Loss: 0.0554, Val Loss: 0.1072\n",
      "Epoch [19/50], Train Loss: 0.0520, Val Loss: 0.1068\n",
      "Epoch [20/50], Train Loss: 0.0526, Val Loss: 0.1063\n",
      "Epoch [21/50], Train Loss: 0.0530, Val Loss: 0.1067\n",
      "Epoch [22/50], Train Loss: 0.0514, Val Loss: 0.1068\n",
      "Epoch [23/50], Train Loss: 0.0503, Val Loss: 0.1062\n",
      "Epoch [24/50], Train Loss: 0.0520, Val Loss: 0.1059\n",
      "Epoch [25/50], Train Loss: 0.0519, Val Loss: 0.1056\n",
      "Epoch [26/50], Train Loss: 0.0506, Val Loss: 0.1052\n",
      "Epoch [27/50], Train Loss: 0.0511, Val Loss: 0.1046\n",
      "Epoch [28/50], Train Loss: 0.0511, Val Loss: 0.1047\n",
      "Epoch [29/50], Train Loss: 0.0518, Val Loss: 0.1052\n",
      "Epoch [30/50], Train Loss: 0.0502, Val Loss: 0.1049\n",
      "Epoch [31/50], Train Loss: 0.0500, Val Loss: 0.1044\n",
      "Epoch [32/50], Train Loss: 0.0502, Val Loss: 0.1044\n",
      "Epoch [33/50], Train Loss: 0.0496, Val Loss: 0.1040\n",
      "Epoch [34/50], Train Loss: 0.0487, Val Loss: 0.1037\n",
      "Epoch [35/50], Train Loss: 0.0503, Val Loss: 0.1033\n",
      "Epoch [36/50], Train Loss: 0.0497, Val Loss: 0.1039\n",
      "Epoch [37/50], Train Loss: 0.0487, Val Loss: 0.1034\n",
      "Epoch [38/50], Train Loss: 0.0496, Val Loss: 0.1034\n",
      "Epoch [39/50], Train Loss: 0.0483, Val Loss: 0.1038\n",
      "Epoch [40/50], Train Loss: 0.0483, Val Loss: 0.1038\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=16, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.2350, Val Loss: 0.3812\n",
      "Epoch [2/50], Train Loss: 0.1753, Val Loss: 0.3039\n",
      "Epoch [3/50], Train Loss: 0.1468, Val Loss: 0.2569\n",
      "Epoch [4/50], Train Loss: 0.1311, Val Loss: 0.2257\n",
      "Epoch [5/50], Train Loss: 0.1141, Val Loss: 0.2024\n",
      "Epoch [6/50], Train Loss: 0.1054, Val Loss: 0.1867\n",
      "Epoch [7/50], Train Loss: 0.0989, Val Loss: 0.1734\n",
      "Epoch [8/50], Train Loss: 0.0955, Val Loss: 0.1643\n",
      "Epoch [9/50], Train Loss: 0.0910, Val Loss: 0.1565\n",
      "Epoch [10/50], Train Loss: 0.0855, Val Loss: 0.1503\n",
      "Epoch [11/50], Train Loss: 0.0866, Val Loss: 0.1450\n",
      "Epoch [12/50], Train Loss: 0.0802, Val Loss: 0.1414\n",
      "Epoch [13/50], Train Loss: 0.0771, Val Loss: 0.1374\n",
      "Epoch [14/50], Train Loss: 0.0738, Val Loss: 0.1356\n",
      "Epoch [15/50], Train Loss: 0.0742, Val Loss: 0.1323\n",
      "Epoch [16/50], Train Loss: 0.0706, Val Loss: 0.1302\n",
      "Epoch [17/50], Train Loss: 0.0722, Val Loss: 0.1277\n",
      "Epoch [18/50], Train Loss: 0.0710, Val Loss: 0.1254\n",
      "Epoch [19/50], Train Loss: 0.0674, Val Loss: 0.1236\n",
      "Epoch [20/50], Train Loss: 0.0669, Val Loss: 0.1213\n",
      "Epoch [21/50], Train Loss: 0.0664, Val Loss: 0.1193\n",
      "Epoch [22/50], Train Loss: 0.0655, Val Loss: 0.1177\n",
      "Epoch [23/50], Train Loss: 0.0649, Val Loss: 0.1158\n",
      "Epoch [24/50], Train Loss: 0.0612, Val Loss: 0.1142\n",
      "Epoch [25/50], Train Loss: 0.0610, Val Loss: 0.1140\n",
      "Epoch [26/50], Train Loss: 0.0593, Val Loss: 0.1126\n",
      "Epoch [27/50], Train Loss: 0.0586, Val Loss: 0.1104\n",
      "Epoch [28/50], Train Loss: 0.0580, Val Loss: 0.1090\n",
      "Epoch [29/50], Train Loss: 0.0566, Val Loss: 0.1076\n",
      "Epoch [30/50], Train Loss: 0.0567, Val Loss: 0.1068\n",
      "Epoch [31/50], Train Loss: 0.0571, Val Loss: 0.1067\n",
      "Epoch [32/50], Train Loss: 0.0560, Val Loss: 0.1065\n",
      "Epoch [33/50], Train Loss: 0.0550, Val Loss: 0.1055\n",
      "Epoch [34/50], Train Loss: 0.0543, Val Loss: 0.1043\n",
      "Epoch [35/50], Train Loss: 0.0536, Val Loss: 0.1031\n",
      "Epoch [36/50], Train Loss: 0.0542, Val Loss: 0.1019\n",
      "Epoch [37/50], Train Loss: 0.0526, Val Loss: 0.1016\n",
      "Epoch [38/50], Train Loss: 0.0501, Val Loss: 0.1003\n",
      "Epoch [39/50], Train Loss: 0.0511, Val Loss: 0.0994\n",
      "Epoch [40/50], Train Loss: 0.0514, Val Loss: 0.0986\n",
      "Epoch [41/50], Train Loss: 0.0504, Val Loss: 0.0971\n",
      "Epoch [42/50], Train Loss: 0.0505, Val Loss: 0.0964\n",
      "Epoch [43/50], Train Loss: 0.0495, Val Loss: 0.0948\n",
      "Epoch [44/50], Train Loss: 0.0493, Val Loss: 0.0942\n",
      "Epoch [45/50], Train Loss: 0.0489, Val Loss: 0.0929\n",
      "Epoch [46/50], Train Loss: 0.0475, Val Loss: 0.0925\n",
      "Epoch [47/50], Train Loss: 0.0483, Val Loss: 0.0917\n",
      "Epoch [48/50], Train Loss: 0.0467, Val Loss: 0.0899\n",
      "Epoch [49/50], Train Loss: 0.0447, Val Loss: 0.0899\n",
      "Epoch [50/50], Train Loss: 0.0458, Val Loss: 0.0892\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=32, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1584, Val Loss: 0.3429\n",
      "Epoch [2/50], Train Loss: 0.1114, Val Loss: 0.2760\n",
      "Epoch [3/50], Train Loss: 0.0834, Val Loss: 0.2293\n",
      "Epoch [4/50], Train Loss: 0.0659, Val Loss: 0.1951\n",
      "Epoch [5/50], Train Loss: 0.0545, Val Loss: 0.1696\n",
      "Epoch [6/50], Train Loss: 0.0472, Val Loss: 0.1502\n",
      "Epoch [7/50], Train Loss: 0.0425, Val Loss: 0.1354\n",
      "Epoch [8/50], Train Loss: 0.0395, Val Loss: 0.1240\n",
      "Epoch [9/50], Train Loss: 0.0375, Val Loss: 0.1151\n",
      "Epoch [10/50], Train Loss: 0.0362, Val Loss: 0.1082\n",
      "Epoch [11/50], Train Loss: 0.0353, Val Loss: 0.1028\n",
      "Epoch [12/50], Train Loss: 0.0346, Val Loss: 0.0985\n",
      "Epoch [13/50], Train Loss: 0.0341, Val Loss: 0.0950\n",
      "Epoch [14/50], Train Loss: 0.0337, Val Loss: 0.0921\n",
      "Epoch [15/50], Train Loss: 0.0333, Val Loss: 0.0897\n",
      "Epoch [16/50], Train Loss: 0.0330, Val Loss: 0.0877\n",
      "Epoch [17/50], Train Loss: 0.0327, Val Loss: 0.0860\n",
      "Epoch [18/50], Train Loss: 0.0324, Val Loss: 0.0845\n",
      "Epoch [19/50], Train Loss: 0.0321, Val Loss: 0.0832\n",
      "Epoch [20/50], Train Loss: 0.0318, Val Loss: 0.0819\n",
      "Epoch [21/50], Train Loss: 0.0315, Val Loss: 0.0808\n",
      "Epoch [22/50], Train Loss: 0.0312, Val Loss: 0.0798\n",
      "Epoch [23/50], Train Loss: 0.0309, Val Loss: 0.0788\n",
      "Epoch [24/50], Train Loss: 0.0306, Val Loss: 0.0779\n",
      "Epoch [25/50], Train Loss: 0.0303, Val Loss: 0.0770\n",
      "Epoch [26/50], Train Loss: 0.0300, Val Loss: 0.0761\n",
      "Epoch [27/50], Train Loss: 0.0297, Val Loss: 0.0753\n",
      "Epoch [28/50], Train Loss: 0.0294, Val Loss: 0.0744\n",
      "Epoch [29/50], Train Loss: 0.0292, Val Loss: 0.0736\n",
      "Epoch [30/50], Train Loss: 0.0289, Val Loss: 0.0728\n",
      "Epoch [31/50], Train Loss: 0.0286, Val Loss: 0.0720\n",
      "Epoch [32/50], Train Loss: 0.0283, Val Loss: 0.0712\n",
      "Epoch [33/50], Train Loss: 0.0280, Val Loss: 0.0704\n",
      "Epoch [34/50], Train Loss: 0.0277, Val Loss: 0.0696\n",
      "Epoch [35/50], Train Loss: 0.0275, Val Loss: 0.0688\n",
      "Epoch [36/50], Train Loss: 0.0272, Val Loss: 0.0680\n",
      "Epoch [37/50], Train Loss: 0.0269, Val Loss: 0.0672\n",
      "Epoch [38/50], Train Loss: 0.0266, Val Loss: 0.0664\n",
      "Epoch [39/50], Train Loss: 0.0263, Val Loss: 0.0657\n",
      "Epoch [40/50], Train Loss: 0.0260, Val Loss: 0.0649\n",
      "Epoch [41/50], Train Loss: 0.0257, Val Loss: 0.0641\n",
      "Epoch [42/50], Train Loss: 0.0255, Val Loss: 0.0633\n",
      "Epoch [43/50], Train Loss: 0.0252, Val Loss: 0.0625\n",
      "Epoch [44/50], Train Loss: 0.0249, Val Loss: 0.0617\n",
      "Epoch [45/50], Train Loss: 0.0246, Val Loss: 0.0609\n",
      "Epoch [46/50], Train Loss: 0.0243, Val Loss: 0.0601\n",
      "Epoch [47/50], Train Loss: 0.0240, Val Loss: 0.0593\n",
      "Epoch [48/50], Train Loss: 0.0238, Val Loss: 0.0585\n",
      "Epoch [49/50], Train Loss: 0.0235, Val Loss: 0.0577\n",
      "Epoch [50/50], Train Loss: 0.0232, Val Loss: 0.0570\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=32, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1499, Val Loss: 0.2555\n",
      "Epoch [2/50], Train Loss: 0.1125, Val Loss: 0.1965\n",
      "Epoch [3/50], Train Loss: 0.0879, Val Loss: 0.1527\n",
      "Epoch [4/50], Train Loss: 0.0682, Val Loss: 0.1206\n",
      "Epoch [5/50], Train Loss: 0.0558, Val Loss: 0.0972\n",
      "Epoch [6/50], Train Loss: 0.0471, Val Loss: 0.0812\n",
      "Epoch [7/50], Train Loss: 0.0419, Val Loss: 0.0705\n",
      "Epoch [8/50], Train Loss: 0.0379, Val Loss: 0.0628\n",
      "Epoch [9/50], Train Loss: 0.0353, Val Loss: 0.0575\n",
      "Epoch [10/50], Train Loss: 0.0336, Val Loss: 0.0542\n",
      "Epoch [11/50], Train Loss: 0.0328, Val Loss: 0.0517\n",
      "Epoch [12/50], Train Loss: 0.0313, Val Loss: 0.0498\n",
      "Epoch [13/50], Train Loss: 0.0303, Val Loss: 0.0485\n",
      "Epoch [14/50], Train Loss: 0.0313, Val Loss: 0.0476\n",
      "Epoch [15/50], Train Loss: 0.0301, Val Loss: 0.0463\n",
      "Epoch [16/50], Train Loss: 0.0296, Val Loss: 0.0453\n",
      "Epoch [17/50], Train Loss: 0.0294, Val Loss: 0.0445\n",
      "Epoch [18/50], Train Loss: 0.0291, Val Loss: 0.0434\n",
      "Epoch [19/50], Train Loss: 0.0287, Val Loss: 0.0427\n",
      "Epoch [20/50], Train Loss: 0.0274, Val Loss: 0.0420\n",
      "Epoch [21/50], Train Loss: 0.0274, Val Loss: 0.0409\n",
      "Epoch [22/50], Train Loss: 0.0279, Val Loss: 0.0402\n",
      "Epoch [23/50], Train Loss: 0.0270, Val Loss: 0.0395\n",
      "Epoch [24/50], Train Loss: 0.0256, Val Loss: 0.0386\n",
      "Epoch [25/50], Train Loss: 0.0268, Val Loss: 0.0378\n",
      "Epoch [26/50], Train Loss: 0.0260, Val Loss: 0.0370\n",
      "Epoch [27/50], Train Loss: 0.0258, Val Loss: 0.0360\n",
      "Epoch [28/50], Train Loss: 0.0252, Val Loss: 0.0349\n",
      "Epoch [29/50], Train Loss: 0.0251, Val Loss: 0.0338\n",
      "Epoch [30/50], Train Loss: 0.0249, Val Loss: 0.0329\n",
      "Epoch [31/50], Train Loss: 0.0251, Val Loss: 0.0320\n",
      "Epoch [32/50], Train Loss: 0.0236, Val Loss: 0.0312\n",
      "Epoch [33/50], Train Loss: 0.0240, Val Loss: 0.0305\n",
      "Epoch [34/50], Train Loss: 0.0237, Val Loss: 0.0297\n",
      "Epoch [35/50], Train Loss: 0.0232, Val Loss: 0.0292\n",
      "Epoch [36/50], Train Loss: 0.0233, Val Loss: 0.0281\n",
      "Epoch [37/50], Train Loss: 0.0225, Val Loss: 0.0276\n",
      "Epoch [38/50], Train Loss: 0.0223, Val Loss: 0.0269\n",
      "Epoch [39/50], Train Loss: 0.0225, Val Loss: 0.0259\n",
      "Epoch [40/50], Train Loss: 0.0213, Val Loss: 0.0254\n",
      "Epoch [41/50], Train Loss: 0.0221, Val Loss: 0.0246\n",
      "Epoch [42/50], Train Loss: 0.0217, Val Loss: 0.0242\n",
      "Epoch [43/50], Train Loss: 0.0212, Val Loss: 0.0233\n",
      "Epoch [44/50], Train Loss: 0.0205, Val Loss: 0.0228\n",
      "Epoch [45/50], Train Loss: 0.0207, Val Loss: 0.0220\n",
      "Epoch [46/50], Train Loss: 0.0202, Val Loss: 0.0213\n",
      "Epoch [47/50], Train Loss: 0.0206, Val Loss: 0.0209\n",
      "Epoch [48/50], Train Loss: 0.0204, Val Loss: 0.0204\n",
      "Epoch [49/50], Train Loss: 0.0202, Val Loss: 0.0199\n",
      "Epoch [50/50], Train Loss: 0.0196, Val Loss: 0.0191\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=32, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1647, Val Loss: 0.2973\n",
      "Epoch [2/50], Train Loss: 0.1224, Val Loss: 0.2446\n",
      "Epoch [3/50], Train Loss: 0.1010, Val Loss: 0.2053\n",
      "Epoch [4/50], Train Loss: 0.0852, Val Loss: 0.1758\n",
      "Epoch [5/50], Train Loss: 0.0727, Val Loss: 0.1535\n",
      "Epoch [6/50], Train Loss: 0.0626, Val Loss: 0.1367\n",
      "Epoch [7/50], Train Loss: 0.0579, Val Loss: 0.1246\n",
      "Epoch [8/50], Train Loss: 0.0535, Val Loss: 0.1143\n",
      "Epoch [9/50], Train Loss: 0.0514, Val Loss: 0.1069\n",
      "Epoch [10/50], Train Loss: 0.0498, Val Loss: 0.1007\n",
      "Epoch [11/50], Train Loss: 0.0480, Val Loss: 0.0962\n",
      "Epoch [12/50], Train Loss: 0.0479, Val Loss: 0.0931\n",
      "Epoch [13/50], Train Loss: 0.0451, Val Loss: 0.0890\n",
      "Epoch [14/50], Train Loss: 0.0455, Val Loss: 0.0861\n",
      "Epoch [15/50], Train Loss: 0.0446, Val Loss: 0.0834\n",
      "Epoch [16/50], Train Loss: 0.0436, Val Loss: 0.0814\n",
      "Epoch [17/50], Train Loss: 0.0430, Val Loss: 0.0793\n",
      "Epoch [18/50], Train Loss: 0.0430, Val Loss: 0.0777\n",
      "Epoch [19/50], Train Loss: 0.0421, Val Loss: 0.0766\n",
      "Epoch [20/50], Train Loss: 0.0420, Val Loss: 0.0743\n",
      "Epoch [21/50], Train Loss: 0.0393, Val Loss: 0.0729\n",
      "Epoch [22/50], Train Loss: 0.0388, Val Loss: 0.0722\n",
      "Epoch [23/50], Train Loss: 0.0383, Val Loss: 0.0710\n",
      "Epoch [24/50], Train Loss: 0.0380, Val Loss: 0.0698\n",
      "Epoch [25/50], Train Loss: 0.0387, Val Loss: 0.0687\n",
      "Epoch [26/50], Train Loss: 0.0385, Val Loss: 0.0673\n",
      "Epoch [27/50], Train Loss: 0.0357, Val Loss: 0.0661\n",
      "Epoch [28/50], Train Loss: 0.0358, Val Loss: 0.0657\n",
      "Epoch [29/50], Train Loss: 0.0361, Val Loss: 0.0644\n",
      "Epoch [30/50], Train Loss: 0.0347, Val Loss: 0.0642\n",
      "Epoch [31/50], Train Loss: 0.0334, Val Loss: 0.0629\n",
      "Epoch [32/50], Train Loss: 0.0360, Val Loss: 0.0616\n",
      "Epoch [33/50], Train Loss: 0.0334, Val Loss: 0.0607\n",
      "Epoch [34/50], Train Loss: 0.0333, Val Loss: 0.0603\n",
      "Epoch [35/50], Train Loss: 0.0339, Val Loss: 0.0594\n",
      "Epoch [36/50], Train Loss: 0.0327, Val Loss: 0.0591\n",
      "Epoch [37/50], Train Loss: 0.0333, Val Loss: 0.0579\n",
      "Epoch [38/50], Train Loss: 0.0332, Val Loss: 0.0570\n",
      "Epoch [39/50], Train Loss: 0.0320, Val Loss: 0.0559\n",
      "Epoch [40/50], Train Loss: 0.0304, Val Loss: 0.0553\n",
      "Epoch [41/50], Train Loss: 0.0323, Val Loss: 0.0547\n",
      "Epoch [42/50], Train Loss: 0.0307, Val Loss: 0.0538\n",
      "Epoch [43/50], Train Loss: 0.0296, Val Loss: 0.0531\n",
      "Epoch [44/50], Train Loss: 0.0305, Val Loss: 0.0522\n",
      "Epoch [45/50], Train Loss: 0.0311, Val Loss: 0.0518\n",
      "Epoch [46/50], Train Loss: 0.0298, Val Loss: 0.0511\n",
      "Epoch [47/50], Train Loss: 0.0303, Val Loss: 0.0503\n",
      "Epoch [48/50], Train Loss: 0.0291, Val Loss: 0.0496\n",
      "Epoch [49/50], Train Loss: 0.0287, Val Loss: 0.0488\n",
      "Epoch [50/50], Train Loss: 0.0290, Val Loss: 0.0479\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=32, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1335, Val Loss: 0.2913\n",
      "Epoch [2/50], Train Loss: 0.0817, Val Loss: 0.2147\n",
      "Epoch [3/50], Train Loss: 0.0568, Val Loss: 0.1682\n",
      "Epoch [4/50], Train Loss: 0.0445, Val Loss: 0.1391\n",
      "Epoch [5/50], Train Loss: 0.0385, Val Loss: 0.1206\n",
      "Epoch [6/50], Train Loss: 0.0354, Val Loss: 0.1086\n",
      "Epoch [7/50], Train Loss: 0.0338, Val Loss: 0.1007\n",
      "Epoch [8/50], Train Loss: 0.0328, Val Loss: 0.0953\n",
      "Epoch [9/50], Train Loss: 0.0323, Val Loss: 0.0916\n",
      "Epoch [10/50], Train Loss: 0.0319, Val Loss: 0.0890\n",
      "Epoch [11/50], Train Loss: 0.0316, Val Loss: 0.0870\n",
      "Epoch [12/50], Train Loss: 0.0313, Val Loss: 0.0855\n",
      "Epoch [13/50], Train Loss: 0.0311, Val Loss: 0.0843\n",
      "Epoch [14/50], Train Loss: 0.0309, Val Loss: 0.0833\n",
      "Epoch [15/50], Train Loss: 0.0307, Val Loss: 0.0824\n",
      "Epoch [16/50], Train Loss: 0.0305, Val Loss: 0.0816\n",
      "Epoch [17/50], Train Loss: 0.0303, Val Loss: 0.0809\n",
      "Epoch [18/50], Train Loss: 0.0302, Val Loss: 0.0803\n",
      "Epoch [19/50], Train Loss: 0.0300, Val Loss: 0.0796\n",
      "Epoch [20/50], Train Loss: 0.0298, Val Loss: 0.0790\n",
      "Epoch [21/50], Train Loss: 0.0296, Val Loss: 0.0784\n",
      "Epoch [22/50], Train Loss: 0.0294, Val Loss: 0.0778\n",
      "Epoch [23/50], Train Loss: 0.0292, Val Loss: 0.0772\n",
      "Epoch [24/50], Train Loss: 0.0290, Val Loss: 0.0766\n",
      "Epoch [25/50], Train Loss: 0.0288, Val Loss: 0.0760\n",
      "Epoch [26/50], Train Loss: 0.0286, Val Loss: 0.0754\n",
      "Epoch [27/50], Train Loss: 0.0284, Val Loss: 0.0748\n",
      "Epoch [28/50], Train Loss: 0.0282, Val Loss: 0.0742\n",
      "Epoch [29/50], Train Loss: 0.0280, Val Loss: 0.0736\n",
      "Epoch [30/50], Train Loss: 0.0278, Val Loss: 0.0730\n",
      "Epoch [31/50], Train Loss: 0.0276, Val Loss: 0.0723\n",
      "Epoch [32/50], Train Loss: 0.0274, Val Loss: 0.0717\n",
      "Epoch [33/50], Train Loss: 0.0272, Val Loss: 0.0710\n",
      "Epoch [34/50], Train Loss: 0.0270, Val Loss: 0.0704\n",
      "Epoch [35/50], Train Loss: 0.0268, Val Loss: 0.0698\n",
      "Epoch [36/50], Train Loss: 0.0265, Val Loss: 0.0691\n",
      "Epoch [37/50], Train Loss: 0.0263, Val Loss: 0.0684\n",
      "Epoch [38/50], Train Loss: 0.0261, Val Loss: 0.0677\n",
      "Epoch [39/50], Train Loss: 0.0259, Val Loss: 0.0670\n",
      "Epoch [40/50], Train Loss: 0.0256, Val Loss: 0.0663\n",
      "Epoch [41/50], Train Loss: 0.0254, Val Loss: 0.0656\n",
      "Epoch [42/50], Train Loss: 0.0252, Val Loss: 0.0649\n",
      "Epoch [43/50], Train Loss: 0.0249, Val Loss: 0.0642\n",
      "Epoch [44/50], Train Loss: 0.0247, Val Loss: 0.0634\n",
      "Epoch [45/50], Train Loss: 0.0245, Val Loss: 0.0627\n",
      "Epoch [46/50], Train Loss: 0.0242, Val Loss: 0.0619\n",
      "Epoch [47/50], Train Loss: 0.0240, Val Loss: 0.0611\n",
      "Epoch [48/50], Train Loss: 0.0237, Val Loss: 0.0603\n",
      "Epoch [49/50], Train Loss: 0.0235, Val Loss: 0.0595\n",
      "Epoch [50/50], Train Loss: 0.0232, Val Loss: 0.0587\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=32, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1460, Val Loss: 0.3247\n",
      "Epoch [2/50], Train Loss: 0.1004, Val Loss: 0.2510\n",
      "Epoch [3/50], Train Loss: 0.0738, Val Loss: 0.2000\n",
      "Epoch [4/50], Train Loss: 0.0589, Val Loss: 0.1652\n",
      "Epoch [5/50], Train Loss: 0.0504, Val Loss: 0.1407\n",
      "Epoch [6/50], Train Loss: 0.0459, Val Loss: 0.1237\n",
      "Epoch [7/50], Train Loss: 0.0428, Val Loss: 0.1123\n",
      "Epoch [8/50], Train Loss: 0.0429, Val Loss: 0.1041\n",
      "Epoch [9/50], Train Loss: 0.0411, Val Loss: 0.0983\n",
      "Epoch [10/50], Train Loss: 0.0411, Val Loss: 0.0940\n",
      "Epoch [11/50], Train Loss: 0.0406, Val Loss: 0.0905\n",
      "Epoch [12/50], Train Loss: 0.0392, Val Loss: 0.0879\n",
      "Epoch [13/50], Train Loss: 0.0399, Val Loss: 0.0860\n",
      "Epoch [14/50], Train Loss: 0.0386, Val Loss: 0.0845\n",
      "Epoch [15/50], Train Loss: 0.0379, Val Loss: 0.0831\n",
      "Epoch [16/50], Train Loss: 0.0381, Val Loss: 0.0817\n",
      "Epoch [17/50], Train Loss: 0.0377, Val Loss: 0.0802\n",
      "Epoch [18/50], Train Loss: 0.0366, Val Loss: 0.0786\n",
      "Epoch [19/50], Train Loss: 0.0364, Val Loss: 0.0769\n",
      "Epoch [20/50], Train Loss: 0.0370, Val Loss: 0.0749\n",
      "Epoch [21/50], Train Loss: 0.0348, Val Loss: 0.0736\n",
      "Epoch [22/50], Train Loss: 0.0341, Val Loss: 0.0722\n",
      "Epoch [23/50], Train Loss: 0.0347, Val Loss: 0.0708\n",
      "Epoch [24/50], Train Loss: 0.0349, Val Loss: 0.0691\n",
      "Epoch [25/50], Train Loss: 0.0333, Val Loss: 0.0673\n",
      "Epoch [26/50], Train Loss: 0.0328, Val Loss: 0.0655\n",
      "Epoch [27/50], Train Loss: 0.0316, Val Loss: 0.0641\n",
      "Epoch [28/50], Train Loss: 0.0325, Val Loss: 0.0627\n",
      "Epoch [29/50], Train Loss: 0.0314, Val Loss: 0.0605\n",
      "Epoch [30/50], Train Loss: 0.0300, Val Loss: 0.0588\n",
      "Epoch [31/50], Train Loss: 0.0299, Val Loss: 0.0573\n",
      "Epoch [32/50], Train Loss: 0.0302, Val Loss: 0.0553\n",
      "Epoch [33/50], Train Loss: 0.0297, Val Loss: 0.0535\n",
      "Epoch [34/50], Train Loss: 0.0287, Val Loss: 0.0520\n",
      "Epoch [35/50], Train Loss: 0.0273, Val Loss: 0.0501\n",
      "Epoch [36/50], Train Loss: 0.0278, Val Loss: 0.0484\n",
      "Epoch [37/50], Train Loss: 0.0270, Val Loss: 0.0467\n",
      "Epoch [38/50], Train Loss: 0.0258, Val Loss: 0.0449\n",
      "Epoch [39/50], Train Loss: 0.0258, Val Loss: 0.0430\n",
      "Epoch [40/50], Train Loss: 0.0252, Val Loss: 0.0413\n",
      "Epoch [41/50], Train Loss: 0.0247, Val Loss: 0.0393\n",
      "Epoch [42/50], Train Loss: 0.0243, Val Loss: 0.0376\n",
      "Epoch [43/50], Train Loss: 0.0236, Val Loss: 0.0359\n",
      "Epoch [44/50], Train Loss: 0.0234, Val Loss: 0.0344\n",
      "Epoch [45/50], Train Loss: 0.0222, Val Loss: 0.0324\n",
      "Epoch [46/50], Train Loss: 0.0219, Val Loss: 0.0312\n",
      "Epoch [47/50], Train Loss: 0.0225, Val Loss: 0.0295\n",
      "Epoch [48/50], Train Loss: 0.0207, Val Loss: 0.0277\n",
      "Epoch [49/50], Train Loss: 0.0211, Val Loss: 0.0262\n",
      "Epoch [50/50], Train Loss: 0.0206, Val Loss: 0.0254\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=32, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0804, Val Loss: 0.1637\n",
      "Epoch [2/50], Train Loss: 0.0689, Val Loss: 0.1399\n",
      "Epoch [3/50], Train Loss: 0.0638, Val Loss: 0.1239\n",
      "Epoch [4/50], Train Loss: 0.0606, Val Loss: 0.1134\n",
      "Epoch [5/50], Train Loss: 0.0561, Val Loss: 0.1051\n",
      "Epoch [6/50], Train Loss: 0.0553, Val Loss: 0.0989\n",
      "Epoch [7/50], Train Loss: 0.0526, Val Loss: 0.0954\n",
      "Epoch [8/50], Train Loss: 0.0527, Val Loss: 0.0912\n",
      "Epoch [9/50], Train Loss: 0.0505, Val Loss: 0.0873\n",
      "Epoch [10/50], Train Loss: 0.0510, Val Loss: 0.0851\n",
      "Epoch [11/50], Train Loss: 0.0482, Val Loss: 0.0836\n",
      "Epoch [12/50], Train Loss: 0.0467, Val Loss: 0.0818\n",
      "Epoch [13/50], Train Loss: 0.0467, Val Loss: 0.0800\n",
      "Epoch [14/50], Train Loss: 0.0461, Val Loss: 0.0784\n",
      "Epoch [15/50], Train Loss: 0.0436, Val Loss: 0.0762\n",
      "Epoch [16/50], Train Loss: 0.0445, Val Loss: 0.0742\n",
      "Epoch [17/50], Train Loss: 0.0444, Val Loss: 0.0718\n",
      "Epoch [18/50], Train Loss: 0.0422, Val Loss: 0.0709\n",
      "Epoch [19/50], Train Loss: 0.0420, Val Loss: 0.0677\n",
      "Epoch [20/50], Train Loss: 0.0402, Val Loss: 0.0665\n",
      "Epoch [21/50], Train Loss: 0.0412, Val Loss: 0.0652\n",
      "Epoch [22/50], Train Loss: 0.0403, Val Loss: 0.0624\n",
      "Epoch [23/50], Train Loss: 0.0406, Val Loss: 0.0599\n",
      "Epoch [24/50], Train Loss: 0.0388, Val Loss: 0.0585\n",
      "Epoch [25/50], Train Loss: 0.0400, Val Loss: 0.0559\n",
      "Epoch [26/50], Train Loss: 0.0377, Val Loss: 0.0538\n",
      "Epoch [27/50], Train Loss: 0.0389, Val Loss: 0.0518\n",
      "Epoch [28/50], Train Loss: 0.0356, Val Loss: 0.0495\n",
      "Epoch [29/50], Train Loss: 0.0349, Val Loss: 0.0479\n",
      "Epoch [30/50], Train Loss: 0.0357, Val Loss: 0.0461\n",
      "Epoch [31/50], Train Loss: 0.0355, Val Loss: 0.0440\n",
      "Epoch [32/50], Train Loss: 0.0339, Val Loss: 0.0423\n",
      "Epoch [33/50], Train Loss: 0.0337, Val Loss: 0.0396\n",
      "Epoch [34/50], Train Loss: 0.0339, Val Loss: 0.0382\n",
      "Epoch [35/50], Train Loss: 0.0322, Val Loss: 0.0366\n",
      "Epoch [36/50], Train Loss: 0.0331, Val Loss: 0.0353\n",
      "Epoch [37/50], Train Loss: 0.0320, Val Loss: 0.0340\n",
      "Epoch [38/50], Train Loss: 0.0319, Val Loss: 0.0323\n",
      "Epoch [39/50], Train Loss: 0.0310, Val Loss: 0.0308\n",
      "Epoch [40/50], Train Loss: 0.0310, Val Loss: 0.0303\n",
      "Epoch [41/50], Train Loss: 0.0306, Val Loss: 0.0289\n",
      "Epoch [42/50], Train Loss: 0.0305, Val Loss: 0.0278\n",
      "Epoch [43/50], Train Loss: 0.0293, Val Loss: 0.0264\n",
      "Epoch [44/50], Train Loss: 0.0291, Val Loss: 0.0251\n",
      "Epoch [45/50], Train Loss: 0.0291, Val Loss: 0.0237\n",
      "Epoch [46/50], Train Loss: 0.0284, Val Loss: 0.0223\n",
      "Epoch [47/50], Train Loss: 0.0282, Val Loss: 0.0218\n",
      "Epoch [48/50], Train Loss: 0.0276, Val Loss: 0.0214\n",
      "Epoch [49/50], Train Loss: 0.0273, Val Loss: 0.0201\n",
      "Epoch [50/50], Train Loss: 0.0287, Val Loss: 0.0192\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=32, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1394, Val Loss: 0.3312\n",
      "Epoch [2/50], Train Loss: 0.0960, Val Loss: 0.2668\n",
      "Epoch [3/50], Train Loss: 0.0709, Val Loss: 0.2220\n",
      "Epoch [4/50], Train Loss: 0.0561, Val Loss: 0.1899\n",
      "Epoch [5/50], Train Loss: 0.0475, Val Loss: 0.1668\n",
      "Epoch [6/50], Train Loss: 0.0427, Val Loss: 0.1502\n",
      "Epoch [7/50], Train Loss: 0.0401, Val Loss: 0.1382\n",
      "Epoch [8/50], Train Loss: 0.0387, Val Loss: 0.1294\n",
      "Epoch [9/50], Train Loss: 0.0380, Val Loss: 0.1231\n",
      "Epoch [10/50], Train Loss: 0.0376, Val Loss: 0.1184\n",
      "Epoch [11/50], Train Loss: 0.0373, Val Loss: 0.1149\n",
      "Epoch [12/50], Train Loss: 0.0372, Val Loss: 0.1122\n",
      "Epoch [13/50], Train Loss: 0.0370, Val Loss: 0.1102\n",
      "Epoch [14/50], Train Loss: 0.0369, Val Loss: 0.1086\n",
      "Epoch [15/50], Train Loss: 0.0368, Val Loss: 0.1073\n",
      "Epoch [16/50], Train Loss: 0.0367, Val Loss: 0.1063\n",
      "Epoch [17/50], Train Loss: 0.0366, Val Loss: 0.1054\n",
      "Epoch [18/50], Train Loss: 0.0365, Val Loss: 0.1047\n",
      "Epoch [19/50], Train Loss: 0.0364, Val Loss: 0.1041\n",
      "Epoch [20/50], Train Loss: 0.0363, Val Loss: 0.1035\n",
      "Epoch [21/50], Train Loss: 0.0361, Val Loss: 0.1029\n",
      "Epoch [22/50], Train Loss: 0.0360, Val Loss: 0.1024\n",
      "Epoch [23/50], Train Loss: 0.0359, Val Loss: 0.1020\n",
      "Epoch [24/50], Train Loss: 0.0358, Val Loss: 0.1015\n",
      "Epoch [25/50], Train Loss: 0.0357, Val Loss: 0.1011\n",
      "Epoch [26/50], Train Loss: 0.0356, Val Loss: 0.1007\n",
      "Epoch [27/50], Train Loss: 0.0354, Val Loss: 0.1002\n",
      "Epoch [28/50], Train Loss: 0.0353, Val Loss: 0.0998\n",
      "Epoch [29/50], Train Loss: 0.0352, Val Loss: 0.0994\n",
      "Epoch [30/50], Train Loss: 0.0351, Val Loss: 0.0989\n",
      "Epoch [31/50], Train Loss: 0.0349, Val Loss: 0.0985\n",
      "Epoch [32/50], Train Loss: 0.0348, Val Loss: 0.0981\n",
      "Epoch [33/50], Train Loss: 0.0347, Val Loss: 0.0977\n",
      "Epoch [34/50], Train Loss: 0.0346, Val Loss: 0.0972\n",
      "Epoch [35/50], Train Loss: 0.0344, Val Loss: 0.0967\n",
      "Epoch [36/50], Train Loss: 0.0343, Val Loss: 0.0963\n",
      "Epoch [37/50], Train Loss: 0.0342, Val Loss: 0.0958\n",
      "Epoch [38/50], Train Loss: 0.0340, Val Loss: 0.0953\n",
      "Epoch [39/50], Train Loss: 0.0339, Val Loss: 0.0948\n",
      "Epoch [40/50], Train Loss: 0.0337, Val Loss: 0.0943\n",
      "Epoch [41/50], Train Loss: 0.0336, Val Loss: 0.0938\n",
      "Epoch [42/50], Train Loss: 0.0334, Val Loss: 0.0932\n",
      "Epoch [43/50], Train Loss: 0.0333, Val Loss: 0.0927\n",
      "Epoch [44/50], Train Loss: 0.0331, Val Loss: 0.0921\n",
      "Epoch [45/50], Train Loss: 0.0330, Val Loss: 0.0915\n",
      "Epoch [46/50], Train Loss: 0.0328, Val Loss: 0.0909\n",
      "Epoch [47/50], Train Loss: 0.0326, Val Loss: 0.0903\n",
      "Epoch [48/50], Train Loss: 0.0324, Val Loss: 0.0896\n",
      "Epoch [49/50], Train Loss: 0.0322, Val Loss: 0.0889\n",
      "Epoch [50/50], Train Loss: 0.0320, Val Loss: 0.0882\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=32, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1180, Val Loss: 0.3144\n",
      "Epoch [2/50], Train Loss: 0.0881, Val Loss: 0.2547\n",
      "Epoch [3/50], Train Loss: 0.0695, Val Loss: 0.2133\n",
      "Epoch [4/50], Train Loss: 0.0603, Val Loss: 0.1838\n",
      "Epoch [5/50], Train Loss: 0.0547, Val Loss: 0.1630\n",
      "Epoch [6/50], Train Loss: 0.0513, Val Loss: 0.1474\n",
      "Epoch [7/50], Train Loss: 0.0483, Val Loss: 0.1361\n",
      "Epoch [8/50], Train Loss: 0.0470, Val Loss: 0.1280\n",
      "Epoch [9/50], Train Loss: 0.0464, Val Loss: 0.1224\n",
      "Epoch [10/50], Train Loss: 0.0458, Val Loss: 0.1178\n",
      "Epoch [11/50], Train Loss: 0.0449, Val Loss: 0.1148\n",
      "Epoch [12/50], Train Loss: 0.0463, Val Loss: 0.1122\n",
      "Epoch [13/50], Train Loss: 0.0450, Val Loss: 0.1104\n",
      "Epoch [14/50], Train Loss: 0.0470, Val Loss: 0.1092\n",
      "Epoch [15/50], Train Loss: 0.0459, Val Loss: 0.1080\n",
      "Epoch [16/50], Train Loss: 0.0457, Val Loss: 0.1070\n",
      "Epoch [17/50], Train Loss: 0.0442, Val Loss: 0.1060\n",
      "Epoch [18/50], Train Loss: 0.0438, Val Loss: 0.1050\n",
      "Epoch [19/50], Train Loss: 0.0441, Val Loss: 0.1051\n",
      "Epoch [20/50], Train Loss: 0.0446, Val Loss: 0.1043\n",
      "Epoch [21/50], Train Loss: 0.0424, Val Loss: 0.1036\n",
      "Epoch [22/50], Train Loss: 0.0437, Val Loss: 0.1032\n",
      "Epoch [23/50], Train Loss: 0.0425, Val Loss: 0.1025\n",
      "Epoch [24/50], Train Loss: 0.0442, Val Loss: 0.1025\n",
      "Epoch [25/50], Train Loss: 0.0436, Val Loss: 0.1022\n",
      "Epoch [26/50], Train Loss: 0.0425, Val Loss: 0.1022\n",
      "Epoch [27/50], Train Loss: 0.0420, Val Loss: 0.1016\n",
      "Epoch [28/50], Train Loss: 0.0415, Val Loss: 0.1009\n",
      "Epoch [29/50], Train Loss: 0.0429, Val Loss: 0.1005\n",
      "Epoch [30/50], Train Loss: 0.0416, Val Loss: 0.1003\n",
      "Epoch [31/50], Train Loss: 0.0417, Val Loss: 0.0997\n",
      "Epoch [32/50], Train Loss: 0.0425, Val Loss: 0.0995\n",
      "Epoch [33/50], Train Loss: 0.0408, Val Loss: 0.0987\n",
      "Epoch [34/50], Train Loss: 0.0424, Val Loss: 0.0986\n",
      "Epoch [35/50], Train Loss: 0.0404, Val Loss: 0.0983\n",
      "Epoch [36/50], Train Loss: 0.0409, Val Loss: 0.0981\n",
      "Epoch [37/50], Train Loss: 0.0406, Val Loss: 0.0982\n",
      "Epoch [38/50], Train Loss: 0.0402, Val Loss: 0.0975\n",
      "Epoch [39/50], Train Loss: 0.0395, Val Loss: 0.0968\n",
      "Epoch [40/50], Train Loss: 0.0393, Val Loss: 0.0959\n",
      "Epoch [41/50], Train Loss: 0.0394, Val Loss: 0.0954\n",
      "Epoch [42/50], Train Loss: 0.0394, Val Loss: 0.0952\n",
      "Epoch [43/50], Train Loss: 0.0390, Val Loss: 0.0947\n",
      "Epoch [44/50], Train Loss: 0.0403, Val Loss: 0.0944\n",
      "Epoch [45/50], Train Loss: 0.0389, Val Loss: 0.0941\n",
      "Epoch [46/50], Train Loss: 0.0396, Val Loss: 0.0934\n",
      "Epoch [47/50], Train Loss: 0.0385, Val Loss: 0.0924\n",
      "Epoch [48/50], Train Loss: 0.0398, Val Loss: 0.0921\n",
      "Epoch [49/50], Train Loss: 0.0382, Val Loss: 0.0918\n",
      "Epoch [50/50], Train Loss: 0.0389, Val Loss: 0.0913\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=32, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.2087, Val Loss: 0.3748\n",
      "Epoch [2/50], Train Loss: 0.1542, Val Loss: 0.2850\n",
      "Epoch [3/50], Train Loss: 0.1217, Val Loss: 0.2295\n",
      "Epoch [4/50], Train Loss: 0.1065, Val Loss: 0.1956\n",
      "Epoch [5/50], Train Loss: 0.0991, Val Loss: 0.1728\n",
      "Epoch [6/50], Train Loss: 0.0884, Val Loss: 0.1564\n",
      "Epoch [7/50], Train Loss: 0.0856, Val Loss: 0.1462\n",
      "Epoch [8/50], Train Loss: 0.0790, Val Loss: 0.1402\n",
      "Epoch [9/50], Train Loss: 0.0743, Val Loss: 0.1352\n",
      "Epoch [10/50], Train Loss: 0.0746, Val Loss: 0.1294\n",
      "Epoch [11/50], Train Loss: 0.0715, Val Loss: 0.1267\n",
      "Epoch [12/50], Train Loss: 0.0700, Val Loss: 0.1234\n",
      "Epoch [13/50], Train Loss: 0.0714, Val Loss: 0.1208\n",
      "Epoch [14/50], Train Loss: 0.0665, Val Loss: 0.1197\n",
      "Epoch [15/50], Train Loss: 0.0642, Val Loss: 0.1181\n",
      "Epoch [16/50], Train Loss: 0.0627, Val Loss: 0.1143\n",
      "Epoch [17/50], Train Loss: 0.0628, Val Loss: 0.1135\n",
      "Epoch [18/50], Train Loss: 0.0598, Val Loss: 0.1104\n",
      "Epoch [19/50], Train Loss: 0.0598, Val Loss: 0.1078\n",
      "Epoch [20/50], Train Loss: 0.0601, Val Loss: 0.1055\n",
      "Epoch [21/50], Train Loss: 0.0591, Val Loss: 0.1044\n",
      "Epoch [22/50], Train Loss: 0.0549, Val Loss: 0.1018\n",
      "Epoch [23/50], Train Loss: 0.0551, Val Loss: 0.1012\n",
      "Epoch [24/50], Train Loss: 0.0556, Val Loss: 0.0985\n",
      "Epoch [25/50], Train Loss: 0.0530, Val Loss: 0.0962\n",
      "Epoch [26/50], Train Loss: 0.0519, Val Loss: 0.0942\n",
      "Epoch [27/50], Train Loss: 0.0504, Val Loss: 0.0938\n",
      "Epoch [28/50], Train Loss: 0.0508, Val Loss: 0.0917\n",
      "Epoch [29/50], Train Loss: 0.0494, Val Loss: 0.0883\n",
      "Epoch [30/50], Train Loss: 0.0490, Val Loss: 0.0857\n",
      "Epoch [31/50], Train Loss: 0.0471, Val Loss: 0.0838\n",
      "Epoch [32/50], Train Loss: 0.0483, Val Loss: 0.0824\n",
      "Epoch [33/50], Train Loss: 0.0452, Val Loss: 0.0785\n",
      "Epoch [34/50], Train Loss: 0.0467, Val Loss: 0.0759\n",
      "Epoch [35/50], Train Loss: 0.0454, Val Loss: 0.0741\n",
      "Epoch [36/50], Train Loss: 0.0446, Val Loss: 0.0715\n",
      "Epoch [37/50], Train Loss: 0.0439, Val Loss: 0.0687\n",
      "Epoch [38/50], Train Loss: 0.0428, Val Loss: 0.0661\n",
      "Epoch [39/50], Train Loss: 0.0424, Val Loss: 0.0642\n",
      "Epoch [40/50], Train Loss: 0.0405, Val Loss: 0.0608\n",
      "Epoch [41/50], Train Loss: 0.0395, Val Loss: 0.0586\n",
      "Epoch [42/50], Train Loss: 0.0382, Val Loss: 0.0569\n",
      "Epoch [43/50], Train Loss: 0.0389, Val Loss: 0.0537\n",
      "Epoch [44/50], Train Loss: 0.0378, Val Loss: 0.0520\n",
      "Epoch [45/50], Train Loss: 0.0377, Val Loss: 0.0483\n",
      "Epoch [46/50], Train Loss: 0.0343, Val Loss: 0.0472\n",
      "Epoch [47/50], Train Loss: 0.0362, Val Loss: 0.0452\n",
      "Epoch [48/50], Train Loss: 0.0347, Val Loss: 0.0418\n",
      "Epoch [49/50], Train Loss: 0.0332, Val Loss: 0.0407\n",
      "Epoch [50/50], Train Loss: 0.0345, Val Loss: 0.0382\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=64, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1208, Val Loss: 0.3078\n",
      "Epoch [2/50], Train Loss: 0.0910, Val Loss: 0.2530\n",
      "Epoch [3/50], Train Loss: 0.0706, Val Loss: 0.2105\n",
      "Epoch [4/50], Train Loss: 0.0567, Val Loss: 0.1773\n",
      "Epoch [5/50], Train Loss: 0.0474, Val Loss: 0.1518\n",
      "Epoch [6/50], Train Loss: 0.0414, Val Loss: 0.1325\n",
      "Epoch [7/50], Train Loss: 0.0377, Val Loss: 0.1181\n",
      "Epoch [8/50], Train Loss: 0.0354, Val Loss: 0.1075\n",
      "Epoch [9/50], Train Loss: 0.0340, Val Loss: 0.0997\n",
      "Epoch [10/50], Train Loss: 0.0331, Val Loss: 0.0938\n",
      "Epoch [11/50], Train Loss: 0.0325, Val Loss: 0.0894\n",
      "Epoch [12/50], Train Loss: 0.0320, Val Loss: 0.0861\n",
      "Epoch [13/50], Train Loss: 0.0316, Val Loss: 0.0834\n",
      "Epoch [14/50], Train Loss: 0.0312, Val Loss: 0.0812\n",
      "Epoch [15/50], Train Loss: 0.0308, Val Loss: 0.0793\n",
      "Epoch [16/50], Train Loss: 0.0304, Val Loss: 0.0776\n",
      "Epoch [17/50], Train Loss: 0.0301, Val Loss: 0.0762\n",
      "Epoch [18/50], Train Loss: 0.0297, Val Loss: 0.0748\n",
      "Epoch [19/50], Train Loss: 0.0293, Val Loss: 0.0735\n",
      "Epoch [20/50], Train Loss: 0.0290, Val Loss: 0.0723\n",
      "Epoch [21/50], Train Loss: 0.0286, Val Loss: 0.0712\n",
      "Epoch [22/50], Train Loss: 0.0283, Val Loss: 0.0701\n",
      "Epoch [23/50], Train Loss: 0.0279, Val Loss: 0.0690\n",
      "Epoch [24/50], Train Loss: 0.0276, Val Loss: 0.0679\n",
      "Epoch [25/50], Train Loss: 0.0273, Val Loss: 0.0669\n",
      "Epoch [26/50], Train Loss: 0.0269, Val Loss: 0.0659\n",
      "Epoch [27/50], Train Loss: 0.0266, Val Loss: 0.0649\n",
      "Epoch [28/50], Train Loss: 0.0262, Val Loss: 0.0639\n",
      "Epoch [29/50], Train Loss: 0.0259, Val Loss: 0.0629\n",
      "Epoch [30/50], Train Loss: 0.0256, Val Loss: 0.0619\n",
      "Epoch [31/50], Train Loss: 0.0253, Val Loss: 0.0610\n",
      "Epoch [32/50], Train Loss: 0.0249, Val Loss: 0.0600\n",
      "Epoch [33/50], Train Loss: 0.0246, Val Loss: 0.0591\n",
      "Epoch [34/50], Train Loss: 0.0243, Val Loss: 0.0581\n",
      "Epoch [35/50], Train Loss: 0.0240, Val Loss: 0.0572\n",
      "Epoch [36/50], Train Loss: 0.0237, Val Loss: 0.0563\n",
      "Epoch [37/50], Train Loss: 0.0233, Val Loss: 0.0553\n",
      "Epoch [38/50], Train Loss: 0.0230, Val Loss: 0.0544\n",
      "Epoch [39/50], Train Loss: 0.0227, Val Loss: 0.0535\n",
      "Epoch [40/50], Train Loss: 0.0224, Val Loss: 0.0526\n",
      "Epoch [41/50], Train Loss: 0.0221, Val Loss: 0.0517\n",
      "Epoch [42/50], Train Loss: 0.0218, Val Loss: 0.0509\n",
      "Epoch [43/50], Train Loss: 0.0215, Val Loss: 0.0500\n",
      "Epoch [44/50], Train Loss: 0.0211, Val Loss: 0.0491\n",
      "Epoch [45/50], Train Loss: 0.0208, Val Loss: 0.0482\n",
      "Epoch [46/50], Train Loss: 0.0205, Val Loss: 0.0474\n",
      "Epoch [47/50], Train Loss: 0.0202, Val Loss: 0.0465\n",
      "Epoch [48/50], Train Loss: 0.0199, Val Loss: 0.0456\n",
      "Epoch [49/50], Train Loss: 0.0196, Val Loss: 0.0448\n",
      "Epoch [50/50], Train Loss: 0.0193, Val Loss: 0.0439\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=64, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1027, Val Loss: 0.2730\n",
      "Epoch [2/50], Train Loss: 0.0774, Val Loss: 0.2215\n",
      "Epoch [3/50], Train Loss: 0.0608, Val Loss: 0.1827\n",
      "Epoch [4/50], Train Loss: 0.0509, Val Loss: 0.1533\n",
      "Epoch [5/50], Train Loss: 0.0433, Val Loss: 0.1312\n",
      "Epoch [6/50], Train Loss: 0.0384, Val Loss: 0.1148\n",
      "Epoch [7/50], Train Loss: 0.0366, Val Loss: 0.1032\n",
      "Epoch [8/50], Train Loss: 0.0347, Val Loss: 0.0947\n",
      "Epoch [9/50], Train Loss: 0.0337, Val Loss: 0.0883\n",
      "Epoch [10/50], Train Loss: 0.0330, Val Loss: 0.0836\n",
      "Epoch [11/50], Train Loss: 0.0325, Val Loss: 0.0804\n",
      "Epoch [12/50], Train Loss: 0.0324, Val Loss: 0.0775\n",
      "Epoch [13/50], Train Loss: 0.0316, Val Loss: 0.0753\n",
      "Epoch [14/50], Train Loss: 0.0318, Val Loss: 0.0736\n",
      "Epoch [15/50], Train Loss: 0.0310, Val Loss: 0.0720\n",
      "Epoch [16/50], Train Loss: 0.0301, Val Loss: 0.0705\n",
      "Epoch [17/50], Train Loss: 0.0300, Val Loss: 0.0693\n",
      "Epoch [18/50], Train Loss: 0.0296, Val Loss: 0.0680\n",
      "Epoch [19/50], Train Loss: 0.0296, Val Loss: 0.0668\n",
      "Epoch [20/50], Train Loss: 0.0285, Val Loss: 0.0656\n",
      "Epoch [21/50], Train Loss: 0.0292, Val Loss: 0.0646\n",
      "Epoch [22/50], Train Loss: 0.0286, Val Loss: 0.0636\n",
      "Epoch [23/50], Train Loss: 0.0283, Val Loss: 0.0625\n",
      "Epoch [24/50], Train Loss: 0.0279, Val Loss: 0.0614\n",
      "Epoch [25/50], Train Loss: 0.0273, Val Loss: 0.0603\n",
      "Epoch [26/50], Train Loss: 0.0270, Val Loss: 0.0590\n",
      "Epoch [27/50], Train Loss: 0.0268, Val Loss: 0.0577\n",
      "Epoch [28/50], Train Loss: 0.0266, Val Loss: 0.0568\n",
      "Epoch [29/50], Train Loss: 0.0254, Val Loss: 0.0561\n",
      "Epoch [30/50], Train Loss: 0.0255, Val Loss: 0.0549\n",
      "Epoch [31/50], Train Loss: 0.0259, Val Loss: 0.0537\n",
      "Epoch [32/50], Train Loss: 0.0249, Val Loss: 0.0524\n",
      "Epoch [33/50], Train Loss: 0.0249, Val Loss: 0.0515\n",
      "Epoch [34/50], Train Loss: 0.0239, Val Loss: 0.0505\n",
      "Epoch [35/50], Train Loss: 0.0239, Val Loss: 0.0491\n",
      "Epoch [36/50], Train Loss: 0.0233, Val Loss: 0.0480\n",
      "Epoch [37/50], Train Loss: 0.0225, Val Loss: 0.0472\n",
      "Epoch [38/50], Train Loss: 0.0228, Val Loss: 0.0462\n",
      "Epoch [39/50], Train Loss: 0.0218, Val Loss: 0.0450\n",
      "Epoch [40/50], Train Loss: 0.0216, Val Loss: 0.0440\n",
      "Epoch [41/50], Train Loss: 0.0214, Val Loss: 0.0433\n",
      "Epoch [42/50], Train Loss: 0.0213, Val Loss: 0.0422\n",
      "Epoch [43/50], Train Loss: 0.0208, Val Loss: 0.0412\n",
      "Epoch [44/50], Train Loss: 0.0204, Val Loss: 0.0400\n",
      "Epoch [45/50], Train Loss: 0.0200, Val Loss: 0.0390\n",
      "Epoch [46/50], Train Loss: 0.0199, Val Loss: 0.0380\n",
      "Epoch [47/50], Train Loss: 0.0197, Val Loss: 0.0369\n",
      "Epoch [48/50], Train Loss: 0.0191, Val Loss: 0.0358\n",
      "Epoch [49/50], Train Loss: 0.0188, Val Loss: 0.0346\n",
      "Epoch [50/50], Train Loss: 0.0182, Val Loss: 0.0336\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=64, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1306, Val Loss: 0.3010\n",
      "Epoch [2/50], Train Loss: 0.0984, Val Loss: 0.2482\n",
      "Epoch [3/50], Train Loss: 0.0791, Val Loss: 0.2070\n",
      "Epoch [4/50], Train Loss: 0.0657, Val Loss: 0.1752\n",
      "Epoch [5/50], Train Loss: 0.0581, Val Loss: 0.1511\n",
      "Epoch [6/50], Train Loss: 0.0526, Val Loss: 0.1332\n",
      "Epoch [7/50], Train Loss: 0.0490, Val Loss: 0.1202\n",
      "Epoch [8/50], Train Loss: 0.0452, Val Loss: 0.1097\n",
      "Epoch [9/50], Train Loss: 0.0438, Val Loss: 0.1026\n",
      "Epoch [10/50], Train Loss: 0.0433, Val Loss: 0.0967\n",
      "Epoch [11/50], Train Loss: 0.0411, Val Loss: 0.0924\n",
      "Epoch [12/50], Train Loss: 0.0406, Val Loss: 0.0891\n",
      "Epoch [13/50], Train Loss: 0.0391, Val Loss: 0.0861\n",
      "Epoch [14/50], Train Loss: 0.0395, Val Loss: 0.0837\n",
      "Epoch [15/50], Train Loss: 0.0390, Val Loss: 0.0815\n",
      "Epoch [16/50], Train Loss: 0.0392, Val Loss: 0.0797\n",
      "Epoch [17/50], Train Loss: 0.0375, Val Loss: 0.0782\n",
      "Epoch [18/50], Train Loss: 0.0368, Val Loss: 0.0764\n",
      "Epoch [19/50], Train Loss: 0.0363, Val Loss: 0.0747\n",
      "Epoch [20/50], Train Loss: 0.0355, Val Loss: 0.0730\n",
      "Epoch [21/50], Train Loss: 0.0359, Val Loss: 0.0720\n",
      "Epoch [22/50], Train Loss: 0.0345, Val Loss: 0.0713\n",
      "Epoch [23/50], Train Loss: 0.0346, Val Loss: 0.0699\n",
      "Epoch [24/50], Train Loss: 0.0337, Val Loss: 0.0691\n",
      "Epoch [25/50], Train Loss: 0.0334, Val Loss: 0.0678\n",
      "Epoch [26/50], Train Loss: 0.0330, Val Loss: 0.0664\n",
      "Epoch [27/50], Train Loss: 0.0325, Val Loss: 0.0657\n",
      "Epoch [28/50], Train Loss: 0.0318, Val Loss: 0.0642\n",
      "Epoch [29/50], Train Loss: 0.0314, Val Loss: 0.0632\n",
      "Epoch [30/50], Train Loss: 0.0304, Val Loss: 0.0620\n",
      "Epoch [31/50], Train Loss: 0.0308, Val Loss: 0.0609\n",
      "Epoch [32/50], Train Loss: 0.0293, Val Loss: 0.0599\n",
      "Epoch [33/50], Train Loss: 0.0300, Val Loss: 0.0589\n",
      "Epoch [34/50], Train Loss: 0.0295, Val Loss: 0.0581\n",
      "Epoch [35/50], Train Loss: 0.0295, Val Loss: 0.0570\n",
      "Epoch [36/50], Train Loss: 0.0290, Val Loss: 0.0562\n",
      "Epoch [37/50], Train Loss: 0.0276, Val Loss: 0.0547\n",
      "Epoch [38/50], Train Loss: 0.0276, Val Loss: 0.0538\n",
      "Epoch [39/50], Train Loss: 0.0278, Val Loss: 0.0527\n",
      "Epoch [40/50], Train Loss: 0.0268, Val Loss: 0.0517\n",
      "Epoch [41/50], Train Loss: 0.0263, Val Loss: 0.0511\n",
      "Epoch [42/50], Train Loss: 0.0258, Val Loss: 0.0503\n",
      "Epoch [43/50], Train Loss: 0.0257, Val Loss: 0.0494\n",
      "Epoch [44/50], Train Loss: 0.0254, Val Loss: 0.0483\n",
      "Epoch [45/50], Train Loss: 0.0252, Val Loss: 0.0471\n",
      "Epoch [46/50], Train Loss: 0.0247, Val Loss: 0.0463\n",
      "Epoch [47/50], Train Loss: 0.0243, Val Loss: 0.0455\n",
      "Epoch [48/50], Train Loss: 0.0239, Val Loss: 0.0443\n",
      "Epoch [49/50], Train Loss: 0.0236, Val Loss: 0.0430\n",
      "Epoch [50/50], Train Loss: 0.0233, Val Loss: 0.0420\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=64, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1173, Val Loss: 0.2742\n",
      "Epoch [2/50], Train Loss: 0.0827, Val Loss: 0.2184\n",
      "Epoch [3/50], Train Loss: 0.0619, Val Loss: 0.1789\n",
      "Epoch [4/50], Train Loss: 0.0495, Val Loss: 0.1508\n",
      "Epoch [5/50], Train Loss: 0.0423, Val Loss: 0.1308\n",
      "Epoch [6/50], Train Loss: 0.0383, Val Loss: 0.1167\n",
      "Epoch [7/50], Train Loss: 0.0362, Val Loss: 0.1068\n",
      "Epoch [8/50], Train Loss: 0.0350, Val Loss: 0.0998\n",
      "Epoch [9/50], Train Loss: 0.0344, Val Loss: 0.0948\n",
      "Epoch [10/50], Train Loss: 0.0339, Val Loss: 0.0911\n",
      "Epoch [11/50], Train Loss: 0.0336, Val Loss: 0.0882\n",
      "Epoch [12/50], Train Loss: 0.0333, Val Loss: 0.0860\n",
      "Epoch [13/50], Train Loss: 0.0330, Val Loss: 0.0842\n",
      "Epoch [14/50], Train Loss: 0.0327, Val Loss: 0.0827\n",
      "Epoch [15/50], Train Loss: 0.0325, Val Loss: 0.0813\n",
      "Epoch [16/50], Train Loss: 0.0322, Val Loss: 0.0800\n",
      "Epoch [17/50], Train Loss: 0.0319, Val Loss: 0.0789\n",
      "Epoch [18/50], Train Loss: 0.0316, Val Loss: 0.0777\n",
      "Epoch [19/50], Train Loss: 0.0313, Val Loss: 0.0766\n",
      "Epoch [20/50], Train Loss: 0.0310, Val Loss: 0.0755\n",
      "Epoch [21/50], Train Loss: 0.0307, Val Loss: 0.0745\n",
      "Epoch [22/50], Train Loss: 0.0304, Val Loss: 0.0734\n",
      "Epoch [23/50], Train Loss: 0.0301, Val Loss: 0.0723\n",
      "Epoch [24/50], Train Loss: 0.0298, Val Loss: 0.0712\n",
      "Epoch [25/50], Train Loss: 0.0295, Val Loss: 0.0701\n",
      "Epoch [26/50], Train Loss: 0.0292, Val Loss: 0.0690\n",
      "Epoch [27/50], Train Loss: 0.0289, Val Loss: 0.0679\n",
      "Epoch [28/50], Train Loss: 0.0285, Val Loss: 0.0667\n",
      "Epoch [29/50], Train Loss: 0.0282, Val Loss: 0.0656\n",
      "Epoch [30/50], Train Loss: 0.0279, Val Loss: 0.0644\n",
      "Epoch [31/50], Train Loss: 0.0275, Val Loss: 0.0632\n",
      "Epoch [32/50], Train Loss: 0.0272, Val Loss: 0.0620\n",
      "Epoch [33/50], Train Loss: 0.0268, Val Loss: 0.0608\n",
      "Epoch [34/50], Train Loss: 0.0265, Val Loss: 0.0596\n",
      "Epoch [35/50], Train Loss: 0.0261, Val Loss: 0.0583\n",
      "Epoch [36/50], Train Loss: 0.0258, Val Loss: 0.0570\n",
      "Epoch [37/50], Train Loss: 0.0254, Val Loss: 0.0557\n",
      "Epoch [38/50], Train Loss: 0.0250, Val Loss: 0.0544\n",
      "Epoch [39/50], Train Loss: 0.0247, Val Loss: 0.0531\n",
      "Epoch [40/50], Train Loss: 0.0243, Val Loss: 0.0517\n",
      "Epoch [41/50], Train Loss: 0.0239, Val Loss: 0.0504\n",
      "Epoch [42/50], Train Loss: 0.0235, Val Loss: 0.0490\n",
      "Epoch [43/50], Train Loss: 0.0231, Val Loss: 0.0476\n",
      "Epoch [44/50], Train Loss: 0.0227, Val Loss: 0.0462\n",
      "Epoch [45/50], Train Loss: 0.0222, Val Loss: 0.0447\n",
      "Epoch [46/50], Train Loss: 0.0218, Val Loss: 0.0433\n",
      "Epoch [47/50], Train Loss: 0.0214, Val Loss: 0.0418\n",
      "Epoch [48/50], Train Loss: 0.0210, Val Loss: 0.0403\n",
      "Epoch [49/50], Train Loss: 0.0205, Val Loss: 0.0388\n",
      "Epoch [50/50], Train Loss: 0.0201, Val Loss: 0.0373\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=64, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1393, Val Loss: 0.3016\n",
      "Epoch [2/50], Train Loss: 0.0918, Val Loss: 0.2233\n",
      "Epoch [3/50], Train Loss: 0.0642, Val Loss: 0.1696\n",
      "Epoch [4/50], Train Loss: 0.0517, Val Loss: 0.1351\n",
      "Epoch [5/50], Train Loss: 0.0435, Val Loss: 0.1130\n",
      "Epoch [6/50], Train Loss: 0.0405, Val Loss: 0.0992\n",
      "Epoch [7/50], Train Loss: 0.0384, Val Loss: 0.0913\n",
      "Epoch [8/50], Train Loss: 0.0375, Val Loss: 0.0857\n",
      "Epoch [9/50], Train Loss: 0.0365, Val Loss: 0.0821\n",
      "Epoch [10/50], Train Loss: 0.0363, Val Loss: 0.0794\n",
      "Epoch [11/50], Train Loss: 0.0355, Val Loss: 0.0771\n",
      "Epoch [12/50], Train Loss: 0.0361, Val Loss: 0.0759\n",
      "Epoch [13/50], Train Loss: 0.0354, Val Loss: 0.0745\n",
      "Epoch [14/50], Train Loss: 0.0349, Val Loss: 0.0733\n",
      "Epoch [15/50], Train Loss: 0.0341, Val Loss: 0.0725\n",
      "Epoch [16/50], Train Loss: 0.0344, Val Loss: 0.0708\n",
      "Epoch [17/50], Train Loss: 0.0333, Val Loss: 0.0691\n",
      "Epoch [18/50], Train Loss: 0.0328, Val Loss: 0.0682\n",
      "Epoch [19/50], Train Loss: 0.0323, Val Loss: 0.0667\n",
      "Epoch [20/50], Train Loss: 0.0321, Val Loss: 0.0655\n",
      "Epoch [21/50], Train Loss: 0.0311, Val Loss: 0.0646\n",
      "Epoch [22/50], Train Loss: 0.0319, Val Loss: 0.0637\n",
      "Epoch [23/50], Train Loss: 0.0316, Val Loss: 0.0623\n",
      "Epoch [24/50], Train Loss: 0.0309, Val Loss: 0.0611\n",
      "Epoch [25/50], Train Loss: 0.0300, Val Loss: 0.0610\n",
      "Epoch [26/50], Train Loss: 0.0300, Val Loss: 0.0594\n",
      "Epoch [27/50], Train Loss: 0.0293, Val Loss: 0.0587\n",
      "Epoch [28/50], Train Loss: 0.0287, Val Loss: 0.0572\n",
      "Epoch [29/50], Train Loss: 0.0285, Val Loss: 0.0556\n",
      "Epoch [30/50], Train Loss: 0.0284, Val Loss: 0.0545\n",
      "Epoch [31/50], Train Loss: 0.0273, Val Loss: 0.0530\n",
      "Epoch [32/50], Train Loss: 0.0271, Val Loss: 0.0516\n",
      "Epoch [33/50], Train Loss: 0.0272, Val Loss: 0.0499\n",
      "Epoch [34/50], Train Loss: 0.0274, Val Loss: 0.0485\n",
      "Epoch [35/50], Train Loss: 0.0265, Val Loss: 0.0479\n",
      "Epoch [36/50], Train Loss: 0.0251, Val Loss: 0.0463\n",
      "Epoch [37/50], Train Loss: 0.0259, Val Loss: 0.0453\n",
      "Epoch [38/50], Train Loss: 0.0252, Val Loss: 0.0438\n",
      "Epoch [39/50], Train Loss: 0.0254, Val Loss: 0.0420\n",
      "Epoch [40/50], Train Loss: 0.0247, Val Loss: 0.0402\n",
      "Epoch [41/50], Train Loss: 0.0235, Val Loss: 0.0386\n",
      "Epoch [42/50], Train Loss: 0.0232, Val Loss: 0.0375\n",
      "Epoch [43/50], Train Loss: 0.0232, Val Loss: 0.0363\n",
      "Epoch [44/50], Train Loss: 0.0220, Val Loss: 0.0353\n",
      "Epoch [45/50], Train Loss: 0.0224, Val Loss: 0.0341\n",
      "Epoch [46/50], Train Loss: 0.0212, Val Loss: 0.0332\n",
      "Epoch [47/50], Train Loss: 0.0211, Val Loss: 0.0314\n",
      "Epoch [48/50], Train Loss: 0.0207, Val Loss: 0.0296\n",
      "Epoch [49/50], Train Loss: 0.0197, Val Loss: 0.0288\n",
      "Epoch [50/50], Train Loss: 0.0194, Val Loss: 0.0280\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=64, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1584, Val Loss: 0.3232\n",
      "Epoch [2/50], Train Loss: 0.1097, Val Loss: 0.2508\n",
      "Epoch [3/50], Train Loss: 0.0848, Val Loss: 0.2011\n",
      "Epoch [4/50], Train Loss: 0.0681, Val Loss: 0.1673\n",
      "Epoch [5/50], Train Loss: 0.0609, Val Loss: 0.1448\n",
      "Epoch [6/50], Train Loss: 0.0577, Val Loss: 0.1290\n",
      "Epoch [7/50], Train Loss: 0.0570, Val Loss: 0.1185\n",
      "Epoch [8/50], Train Loss: 0.0513, Val Loss: 0.1108\n",
      "Epoch [9/50], Train Loss: 0.0513, Val Loss: 0.1060\n",
      "Epoch [10/50], Train Loss: 0.0505, Val Loss: 0.1024\n",
      "Epoch [11/50], Train Loss: 0.0524, Val Loss: 0.0999\n",
      "Epoch [12/50], Train Loss: 0.0500, Val Loss: 0.0977\n",
      "Epoch [13/50], Train Loss: 0.0504, Val Loss: 0.0955\n",
      "Epoch [14/50], Train Loss: 0.0492, Val Loss: 0.0942\n",
      "Epoch [15/50], Train Loss: 0.0476, Val Loss: 0.0929\n",
      "Epoch [16/50], Train Loss: 0.0476, Val Loss: 0.0907\n",
      "Epoch [17/50], Train Loss: 0.0462, Val Loss: 0.0904\n",
      "Epoch [18/50], Train Loss: 0.0446, Val Loss: 0.0894\n",
      "Epoch [19/50], Train Loss: 0.0440, Val Loss: 0.0876\n",
      "Epoch [20/50], Train Loss: 0.0434, Val Loss: 0.0866\n",
      "Epoch [21/50], Train Loss: 0.0426, Val Loss: 0.0844\n",
      "Epoch [22/50], Train Loss: 0.0425, Val Loss: 0.0832\n",
      "Epoch [23/50], Train Loss: 0.0421, Val Loss: 0.0827\n",
      "Epoch [24/50], Train Loss: 0.0418, Val Loss: 0.0818\n",
      "Epoch [25/50], Train Loss: 0.0415, Val Loss: 0.0810\n",
      "Epoch [26/50], Train Loss: 0.0409, Val Loss: 0.0802\n",
      "Epoch [27/50], Train Loss: 0.0412, Val Loss: 0.0789\n",
      "Epoch [28/50], Train Loss: 0.0400, Val Loss: 0.0782\n",
      "Epoch [29/50], Train Loss: 0.0402, Val Loss: 0.0769\n",
      "Epoch [30/50], Train Loss: 0.0388, Val Loss: 0.0762\n",
      "Epoch [31/50], Train Loss: 0.0391, Val Loss: 0.0754\n",
      "Epoch [32/50], Train Loss: 0.0371, Val Loss: 0.0748\n",
      "Epoch [33/50], Train Loss: 0.0361, Val Loss: 0.0736\n",
      "Epoch [34/50], Train Loss: 0.0362, Val Loss: 0.0716\n",
      "Epoch [35/50], Train Loss: 0.0375, Val Loss: 0.0695\n",
      "Epoch [36/50], Train Loss: 0.0367, Val Loss: 0.0687\n",
      "Epoch [37/50], Train Loss: 0.0363, Val Loss: 0.0678\n",
      "Epoch [38/50], Train Loss: 0.0355, Val Loss: 0.0666\n",
      "Epoch [39/50], Train Loss: 0.0351, Val Loss: 0.0656\n",
      "Epoch [40/50], Train Loss: 0.0345, Val Loss: 0.0638\n",
      "Epoch [41/50], Train Loss: 0.0351, Val Loss: 0.0623\n",
      "Epoch [42/50], Train Loss: 0.0335, Val Loss: 0.0611\n",
      "Epoch [43/50], Train Loss: 0.0339, Val Loss: 0.0597\n",
      "Epoch [44/50], Train Loss: 0.0335, Val Loss: 0.0583\n",
      "Epoch [45/50], Train Loss: 0.0323, Val Loss: 0.0571\n",
      "Epoch [46/50], Train Loss: 0.0314, Val Loss: 0.0555\n",
      "Epoch [47/50], Train Loss: 0.0311, Val Loss: 0.0541\n",
      "Epoch [48/50], Train Loss: 0.0299, Val Loss: 0.0531\n",
      "Epoch [49/50], Train Loss: 0.0302, Val Loss: 0.0519\n",
      "Epoch [50/50], Train Loss: 0.0288, Val Loss: 0.0501\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=64, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0729, Val Loss: 0.2166\n",
      "Epoch [2/50], Train Loss: 0.0532, Val Loss: 0.1711\n",
      "Epoch [3/50], Train Loss: 0.0433, Val Loss: 0.1411\n",
      "Epoch [4/50], Train Loss: 0.0385, Val Loss: 0.1217\n",
      "Epoch [5/50], Train Loss: 0.0364, Val Loss: 0.1092\n",
      "Epoch [6/50], Train Loss: 0.0355, Val Loss: 0.1011\n",
      "Epoch [7/50], Train Loss: 0.0351, Val Loss: 0.0958\n",
      "Epoch [8/50], Train Loss: 0.0349, Val Loss: 0.0922\n",
      "Epoch [9/50], Train Loss: 0.0347, Val Loss: 0.0896\n",
      "Epoch [10/50], Train Loss: 0.0345, Val Loss: 0.0877\n",
      "Epoch [11/50], Train Loss: 0.0343, Val Loss: 0.0861\n",
      "Epoch [12/50], Train Loss: 0.0341, Val Loss: 0.0848\n",
      "Epoch [13/50], Train Loss: 0.0339, Val Loss: 0.0836\n",
      "Epoch [14/50], Train Loss: 0.0336, Val Loss: 0.0825\n",
      "Epoch [15/50], Train Loss: 0.0334, Val Loss: 0.0813\n",
      "Epoch [16/50], Train Loss: 0.0331, Val Loss: 0.0802\n",
      "Epoch [17/50], Train Loss: 0.0329, Val Loss: 0.0791\n",
      "Epoch [18/50], Train Loss: 0.0326, Val Loss: 0.0780\n",
      "Epoch [19/50], Train Loss: 0.0323, Val Loss: 0.0768\n",
      "Epoch [20/50], Train Loss: 0.0320, Val Loss: 0.0756\n",
      "Epoch [21/50], Train Loss: 0.0317, Val Loss: 0.0744\n",
      "Epoch [22/50], Train Loss: 0.0314, Val Loss: 0.0731\n",
      "Epoch [23/50], Train Loss: 0.0311, Val Loss: 0.0718\n",
      "Epoch [24/50], Train Loss: 0.0308, Val Loss: 0.0704\n",
      "Epoch [25/50], Train Loss: 0.0304, Val Loss: 0.0690\n",
      "Epoch [26/50], Train Loss: 0.0300, Val Loss: 0.0675\n",
      "Epoch [27/50], Train Loss: 0.0297, Val Loss: 0.0659\n",
      "Epoch [28/50], Train Loss: 0.0293, Val Loss: 0.0643\n",
      "Epoch [29/50], Train Loss: 0.0288, Val Loss: 0.0627\n",
      "Epoch [30/50], Train Loss: 0.0284, Val Loss: 0.0609\n",
      "Epoch [31/50], Train Loss: 0.0279, Val Loss: 0.0591\n",
      "Epoch [32/50], Train Loss: 0.0275, Val Loss: 0.0573\n",
      "Epoch [33/50], Train Loss: 0.0270, Val Loss: 0.0554\n",
      "Epoch [34/50], Train Loss: 0.0265, Val Loss: 0.0534\n",
      "Epoch [35/50], Train Loss: 0.0259, Val Loss: 0.0513\n",
      "Epoch [36/50], Train Loss: 0.0254, Val Loss: 0.0492\n",
      "Epoch [37/50], Train Loss: 0.0248, Val Loss: 0.0470\n",
      "Epoch [38/50], Train Loss: 0.0242, Val Loss: 0.0448\n",
      "Epoch [39/50], Train Loss: 0.0236, Val Loss: 0.0426\n",
      "Epoch [40/50], Train Loss: 0.0230, Val Loss: 0.0403\n",
      "Epoch [41/50], Train Loss: 0.0223, Val Loss: 0.0381\n",
      "Epoch [42/50], Train Loss: 0.0217, Val Loss: 0.0358\n",
      "Epoch [43/50], Train Loss: 0.0211, Val Loss: 0.0335\n",
      "Epoch [44/50], Train Loss: 0.0204, Val Loss: 0.0314\n",
      "Epoch [45/50], Train Loss: 0.0198, Val Loss: 0.0292\n",
      "Epoch [46/50], Train Loss: 0.0192, Val Loss: 0.0272\n",
      "Epoch [47/50], Train Loss: 0.0186, Val Loss: 0.0252\n",
      "Epoch [48/50], Train Loss: 0.0181, Val Loss: 0.0234\n",
      "Epoch [49/50], Train Loss: 0.0176, Val Loss: 0.0217\n",
      "Epoch [50/50], Train Loss: 0.0171, Val Loss: 0.0202\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=64, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0970, Val Loss: 0.2225\n",
      "Epoch [2/50], Train Loss: 0.0704, Val Loss: 0.1757\n",
      "Epoch [3/50], Train Loss: 0.0559, Val Loss: 0.1463\n",
      "Epoch [4/50], Train Loss: 0.0493, Val Loss: 0.1273\n",
      "Epoch [5/50], Train Loss: 0.0454, Val Loss: 0.1152\n",
      "Epoch [6/50], Train Loss: 0.0437, Val Loss: 0.1072\n",
      "Epoch [7/50], Train Loss: 0.0432, Val Loss: 0.1022\n",
      "Epoch [8/50], Train Loss: 0.0426, Val Loss: 0.0983\n",
      "Epoch [9/50], Train Loss: 0.0422, Val Loss: 0.0960\n",
      "Epoch [10/50], Train Loss: 0.0415, Val Loss: 0.0938\n",
      "Epoch [11/50], Train Loss: 0.0400, Val Loss: 0.0928\n",
      "Epoch [12/50], Train Loss: 0.0405, Val Loss: 0.0916\n",
      "Epoch [13/50], Train Loss: 0.0411, Val Loss: 0.0908\n",
      "Epoch [14/50], Train Loss: 0.0398, Val Loss: 0.0895\n",
      "Epoch [15/50], Train Loss: 0.0404, Val Loss: 0.0893\n",
      "Epoch [16/50], Train Loss: 0.0393, Val Loss: 0.0881\n",
      "Epoch [17/50], Train Loss: 0.0395, Val Loss: 0.0878\n",
      "Epoch [18/50], Train Loss: 0.0387, Val Loss: 0.0866\n",
      "Epoch [19/50], Train Loss: 0.0386, Val Loss: 0.0849\n",
      "Epoch [20/50], Train Loss: 0.0394, Val Loss: 0.0840\n",
      "Epoch [21/50], Train Loss: 0.0382, Val Loss: 0.0833\n",
      "Epoch [22/50], Train Loss: 0.0376, Val Loss: 0.0825\n",
      "Epoch [23/50], Train Loss: 0.0368, Val Loss: 0.0816\n",
      "Epoch [24/50], Train Loss: 0.0372, Val Loss: 0.0804\n",
      "Epoch [25/50], Train Loss: 0.0363, Val Loss: 0.0791\n",
      "Epoch [26/50], Train Loss: 0.0362, Val Loss: 0.0780\n",
      "Epoch [27/50], Train Loss: 0.0360, Val Loss: 0.0770\n",
      "Epoch [28/50], Train Loss: 0.0352, Val Loss: 0.0760\n",
      "Epoch [29/50], Train Loss: 0.0348, Val Loss: 0.0744\n",
      "Epoch [30/50], Train Loss: 0.0353, Val Loss: 0.0733\n",
      "Epoch [31/50], Train Loss: 0.0348, Val Loss: 0.0722\n",
      "Epoch [32/50], Train Loss: 0.0344, Val Loss: 0.0712\n",
      "Epoch [33/50], Train Loss: 0.0336, Val Loss: 0.0699\n",
      "Epoch [34/50], Train Loss: 0.0337, Val Loss: 0.0686\n",
      "Epoch [35/50], Train Loss: 0.0335, Val Loss: 0.0675\n",
      "Epoch [36/50], Train Loss: 0.0323, Val Loss: 0.0662\n",
      "Epoch [37/50], Train Loss: 0.0320, Val Loss: 0.0648\n",
      "Epoch [38/50], Train Loss: 0.0311, Val Loss: 0.0634\n",
      "Epoch [39/50], Train Loss: 0.0315, Val Loss: 0.0614\n",
      "Epoch [40/50], Train Loss: 0.0308, Val Loss: 0.0600\n",
      "Epoch [41/50], Train Loss: 0.0304, Val Loss: 0.0576\n",
      "Epoch [42/50], Train Loss: 0.0299, Val Loss: 0.0557\n",
      "Epoch [43/50], Train Loss: 0.0287, Val Loss: 0.0546\n",
      "Epoch [44/50], Train Loss: 0.0289, Val Loss: 0.0529\n",
      "Epoch [45/50], Train Loss: 0.0284, Val Loss: 0.0516\n",
      "Epoch [46/50], Train Loss: 0.0277, Val Loss: 0.0496\n",
      "Epoch [47/50], Train Loss: 0.0274, Val Loss: 0.0473\n",
      "Epoch [48/50], Train Loss: 0.0267, Val Loss: 0.0459\n",
      "Epoch [49/50], Train Loss: 0.0256, Val Loss: 0.0436\n",
      "Epoch [50/50], Train Loss: 0.0258, Val Loss: 0.0419\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=64, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1295, Val Loss: 0.2770\n",
      "Epoch [2/50], Train Loss: 0.0940, Val Loss: 0.2150\n",
      "Epoch [3/50], Train Loss: 0.0774, Val Loss: 0.1754\n",
      "Epoch [4/50], Train Loss: 0.0681, Val Loss: 0.1511\n",
      "Epoch [5/50], Train Loss: 0.0642, Val Loss: 0.1362\n",
      "Epoch [6/50], Train Loss: 0.0640, Val Loss: 0.1253\n",
      "Epoch [7/50], Train Loss: 0.0623, Val Loss: 0.1192\n",
      "Epoch [8/50], Train Loss: 0.0587, Val Loss: 0.1147\n",
      "Epoch [9/50], Train Loss: 0.0575, Val Loss: 0.1117\n",
      "Epoch [10/50], Train Loss: 0.0575, Val Loss: 0.1099\n",
      "Epoch [11/50], Train Loss: 0.0577, Val Loss: 0.1077\n",
      "Epoch [12/50], Train Loss: 0.0554, Val Loss: 0.1053\n",
      "Epoch [13/50], Train Loss: 0.0545, Val Loss: 0.1044\n",
      "Epoch [14/50], Train Loss: 0.0544, Val Loss: 0.1034\n",
      "Epoch [15/50], Train Loss: 0.0535, Val Loss: 0.1032\n",
      "Epoch [16/50], Train Loss: 0.0526, Val Loss: 0.1021\n",
      "Epoch [17/50], Train Loss: 0.0515, Val Loss: 0.1022\n",
      "Epoch [18/50], Train Loss: 0.0529, Val Loss: 0.1008\n",
      "Epoch [19/50], Train Loss: 0.0497, Val Loss: 0.0998\n",
      "Epoch [20/50], Train Loss: 0.0503, Val Loss: 0.0990\n",
      "Epoch [21/50], Train Loss: 0.0496, Val Loss: 0.0981\n",
      "Epoch [22/50], Train Loss: 0.0500, Val Loss: 0.0979\n",
      "Epoch [23/50], Train Loss: 0.0484, Val Loss: 0.0964\n",
      "Epoch [24/50], Train Loss: 0.0485, Val Loss: 0.0957\n",
      "Epoch [25/50], Train Loss: 0.0471, Val Loss: 0.0952\n",
      "Epoch [26/50], Train Loss: 0.0468, Val Loss: 0.0946\n",
      "Epoch [27/50], Train Loss: 0.0461, Val Loss: 0.0940\n",
      "Epoch [28/50], Train Loss: 0.0455, Val Loss: 0.0932\n",
      "Epoch [29/50], Train Loss: 0.0454, Val Loss: 0.0915\n",
      "Epoch [30/50], Train Loss: 0.0450, Val Loss: 0.0905\n",
      "Epoch [31/50], Train Loss: 0.0448, Val Loss: 0.0888\n",
      "Epoch [32/50], Train Loss: 0.0433, Val Loss: 0.0877\n",
      "Epoch [33/50], Train Loss: 0.0445, Val Loss: 0.0869\n",
      "Epoch [34/50], Train Loss: 0.0423, Val Loss: 0.0855\n",
      "Epoch [35/50], Train Loss: 0.0424, Val Loss: 0.0841\n",
      "Epoch [36/50], Train Loss: 0.0435, Val Loss: 0.0841\n",
      "Epoch [37/50], Train Loss: 0.0422, Val Loss: 0.0835\n",
      "Epoch [38/50], Train Loss: 0.0415, Val Loss: 0.0822\n",
      "Epoch [39/50], Train Loss: 0.0411, Val Loss: 0.0806\n",
      "Epoch [40/50], Train Loss: 0.0402, Val Loss: 0.0793\n",
      "Epoch [41/50], Train Loss: 0.0410, Val Loss: 0.0781\n",
      "Epoch [42/50], Train Loss: 0.0408, Val Loss: 0.0769\n",
      "Epoch [43/50], Train Loss: 0.0403, Val Loss: 0.0760\n",
      "Epoch [44/50], Train Loss: 0.0396, Val Loss: 0.0739\n",
      "Epoch [45/50], Train Loss: 0.0383, Val Loss: 0.0723\n",
      "Epoch [46/50], Train Loss: 0.0379, Val Loss: 0.0714\n",
      "Epoch [47/50], Train Loss: 0.0377, Val Loss: 0.0702\n",
      "Epoch [48/50], Train Loss: 0.0372, Val Loss: 0.0686\n",
      "Epoch [49/50], Train Loss: 0.0376, Val Loss: 0.0666\n",
      "Epoch [50/50], Train Loss: 0.0357, Val Loss: 0.0654\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=128, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1144, Val Loss: 0.3059\n",
      "Epoch [2/50], Train Loss: 0.0857, Val Loss: 0.2509\n",
      "Epoch [3/50], Train Loss: 0.0670, Val Loss: 0.2098\n",
      "Epoch [4/50], Train Loss: 0.0546, Val Loss: 0.1788\n",
      "Epoch [5/50], Train Loss: 0.0466, Val Loss: 0.1552\n",
      "Epoch [6/50], Train Loss: 0.0414, Val Loss: 0.1373\n",
      "Epoch [7/50], Train Loss: 0.0382, Val Loss: 0.1238\n",
      "Epoch [8/50], Train Loss: 0.0363, Val Loss: 0.1136\n",
      "Epoch [9/50], Train Loss: 0.0350, Val Loss: 0.1059\n",
      "Epoch [10/50], Train Loss: 0.0342, Val Loss: 0.0999\n",
      "Epoch [11/50], Train Loss: 0.0336, Val Loss: 0.0954\n",
      "Epoch [12/50], Train Loss: 0.0332, Val Loss: 0.0917\n",
      "Epoch [13/50], Train Loss: 0.0328, Val Loss: 0.0888\n",
      "Epoch [14/50], Train Loss: 0.0324, Val Loss: 0.0864\n",
      "Epoch [15/50], Train Loss: 0.0321, Val Loss: 0.0844\n",
      "Epoch [16/50], Train Loss: 0.0318, Val Loss: 0.0826\n",
      "Epoch [17/50], Train Loss: 0.0314, Val Loss: 0.0811\n",
      "Epoch [18/50], Train Loss: 0.0311, Val Loss: 0.0797\n",
      "Epoch [19/50], Train Loss: 0.0307, Val Loss: 0.0784\n",
      "Epoch [20/50], Train Loss: 0.0304, Val Loss: 0.0771\n",
      "Epoch [21/50], Train Loss: 0.0301, Val Loss: 0.0760\n",
      "Epoch [22/50], Train Loss: 0.0297, Val Loss: 0.0748\n",
      "Epoch [23/50], Train Loss: 0.0294, Val Loss: 0.0737\n",
      "Epoch [24/50], Train Loss: 0.0291, Val Loss: 0.0727\n",
      "Epoch [25/50], Train Loss: 0.0287, Val Loss: 0.0716\n",
      "Epoch [26/50], Train Loss: 0.0284, Val Loss: 0.0706\n",
      "Epoch [27/50], Train Loss: 0.0281, Val Loss: 0.0696\n",
      "Epoch [28/50], Train Loss: 0.0277, Val Loss: 0.0686\n",
      "Epoch [29/50], Train Loss: 0.0274, Val Loss: 0.0676\n",
      "Epoch [30/50], Train Loss: 0.0271, Val Loss: 0.0666\n",
      "Epoch [31/50], Train Loss: 0.0268, Val Loss: 0.0656\n",
      "Epoch [32/50], Train Loss: 0.0264, Val Loss: 0.0647\n",
      "Epoch [33/50], Train Loss: 0.0261, Val Loss: 0.0637\n",
      "Epoch [34/50], Train Loss: 0.0258, Val Loss: 0.0627\n",
      "Epoch [35/50], Train Loss: 0.0254, Val Loss: 0.0617\n",
      "Epoch [36/50], Train Loss: 0.0251, Val Loss: 0.0608\n",
      "Epoch [37/50], Train Loss: 0.0248, Val Loss: 0.0598\n",
      "Epoch [38/50], Train Loss: 0.0244, Val Loss: 0.0589\n",
      "Epoch [39/50], Train Loss: 0.0241, Val Loss: 0.0579\n",
      "Epoch [40/50], Train Loss: 0.0238, Val Loss: 0.0569\n",
      "Epoch [41/50], Train Loss: 0.0235, Val Loss: 0.0560\n",
      "Epoch [42/50], Train Loss: 0.0231, Val Loss: 0.0550\n",
      "Epoch [43/50], Train Loss: 0.0228, Val Loss: 0.0541\n",
      "Epoch [44/50], Train Loss: 0.0225, Val Loss: 0.0531\n",
      "Epoch [45/50], Train Loss: 0.0221, Val Loss: 0.0522\n",
      "Epoch [46/50], Train Loss: 0.0218, Val Loss: 0.0512\n",
      "Epoch [47/50], Train Loss: 0.0215, Val Loss: 0.0503\n",
      "Epoch [48/50], Train Loss: 0.0211, Val Loss: 0.0493\n",
      "Epoch [49/50], Train Loss: 0.0208, Val Loss: 0.0484\n",
      "Epoch [50/50], Train Loss: 0.0204, Val Loss: 0.0475\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=128, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1260, Val Loss: 0.3211\n",
      "Epoch [2/50], Train Loss: 0.0892, Val Loss: 0.2548\n",
      "Epoch [3/50], Train Loss: 0.0664, Val Loss: 0.2066\n",
      "Epoch [4/50], Train Loss: 0.0525, Val Loss: 0.1711\n",
      "Epoch [5/50], Train Loss: 0.0437, Val Loss: 0.1446\n",
      "Epoch [6/50], Train Loss: 0.0383, Val Loss: 0.1250\n",
      "Epoch [7/50], Train Loss: 0.0357, Val Loss: 0.1106\n",
      "Epoch [8/50], Train Loss: 0.0333, Val Loss: 0.0999\n",
      "Epoch [9/50], Train Loss: 0.0325, Val Loss: 0.0917\n",
      "Epoch [10/50], Train Loss: 0.0319, Val Loss: 0.0857\n",
      "Epoch [11/50], Train Loss: 0.0310, Val Loss: 0.0814\n",
      "Epoch [12/50], Train Loss: 0.0306, Val Loss: 0.0778\n",
      "Epoch [13/50], Train Loss: 0.0307, Val Loss: 0.0749\n",
      "Epoch [14/50], Train Loss: 0.0297, Val Loss: 0.0728\n",
      "Epoch [15/50], Train Loss: 0.0296, Val Loss: 0.0710\n",
      "Epoch [16/50], Train Loss: 0.0291, Val Loss: 0.0693\n",
      "Epoch [17/50], Train Loss: 0.0286, Val Loss: 0.0677\n",
      "Epoch [18/50], Train Loss: 0.0282, Val Loss: 0.0664\n",
      "Epoch [19/50], Train Loss: 0.0280, Val Loss: 0.0652\n",
      "Epoch [20/50], Train Loss: 0.0279, Val Loss: 0.0639\n",
      "Epoch [21/50], Train Loss: 0.0273, Val Loss: 0.0630\n",
      "Epoch [22/50], Train Loss: 0.0269, Val Loss: 0.0618\n",
      "Epoch [23/50], Train Loss: 0.0263, Val Loss: 0.0610\n",
      "Epoch [24/50], Train Loss: 0.0259, Val Loss: 0.0602\n",
      "Epoch [25/50], Train Loss: 0.0256, Val Loss: 0.0591\n",
      "Epoch [26/50], Train Loss: 0.0256, Val Loss: 0.0580\n",
      "Epoch [27/50], Train Loss: 0.0251, Val Loss: 0.0571\n",
      "Epoch [28/50], Train Loss: 0.0244, Val Loss: 0.0562\n",
      "Epoch [29/50], Train Loss: 0.0245, Val Loss: 0.0553\n",
      "Epoch [30/50], Train Loss: 0.0247, Val Loss: 0.0541\n",
      "Epoch [31/50], Train Loss: 0.0236, Val Loss: 0.0531\n",
      "Epoch [32/50], Train Loss: 0.0230, Val Loss: 0.0522\n",
      "Epoch [33/50], Train Loss: 0.0229, Val Loss: 0.0510\n",
      "Epoch [34/50], Train Loss: 0.0222, Val Loss: 0.0501\n",
      "Epoch [35/50], Train Loss: 0.0223, Val Loss: 0.0489\n",
      "Epoch [36/50], Train Loss: 0.0218, Val Loss: 0.0480\n",
      "Epoch [37/50], Train Loss: 0.0212, Val Loss: 0.0471\n",
      "Epoch [38/50], Train Loss: 0.0209, Val Loss: 0.0461\n",
      "Epoch [39/50], Train Loss: 0.0207, Val Loss: 0.0453\n",
      "Epoch [40/50], Train Loss: 0.0202, Val Loss: 0.0443\n",
      "Epoch [41/50], Train Loss: 0.0197, Val Loss: 0.0434\n",
      "Epoch [42/50], Train Loss: 0.0195, Val Loss: 0.0423\n",
      "Epoch [43/50], Train Loss: 0.0193, Val Loss: 0.0414\n",
      "Epoch [44/50], Train Loss: 0.0187, Val Loss: 0.0403\n",
      "Epoch [45/50], Train Loss: 0.0181, Val Loss: 0.0396\n",
      "Epoch [46/50], Train Loss: 0.0177, Val Loss: 0.0386\n",
      "Epoch [47/50], Train Loss: 0.0179, Val Loss: 0.0376\n",
      "Epoch [48/50], Train Loss: 0.0173, Val Loss: 0.0365\n",
      "Epoch [49/50], Train Loss: 0.0167, Val Loss: 0.0356\n",
      "Epoch [50/50], Train Loss: 0.0163, Val Loss: 0.0348\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=128, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1490, Val Loss: 0.3359\n",
      "Epoch [2/50], Train Loss: 0.1046, Val Loss: 0.2617\n",
      "Epoch [3/50], Train Loss: 0.0770, Val Loss: 0.2100\n",
      "Epoch [4/50], Train Loss: 0.0617, Val Loss: 0.1728\n",
      "Epoch [5/50], Train Loss: 0.0530, Val Loss: 0.1456\n",
      "Epoch [6/50], Train Loss: 0.0458, Val Loss: 0.1263\n",
      "Epoch [7/50], Train Loss: 0.0421, Val Loss: 0.1125\n",
      "Epoch [8/50], Train Loss: 0.0398, Val Loss: 0.1022\n",
      "Epoch [9/50], Train Loss: 0.0388, Val Loss: 0.0954\n",
      "Epoch [10/50], Train Loss: 0.0379, Val Loss: 0.0896\n",
      "Epoch [11/50], Train Loss: 0.0377, Val Loss: 0.0855\n",
      "Epoch [12/50], Train Loss: 0.0374, Val Loss: 0.0824\n",
      "Epoch [13/50], Train Loss: 0.0361, Val Loss: 0.0798\n",
      "Epoch [14/50], Train Loss: 0.0354, Val Loss: 0.0776\n",
      "Epoch [15/50], Train Loss: 0.0357, Val Loss: 0.0758\n",
      "Epoch [16/50], Train Loss: 0.0346, Val Loss: 0.0744\n",
      "Epoch [17/50], Train Loss: 0.0353, Val Loss: 0.0736\n",
      "Epoch [18/50], Train Loss: 0.0341, Val Loss: 0.0725\n",
      "Epoch [19/50], Train Loss: 0.0341, Val Loss: 0.0715\n",
      "Epoch [20/50], Train Loss: 0.0331, Val Loss: 0.0706\n",
      "Epoch [21/50], Train Loss: 0.0328, Val Loss: 0.0695\n",
      "Epoch [22/50], Train Loss: 0.0322, Val Loss: 0.0687\n",
      "Epoch [23/50], Train Loss: 0.0319, Val Loss: 0.0674\n",
      "Epoch [24/50], Train Loss: 0.0317, Val Loss: 0.0666\n",
      "Epoch [25/50], Train Loss: 0.0314, Val Loss: 0.0656\n",
      "Epoch [26/50], Train Loss: 0.0307, Val Loss: 0.0644\n",
      "Epoch [27/50], Train Loss: 0.0306, Val Loss: 0.0636\n",
      "Epoch [28/50], Train Loss: 0.0297, Val Loss: 0.0624\n",
      "Epoch [29/50], Train Loss: 0.0300, Val Loss: 0.0617\n",
      "Epoch [30/50], Train Loss: 0.0291, Val Loss: 0.0606\n",
      "Epoch [31/50], Train Loss: 0.0286, Val Loss: 0.0592\n",
      "Epoch [32/50], Train Loss: 0.0291, Val Loss: 0.0585\n",
      "Epoch [33/50], Train Loss: 0.0287, Val Loss: 0.0578\n",
      "Epoch [34/50], Train Loss: 0.0270, Val Loss: 0.0570\n",
      "Epoch [35/50], Train Loss: 0.0271, Val Loss: 0.0560\n",
      "Epoch [36/50], Train Loss: 0.0272, Val Loss: 0.0549\n",
      "Epoch [37/50], Train Loss: 0.0265, Val Loss: 0.0539\n",
      "Epoch [38/50], Train Loss: 0.0263, Val Loss: 0.0530\n",
      "Epoch [39/50], Train Loss: 0.0258, Val Loss: 0.0522\n",
      "Epoch [40/50], Train Loss: 0.0254, Val Loss: 0.0512\n",
      "Epoch [41/50], Train Loss: 0.0252, Val Loss: 0.0503\n",
      "Epoch [42/50], Train Loss: 0.0251, Val Loss: 0.0494\n",
      "Epoch [43/50], Train Loss: 0.0239, Val Loss: 0.0487\n",
      "Epoch [44/50], Train Loss: 0.0241, Val Loss: 0.0475\n",
      "Epoch [45/50], Train Loss: 0.0235, Val Loss: 0.0464\n",
      "Epoch [46/50], Train Loss: 0.0234, Val Loss: 0.0457\n",
      "Epoch [47/50], Train Loss: 0.0230, Val Loss: 0.0446\n",
      "Epoch [48/50], Train Loss: 0.0218, Val Loss: 0.0435\n",
      "Epoch [49/50], Train Loss: 0.0227, Val Loss: 0.0429\n",
      "Epoch [50/50], Train Loss: 0.0224, Val Loss: 0.0419\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=128, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1239, Val Loss: 0.3230\n",
      "Epoch [2/50], Train Loss: 0.0896, Val Loss: 0.2568\n",
      "Epoch [3/50], Train Loss: 0.0663, Val Loss: 0.2048\n",
      "Epoch [4/50], Train Loss: 0.0509, Val Loss: 0.1651\n",
      "Epoch [5/50], Train Loss: 0.0417, Val Loss: 0.1361\n",
      "Epoch [6/50], Train Loss: 0.0368, Val Loss: 0.1161\n",
      "Epoch [7/50], Train Loss: 0.0344, Val Loss: 0.1029\n",
      "Epoch [8/50], Train Loss: 0.0333, Val Loss: 0.0943\n",
      "Epoch [9/50], Train Loss: 0.0327, Val Loss: 0.0886\n",
      "Epoch [10/50], Train Loss: 0.0323, Val Loss: 0.0848\n",
      "Epoch [11/50], Train Loss: 0.0320, Val Loss: 0.0821\n",
      "Epoch [12/50], Train Loss: 0.0317, Val Loss: 0.0801\n",
      "Epoch [13/50], Train Loss: 0.0314, Val Loss: 0.0784\n",
      "Epoch [14/50], Train Loss: 0.0310, Val Loss: 0.0770\n",
      "Epoch [15/50], Train Loss: 0.0307, Val Loss: 0.0757\n",
      "Epoch [16/50], Train Loss: 0.0303, Val Loss: 0.0745\n",
      "Epoch [17/50], Train Loss: 0.0300, Val Loss: 0.0733\n",
      "Epoch [18/50], Train Loss: 0.0296, Val Loss: 0.0721\n",
      "Epoch [19/50], Train Loss: 0.0293, Val Loss: 0.0709\n",
      "Epoch [20/50], Train Loss: 0.0289, Val Loss: 0.0698\n",
      "Epoch [21/50], Train Loss: 0.0285, Val Loss: 0.0686\n",
      "Epoch [22/50], Train Loss: 0.0281, Val Loss: 0.0674\n",
      "Epoch [23/50], Train Loss: 0.0277, Val Loss: 0.0662\n",
      "Epoch [24/50], Train Loss: 0.0273, Val Loss: 0.0649\n",
      "Epoch [25/50], Train Loss: 0.0269, Val Loss: 0.0637\n",
      "Epoch [26/50], Train Loss: 0.0265, Val Loss: 0.0624\n",
      "Epoch [27/50], Train Loss: 0.0261, Val Loss: 0.0611\n",
      "Epoch [28/50], Train Loss: 0.0257, Val Loss: 0.0598\n",
      "Epoch [29/50], Train Loss: 0.0252, Val Loss: 0.0585\n",
      "Epoch [30/50], Train Loss: 0.0248, Val Loss: 0.0571\n",
      "Epoch [31/50], Train Loss: 0.0243, Val Loss: 0.0558\n",
      "Epoch [32/50], Train Loss: 0.0239, Val Loss: 0.0544\n",
      "Epoch [33/50], Train Loss: 0.0234, Val Loss: 0.0529\n",
      "Epoch [34/50], Train Loss: 0.0229, Val Loss: 0.0515\n",
      "Epoch [35/50], Train Loss: 0.0224, Val Loss: 0.0500\n",
      "Epoch [36/50], Train Loss: 0.0219, Val Loss: 0.0485\n",
      "Epoch [37/50], Train Loss: 0.0214, Val Loss: 0.0469\n",
      "Epoch [38/50], Train Loss: 0.0209, Val Loss: 0.0454\n",
      "Epoch [39/50], Train Loss: 0.0203, Val Loss: 0.0438\n",
      "Epoch [40/50], Train Loss: 0.0198, Val Loss: 0.0422\n",
      "Epoch [41/50], Train Loss: 0.0192, Val Loss: 0.0406\n",
      "Epoch [42/50], Train Loss: 0.0187, Val Loss: 0.0389\n",
      "Epoch [43/50], Train Loss: 0.0181, Val Loss: 0.0373\n",
      "Epoch [44/50], Train Loss: 0.0175, Val Loss: 0.0356\n",
      "Epoch [45/50], Train Loss: 0.0169, Val Loss: 0.0340\n",
      "Epoch [46/50], Train Loss: 0.0163, Val Loss: 0.0323\n",
      "Epoch [47/50], Train Loss: 0.0157, Val Loss: 0.0306\n",
      "Epoch [48/50], Train Loss: 0.0151, Val Loss: 0.0289\n",
      "Epoch [49/50], Train Loss: 0.0144, Val Loss: 0.0273\n",
      "Epoch [50/50], Train Loss: 0.0138, Val Loss: 0.0257\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=128, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1019, Val Loss: 0.2753\n",
      "Epoch [2/50], Train Loss: 0.0709, Val Loss: 0.2141\n",
      "Epoch [3/50], Train Loss: 0.0541, Val Loss: 0.1709\n",
      "Epoch [4/50], Train Loss: 0.0449, Val Loss: 0.1422\n",
      "Epoch [5/50], Train Loss: 0.0407, Val Loss: 0.1236\n",
      "Epoch [6/50], Train Loss: 0.0388, Val Loss: 0.1111\n",
      "Epoch [7/50], Train Loss: 0.0378, Val Loss: 0.1030\n",
      "Epoch [8/50], Train Loss: 0.0368, Val Loss: 0.0978\n",
      "Epoch [9/50], Train Loss: 0.0370, Val Loss: 0.0942\n",
      "Epoch [10/50], Train Loss: 0.0364, Val Loss: 0.0913\n",
      "Epoch [11/50], Train Loss: 0.0366, Val Loss: 0.0893\n",
      "Epoch [12/50], Train Loss: 0.0358, Val Loss: 0.0875\n",
      "Epoch [13/50], Train Loss: 0.0361, Val Loss: 0.0861\n",
      "Epoch [14/50], Train Loss: 0.0355, Val Loss: 0.0846\n",
      "Epoch [15/50], Train Loss: 0.0349, Val Loss: 0.0829\n",
      "Epoch [16/50], Train Loss: 0.0354, Val Loss: 0.0821\n",
      "Epoch [17/50], Train Loss: 0.0345, Val Loss: 0.0812\n",
      "Epoch [18/50], Train Loss: 0.0340, Val Loss: 0.0798\n",
      "Epoch [19/50], Train Loss: 0.0333, Val Loss: 0.0790\n",
      "Epoch [20/50], Train Loss: 0.0332, Val Loss: 0.0780\n",
      "Epoch [21/50], Train Loss: 0.0326, Val Loss: 0.0767\n",
      "Epoch [22/50], Train Loss: 0.0328, Val Loss: 0.0752\n",
      "Epoch [23/50], Train Loss: 0.0327, Val Loss: 0.0740\n",
      "Epoch [24/50], Train Loss: 0.0319, Val Loss: 0.0725\n",
      "Epoch [25/50], Train Loss: 0.0315, Val Loss: 0.0714\n",
      "Epoch [26/50], Train Loss: 0.0315, Val Loss: 0.0703\n",
      "Epoch [27/50], Train Loss: 0.0309, Val Loss: 0.0687\n",
      "Epoch [28/50], Train Loss: 0.0305, Val Loss: 0.0679\n",
      "Epoch [29/50], Train Loss: 0.0302, Val Loss: 0.0663\n",
      "Epoch [30/50], Train Loss: 0.0303, Val Loss: 0.0654\n",
      "Epoch [31/50], Train Loss: 0.0293, Val Loss: 0.0640\n",
      "Epoch [32/50], Train Loss: 0.0292, Val Loss: 0.0629\n",
      "Epoch [33/50], Train Loss: 0.0287, Val Loss: 0.0616\n",
      "Epoch [34/50], Train Loss: 0.0282, Val Loss: 0.0602\n",
      "Epoch [35/50], Train Loss: 0.0281, Val Loss: 0.0592\n",
      "Epoch [36/50], Train Loss: 0.0279, Val Loss: 0.0578\n",
      "Epoch [37/50], Train Loss: 0.0272, Val Loss: 0.0566\n",
      "Epoch [38/50], Train Loss: 0.0267, Val Loss: 0.0549\n",
      "Epoch [39/50], Train Loss: 0.0264, Val Loss: 0.0538\n",
      "Epoch [40/50], Train Loss: 0.0261, Val Loss: 0.0524\n",
      "Epoch [41/50], Train Loss: 0.0260, Val Loss: 0.0510\n",
      "Epoch [42/50], Train Loss: 0.0250, Val Loss: 0.0494\n",
      "Epoch [43/50], Train Loss: 0.0247, Val Loss: 0.0481\n",
      "Epoch [44/50], Train Loss: 0.0247, Val Loss: 0.0463\n",
      "Epoch [45/50], Train Loss: 0.0242, Val Loss: 0.0451\n",
      "Epoch [46/50], Train Loss: 0.0235, Val Loss: 0.0435\n",
      "Epoch [47/50], Train Loss: 0.0228, Val Loss: 0.0424\n",
      "Epoch [48/50], Train Loss: 0.0222, Val Loss: 0.0407\n",
      "Epoch [49/50], Train Loss: 0.0221, Val Loss: 0.0391\n",
      "Epoch [50/50], Train Loss: 0.0216, Val Loss: 0.0379\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=128, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1181, Val Loss: 0.2796\n",
      "Epoch [2/50], Train Loss: 0.0825, Val Loss: 0.2125\n",
      "Epoch [3/50], Train Loss: 0.0633, Val Loss: 0.1680\n",
      "Epoch [4/50], Train Loss: 0.0534, Val Loss: 0.1374\n",
      "Epoch [5/50], Train Loss: 0.0473, Val Loss: 0.1183\n",
      "Epoch [6/50], Train Loss: 0.0457, Val Loss: 0.1060\n",
      "Epoch [7/50], Train Loss: 0.0453, Val Loss: 0.0969\n",
      "Epoch [8/50], Train Loss: 0.0437, Val Loss: 0.0913\n",
      "Epoch [9/50], Train Loss: 0.0428, Val Loss: 0.0878\n",
      "Epoch [10/50], Train Loss: 0.0437, Val Loss: 0.0856\n",
      "Epoch [11/50], Train Loss: 0.0429, Val Loss: 0.0828\n",
      "Epoch [12/50], Train Loss: 0.0413, Val Loss: 0.0810\n",
      "Epoch [13/50], Train Loss: 0.0403, Val Loss: 0.0796\n",
      "Epoch [14/50], Train Loss: 0.0396, Val Loss: 0.0779\n",
      "Epoch [15/50], Train Loss: 0.0391, Val Loss: 0.0768\n",
      "Epoch [16/50], Train Loss: 0.0395, Val Loss: 0.0754\n",
      "Epoch [17/50], Train Loss: 0.0396, Val Loss: 0.0742\n",
      "Epoch [18/50], Train Loss: 0.0384, Val Loss: 0.0717\n",
      "Epoch [19/50], Train Loss: 0.0378, Val Loss: 0.0699\n",
      "Epoch [20/50], Train Loss: 0.0369, Val Loss: 0.0689\n",
      "Epoch [21/50], Train Loss: 0.0372, Val Loss: 0.0674\n",
      "Epoch [22/50], Train Loss: 0.0355, Val Loss: 0.0661\n",
      "Epoch [23/50], Train Loss: 0.0358, Val Loss: 0.0656\n",
      "Epoch [24/50], Train Loss: 0.0347, Val Loss: 0.0629\n",
      "Epoch [25/50], Train Loss: 0.0341, Val Loss: 0.0612\n",
      "Epoch [26/50], Train Loss: 0.0334, Val Loss: 0.0593\n",
      "Epoch [27/50], Train Loss: 0.0344, Val Loss: 0.0583\n",
      "Epoch [28/50], Train Loss: 0.0329, Val Loss: 0.0558\n",
      "Epoch [29/50], Train Loss: 0.0313, Val Loss: 0.0545\n",
      "Epoch [30/50], Train Loss: 0.0315, Val Loss: 0.0524\n",
      "Epoch [31/50], Train Loss: 0.0311, Val Loss: 0.0511\n",
      "Epoch [32/50], Train Loss: 0.0307, Val Loss: 0.0497\n",
      "Epoch [33/50], Train Loss: 0.0305, Val Loss: 0.0478\n",
      "Epoch [34/50], Train Loss: 0.0294, Val Loss: 0.0453\n",
      "Epoch [35/50], Train Loss: 0.0290, Val Loss: 0.0431\n",
      "Epoch [36/50], Train Loss: 0.0283, Val Loss: 0.0413\n",
      "Epoch [37/50], Train Loss: 0.0278, Val Loss: 0.0391\n",
      "Epoch [38/50], Train Loss: 0.0280, Val Loss: 0.0376\n",
      "Epoch [39/50], Train Loss: 0.0267, Val Loss: 0.0349\n",
      "Epoch [40/50], Train Loss: 0.0255, Val Loss: 0.0331\n",
      "Epoch [41/50], Train Loss: 0.0252, Val Loss: 0.0321\n",
      "Epoch [42/50], Train Loss: 0.0244, Val Loss: 0.0299\n",
      "Epoch [43/50], Train Loss: 0.0241, Val Loss: 0.0279\n",
      "Epoch [44/50], Train Loss: 0.0232, Val Loss: 0.0258\n",
      "Epoch [45/50], Train Loss: 0.0226, Val Loss: 0.0238\n",
      "Epoch [46/50], Train Loss: 0.0224, Val Loss: 0.0227\n",
      "Epoch [47/50], Train Loss: 0.0218, Val Loss: 0.0210\n",
      "Epoch [48/50], Train Loss: 0.0220, Val Loss: 0.0187\n",
      "Epoch [49/50], Train Loss: 0.0209, Val Loss: 0.0178\n",
      "Epoch [50/50], Train Loss: 0.0210, Val Loss: 0.0163\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=128, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0726, Val Loss: 0.2058\n",
      "Epoch [2/50], Train Loss: 0.0504, Val Loss: 0.1584\n",
      "Epoch [3/50], Train Loss: 0.0408, Val Loss: 0.1294\n",
      "Epoch [4/50], Train Loss: 0.0372, Val Loss: 0.1122\n",
      "Epoch [5/50], Train Loss: 0.0360, Val Loss: 0.1022\n",
      "Epoch [6/50], Train Loss: 0.0356, Val Loss: 0.0964\n",
      "Epoch [7/50], Train Loss: 0.0354, Val Loss: 0.0928\n",
      "Epoch [8/50], Train Loss: 0.0353, Val Loss: 0.0905\n",
      "Epoch [9/50], Train Loss: 0.0351, Val Loss: 0.0889\n",
      "Epoch [10/50], Train Loss: 0.0349, Val Loss: 0.0877\n",
      "Epoch [11/50], Train Loss: 0.0346, Val Loss: 0.0866\n",
      "Epoch [12/50], Train Loss: 0.0344, Val Loss: 0.0857\n",
      "Epoch [13/50], Train Loss: 0.0342, Val Loss: 0.0847\n",
      "Epoch [14/50], Train Loss: 0.0339, Val Loss: 0.0838\n",
      "Epoch [15/50], Train Loss: 0.0337, Val Loss: 0.0829\n",
      "Epoch [16/50], Train Loss: 0.0334, Val Loss: 0.0820\n",
      "Epoch [17/50], Train Loss: 0.0331, Val Loss: 0.0811\n",
      "Epoch [18/50], Train Loss: 0.0329, Val Loss: 0.0801\n",
      "Epoch [19/50], Train Loss: 0.0326, Val Loss: 0.0792\n",
      "Epoch [20/50], Train Loss: 0.0323, Val Loss: 0.0782\n",
      "Epoch [21/50], Train Loss: 0.0320, Val Loss: 0.0772\n",
      "Epoch [22/50], Train Loss: 0.0318, Val Loss: 0.0761\n",
      "Epoch [23/50], Train Loss: 0.0315, Val Loss: 0.0750\n",
      "Epoch [24/50], Train Loss: 0.0312, Val Loss: 0.0739\n",
      "Epoch [25/50], Train Loss: 0.0308, Val Loss: 0.0728\n",
      "Epoch [26/50], Train Loss: 0.0305, Val Loss: 0.0716\n",
      "Epoch [27/50], Train Loss: 0.0302, Val Loss: 0.0704\n",
      "Epoch [28/50], Train Loss: 0.0299, Val Loss: 0.0692\n",
      "Epoch [29/50], Train Loss: 0.0295, Val Loss: 0.0679\n",
      "Epoch [30/50], Train Loss: 0.0292, Val Loss: 0.0665\n",
      "Epoch [31/50], Train Loss: 0.0288, Val Loss: 0.0651\n",
      "Epoch [32/50], Train Loss: 0.0284, Val Loss: 0.0637\n",
      "Epoch [33/50], Train Loss: 0.0280, Val Loss: 0.0622\n",
      "Epoch [34/50], Train Loss: 0.0276, Val Loss: 0.0607\n",
      "Epoch [35/50], Train Loss: 0.0272, Val Loss: 0.0591\n",
      "Epoch [36/50], Train Loss: 0.0268, Val Loss: 0.0575\n",
      "Epoch [37/50], Train Loss: 0.0263, Val Loss: 0.0558\n",
      "Epoch [38/50], Train Loss: 0.0258, Val Loss: 0.0541\n",
      "Epoch [39/50], Train Loss: 0.0254, Val Loss: 0.0523\n",
      "Epoch [40/50], Train Loss: 0.0249, Val Loss: 0.0504\n",
      "Epoch [41/50], Train Loss: 0.0243, Val Loss: 0.0485\n",
      "Epoch [42/50], Train Loss: 0.0238, Val Loss: 0.0465\n",
      "Epoch [43/50], Train Loss: 0.0232, Val Loss: 0.0445\n",
      "Epoch [44/50], Train Loss: 0.0227, Val Loss: 0.0424\n",
      "Epoch [45/50], Train Loss: 0.0221, Val Loss: 0.0403\n",
      "Epoch [46/50], Train Loss: 0.0214, Val Loss: 0.0381\n",
      "Epoch [47/50], Train Loss: 0.0208, Val Loss: 0.0359\n",
      "Epoch [48/50], Train Loss: 0.0202, Val Loss: 0.0337\n",
      "Epoch [49/50], Train Loss: 0.0195, Val Loss: 0.0314\n",
      "Epoch [50/50], Train Loss: 0.0188, Val Loss: 0.0292\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=128, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1291, Val Loss: 0.2936\n",
      "Epoch [2/50], Train Loss: 0.0796, Val Loss: 0.2140\n",
      "Epoch [3/50], Train Loss: 0.0544, Val Loss: 0.1620\n",
      "Epoch [4/50], Train Loss: 0.0438, Val Loss: 0.1291\n",
      "Epoch [5/50], Train Loss: 0.0388, Val Loss: 0.1104\n",
      "Epoch [6/50], Train Loss: 0.0379, Val Loss: 0.0992\n",
      "Epoch [7/50], Train Loss: 0.0379, Val Loss: 0.0925\n",
      "Epoch [8/50], Train Loss: 0.0376, Val Loss: 0.0883\n",
      "Epoch [9/50], Train Loss: 0.0372, Val Loss: 0.0857\n",
      "Epoch [10/50], Train Loss: 0.0370, Val Loss: 0.0837\n",
      "Epoch [11/50], Train Loss: 0.0366, Val Loss: 0.0829\n",
      "Epoch [12/50], Train Loss: 0.0367, Val Loss: 0.0816\n",
      "Epoch [13/50], Train Loss: 0.0359, Val Loss: 0.0802\n",
      "Epoch [14/50], Train Loss: 0.0362, Val Loss: 0.0794\n",
      "Epoch [15/50], Train Loss: 0.0351, Val Loss: 0.0783\n",
      "Epoch [16/50], Train Loss: 0.0358, Val Loss: 0.0773\n",
      "Epoch [17/50], Train Loss: 0.0344, Val Loss: 0.0763\n",
      "Epoch [18/50], Train Loss: 0.0346, Val Loss: 0.0762\n",
      "Epoch [19/50], Train Loss: 0.0346, Val Loss: 0.0753\n",
      "Epoch [20/50], Train Loss: 0.0346, Val Loss: 0.0736\n",
      "Epoch [21/50], Train Loss: 0.0336, Val Loss: 0.0726\n",
      "Epoch [22/50], Train Loss: 0.0328, Val Loss: 0.0713\n",
      "Epoch [23/50], Train Loss: 0.0329, Val Loss: 0.0701\n",
      "Epoch [24/50], Train Loss: 0.0330, Val Loss: 0.0695\n",
      "Epoch [25/50], Train Loss: 0.0325, Val Loss: 0.0686\n",
      "Epoch [26/50], Train Loss: 0.0316, Val Loss: 0.0676\n",
      "Epoch [27/50], Train Loss: 0.0310, Val Loss: 0.0664\n",
      "Epoch [28/50], Train Loss: 0.0309, Val Loss: 0.0647\n",
      "Epoch [29/50], Train Loss: 0.0305, Val Loss: 0.0635\n",
      "Epoch [30/50], Train Loss: 0.0299, Val Loss: 0.0619\n",
      "Epoch [31/50], Train Loss: 0.0298, Val Loss: 0.0608\n",
      "Epoch [32/50], Train Loss: 0.0287, Val Loss: 0.0596\n",
      "Epoch [33/50], Train Loss: 0.0290, Val Loss: 0.0580\n",
      "Epoch [34/50], Train Loss: 0.0285, Val Loss: 0.0560\n",
      "Epoch [35/50], Train Loss: 0.0276, Val Loss: 0.0542\n",
      "Epoch [36/50], Train Loss: 0.0273, Val Loss: 0.0526\n",
      "Epoch [37/50], Train Loss: 0.0270, Val Loss: 0.0512\n",
      "Epoch [38/50], Train Loss: 0.0266, Val Loss: 0.0496\n",
      "Epoch [39/50], Train Loss: 0.0254, Val Loss: 0.0482\n",
      "Epoch [40/50], Train Loss: 0.0255, Val Loss: 0.0461\n",
      "Epoch [41/50], Train Loss: 0.0243, Val Loss: 0.0444\n",
      "Epoch [42/50], Train Loss: 0.0241, Val Loss: 0.0424\n",
      "Epoch [43/50], Train Loss: 0.0239, Val Loss: 0.0406\n",
      "Epoch [44/50], Train Loss: 0.0231, Val Loss: 0.0386\n",
      "Epoch [45/50], Train Loss: 0.0225, Val Loss: 0.0364\n",
      "Epoch [46/50], Train Loss: 0.0218, Val Loss: 0.0341\n",
      "Epoch [47/50], Train Loss: 0.0212, Val Loss: 0.0322\n",
      "Epoch [48/50], Train Loss: 0.0204, Val Loss: 0.0309\n",
      "Epoch [49/50], Train Loss: 0.0203, Val Loss: 0.0298\n",
      "Epoch [50/50], Train Loss: 0.0199, Val Loss: 0.0275\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=128, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1460, Val Loss: 0.2958\n",
      "Epoch [2/50], Train Loss: 0.0865, Val Loss: 0.1980\n",
      "Epoch [3/50], Train Loss: 0.0634, Val Loss: 0.1458\n",
      "Epoch [4/50], Train Loss: 0.0565, Val Loss: 0.1177\n",
      "Epoch [5/50], Train Loss: 0.0559, Val Loss: 0.1049\n",
      "Epoch [6/50], Train Loss: 0.0543, Val Loss: 0.0965\n",
      "Epoch [7/50], Train Loss: 0.0512, Val Loss: 0.0961\n",
      "Epoch [8/50], Train Loss: 0.0524, Val Loss: 0.0915\n",
      "Epoch [9/50], Train Loss: 0.0523, Val Loss: 0.0900\n",
      "Epoch [10/50], Train Loss: 0.0517, Val Loss: 0.0875\n",
      "Epoch [11/50], Train Loss: 0.0506, Val Loss: 0.0873\n",
      "Epoch [12/50], Train Loss: 0.0493, Val Loss: 0.0878\n",
      "Epoch [13/50], Train Loss: 0.0486, Val Loss: 0.0856\n",
      "Epoch [14/50], Train Loss: 0.0471, Val Loss: 0.0849\n",
      "Epoch [15/50], Train Loss: 0.0475, Val Loss: 0.0848\n",
      "Epoch [16/50], Train Loss: 0.0464, Val Loss: 0.0827\n",
      "Epoch [17/50], Train Loss: 0.0468, Val Loss: 0.0822\n",
      "Epoch [18/50], Train Loss: 0.0458, Val Loss: 0.0808\n",
      "Epoch [19/50], Train Loss: 0.0433, Val Loss: 0.0779\n",
      "Epoch [20/50], Train Loss: 0.0450, Val Loss: 0.0767\n",
      "Epoch [21/50], Train Loss: 0.0432, Val Loss: 0.0753\n",
      "Epoch [22/50], Train Loss: 0.0428, Val Loss: 0.0741\n",
      "Epoch [23/50], Train Loss: 0.0425, Val Loss: 0.0721\n",
      "Epoch [24/50], Train Loss: 0.0405, Val Loss: 0.0719\n",
      "Epoch [25/50], Train Loss: 0.0403, Val Loss: 0.0685\n",
      "Epoch [26/50], Train Loss: 0.0416, Val Loss: 0.0680\n",
      "Epoch [27/50], Train Loss: 0.0400, Val Loss: 0.0671\n",
      "Epoch [28/50], Train Loss: 0.0394, Val Loss: 0.0650\n",
      "Epoch [29/50], Train Loss: 0.0393, Val Loss: 0.0632\n",
      "Epoch [30/50], Train Loss: 0.0386, Val Loss: 0.0619\n",
      "Epoch [31/50], Train Loss: 0.0381, Val Loss: 0.0616\n",
      "Epoch [32/50], Train Loss: 0.0373, Val Loss: 0.0600\n",
      "Epoch [33/50], Train Loss: 0.0373, Val Loss: 0.0568\n",
      "Epoch [34/50], Train Loss: 0.0367, Val Loss: 0.0543\n",
      "Epoch [35/50], Train Loss: 0.0353, Val Loss: 0.0533\n",
      "Epoch [36/50], Train Loss: 0.0330, Val Loss: 0.0511\n",
      "Epoch [37/50], Train Loss: 0.0343, Val Loss: 0.0483\n",
      "Epoch [38/50], Train Loss: 0.0341, Val Loss: 0.0467\n",
      "Epoch [39/50], Train Loss: 0.0341, Val Loss: 0.0453\n",
      "Epoch [40/50], Train Loss: 0.0321, Val Loss: 0.0422\n",
      "Epoch [41/50], Train Loss: 0.0321, Val Loss: 0.0414\n",
      "Epoch [42/50], Train Loss: 0.0304, Val Loss: 0.0377\n",
      "Epoch [43/50], Train Loss: 0.0305, Val Loss: 0.0347\n",
      "Epoch [44/50], Train Loss: 0.0312, Val Loss: 0.0332\n",
      "Epoch [45/50], Train Loss: 0.0291, Val Loss: 0.0301\n",
      "Epoch [46/50], Train Loss: 0.0290, Val Loss: 0.0294\n",
      "Epoch [47/50], Train Loss: 0.0286, Val Loss: 0.0265\n",
      "Epoch [48/50], Train Loss: 0.0271, Val Loss: 0.0243\n",
      "Epoch [49/50], Train Loss: 0.0274, Val Loss: 0.0227\n",
      "Epoch [50/50], Train Loss: 0.0267, Val Loss: 0.0206\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=16, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0270, Val Loss: 0.0401\n",
      "Epoch [2/50], Train Loss: 0.0391, Val Loss: 0.0232\n",
      "Epoch [3/50], Train Loss: 0.0307, Val Loss: 0.0177\n",
      "Epoch [4/50], Train Loss: 0.0244, Val Loss: 0.0078\n",
      "Epoch [5/50], Train Loss: 0.0120, Val Loss: 0.0149\n",
      "Epoch [6/50], Train Loss: 0.0056, Val Loss: 0.0094\n",
      "Epoch [7/50], Train Loss: 0.0042, Val Loss: 0.0050\n",
      "Epoch [8/50], Train Loss: 0.0027, Val Loss: 0.0097\n",
      "Epoch [9/50], Train Loss: 0.0024, Val Loss: 0.0056\n",
      "Epoch [10/50], Train Loss: 0.0036, Val Loss: 0.0032\n",
      "Epoch [11/50], Train Loss: 0.0027, Val Loss: 0.0114\n",
      "Epoch [12/50], Train Loss: 0.0021, Val Loss: 0.0039\n",
      "Epoch [13/50], Train Loss: 0.0037, Val Loss: 0.0033\n",
      "Epoch [14/50], Train Loss: 0.0030, Val Loss: 0.0118\n",
      "Epoch [15/50], Train Loss: 0.0022, Val Loss: 0.0033\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=16, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0456, Val Loss: 0.0633\n",
      "Epoch [2/50], Train Loss: 0.0519, Val Loss: 0.0345\n",
      "Epoch [3/50], Train Loss: 0.0373, Val Loss: 0.0105\n",
      "Epoch [4/50], Train Loss: 0.0193, Val Loss: 0.0113\n",
      "Epoch [5/50], Train Loss: 0.0126, Val Loss: 0.0026\n",
      "Epoch [6/50], Train Loss: 0.0073, Val Loss: 0.0101\n",
      "Epoch [7/50], Train Loss: 0.0067, Val Loss: 0.0066\n",
      "Epoch [8/50], Train Loss: 0.0044, Val Loss: 0.0022\n",
      "Epoch [9/50], Train Loss: 0.0049, Val Loss: 0.0027\n",
      "Epoch [10/50], Train Loss: 0.0046, Val Loss: 0.0097\n",
      "Epoch [11/50], Train Loss: 0.0050, Val Loss: 0.0036\n",
      "Epoch [12/50], Train Loss: 0.0045, Val Loss: 0.0020\n",
      "Epoch [13/50], Train Loss: 0.0049, Val Loss: 0.0090\n",
      "Epoch [14/50], Train Loss: 0.0041, Val Loss: 0.0031\n",
      "Epoch [15/50], Train Loss: 0.0039, Val Loss: 0.0017\n",
      "Epoch [16/50], Train Loss: 0.0045, Val Loss: 0.0094\n",
      "Epoch [17/50], Train Loss: 0.0044, Val Loss: 0.0030\n",
      "Epoch [18/50], Train Loss: 0.0051, Val Loss: 0.0034\n",
      "Epoch [19/50], Train Loss: 0.0060, Val Loss: 0.0141\n",
      "Epoch [20/50], Train Loss: 0.0061, Val Loss: 0.0022\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=16, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0594, Val Loss: 0.0595\n",
      "Epoch [2/50], Train Loss: 0.0565, Val Loss: 0.0264\n",
      "Epoch [3/50], Train Loss: 0.0252, Val Loss: 0.0063\n",
      "Epoch [4/50], Train Loss: 0.0154, Val Loss: 0.0096\n",
      "Epoch [5/50], Train Loss: 0.0115, Val Loss: 0.0030\n",
      "Epoch [6/50], Train Loss: 0.0112, Val Loss: 0.0065\n",
      "Epoch [7/50], Train Loss: 0.0095, Val Loss: 0.0107\n",
      "Epoch [8/50], Train Loss: 0.0090, Val Loss: 0.0065\n",
      "Epoch [9/50], Train Loss: 0.0092, Val Loss: 0.0051\n",
      "Epoch [10/50], Train Loss: 0.0075, Val Loss: 0.0072\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=16, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0301, Val Loss: 0.0637\n",
      "Epoch [2/50], Train Loss: 0.0519, Val Loss: 0.0715\n",
      "Epoch [3/50], Train Loss: 0.0517, Val Loss: 0.0349\n",
      "Epoch [4/50], Train Loss: 0.0378, Val Loss: 0.0217\n",
      "Epoch [5/50], Train Loss: 0.0214, Val Loss: 0.0095\n",
      "Epoch [6/50], Train Loss: 0.0107, Val Loss: 0.0120\n",
      "Epoch [7/50], Train Loss: 0.0167, Val Loss: 0.0267\n",
      "Epoch [8/50], Train Loss: 0.0252, Val Loss: 0.0042\n",
      "Epoch [9/50], Train Loss: 0.0236, Val Loss: 0.0262\n",
      "Epoch [10/50], Train Loss: 0.0073, Val Loss: 0.0457\n",
      "Epoch [11/50], Train Loss: 0.0039, Val Loss: 0.0037\n",
      "Epoch [12/50], Train Loss: 0.0058, Val Loss: 0.0045\n",
      "Epoch [13/50], Train Loss: 0.0072, Val Loss: 0.0150\n",
      "Epoch [14/50], Train Loss: 0.0078, Val Loss: 0.0035\n",
      "Epoch [15/50], Train Loss: 0.0040, Val Loss: 0.0040\n",
      "Epoch [16/50], Train Loss: 0.0036, Val Loss: 0.0167\n",
      "Epoch [17/50], Train Loss: 0.0027, Val Loss: 0.0049\n",
      "Epoch [18/50], Train Loss: 0.0035, Val Loss: 0.0033\n",
      "Epoch [19/50], Train Loss: 0.0031, Val Loss: 0.0126\n",
      "Epoch [20/50], Train Loss: 0.0030, Val Loss: 0.0143\n",
      "Epoch [21/50], Train Loss: 0.0037, Val Loss: 0.0030\n",
      "Epoch [22/50], Train Loss: 0.0028, Val Loss: 0.0064\n",
      "Epoch [23/50], Train Loss: 0.0041, Val Loss: 0.0170\n",
      "Epoch [24/50], Train Loss: 0.0027, Val Loss: 0.0020\n",
      "Epoch [25/50], Train Loss: 0.0032, Val Loss: 0.0053\n",
      "Epoch [26/50], Train Loss: 0.0042, Val Loss: 0.0117\n",
      "Epoch [27/50], Train Loss: 0.0029, Val Loss: 0.0029\n",
      "Epoch [28/50], Train Loss: 0.0043, Val Loss: 0.0064\n",
      "Epoch [29/50], Train Loss: 0.0030, Val Loss: 0.0102\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=16, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0527, Val Loss: 0.0958\n",
      "Epoch [2/50], Train Loss: 0.0605, Val Loss: 0.0249\n",
      "Epoch [3/50], Train Loss: 0.0427, Val Loss: 0.0360\n",
      "Epoch [4/50], Train Loss: 0.0398, Val Loss: 0.0181\n",
      "Epoch [5/50], Train Loss: 0.0195, Val Loss: 0.0071\n",
      "Epoch [6/50], Train Loss: 0.0171, Val Loss: 0.0173\n",
      "Epoch [7/50], Train Loss: 0.0119, Val Loss: 0.0044\n",
      "Epoch [8/50], Train Loss: 0.0199, Val Loss: 0.0041\n",
      "Epoch [9/50], Train Loss: 0.0124, Val Loss: 0.0250\n",
      "Epoch [10/50], Train Loss: 0.0206, Val Loss: 0.0108\n",
      "Epoch [11/50], Train Loss: 0.0116, Val Loss: 0.0037\n",
      "Epoch [12/50], Train Loss: 0.0109, Val Loss: 0.0205\n",
      "Epoch [13/50], Train Loss: 0.0205, Val Loss: 0.0134\n",
      "Epoch [14/50], Train Loss: 0.0100, Val Loss: 0.0050\n",
      "Epoch [15/50], Train Loss: 0.0097, Val Loss: 0.0132\n",
      "Epoch [16/50], Train Loss: 0.0190, Val Loss: 0.0113\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=16, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0935, Val Loss: 0.1076\n",
      "Epoch [2/50], Train Loss: 0.0605, Val Loss: 0.0474\n",
      "Epoch [3/50], Train Loss: 0.0453, Val Loss: 0.0327\n",
      "Epoch [4/50], Train Loss: 0.0383, Val Loss: 0.0249\n",
      "Epoch [5/50], Train Loss: 0.0356, Val Loss: 0.0224\n",
      "Epoch [6/50], Train Loss: 0.0288, Val Loss: 0.0128\n",
      "Epoch [7/50], Train Loss: 0.0301, Val Loss: 0.0172\n",
      "Epoch [8/50], Train Loss: 0.0170, Val Loss: 0.0157\n",
      "Epoch [9/50], Train Loss: 0.0265, Val Loss: 0.0051\n",
      "Epoch [10/50], Train Loss: 0.0163, Val Loss: 0.0384\n",
      "Epoch [11/50], Train Loss: 0.0184, Val Loss: 0.0041\n",
      "Epoch [12/50], Train Loss: 0.0116, Val Loss: 0.0049\n",
      "Epoch [13/50], Train Loss: 0.0154, Val Loss: 0.0401\n",
      "Epoch [14/50], Train Loss: 0.0269, Val Loss: 0.0121\n",
      "Epoch [15/50], Train Loss: 0.0138, Val Loss: 0.0168\n",
      "Epoch [16/50], Train Loss: 0.0149, Val Loss: 0.0220\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=16, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0303, Val Loss: 0.1032\n",
      "Epoch [2/50], Train Loss: 0.0509, Val Loss: 0.0933\n",
      "Epoch [3/50], Train Loss: 0.0450, Val Loss: 0.0659\n",
      "Epoch [4/50], Train Loss: 0.0500, Val Loss: 0.2950\n",
      "Epoch [5/50], Train Loss: 0.0398, Val Loss: 0.0854\n",
      "Epoch [6/50], Train Loss: 0.0574, Val Loss: 0.0426\n",
      "Epoch [7/50], Train Loss: 0.0553, Val Loss: 0.0122\n",
      "Epoch [8/50], Train Loss: 0.0356, Val Loss: 0.0767\n",
      "Epoch [9/50], Train Loss: 0.0814, Val Loss: 0.0186\n",
      "Epoch [10/50], Train Loss: 0.0415, Val Loss: 0.0720\n",
      "Epoch [11/50], Train Loss: 0.0664, Val Loss: 0.0383\n",
      "Epoch [12/50], Train Loss: 0.0540, Val Loss: 0.0615\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=16, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0460, Val Loss: 0.1089\n",
      "Epoch [2/50], Train Loss: 0.0564, Val Loss: 0.1016\n",
      "Epoch [3/50], Train Loss: 0.0541, Val Loss: 0.1387\n",
      "Epoch [4/50], Train Loss: 0.0575, Val Loss: 0.1186\n",
      "Epoch [5/50], Train Loss: 0.0335, Val Loss: 0.0633\n",
      "Epoch [6/50], Train Loss: 0.0537, Val Loss: 0.0733\n",
      "Epoch [7/50], Train Loss: 0.0559, Val Loss: 0.0453\n",
      "Epoch [8/50], Train Loss: 0.0484, Val Loss: 0.0313\n",
      "Epoch [9/50], Train Loss: 0.0307, Val Loss: 0.0424\n",
      "Epoch [10/50], Train Loss: 0.0377, Val Loss: 0.0283\n",
      "Epoch [11/50], Train Loss: 0.0308, Val Loss: 0.0238\n",
      "Epoch [12/50], Train Loss: 0.0260, Val Loss: 0.0065\n",
      "Epoch [13/50], Train Loss: 0.0203, Val Loss: 0.0122\n",
      "Epoch [14/50], Train Loss: 0.0183, Val Loss: 0.0095\n",
      "Epoch [15/50], Train Loss: 0.0109, Val Loss: 0.0076\n",
      "Epoch [16/50], Train Loss: 0.0130, Val Loss: 0.0197\n",
      "Epoch [17/50], Train Loss: 0.0099, Val Loss: 0.0121\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=16, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0997, Val Loss: 0.1103\n",
      "Epoch [2/50], Train Loss: 0.0665, Val Loss: 0.0806\n",
      "Epoch [3/50], Train Loss: 0.0666, Val Loss: 0.1025\n",
      "Epoch [4/50], Train Loss: 0.0504, Val Loss: 0.0250\n",
      "Epoch [5/50], Train Loss: 0.0432, Val Loss: 0.0361\n",
      "Epoch [6/50], Train Loss: 0.0426, Val Loss: 0.0134\n",
      "Epoch [7/50], Train Loss: 0.0359, Val Loss: 0.0280\n",
      "Epoch [8/50], Train Loss: 0.0300, Val Loss: 0.0197\n",
      "Epoch [9/50], Train Loss: 0.0268, Val Loss: 0.0426\n",
      "Epoch [10/50], Train Loss: 0.0262, Val Loss: 0.0300\n",
      "Epoch [11/50], Train Loss: 0.0256, Val Loss: 0.0087\n",
      "Epoch [12/50], Train Loss: 0.0241, Val Loss: 0.0125\n",
      "Epoch [13/50], Train Loss: 0.0282, Val Loss: 0.0253\n",
      "Epoch [14/50], Train Loss: 0.0467, Val Loss: 0.0661\n",
      "Epoch [15/50], Train Loss: 0.0281, Val Loss: 0.0228\n",
      "Epoch [16/50], Train Loss: 0.0244, Val Loss: 0.0139\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=32, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0242, Val Loss: 0.0566\n",
      "Epoch [2/50], Train Loss: 0.0406, Val Loss: 0.0271\n",
      "Epoch [3/50], Train Loss: 0.0679, Val Loss: 0.0247\n",
      "Epoch [4/50], Train Loss: 0.0231, Val Loss: 0.0065\n",
      "Epoch [5/50], Train Loss: 0.0156, Val Loss: 0.0077\n",
      "Epoch [6/50], Train Loss: 0.0127, Val Loss: 0.0168\n",
      "Epoch [7/50], Train Loss: 0.0074, Val Loss: 0.0021\n",
      "Epoch [8/50], Train Loss: 0.0044, Val Loss: 0.0073\n",
      "Epoch [9/50], Train Loss: 0.0038, Val Loss: 0.0031\n",
      "Epoch [10/50], Train Loss: 0.0025, Val Loss: 0.0016\n",
      "Epoch [11/50], Train Loss: 0.0036, Val Loss: 0.0083\n",
      "Epoch [12/50], Train Loss: 0.0026, Val Loss: 0.0022\n",
      "Epoch [13/50], Train Loss: 0.0034, Val Loss: 0.0036\n",
      "Epoch [14/50], Train Loss: 0.0029, Val Loss: 0.0091\n",
      "Epoch [15/50], Train Loss: 0.0019, Val Loss: 0.0023\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=32, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0298, Val Loss: 0.0653\n",
      "Epoch [2/50], Train Loss: 0.0537, Val Loss: 0.0321\n",
      "Epoch [3/50], Train Loss: 0.0410, Val Loss: 0.0181\n",
      "Epoch [4/50], Train Loss: 0.0412, Val Loss: 0.0080\n",
      "Epoch [5/50], Train Loss: 0.0162, Val Loss: 0.0074\n",
      "Epoch [6/50], Train Loss: 0.0138, Val Loss: 0.0126\n",
      "Epoch [7/50], Train Loss: 0.0070, Val Loss: 0.0027\n",
      "Epoch [8/50], Train Loss: 0.0057, Val Loss: 0.0138\n",
      "Epoch [9/50], Train Loss: 0.0049, Val Loss: 0.0016\n",
      "Epoch [10/50], Train Loss: 0.0071, Val Loss: 0.0088\n",
      "Epoch [11/50], Train Loss: 0.0038, Val Loss: 0.0032\n",
      "Epoch [12/50], Train Loss: 0.0037, Val Loss: 0.0046\n",
      "Epoch [13/50], Train Loss: 0.0036, Val Loss: 0.0042\n",
      "Epoch [14/50], Train Loss: 0.0039, Val Loss: 0.0034\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=32, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0323, Val Loss: 0.0447\n",
      "Epoch [2/50], Train Loss: 0.0668, Val Loss: 0.0602\n",
      "Epoch [3/50], Train Loss: 0.0395, Val Loss: 0.0049\n",
      "Epoch [4/50], Train Loss: 0.0215, Val Loss: 0.0028\n",
      "Epoch [5/50], Train Loss: 0.0159, Val Loss: 0.0046\n",
      "Epoch [6/50], Train Loss: 0.0118, Val Loss: 0.0054\n",
      "Epoch [7/50], Train Loss: 0.0083, Val Loss: 0.0035\n",
      "Epoch [8/50], Train Loss: 0.0083, Val Loss: 0.0067\n",
      "Epoch [9/50], Train Loss: 0.0070, Val Loss: 0.0037\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=32, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0206, Val Loss: 0.0179\n",
      "Epoch [2/50], Train Loss: 0.0522, Val Loss: 0.0495\n",
      "Epoch [3/50], Train Loss: 0.0456, Val Loss: 0.0472\n",
      "Epoch [4/50], Train Loss: 0.0308, Val Loss: 0.0169\n",
      "Epoch [5/50], Train Loss: 0.0220, Val Loss: 0.0581\n",
      "Epoch [6/50], Train Loss: 0.0234, Val Loss: 0.0136\n",
      "Epoch [7/50], Train Loss: 0.0212, Val Loss: 0.0064\n",
      "Epoch [8/50], Train Loss: 0.0125, Val Loss: 0.0083\n",
      "Epoch [9/50], Train Loss: 0.0140, Val Loss: 0.0112\n",
      "Epoch [10/50], Train Loss: 0.0109, Val Loss: 0.0198\n",
      "Epoch [11/50], Train Loss: 0.0092, Val Loss: 0.0072\n",
      "Epoch [12/50], Train Loss: 0.0035, Val Loss: 0.0036\n",
      "Epoch [13/50], Train Loss: 0.0042, Val Loss: 0.0072\n",
      "Epoch [14/50], Train Loss: 0.0056, Val Loss: 0.0066\n",
      "Epoch [15/50], Train Loss: 0.0027, Val Loss: 0.0040\n",
      "Epoch [16/50], Train Loss: 0.0041, Val Loss: 0.0044\n",
      "Epoch [17/50], Train Loss: 0.0034, Val Loss: 0.0054\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=32, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0326, Val Loss: 0.0490\n",
      "Epoch [2/50], Train Loss: 0.0536, Val Loss: 0.0867\n",
      "Epoch [3/50], Train Loss: 0.0467, Val Loss: 0.0412\n",
      "Epoch [4/50], Train Loss: 0.0486, Val Loss: 0.0206\n",
      "Epoch [5/50], Train Loss: 0.0321, Val Loss: 0.0280\n",
      "Epoch [6/50], Train Loss: 0.0336, Val Loss: 0.0048\n",
      "Epoch [7/50], Train Loss: 0.0135, Val Loss: 0.0044\n",
      "Epoch [8/50], Train Loss: 0.0176, Val Loss: 0.0120\n",
      "Epoch [9/50], Train Loss: 0.0104, Val Loss: 0.0037\n",
      "Epoch [10/50], Train Loss: 0.0077, Val Loss: 0.0024\n",
      "Epoch [11/50], Train Loss: 0.0063, Val Loss: 0.0026\n",
      "Epoch [12/50], Train Loss: 0.0074, Val Loss: 0.0042\n",
      "Epoch [13/50], Train Loss: 0.0158, Val Loss: 0.0145\n",
      "Epoch [14/50], Train Loss: 0.0067, Val Loss: 0.0067\n",
      "Epoch [15/50], Train Loss: 0.0081, Val Loss: 0.0072\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=32, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0447, Val Loss: 0.0390\n",
      "Epoch [2/50], Train Loss: 0.0630, Val Loss: 0.0663\n",
      "Epoch [3/50], Train Loss: 0.0629, Val Loss: 0.0385\n",
      "Epoch [4/50], Train Loss: 0.0378, Val Loss: 0.0146\n",
      "Epoch [5/50], Train Loss: 0.0255, Val Loss: 0.0047\n",
      "Epoch [6/50], Train Loss: 0.0285, Val Loss: 0.0047\n",
      "Epoch [7/50], Train Loss: 0.0250, Val Loss: 0.0224\n",
      "Epoch [8/50], Train Loss: 0.0268, Val Loss: 0.0045\n",
      "Epoch [9/50], Train Loss: 0.0155, Val Loss: 0.0121\n",
      "Epoch [10/50], Train Loss: 0.0125, Val Loss: 0.0089\n",
      "Epoch [11/50], Train Loss: 0.0089, Val Loss: 0.0071\n",
      "Epoch [12/50], Train Loss: 0.0181, Val Loss: 0.0057\n",
      "Epoch [13/50], Train Loss: 0.0135, Val Loss: 0.0044\n",
      "Epoch [14/50], Train Loss: 0.0469, Val Loss: 0.0542\n",
      "Epoch [15/50], Train Loss: 0.0208, Val Loss: 0.0166\n",
      "Epoch [16/50], Train Loss: 0.0152, Val Loss: 0.0043\n",
      "Epoch [17/50], Train Loss: 0.0152, Val Loss: 0.0240\n",
      "Epoch [18/50], Train Loss: 0.0158, Val Loss: 0.0096\n",
      "Epoch [19/50], Train Loss: 0.0086, Val Loss: 0.0066\n",
      "Epoch [20/50], Train Loss: 0.0155, Val Loss: 0.0032\n",
      "Epoch [21/50], Train Loss: 0.0097, Val Loss: 0.0073\n",
      "Epoch [22/50], Train Loss: 0.0266, Val Loss: 0.0211\n",
      "Epoch [23/50], Train Loss: 0.0101, Val Loss: 0.0145\n",
      "Epoch [24/50], Train Loss: 0.0119, Val Loss: 0.0142\n",
      "Epoch [25/50], Train Loss: 0.0124, Val Loss: 0.0015\n",
      "Epoch [26/50], Train Loss: 0.0117, Val Loss: 0.0371\n",
      "Epoch [27/50], Train Loss: 0.0149, Val Loss: 0.0044\n",
      "Epoch [28/50], Train Loss: 0.0161, Val Loss: 0.0416\n",
      "Epoch [29/50], Train Loss: 0.0128, Val Loss: 0.0062\n",
      "Epoch [30/50], Train Loss: 0.0092, Val Loss: 0.0074\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=32, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0238, Val Loss: 0.0693\n",
      "Epoch [2/50], Train Loss: 0.0459, Val Loss: 0.1106\n",
      "Epoch [3/50], Train Loss: 0.0439, Val Loss: 0.0680\n",
      "Epoch [4/50], Train Loss: 0.0361, Val Loss: 0.0437\n",
      "Epoch [5/50], Train Loss: 0.0407, Val Loss: 0.0238\n",
      "Epoch [6/50], Train Loss: 0.0419, Val Loss: 0.0693\n",
      "Epoch [7/50], Train Loss: 0.0407, Val Loss: 0.0528\n",
      "Epoch [8/50], Train Loss: 0.0357, Val Loss: 0.0165\n",
      "Epoch [9/50], Train Loss: 0.0514, Val Loss: 0.0764\n",
      "Epoch [10/50], Train Loss: 0.0471, Val Loss: 0.1489\n",
      "Epoch [11/50], Train Loss: 0.0348, Val Loss: 0.1071\n",
      "Epoch [12/50], Train Loss: 0.0478, Val Loss: 0.0638\n",
      "Epoch [13/50], Train Loss: 0.0380, Val Loss: 0.0623\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=32, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0298, Val Loss: 0.0686\n",
      "Epoch [2/50], Train Loss: 0.0486, Val Loss: 0.0703\n",
      "Epoch [3/50], Train Loss: 0.0576, Val Loss: 0.0920\n",
      "Epoch [4/50], Train Loss: 0.0500, Val Loss: 0.0945\n",
      "Epoch [5/50], Train Loss: 0.0616, Val Loss: 0.0630\n",
      "Epoch [6/50], Train Loss: 0.0851, Val Loss: 0.0332\n",
      "Epoch [7/50], Train Loss: 0.0338, Val Loss: 0.0108\n",
      "Epoch [8/50], Train Loss: 0.0296, Val Loss: 0.0405\n",
      "Epoch [9/50], Train Loss: 0.0300, Val Loss: 0.0118\n",
      "Epoch [10/50], Train Loss: 0.0258, Val Loss: 0.0835\n",
      "Epoch [11/50], Train Loss: 0.0363, Val Loss: 0.0632\n",
      "Epoch [12/50], Train Loss: 0.0378, Val Loss: 0.0374\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=32, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0556, Val Loss: 0.0338\n",
      "Epoch [2/50], Train Loss: 0.0580, Val Loss: 0.0734\n",
      "Epoch [3/50], Train Loss: 0.0538, Val Loss: 0.0599\n",
      "Epoch [4/50], Train Loss: 0.0570, Val Loss: 0.0789\n",
      "Epoch [5/50], Train Loss: 0.0400, Val Loss: 0.0284\n",
      "Epoch [6/50], Train Loss: 0.0325, Val Loss: 0.0263\n",
      "Epoch [7/50], Train Loss: 0.0313, Val Loss: 0.0228\n",
      "Epoch [8/50], Train Loss: 0.0250, Val Loss: 0.0128\n",
      "Epoch [9/50], Train Loss: 0.0241, Val Loss: 0.0071\n",
      "Epoch [10/50], Train Loss: 0.0239, Val Loss: 0.0118\n",
      "Epoch [11/50], Train Loss: 0.0372, Val Loss: 0.0468\n",
      "Epoch [12/50], Train Loss: 0.0428, Val Loss: 0.0131\n",
      "Epoch [13/50], Train Loss: 0.0356, Val Loss: 0.0126\n",
      "Epoch [14/50], Train Loss: 0.0198, Val Loss: 0.0556\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=64, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0210, Val Loss: 0.0288\n",
      "Epoch [2/50], Train Loss: 0.0271, Val Loss: 0.0363\n",
      "Epoch [3/50], Train Loss: 0.0349, Val Loss: 0.0380\n",
      "Epoch [4/50], Train Loss: 0.0567, Val Loss: 0.0104\n",
      "Epoch [5/50], Train Loss: 0.0685, Val Loss: 0.0344\n",
      "Epoch [6/50], Train Loss: 0.0621, Val Loss: 0.0266\n",
      "Epoch [7/50], Train Loss: 0.0592, Val Loss: 0.0242\n",
      "Epoch [8/50], Train Loss: 0.0245, Val Loss: 0.0173\n",
      "Epoch [9/50], Train Loss: 0.0183, Val Loss: 0.0167\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=64, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0180, Val Loss: 0.0242\n",
      "Epoch [2/50], Train Loss: 0.0607, Val Loss: 0.0306\n",
      "Epoch [3/50], Train Loss: 0.0629, Val Loss: 0.0090\n",
      "Epoch [4/50], Train Loss: 0.0506, Val Loss: 0.0137\n",
      "Epoch [5/50], Train Loss: 0.0364, Val Loss: 0.0269\n",
      "Epoch [6/50], Train Loss: 0.0250, Val Loss: 0.0110\n",
      "Epoch [7/50], Train Loss: 0.0150, Val Loss: 0.0012\n",
      "Epoch [8/50], Train Loss: 0.0086, Val Loss: 0.0170\n",
      "Epoch [9/50], Train Loss: 0.0067, Val Loss: 0.0152\n",
      "Epoch [10/50], Train Loss: 0.0047, Val Loss: 0.0024\n",
      "Epoch [11/50], Train Loss: 0.0043, Val Loss: 0.0010\n",
      "Epoch [12/50], Train Loss: 0.0042, Val Loss: 0.0074\n",
      "Epoch [13/50], Train Loss: 0.0038, Val Loss: 0.0069\n",
      "Epoch [14/50], Train Loss: 0.0035, Val Loss: 0.0032\n",
      "Epoch [15/50], Train Loss: 0.0033, Val Loss: 0.0023\n",
      "Epoch [16/50], Train Loss: 0.0044, Val Loss: 0.0135\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=64, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0218, Val Loss: 0.0421\n",
      "Epoch [2/50], Train Loss: 0.0278, Val Loss: 0.0216\n",
      "Epoch [3/50], Train Loss: 0.0279, Val Loss: 0.0267\n",
      "Epoch [4/50], Train Loss: 0.0231, Val Loss: 0.0270\n",
      "Epoch [5/50], Train Loss: 0.0255, Val Loss: 0.0118\n",
      "Epoch [6/50], Train Loss: 0.0121, Val Loss: 0.0062\n",
      "Epoch [7/50], Train Loss: 0.0140, Val Loss: 0.0041\n",
      "Epoch [8/50], Train Loss: 0.0098, Val Loss: 0.0048\n",
      "Epoch [9/50], Train Loss: 0.0168, Val Loss: 0.0141\n",
      "Epoch [10/50], Train Loss: 0.0071, Val Loss: 0.0028\n",
      "Epoch [11/50], Train Loss: 0.0063, Val Loss: 0.0096\n",
      "Epoch [12/50], Train Loss: 0.0078, Val Loss: 0.0071\n",
      "Epoch [13/50], Train Loss: 0.0081, Val Loss: 0.0091\n",
      "Epoch [14/50], Train Loss: 0.0055, Val Loss: 0.0073\n",
      "Epoch [15/50], Train Loss: 0.0073, Val Loss: 0.0151\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=64, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0207, Val Loss: 0.0412\n",
      "Epoch [2/50], Train Loss: 0.0302, Val Loss: 0.0364\n",
      "Epoch [3/50], Train Loss: 0.0749, Val Loss: 0.1049\n",
      "Epoch [4/50], Train Loss: 0.0604, Val Loss: 0.0346\n",
      "Epoch [5/50], Train Loss: 0.0643, Val Loss: 0.0663\n",
      "Epoch [6/50], Train Loss: 0.0556, Val Loss: 0.1113\n",
      "Epoch [7/50], Train Loss: 0.0482, Val Loss: 0.0800\n",
      "Epoch [8/50], Train Loss: 0.0512, Val Loss: 0.1072\n",
      "Epoch [9/50], Train Loss: 0.0442, Val Loss: 0.1084\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=64, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0265, Val Loss: 0.0519\n",
      "Epoch [2/50], Train Loss: 0.0327, Val Loss: 0.0340\n",
      "Epoch [3/50], Train Loss: 0.0346, Val Loss: 0.0114\n",
      "Epoch [4/50], Train Loss: 0.0456, Val Loss: 0.0165\n",
      "Epoch [5/50], Train Loss: 0.0329, Val Loss: 0.0197\n",
      "Epoch [6/50], Train Loss: 0.0300, Val Loss: 0.0583\n",
      "Epoch [7/50], Train Loss: 0.0201, Val Loss: 0.0129\n",
      "Epoch [8/50], Train Loss: 0.0079, Val Loss: 0.0060\n",
      "Epoch [9/50], Train Loss: 0.0096, Val Loss: 0.0101\n",
      "Epoch [10/50], Train Loss: 0.0098, Val Loss: 0.0297\n",
      "Epoch [11/50], Train Loss: 0.0177, Val Loss: 0.0103\n",
      "Epoch [12/50], Train Loss: 0.0118, Val Loss: 0.0037\n",
      "Epoch [13/50], Train Loss: 0.0147, Val Loss: 0.0176\n",
      "Epoch [14/50], Train Loss: 0.0203, Val Loss: 0.0133\n",
      "Epoch [15/50], Train Loss: 0.0130, Val Loss: 0.0070\n",
      "Epoch [16/50], Train Loss: 0.0209, Val Loss: 0.0509\n",
      "Epoch [17/50], Train Loss: 0.0092, Val Loss: 0.0161\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=64, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0336, Val Loss: 0.0451\n",
      "Epoch [2/50], Train Loss: 0.0451, Val Loss: 0.0311\n",
      "Epoch [3/50], Train Loss: 0.0590, Val Loss: 0.0689\n",
      "Epoch [4/50], Train Loss: 0.0517, Val Loss: 0.0510\n",
      "Epoch [5/50], Train Loss: 0.0376, Val Loss: 0.0204\n",
      "Epoch [6/50], Train Loss: 0.0272, Val Loss: 0.0445\n",
      "Epoch [7/50], Train Loss: 0.0186, Val Loss: 0.0137\n",
      "Epoch [8/50], Train Loss: 0.0099, Val Loss: 0.0044\n",
      "Epoch [9/50], Train Loss: 0.0129, Val Loss: 0.0024\n",
      "Epoch [10/50], Train Loss: 0.0168, Val Loss: 0.0068\n",
      "Epoch [11/50], Train Loss: 0.0400, Val Loss: 0.0357\n",
      "Epoch [12/50], Train Loss: 0.0244, Val Loss: 0.0223\n",
      "Epoch [13/50], Train Loss: 0.0201, Val Loss: 0.0035\n",
      "Epoch [14/50], Train Loss: 0.0170, Val Loss: 0.0407\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=64, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0246, Val Loss: 0.0590\n",
      "Epoch [2/50], Train Loss: 0.0236, Val Loss: 0.0559\n",
      "Epoch [3/50], Train Loss: 0.0278, Val Loss: 0.2483\n",
      "Epoch [4/50], Train Loss: 0.0846, Val Loss: 0.0117\n",
      "Epoch [5/50], Train Loss: 0.0986, Val Loss: 0.0745\n",
      "Epoch [6/50], Train Loss: 0.1277, Val Loss: 0.0862\n",
      "Epoch [7/50], Train Loss: 0.0595, Val Loss: 0.1130\n",
      "Epoch [8/50], Train Loss: 0.0503, Val Loss: 0.1327\n",
      "Epoch [9/50], Train Loss: 0.0482, Val Loss: 0.1240\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=64, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0285, Val Loss: 0.0469\n",
      "Epoch [2/50], Train Loss: 0.0327, Val Loss: 0.0557\n",
      "Epoch [3/50], Train Loss: 0.1234, Val Loss: 0.2100\n",
      "Epoch [4/50], Train Loss: 0.1081, Val Loss: 0.0412\n",
      "Epoch [5/50], Train Loss: 0.0655, Val Loss: 0.0945\n",
      "Epoch [6/50], Train Loss: 0.0556, Val Loss: 0.0912\n",
      "Epoch [7/50], Train Loss: 0.0537, Val Loss: 0.1042\n",
      "Epoch [8/50], Train Loss: 0.0497, Val Loss: 0.1222\n",
      "Epoch [9/50], Train Loss: 0.0437, Val Loss: 0.1103\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=64, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0303, Val Loss: 0.0439\n",
      "Epoch [2/50], Train Loss: 0.0412, Val Loss: 0.0506\n",
      "Epoch [3/50], Train Loss: 0.0481, Val Loss: 0.0425\n",
      "Epoch [4/50], Train Loss: 0.0488, Val Loss: 0.0304\n",
      "Epoch [5/50], Train Loss: 0.0381, Val Loss: 0.0525\n",
      "Epoch [6/50], Train Loss: 0.0374, Val Loss: 0.0244\n",
      "Epoch [7/50], Train Loss: 0.0689, Val Loss: 0.0362\n",
      "Epoch [8/50], Train Loss: 0.0441, Val Loss: 0.0369\n",
      "Epoch [9/50], Train Loss: 0.0530, Val Loss: 0.0633\n",
      "Epoch [10/50], Train Loss: 0.0632, Val Loss: 0.0169\n",
      "Epoch [11/50], Train Loss: 0.0491, Val Loss: 0.0458\n",
      "Epoch [12/50], Train Loss: 0.0450, Val Loss: 0.0686\n",
      "Epoch [13/50], Train Loss: 0.0458, Val Loss: 0.0759\n",
      "Epoch [14/50], Train Loss: 0.0500, Val Loss: 0.1115\n",
      "Epoch [15/50], Train Loss: 0.0331, Val Loss: 0.0243\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=128, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0202, Val Loss: 0.0509\n",
      "Epoch [2/50], Train Loss: 0.2284, Val Loss: 0.1233\n",
      "Epoch [3/50], Train Loss: 0.2768, Val Loss: 0.1184\n",
      "Epoch [4/50], Train Loss: 0.2790, Val Loss: 0.5931\n",
      "Epoch [5/50], Train Loss: 0.0886, Val Loss: 0.0438\n",
      "Epoch [6/50], Train Loss: 0.2247, Val Loss: 0.3355\n",
      "Epoch [7/50], Train Loss: 0.0614, Val Loss: 0.0859\n",
      "Epoch [8/50], Train Loss: 0.0565, Val Loss: 0.0580\n",
      "Epoch [9/50], Train Loss: 0.0677, Val Loss: 0.0571\n",
      "Epoch [10/50], Train Loss: 0.0698, Val Loss: 0.0634\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=128, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0280, Val Loss: 0.0419\n",
      "Epoch [2/50], Train Loss: 0.0342, Val Loss: 0.0257\n",
      "Epoch [3/50], Train Loss: 0.0264, Val Loss: 0.0283\n",
      "Epoch [4/50], Train Loss: 0.0197, Val Loss: 0.0283\n",
      "Epoch [5/50], Train Loss: 0.0144, Val Loss: 0.0360\n",
      "Epoch [6/50], Train Loss: 0.0139, Val Loss: 0.0244\n",
      "Epoch [7/50], Train Loss: 0.0121, Val Loss: 0.0225\n",
      "Epoch [8/50], Train Loss: 0.0118, Val Loss: 0.0158\n",
      "Epoch [9/50], Train Loss: 0.0099, Val Loss: 0.0036\n",
      "Epoch [10/50], Train Loss: 0.0057, Val Loss: 0.0027\n",
      "Epoch [11/50], Train Loss: 0.0079, Val Loss: 0.0045\n",
      "Epoch [12/50], Train Loss: 0.0040, Val Loss: 0.0025\n",
      "Epoch [13/50], Train Loss: 0.0041, Val Loss: 0.0091\n",
      "Epoch [14/50], Train Loss: 0.0025, Val Loss: 0.0038\n",
      "Epoch [15/50], Train Loss: 0.0019, Val Loss: 0.0014\n",
      "Epoch [16/50], Train Loss: 0.0022, Val Loss: 0.0022\n",
      "Epoch [17/50], Train Loss: 0.0024, Val Loss: 0.0043\n",
      "Epoch [18/50], Train Loss: 0.0020, Val Loss: 0.0027\n",
      "Epoch [19/50], Train Loss: 0.0021, Val Loss: 0.0022\n",
      "Epoch [20/50], Train Loss: 0.0028, Val Loss: 0.0057\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=128, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0194, Val Loss: 0.0476\n",
      "Epoch [2/50], Train Loss: 0.0820, Val Loss: 0.0329\n",
      "Epoch [3/50], Train Loss: 0.0408, Val Loss: 0.0236\n",
      "Epoch [4/50], Train Loss: 0.0370, Val Loss: 0.0389\n",
      "Epoch [5/50], Train Loss: 0.0294, Val Loss: 0.0058\n",
      "Epoch [6/50], Train Loss: 0.0204, Val Loss: 0.0059\n",
      "Epoch [7/50], Train Loss: 0.0116, Val Loss: 0.0083\n",
      "Epoch [8/50], Train Loss: 0.0094, Val Loss: 0.0160\n",
      "Epoch [9/50], Train Loss: 0.0041, Val Loss: 0.0026\n",
      "Epoch [10/50], Train Loss: 0.0053, Val Loss: 0.0043\n",
      "Epoch [11/50], Train Loss: 0.0039, Val Loss: 0.0024\n",
      "Epoch [12/50], Train Loss: 0.0042, Val Loss: 0.0081\n",
      "Epoch [13/50], Train Loss: 0.0040, Val Loss: 0.0026\n",
      "Epoch [14/50], Train Loss: 0.0047, Val Loss: 0.0046\n",
      "Epoch [15/50], Train Loss: 0.0046, Val Loss: 0.0075\n",
      "Epoch [16/50], Train Loss: 0.0031, Val Loss: 0.0013\n",
      "Epoch [17/50], Train Loss: 0.0037, Val Loss: 0.0031\n",
      "Epoch [18/50], Train Loss: 0.0033, Val Loss: 0.0024\n",
      "Epoch [19/50], Train Loss: 0.0035, Val Loss: 0.0028\n",
      "Epoch [20/50], Train Loss: 0.0035, Val Loss: 0.0038\n",
      "Epoch [21/50], Train Loss: 0.0034, Val Loss: 0.0013\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=128, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0241, Val Loss: 0.0100\n",
      "Epoch [2/50], Train Loss: 0.2967, Val Loss: 0.4291\n",
      "Epoch [3/50], Train Loss: 0.0382, Val Loss: 0.0764\n",
      "Epoch [4/50], Train Loss: 0.0463, Val Loss: 0.0438\n",
      "Epoch [5/50], Train Loss: 0.0600, Val Loss: 0.0540\n",
      "Epoch [6/50], Train Loss: 0.0622, Val Loss: 0.0600\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=128, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1229, Val Loss: 0.4585\n",
      "Epoch [2/50], Train Loss: 0.1104, Val Loss: 0.0618\n",
      "Epoch [3/50], Train Loss: 0.0794, Val Loss: 0.0322\n",
      "Epoch [4/50], Train Loss: 0.0699, Val Loss: 0.0262\n",
      "Epoch [5/50], Train Loss: 0.0528, Val Loss: 0.1198\n",
      "Epoch [6/50], Train Loss: 0.0402, Val Loss: 0.0469\n",
      "Epoch [7/50], Train Loss: 0.0722, Val Loss: 0.0224\n",
      "Epoch [8/50], Train Loss: 0.0735, Val Loss: 0.0763\n",
      "Epoch [9/50], Train Loss: 0.0477, Val Loss: 0.0451\n",
      "Epoch [10/50], Train Loss: 0.0528, Val Loss: 0.0577\n",
      "Epoch [11/50], Train Loss: 0.0533, Val Loss: 0.0590\n",
      "Epoch [12/50], Train Loss: 0.0520, Val Loss: 0.0547\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=128, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0291, Val Loss: 0.0143\n",
      "Epoch [2/50], Train Loss: 0.1025, Val Loss: 0.0522\n",
      "Epoch [3/50], Train Loss: 0.0445, Val Loss: 0.0380\n",
      "Epoch [4/50], Train Loss: 0.0420, Val Loss: 0.0173\n",
      "Epoch [5/50], Train Loss: 0.0505, Val Loss: 0.0231\n",
      "Epoch [6/50], Train Loss: 0.0546, Val Loss: 0.0237\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=128, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1548, Val Loss: 1.7902\n",
      "Epoch [2/50], Train Loss: 0.2177, Val Loss: 0.1403\n",
      "Epoch [3/50], Train Loss: 0.2715, Val Loss: 1.5736\n",
      "Epoch [4/50], Train Loss: 0.1871, Val Loss: 0.1795\n",
      "Epoch [5/50], Train Loss: 0.0524, Val Loss: 0.1209\n",
      "Epoch [6/50], Train Loss: 0.0557, Val Loss: 0.0921\n",
      "Epoch [7/50], Train Loss: 0.0606, Val Loss: 0.0846\n",
      "Epoch [8/50], Train Loss: 0.0620, Val Loss: 0.0813\n",
      "Epoch [9/50], Train Loss: 0.0630, Val Loss: 0.0815\n",
      "Epoch [10/50], Train Loss: 0.0623, Val Loss: 0.0822\n",
      "Epoch [11/50], Train Loss: 0.0624, Val Loss: 0.0856\n",
      "Epoch [12/50], Train Loss: 0.0613, Val Loss: 0.0869\n",
      "Epoch [13/50], Train Loss: 0.0613, Val Loss: 0.0894\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=128, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.3049, Val Loss: 0.0523\n",
      "Epoch [2/50], Train Loss: 0.1358, Val Loss: 0.2583\n",
      "Epoch [3/50], Train Loss: 0.0956, Val Loss: 0.0695\n",
      "Epoch [4/50], Train Loss: 0.0896, Val Loss: 0.0687\n",
      "Epoch [5/50], Train Loss: 0.0890, Val Loss: 0.0879\n",
      "Epoch [6/50], Train Loss: 0.0781, Val Loss: 0.0860\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=128, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0270, Val Loss: 0.0370\n",
      "Epoch [2/50], Train Loss: 0.0715, Val Loss: 0.0822\n",
      "Epoch [3/50], Train Loss: 0.0940, Val Loss: 0.2522\n",
      "Epoch [4/50], Train Loss: 0.0741, Val Loss: 0.0908\n",
      "Epoch [5/50], Train Loss: 0.0837, Val Loss: 0.0192\n",
      "Epoch [6/50], Train Loss: 0.0658, Val Loss: 0.0421\n",
      "Epoch [7/50], Train Loss: 0.0541, Val Loss: 0.0298\n",
      "Epoch [8/50], Train Loss: 0.0580, Val Loss: 0.0254\n",
      "Epoch [9/50], Train Loss: 0.0585, Val Loss: 0.0388\n",
      "Epoch [10/50], Train Loss: 0.0503, Val Loss: 0.0194\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=16, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0991, Val Loss: 0.1853\n",
      "Epoch [2/50], Train Loss: 0.0364, Val Loss: 0.0749\n",
      "Epoch [3/50], Train Loss: 0.0312, Val Loss: 0.0518\n",
      "Epoch [4/50], Train Loss: 0.0266, Val Loss: 0.0410\n",
      "Epoch [5/50], Train Loss: 0.0216, Val Loss: 0.0313\n",
      "Epoch [6/50], Train Loss: 0.0175, Val Loss: 0.0237\n",
      "Epoch [7/50], Train Loss: 0.0135, Val Loss: 0.0177\n",
      "Epoch [8/50], Train Loss: 0.0093, Val Loss: 0.0128\n",
      "Epoch [9/50], Train Loss: 0.0059, Val Loss: 0.0122\n",
      "Epoch [10/50], Train Loss: 0.0049, Val Loss: 0.0121\n",
      "Epoch [11/50], Train Loss: 0.0036, Val Loss: 0.0055\n",
      "Epoch [12/50], Train Loss: 0.0034, Val Loss: 0.0058\n",
      "Epoch [13/50], Train Loss: 0.0029, Val Loss: 0.0065\n",
      "Epoch [14/50], Train Loss: 0.0028, Val Loss: 0.0048\n",
      "Epoch [15/50], Train Loss: 0.0026, Val Loss: 0.0045\n",
      "Epoch [16/50], Train Loss: 0.0024, Val Loss: 0.0050\n",
      "Epoch [17/50], Train Loss: 0.0023, Val Loss: 0.0045\n",
      "Epoch [18/50], Train Loss: 0.0023, Val Loss: 0.0042\n",
      "Epoch [19/50], Train Loss: 0.0023, Val Loss: 0.0044\n",
      "Epoch [20/50], Train Loss: 0.0022, Val Loss: 0.0046\n",
      "Epoch [21/50], Train Loss: 0.0021, Val Loss: 0.0039\n",
      "Epoch [22/50], Train Loss: 0.0022, Val Loss: 0.0042\n",
      "Epoch [23/50], Train Loss: 0.0021, Val Loss: 0.0041\n",
      "Epoch [24/50], Train Loss: 0.0021, Val Loss: 0.0044\n",
      "Epoch [25/50], Train Loss: 0.0020, Val Loss: 0.0034\n",
      "Epoch [26/50], Train Loss: 0.0022, Val Loss: 0.0043\n",
      "Epoch [27/50], Train Loss: 0.0020, Val Loss: 0.0038\n",
      "Epoch [28/50], Train Loss: 0.0020, Val Loss: 0.0043\n",
      "Epoch [29/50], Train Loss: 0.0019, Val Loss: 0.0032\n",
      "Epoch [30/50], Train Loss: 0.0021, Val Loss: 0.0039\n",
      "Epoch [31/50], Train Loss: 0.0019, Val Loss: 0.0038\n",
      "Epoch [32/50], Train Loss: 0.0019, Val Loss: 0.0039\n",
      "Epoch [33/50], Train Loss: 0.0019, Val Loss: 0.0035\n",
      "Epoch [34/50], Train Loss: 0.0019, Val Loss: 0.0031\n",
      "Epoch [35/50], Train Loss: 0.0019, Val Loss: 0.0039\n",
      "Epoch [36/50], Train Loss: 0.0018, Val Loss: 0.0034\n",
      "Epoch [37/50], Train Loss: 0.0019, Val Loss: 0.0038\n",
      "Epoch [38/50], Train Loss: 0.0018, Val Loss: 0.0028\n",
      "Epoch [39/50], Train Loss: 0.0019, Val Loss: 0.0034\n",
      "Epoch [40/50], Train Loss: 0.0018, Val Loss: 0.0033\n",
      "Epoch [41/50], Train Loss: 0.0018, Val Loss: 0.0035\n",
      "Epoch [42/50], Train Loss: 0.0018, Val Loss: 0.0033\n",
      "Epoch [43/50], Train Loss: 0.0017, Val Loss: 0.0027\n",
      "Epoch [44/50], Train Loss: 0.0018, Val Loss: 0.0034\n",
      "Epoch [45/50], Train Loss: 0.0017, Val Loss: 0.0029\n",
      "Epoch [46/50], Train Loss: 0.0018, Val Loss: 0.0035\n",
      "Epoch [47/50], Train Loss: 0.0017, Val Loss: 0.0029\n",
      "Epoch [48/50], Train Loss: 0.0017, Val Loss: 0.0027\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=16, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1224, Val Loss: 0.2029\n",
      "Epoch [2/50], Train Loss: 0.0542, Val Loss: 0.0883\n",
      "Epoch [3/50], Train Loss: 0.0404, Val Loss: 0.0561\n",
      "Epoch [4/50], Train Loss: 0.0364, Val Loss: 0.0411\n",
      "Epoch [5/50], Train Loss: 0.0297, Val Loss: 0.0283\n",
      "Epoch [6/50], Train Loss: 0.0237, Val Loss: 0.0199\n",
      "Epoch [7/50], Train Loss: 0.0220, Val Loss: 0.0167\n",
      "Epoch [8/50], Train Loss: 0.0185, Val Loss: 0.0151\n",
      "Epoch [9/50], Train Loss: 0.0178, Val Loss: 0.0118\n",
      "Epoch [10/50], Train Loss: 0.0152, Val Loss: 0.0106\n",
      "Epoch [11/50], Train Loss: 0.0136, Val Loss: 0.0100\n",
      "Epoch [12/50], Train Loss: 0.0125, Val Loss: 0.0086\n",
      "Epoch [13/50], Train Loss: 0.0117, Val Loss: 0.0095\n",
      "Epoch [14/50], Train Loss: 0.0100, Val Loss: 0.0069\n",
      "Epoch [15/50], Train Loss: 0.0097, Val Loss: 0.0056\n",
      "Epoch [16/50], Train Loss: 0.0092, Val Loss: 0.0060\n",
      "Epoch [17/50], Train Loss: 0.0084, Val Loss: 0.0040\n",
      "Epoch [18/50], Train Loss: 0.0081, Val Loss: 0.0036\n",
      "Epoch [19/50], Train Loss: 0.0084, Val Loss: 0.0059\n",
      "Epoch [20/50], Train Loss: 0.0084, Val Loss: 0.0051\n",
      "Epoch [21/50], Train Loss: 0.0073, Val Loss: 0.0024\n",
      "Epoch [22/50], Train Loss: 0.0075, Val Loss: 0.0031\n",
      "Epoch [23/50], Train Loss: 0.0070, Val Loss: 0.0052\n",
      "Epoch [24/50], Train Loss: 0.0069, Val Loss: 0.0057\n",
      "Epoch [25/50], Train Loss: 0.0071, Val Loss: 0.0031\n",
      "Epoch [26/50], Train Loss: 0.0076, Val Loss: 0.0024\n",
      "Epoch [27/50], Train Loss: 0.0074, Val Loss: 0.0027\n",
      "Epoch [28/50], Train Loss: 0.0071, Val Loss: 0.0053\n",
      "Epoch [29/50], Train Loss: 0.0064, Val Loss: 0.0030\n",
      "Epoch [30/50], Train Loss: 0.0066, Val Loss: 0.0027\n",
      "Epoch [31/50], Train Loss: 0.0066, Val Loss: 0.0035\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=16, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1059, Val Loss: 0.1607\n",
      "Epoch [2/50], Train Loss: 0.0683, Val Loss: 0.0870\n",
      "Epoch [3/50], Train Loss: 0.0533, Val Loss: 0.0589\n",
      "Epoch [4/50], Train Loss: 0.0447, Val Loss: 0.0408\n",
      "Epoch [5/50], Train Loss: 0.0405, Val Loss: 0.0266\n",
      "Epoch [6/50], Train Loss: 0.0345, Val Loss: 0.0208\n",
      "Epoch [7/50], Train Loss: 0.0314, Val Loss: 0.0227\n",
      "Epoch [8/50], Train Loss: 0.0295, Val Loss: 0.0170\n",
      "Epoch [9/50], Train Loss: 0.0280, Val Loss: 0.0163\n",
      "Epoch [10/50], Train Loss: 0.0267, Val Loss: 0.0106\n",
      "Epoch [11/50], Train Loss: 0.0243, Val Loss: 0.0084\n",
      "Epoch [12/50], Train Loss: 0.0229, Val Loss: 0.0116\n",
      "Epoch [13/50], Train Loss: 0.0209, Val Loss: 0.0145\n",
      "Epoch [14/50], Train Loss: 0.0183, Val Loss: 0.0117\n",
      "Epoch [15/50], Train Loss: 0.0185, Val Loss: 0.0046\n",
      "Epoch [16/50], Train Loss: 0.0176, Val Loss: 0.0048\n",
      "Epoch [17/50], Train Loss: 0.0161, Val Loss: 0.0068\n",
      "Epoch [18/50], Train Loss: 0.0146, Val Loss: 0.0079\n",
      "Epoch [19/50], Train Loss: 0.0136, Val Loss: 0.0082\n",
      "Epoch [20/50], Train Loss: 0.0138, Val Loss: 0.0066\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=16, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0614, Val Loss: 0.0720\n",
      "Epoch [2/50], Train Loss: 0.0522, Val Loss: 0.0785\n",
      "Epoch [3/50], Train Loss: 0.0354, Val Loss: 0.0564\n",
      "Epoch [4/50], Train Loss: 0.0310, Val Loss: 0.0355\n",
      "Epoch [5/50], Train Loss: 0.0227, Val Loss: 0.0146\n",
      "Epoch [6/50], Train Loss: 0.0214, Val Loss: 0.0221\n",
      "Epoch [7/50], Train Loss: 0.0146, Val Loss: 0.0125\n",
      "Epoch [8/50], Train Loss: 0.0126, Val Loss: 0.0105\n",
      "Epoch [9/50], Train Loss: 0.0098, Val Loss: 0.0093\n",
      "Epoch [10/50], Train Loss: 0.0065, Val Loss: 0.0097\n",
      "Epoch [11/50], Train Loss: 0.0041, Val Loss: 0.0068\n",
      "Epoch [12/50], Train Loss: 0.0054, Val Loss: 0.0047\n",
      "Epoch [13/50], Train Loss: 0.0033, Val Loss: 0.0043\n",
      "Epoch [14/50], Train Loss: 0.0043, Val Loss: 0.0091\n",
      "Epoch [15/50], Train Loss: 0.0036, Val Loss: 0.0054\n",
      "Epoch [16/50], Train Loss: 0.0028, Val Loss: 0.0025\n",
      "Epoch [17/50], Train Loss: 0.0035, Val Loss: 0.0056\n",
      "Epoch [18/50], Train Loss: 0.0027, Val Loss: 0.0068\n",
      "Epoch [19/50], Train Loss: 0.0026, Val Loss: 0.0027\n",
      "Epoch [20/50], Train Loss: 0.0029, Val Loss: 0.0027\n",
      "Epoch [21/50], Train Loss: 0.0043, Val Loss: 0.0118\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=16, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1132, Val Loss: 0.1831\n",
      "Epoch [2/50], Train Loss: 0.0569, Val Loss: 0.0901\n",
      "Epoch [3/50], Train Loss: 0.0528, Val Loss: 0.0826\n",
      "Epoch [4/50], Train Loss: 0.0476, Val Loss: 0.0710\n",
      "Epoch [5/50], Train Loss: 0.0433, Val Loss: 0.0493\n",
      "Epoch [6/50], Train Loss: 0.0362, Val Loss: 0.0318\n",
      "Epoch [7/50], Train Loss: 0.0296, Val Loss: 0.0180\n",
      "Epoch [8/50], Train Loss: 0.0246, Val Loss: 0.0160\n",
      "Epoch [9/50], Train Loss: 0.0222, Val Loss: 0.0209\n",
      "Epoch [10/50], Train Loss: 0.0173, Val Loss: 0.0121\n",
      "Epoch [11/50], Train Loss: 0.0175, Val Loss: 0.0115\n",
      "Epoch [12/50], Train Loss: 0.0153, Val Loss: 0.0124\n",
      "Epoch [13/50], Train Loss: 0.0128, Val Loss: 0.0094\n",
      "Epoch [14/50], Train Loss: 0.0134, Val Loss: 0.0084\n",
      "Epoch [15/50], Train Loss: 0.0118, Val Loss: 0.0113\n",
      "Epoch [16/50], Train Loss: 0.0124, Val Loss: 0.0122\n",
      "Epoch [17/50], Train Loss: 0.0115, Val Loss: 0.0075\n",
      "Epoch [18/50], Train Loss: 0.0113, Val Loss: 0.0066\n",
      "Epoch [19/50], Train Loss: 0.0109, Val Loss: 0.0127\n",
      "Epoch [20/50], Train Loss: 0.0097, Val Loss: 0.0103\n",
      "Epoch [21/50], Train Loss: 0.0097, Val Loss: 0.0079\n",
      "Epoch [22/50], Train Loss: 0.0091, Val Loss: 0.0128\n",
      "Epoch [23/50], Train Loss: 0.0090, Val Loss: 0.0097\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=16, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1423, Val Loss: 0.1774\n",
      "Epoch [2/50], Train Loss: 0.0785, Val Loss: 0.1196\n",
      "Epoch [3/50], Train Loss: 0.0574, Val Loss: 0.0755\n",
      "Epoch [4/50], Train Loss: 0.0474, Val Loss: 0.0496\n",
      "Epoch [5/50], Train Loss: 0.0374, Val Loss: 0.0396\n",
      "Epoch [6/50], Train Loss: 0.0343, Val Loss: 0.0376\n",
      "Epoch [7/50], Train Loss: 0.0310, Val Loss: 0.0227\n",
      "Epoch [8/50], Train Loss: 0.0288, Val Loss: 0.0103\n",
      "Epoch [9/50], Train Loss: 0.0251, Val Loss: 0.0301\n",
      "Epoch [10/50], Train Loss: 0.0243, Val Loss: 0.0326\n",
      "Epoch [11/50], Train Loss: 0.0258, Val Loss: 0.0117\n",
      "Epoch [12/50], Train Loss: 0.0223, Val Loss: 0.0034\n",
      "Epoch [13/50], Train Loss: 0.0208, Val Loss: 0.0271\n",
      "Epoch [14/50], Train Loss: 0.0194, Val Loss: 0.0208\n",
      "Epoch [15/50], Train Loss: 0.0199, Val Loss: 0.0086\n",
      "Epoch [16/50], Train Loss: 0.0192, Val Loss: 0.0090\n",
      "Epoch [17/50], Train Loss: 0.0176, Val Loss: 0.0225\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=16, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0646, Val Loss: 0.1036\n",
      "Epoch [2/50], Train Loss: 0.0465, Val Loss: 0.0827\n",
      "Epoch [3/50], Train Loss: 0.0398, Val Loss: 0.0729\n",
      "Epoch [4/50], Train Loss: 0.0342, Val Loss: 0.0500\n",
      "Epoch [5/50], Train Loss: 0.0256, Val Loss: 0.0227\n",
      "Epoch [6/50], Train Loss: 0.0144, Val Loss: 0.0130\n",
      "Epoch [7/50], Train Loss: 0.0125, Val Loss: 0.0385\n",
      "Epoch [8/50], Train Loss: 0.0093, Val Loss: 0.0230\n",
      "Epoch [9/50], Train Loss: 0.0050, Val Loss: 0.0177\n",
      "Epoch [10/50], Train Loss: 0.0050, Val Loss: 0.0338\n",
      "Epoch [11/50], Train Loss: 0.0042, Val Loss: 0.0206\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=16, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0671, Val Loss: 0.0508\n",
      "Epoch [2/50], Train Loss: 0.0783, Val Loss: 0.1086\n",
      "Epoch [3/50], Train Loss: 0.0545, Val Loss: 0.0934\n",
      "Epoch [4/50], Train Loss: 0.0555, Val Loss: 0.0789\n",
      "Epoch [5/50], Train Loss: 0.0506, Val Loss: 0.0583\n",
      "Epoch [6/50], Train Loss: 0.0411, Val Loss: 0.0345\n",
      "Epoch [7/50], Train Loss: 0.0381, Val Loss: 0.0475\n",
      "Epoch [8/50], Train Loss: 0.0323, Val Loss: 0.0388\n",
      "Epoch [9/50], Train Loss: 0.0326, Val Loss: 0.0490\n",
      "Epoch [10/50], Train Loss: 0.0283, Val Loss: 0.0255\n",
      "Epoch [11/50], Train Loss: 0.0285, Val Loss: 0.0272\n",
      "Epoch [12/50], Train Loss: 0.0246, Val Loss: 0.0238\n",
      "Epoch [13/50], Train Loss: 0.0225, Val Loss: 0.0263\n",
      "Epoch [14/50], Train Loss: 0.0181, Val Loss: 0.0140\n",
      "Epoch [15/50], Train Loss: 0.0161, Val Loss: 0.0097\n",
      "Epoch [16/50], Train Loss: 0.0188, Val Loss: 0.0170\n",
      "Epoch [17/50], Train Loss: 0.0148, Val Loss: 0.0118\n",
      "Epoch [18/50], Train Loss: 0.0162, Val Loss: 0.0059\n",
      "Epoch [19/50], Train Loss: 0.0136, Val Loss: 0.0183\n",
      "Epoch [20/50], Train Loss: 0.0142, Val Loss: 0.0183\n",
      "Epoch [21/50], Train Loss: 0.0151, Val Loss: 0.0059\n",
      "Epoch [22/50], Train Loss: 0.0150, Val Loss: 0.0231\n",
      "Epoch [23/50], Train Loss: 0.0127, Val Loss: 0.0107\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=16, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0988, Val Loss: 0.1725\n",
      "Epoch [2/50], Train Loss: 0.0610, Val Loss: 0.1021\n",
      "Epoch [3/50], Train Loss: 0.0551, Val Loss: 0.0629\n",
      "Epoch [4/50], Train Loss: 0.0437, Val Loss: 0.0350\n",
      "Epoch [5/50], Train Loss: 0.0391, Val Loss: 0.0450\n",
      "Epoch [6/50], Train Loss: 0.0344, Val Loss: 0.0308\n",
      "Epoch [7/50], Train Loss: 0.0314, Val Loss: 0.0297\n",
      "Epoch [8/50], Train Loss: 0.0325, Val Loss: 0.0262\n",
      "Epoch [9/50], Train Loss: 0.0286, Val Loss: 0.0361\n",
      "Epoch [10/50], Train Loss: 0.0270, Val Loss: 0.0260\n",
      "Epoch [11/50], Train Loss: 0.0249, Val Loss: 0.0116\n",
      "Epoch [12/50], Train Loss: 0.0237, Val Loss: 0.0134\n",
      "Epoch [13/50], Train Loss: 0.0226, Val Loss: 0.0103\n",
      "Epoch [14/50], Train Loss: 0.0224, Val Loss: 0.0074\n",
      "Epoch [15/50], Train Loss: 0.0205, Val Loss: 0.0282\n",
      "Epoch [16/50], Train Loss: 0.0206, Val Loss: 0.0110\n",
      "Epoch [17/50], Train Loss: 0.0179, Val Loss: 0.0125\n",
      "Epoch [18/50], Train Loss: 0.0173, Val Loss: 0.0185\n",
      "Epoch [19/50], Train Loss: 0.0177, Val Loss: 0.0314\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=32, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0961, Val Loss: 0.1513\n",
      "Epoch [2/50], Train Loss: 0.0320, Val Loss: 0.0362\n",
      "Epoch [3/50], Train Loss: 0.0330, Val Loss: 0.0308\n",
      "Epoch [4/50], Train Loss: 0.0202, Val Loss: 0.0122\n",
      "Epoch [5/50], Train Loss: 0.0115, Val Loss: 0.0127\n",
      "Epoch [6/50], Train Loss: 0.0133, Val Loss: 0.0113\n",
      "Epoch [7/50], Train Loss: 0.0067, Val Loss: 0.0052\n",
      "Epoch [8/50], Train Loss: 0.0044, Val Loss: 0.0144\n",
      "Epoch [9/50], Train Loss: 0.0031, Val Loss: 0.0035\n",
      "Epoch [10/50], Train Loss: 0.0050, Val Loss: 0.0072\n",
      "Epoch [11/50], Train Loss: 0.0032, Val Loss: 0.0110\n",
      "Epoch [12/50], Train Loss: 0.0028, Val Loss: 0.0041\n",
      "Epoch [13/50], Train Loss: 0.0028, Val Loss: 0.0025\n",
      "Epoch [14/50], Train Loss: 0.0027, Val Loss: 0.0092\n",
      "Epoch [15/50], Train Loss: 0.0021, Val Loss: 0.0030\n",
      "Epoch [16/50], Train Loss: 0.0030, Val Loss: 0.0025\n",
      "Epoch [17/50], Train Loss: 0.0026, Val Loss: 0.0095\n",
      "Epoch [18/50], Train Loss: 0.0021, Val Loss: 0.0031\n",
      "Epoch [19/50], Train Loss: 0.0030, Val Loss: 0.0025\n",
      "Epoch [20/50], Train Loss: 0.0025, Val Loss: 0.0085\n",
      "Epoch [21/50], Train Loss: 0.0020, Val Loss: 0.0036\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=32, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0428, Val Loss: 0.0524\n",
      "Epoch [2/50], Train Loss: 0.0466, Val Loss: 0.0473\n",
      "Epoch [3/50], Train Loss: 0.0290, Val Loss: 0.0400\n",
      "Epoch [4/50], Train Loss: 0.0200, Val Loss: 0.0214\n",
      "Epoch [5/50], Train Loss: 0.0151, Val Loss: 0.0129\n",
      "Epoch [6/50], Train Loss: 0.0111, Val Loss: 0.0093\n",
      "Epoch [7/50], Train Loss: 0.0087, Val Loss: 0.0057\n",
      "Epoch [8/50], Train Loss: 0.0062, Val Loss: 0.0036\n",
      "Epoch [9/50], Train Loss: 0.0063, Val Loss: 0.0044\n",
      "Epoch [10/50], Train Loss: 0.0055, Val Loss: 0.0049\n",
      "Epoch [11/50], Train Loss: 0.0054, Val Loss: 0.0038\n",
      "Epoch [12/50], Train Loss: 0.0050, Val Loss: 0.0031\n",
      "Epoch [13/50], Train Loss: 0.0052, Val Loss: 0.0035\n",
      "Epoch [14/50], Train Loss: 0.0050, Val Loss: 0.0061\n",
      "Epoch [15/50], Train Loss: 0.0048, Val Loss: 0.0022\n",
      "Epoch [16/50], Train Loss: 0.0055, Val Loss: 0.0034\n",
      "Epoch [17/50], Train Loss: 0.0051, Val Loss: 0.0040\n",
      "Epoch [18/50], Train Loss: 0.0047, Val Loss: 0.0042\n",
      "Epoch [19/50], Train Loss: 0.0046, Val Loss: 0.0023\n",
      "Epoch [20/50], Train Loss: 0.0045, Val Loss: 0.0025\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=32, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0823, Val Loss: 0.0663\n",
      "Epoch [2/50], Train Loss: 0.0634, Val Loss: 0.0810\n",
      "Epoch [3/50], Train Loss: 0.0378, Val Loss: 0.0318\n",
      "Epoch [4/50], Train Loss: 0.0318, Val Loss: 0.0177\n",
      "Epoch [5/50], Train Loss: 0.0239, Val Loss: 0.0108\n",
      "Epoch [6/50], Train Loss: 0.0179, Val Loss: 0.0119\n",
      "Epoch [7/50], Train Loss: 0.0171, Val Loss: 0.0116\n",
      "Epoch [8/50], Train Loss: 0.0141, Val Loss: 0.0053\n",
      "Epoch [9/50], Train Loss: 0.0189, Val Loss: 0.0071\n",
      "Epoch [10/50], Train Loss: 0.0140, Val Loss: 0.0198\n",
      "Epoch [11/50], Train Loss: 0.0111, Val Loss: 0.0052\n",
      "Epoch [12/50], Train Loss: 0.0121, Val Loss: 0.0041\n",
      "Epoch [13/50], Train Loss: 0.0104, Val Loss: 0.0035\n",
      "Epoch [14/50], Train Loss: 0.0101, Val Loss: 0.0119\n",
      "Epoch [15/50], Train Loss: 0.0106, Val Loss: 0.0079\n",
      "Epoch [16/50], Train Loss: 0.0103, Val Loss: 0.0037\n",
      "Epoch [17/50], Train Loss: 0.0096, Val Loss: 0.0036\n",
      "Epoch [18/50], Train Loss: 0.0092, Val Loss: 0.0124\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=32, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0303, Val Loss: 0.0579\n",
      "Epoch [2/50], Train Loss: 0.0692, Val Loss: 0.0596\n",
      "Epoch [3/50], Train Loss: 0.0347, Val Loss: 0.0370\n",
      "Epoch [4/50], Train Loss: 0.0228, Val Loss: 0.0207\n",
      "Epoch [5/50], Train Loss: 0.0147, Val Loss: 0.0127\n",
      "Epoch [6/50], Train Loss: 0.0056, Val Loss: 0.0044\n",
      "Epoch [7/50], Train Loss: 0.0049, Val Loss: 0.0080\n",
      "Epoch [8/50], Train Loss: 0.0040, Val Loss: 0.0098\n",
      "Epoch [9/50], Train Loss: 0.0063, Val Loss: 0.0036\n",
      "Epoch [10/50], Train Loss: 0.0030, Val Loss: 0.0044\n",
      "Epoch [11/50], Train Loss: 0.0042, Val Loss: 0.0110\n",
      "Epoch [12/50], Train Loss: 0.0059, Val Loss: 0.0085\n",
      "Epoch [13/50], Train Loss: 0.0043, Val Loss: 0.0029\n",
      "Epoch [14/50], Train Loss: 0.0075, Val Loss: 0.0126\n",
      "Epoch [15/50], Train Loss: 0.0105, Val Loss: 0.0060\n",
      "Epoch [16/50], Train Loss: 0.0094, Val Loss: 0.0084\n",
      "Epoch [17/50], Train Loss: 0.0060, Val Loss: 0.0112\n",
      "Epoch [18/50], Train Loss: 0.0096, Val Loss: 0.0084\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=32, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0644, Val Loss: 0.0299\n",
      "Epoch [2/50], Train Loss: 0.0823, Val Loss: 0.0591\n",
      "Epoch [3/50], Train Loss: 0.0476, Val Loss: 0.0412\n",
      "Epoch [4/50], Train Loss: 0.0291, Val Loss: 0.0222\n",
      "Epoch [5/50], Train Loss: 0.0255, Val Loss: 0.0228\n",
      "Epoch [6/50], Train Loss: 0.0180, Val Loss: 0.0086\n",
      "Epoch [7/50], Train Loss: 0.0147, Val Loss: 0.0136\n",
      "Epoch [8/50], Train Loss: 0.0158, Val Loss: 0.0153\n",
      "Epoch [9/50], Train Loss: 0.0172, Val Loss: 0.0050\n",
      "Epoch [10/50], Train Loss: 0.0166, Val Loss: 0.0109\n",
      "Epoch [11/50], Train Loss: 0.0137, Val Loss: 0.0261\n",
      "Epoch [12/50], Train Loss: 0.0127, Val Loss: 0.0082\n",
      "Epoch [13/50], Train Loss: 0.0155, Val Loss: 0.0093\n",
      "Epoch [14/50], Train Loss: 0.0099, Val Loss: 0.0160\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=32, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0562, Val Loss: 0.0737\n",
      "Epoch [2/50], Train Loss: 0.0570, Val Loss: 0.0544\n",
      "Epoch [3/50], Train Loss: 0.0398, Val Loss: 0.0360\n",
      "Epoch [4/50], Train Loss: 0.0222, Val Loss: 0.0319\n",
      "Epoch [5/50], Train Loss: 0.0178, Val Loss: 0.0097\n",
      "Epoch [6/50], Train Loss: 0.0215, Val Loss: 0.0069\n",
      "Epoch [7/50], Train Loss: 0.0168, Val Loss: 0.0266\n",
      "Epoch [8/50], Train Loss: 0.0215, Val Loss: 0.0120\n",
      "Epoch [9/50], Train Loss: 0.0176, Val Loss: 0.0165\n",
      "Epoch [10/50], Train Loss: 0.0179, Val Loss: 0.0211\n",
      "Epoch [11/50], Train Loss: 0.0215, Val Loss: 0.0235\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=32, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0403, Val Loss: 0.0612\n",
      "Epoch [2/50], Train Loss: 0.0710, Val Loss: 0.0775\n",
      "Epoch [3/50], Train Loss: 0.0488, Val Loss: 0.0697\n",
      "Epoch [4/50], Train Loss: 0.0352, Val Loss: 0.0290\n",
      "Epoch [5/50], Train Loss: 0.0283, Val Loss: 0.0284\n",
      "Epoch [6/50], Train Loss: 0.0239, Val Loss: 0.0474\n",
      "Epoch [7/50], Train Loss: 0.0189, Val Loss: 0.0235\n",
      "Epoch [8/50], Train Loss: 0.0196, Val Loss: 0.0103\n",
      "Epoch [9/50], Train Loss: 0.0081, Val Loss: 0.0148\n",
      "Epoch [10/50], Train Loss: 0.0173, Val Loss: 0.0086\n",
      "Epoch [11/50], Train Loss: 0.0073, Val Loss: 0.0212\n",
      "Epoch [12/50], Train Loss: 0.0077, Val Loss: 0.0077\n",
      "Epoch [13/50], Train Loss: 0.0063, Val Loss: 0.0087\n",
      "Epoch [14/50], Train Loss: 0.0061, Val Loss: 0.0101\n",
      "Epoch [15/50], Train Loss: 0.0081, Val Loss: 0.0038\n",
      "Epoch [16/50], Train Loss: 0.0047, Val Loss: 0.0101\n",
      "Epoch [17/50], Train Loss: 0.0069, Val Loss: 0.0042\n",
      "Epoch [18/50], Train Loss: 0.0052, Val Loss: 0.0107\n",
      "Epoch [19/50], Train Loss: 0.0052, Val Loss: 0.0040\n",
      "Epoch [20/50], Train Loss: 0.0047, Val Loss: 0.0152\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=32, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0559, Val Loss: 0.0584\n",
      "Epoch [2/50], Train Loss: 0.0903, Val Loss: 0.0755\n",
      "Epoch [3/50], Train Loss: 0.0595, Val Loss: 0.0650\n",
      "Epoch [4/50], Train Loss: 0.0421, Val Loss: 0.0370\n",
      "Epoch [5/50], Train Loss: 0.0254, Val Loss: 0.0370\n",
      "Epoch [6/50], Train Loss: 0.0242, Val Loss: 0.0343\n",
      "Epoch [7/50], Train Loss: 0.0179, Val Loss: 0.0388\n",
      "Epoch [8/50], Train Loss: 0.0223, Val Loss: 0.0489\n",
      "Epoch [9/50], Train Loss: 0.0174, Val Loss: 0.0197\n",
      "Epoch [10/50], Train Loss: 0.0228, Val Loss: 0.0262\n",
      "Epoch [11/50], Train Loss: 0.0182, Val Loss: 0.0341\n",
      "Epoch [12/50], Train Loss: 0.0331, Val Loss: 0.0343\n",
      "Epoch [13/50], Train Loss: 0.0169, Val Loss: 0.0231\n",
      "Epoch [14/50], Train Loss: 0.0138, Val Loss: 0.0365\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=32, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0717, Val Loss: 0.0583\n",
      "Epoch [2/50], Train Loss: 0.0688, Val Loss: 0.0657\n",
      "Epoch [3/50], Train Loss: 0.0460, Val Loss: 0.0247\n",
      "Epoch [4/50], Train Loss: 0.0388, Val Loss: 0.0275\n",
      "Epoch [5/50], Train Loss: 0.0306, Val Loss: 0.0251\n",
      "Epoch [6/50], Train Loss: 0.0276, Val Loss: 0.0181\n",
      "Epoch [7/50], Train Loss: 0.0257, Val Loss: 0.0074\n",
      "Epoch [8/50], Train Loss: 0.0202, Val Loss: 0.0043\n",
      "Epoch [9/50], Train Loss: 0.0189, Val Loss: 0.0076\n",
      "Epoch [10/50], Train Loss: 0.0175, Val Loss: 0.0093\n",
      "Epoch [11/50], Train Loss: 0.0320, Val Loss: 0.0135\n",
      "Epoch [12/50], Train Loss: 0.0332, Val Loss: 0.0715\n",
      "Epoch [13/50], Train Loss: 0.0351, Val Loss: 0.0451\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=64, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0354, Val Loss: 0.0772\n",
      "Epoch [2/50], Train Loss: 0.0555, Val Loss: 0.0262\n",
      "Epoch [3/50], Train Loss: 0.0387, Val Loss: 0.0246\n",
      "Epoch [4/50], Train Loss: 0.0239, Val Loss: 0.0096\n",
      "Epoch [5/50], Train Loss: 0.0158, Val Loss: 0.0022\n",
      "Epoch [6/50], Train Loss: 0.0057, Val Loss: 0.0082\n",
      "Epoch [7/50], Train Loss: 0.0145, Val Loss: 0.0115\n",
      "Epoch [8/50], Train Loss: 0.0073, Val Loss: 0.0045\n",
      "Epoch [9/50], Train Loss: 0.0030, Val Loss: 0.0025\n",
      "Epoch [10/50], Train Loss: 0.0037, Val Loss: 0.0091\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=64, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0307, Val Loss: 0.0510\n",
      "Epoch [2/50], Train Loss: 0.0722, Val Loss: 0.0391\n",
      "Epoch [3/50], Train Loss: 0.0318, Val Loss: 0.0150\n",
      "Epoch [4/50], Train Loss: 0.0175, Val Loss: 0.0075\n",
      "Epoch [5/50], Train Loss: 0.0144, Val Loss: 0.0058\n",
      "Epoch [6/50], Train Loss: 0.0076, Val Loss: 0.0021\n",
      "Epoch [7/50], Train Loss: 0.0060, Val Loss: 0.0082\n",
      "Epoch [8/50], Train Loss: 0.0051, Val Loss: 0.0056\n",
      "Epoch [9/50], Train Loss: 0.0049, Val Loss: 0.0025\n",
      "Epoch [10/50], Train Loss: 0.0046, Val Loss: 0.0052\n",
      "Epoch [11/50], Train Loss: 0.0046, Val Loss: 0.0083\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=64, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0441, Val Loss: 0.0377\n",
      "Epoch [2/50], Train Loss: 0.0520, Val Loss: 0.0397\n",
      "Epoch [3/50], Train Loss: 0.0336, Val Loss: 0.0221\n",
      "Epoch [4/50], Train Loss: 0.0217, Val Loss: 0.0064\n",
      "Epoch [5/50], Train Loss: 0.0154, Val Loss: 0.0082\n",
      "Epoch [6/50], Train Loss: 0.0096, Val Loss: 0.0059\n",
      "Epoch [7/50], Train Loss: 0.0084, Val Loss: 0.0028\n",
      "Epoch [8/50], Train Loss: 0.0082, Val Loss: 0.0103\n",
      "Epoch [9/50], Train Loss: 0.0081, Val Loss: 0.0048\n",
      "Epoch [10/50], Train Loss: 0.0087, Val Loss: 0.0032\n",
      "Epoch [11/50], Train Loss: 0.0085, Val Loss: 0.0128\n",
      "Epoch [12/50], Train Loss: 0.0079, Val Loss: 0.0018\n",
      "Epoch [13/50], Train Loss: 0.0068, Val Loss: 0.0025\n",
      "Epoch [14/50], Train Loss: 0.0071, Val Loss: 0.0142\n",
      "Epoch [15/50], Train Loss: 0.0070, Val Loss: 0.0030\n",
      "Epoch [16/50], Train Loss: 0.0084, Val Loss: 0.0050\n",
      "Epoch [17/50], Train Loss: 0.0067, Val Loss: 0.0106\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=64, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0259, Val Loss: 0.1164\n",
      "Epoch [2/50], Train Loss: 0.0551, Val Loss: 0.0289\n",
      "Epoch [3/50], Train Loss: 0.0539, Val Loss: 0.0099\n",
      "Epoch [4/50], Train Loss: 0.0326, Val Loss: 0.0153\n",
      "Epoch [5/50], Train Loss: 0.0200, Val Loss: 0.0047\n",
      "Epoch [6/50], Train Loss: 0.0064, Val Loss: 0.0016\n",
      "Epoch [7/50], Train Loss: 0.0069, Val Loss: 0.0069\n",
      "Epoch [8/50], Train Loss: 0.0105, Val Loss: 0.0052\n",
      "Epoch [9/50], Train Loss: 0.0058, Val Loss: 0.0060\n",
      "Epoch [10/50], Train Loss: 0.0048, Val Loss: 0.0109\n",
      "Epoch [11/50], Train Loss: 0.0066, Val Loss: 0.0020\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=64, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0250, Val Loss: 0.0696\n",
      "Epoch [2/50], Train Loss: 0.0527, Val Loss: 0.0218\n",
      "Epoch [3/50], Train Loss: 0.0405, Val Loss: 0.0101\n",
      "Epoch [4/50], Train Loss: 0.0306, Val Loss: 0.0105\n",
      "Epoch [5/50], Train Loss: 0.0177, Val Loss: 0.0098\n",
      "Epoch [6/50], Train Loss: 0.0175, Val Loss: 0.0080\n",
      "Epoch [7/50], Train Loss: 0.0234, Val Loss: 0.0704\n",
      "Epoch [8/50], Train Loss: 0.0168, Val Loss: 0.0170\n",
      "Epoch [9/50], Train Loss: 0.0208, Val Loss: 0.0412\n",
      "Epoch [10/50], Train Loss: 0.0060, Val Loss: 0.0131\n",
      "Epoch [11/50], Train Loss: 0.0059, Val Loss: 0.0021\n",
      "Epoch [12/50], Train Loss: 0.0071, Val Loss: 0.0120\n",
      "Epoch [13/50], Train Loss: 0.0056, Val Loss: 0.0122\n",
      "Epoch [14/50], Train Loss: 0.0050, Val Loss: 0.0021\n",
      "Epoch [15/50], Train Loss: 0.0058, Val Loss: 0.0039\n",
      "Epoch [16/50], Train Loss: 0.0062, Val Loss: 0.0185\n",
      "Epoch [17/50], Train Loss: 0.0062, Val Loss: 0.0021\n",
      "Epoch [18/50], Train Loss: 0.0078, Val Loss: 0.0115\n",
      "Epoch [19/50], Train Loss: 0.0053, Val Loss: 0.0158\n",
      "Epoch [20/50], Train Loss: 0.0072, Val Loss: 0.0052\n",
      "Epoch [21/50], Train Loss: 0.0079, Val Loss: 0.0092\n",
      "Epoch [22/50], Train Loss: 0.0051, Val Loss: 0.0148\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=64, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0405, Val Loss: 0.1072\n",
      "Epoch [2/50], Train Loss: 0.0604, Val Loss: 0.0462\n",
      "Epoch [3/50], Train Loss: 0.0517, Val Loss: 0.0168\n",
      "Epoch [4/50], Train Loss: 0.0346, Val Loss: 0.0025\n",
      "Epoch [5/50], Train Loss: 0.0201, Val Loss: 0.0214\n",
      "Epoch [6/50], Train Loss: 0.0178, Val Loss: 0.0023\n",
      "Epoch [7/50], Train Loss: 0.0178, Val Loss: 0.0053\n",
      "Epoch [8/50], Train Loss: 0.0188, Val Loss: 0.0456\n",
      "Epoch [9/50], Train Loss: 0.0149, Val Loss: 0.0089\n",
      "Epoch [10/50], Train Loss: 0.0195, Val Loss: 0.0152\n",
      "Epoch [11/50], Train Loss: 0.0106, Val Loss: 0.0158\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=64, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0198, Val Loss: 0.1022\n",
      "Epoch [2/50], Train Loss: 0.0431, Val Loss: 0.1108\n",
      "Epoch [3/50], Train Loss: 0.0385, Val Loss: 0.0767\n",
      "Epoch [4/50], Train Loss: 0.0381, Val Loss: 0.0424\n",
      "Epoch [5/50], Train Loss: 0.0344, Val Loss: 0.0088\n",
      "Epoch [6/50], Train Loss: 0.0247, Val Loss: 0.0264\n",
      "Epoch [7/50], Train Loss: 0.0516, Val Loss: 0.0200\n",
      "Epoch [8/50], Train Loss: 0.0317, Val Loss: 0.0039\n",
      "Epoch [9/50], Train Loss: 0.0260, Val Loss: 0.0163\n",
      "Epoch [10/50], Train Loss: 0.0206, Val Loss: 0.0080\n",
      "Epoch [11/50], Train Loss: 0.0223, Val Loss: 0.0250\n",
      "Epoch [12/50], Train Loss: 0.0191, Val Loss: 0.0051\n",
      "Epoch [13/50], Train Loss: 0.0118, Val Loss: 0.0067\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=64, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0295, Val Loss: 0.0683\n",
      "Epoch [2/50], Train Loss: 0.0650, Val Loss: 0.1145\n",
      "Epoch [3/50], Train Loss: 0.0510, Val Loss: 0.0272\n",
      "Epoch [4/50], Train Loss: 0.0395, Val Loss: 0.0077\n",
      "Epoch [5/50], Train Loss: 0.0257, Val Loss: 0.0055\n",
      "Epoch [6/50], Train Loss: 0.0126, Val Loss: 0.0340\n",
      "Epoch [7/50], Train Loss: 0.0124, Val Loss: 0.0078\n",
      "Epoch [8/50], Train Loss: 0.0087, Val Loss: 0.0074\n",
      "Epoch [9/50], Train Loss: 0.0163, Val Loss: 0.0031\n",
      "Epoch [10/50], Train Loss: 0.0117, Val Loss: 0.0102\n",
      "Epoch [11/50], Train Loss: 0.0133, Val Loss: 0.0048\n",
      "Epoch [12/50], Train Loss: 0.0305, Val Loss: 0.0296\n",
      "Epoch [13/50], Train Loss: 0.0157, Val Loss: 0.0090\n",
      "Epoch [14/50], Train Loss: 0.0086, Val Loss: 0.0028\n",
      "Epoch [15/50], Train Loss: 0.0087, Val Loss: 0.0091\n",
      "Epoch [16/50], Train Loss: 0.0130, Val Loss: 0.0022\n",
      "Epoch [17/50], Train Loss: 0.0078, Val Loss: 0.0098\n",
      "Epoch [18/50], Train Loss: 0.0094, Val Loss: 0.0047\n",
      "Epoch [19/50], Train Loss: 0.0315, Val Loss: 0.0342\n",
      "Epoch [20/50], Train Loss: 0.0156, Val Loss: 0.0335\n",
      "Epoch [21/50], Train Loss: 0.0092, Val Loss: 0.0040\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=64, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0522, Val Loss: 0.0852\n",
      "Epoch [2/50], Train Loss: 0.0566, Val Loss: 0.0873\n",
      "Epoch [3/50], Train Loss: 0.0565, Val Loss: 0.0619\n",
      "Epoch [4/50], Train Loss: 0.0468, Val Loss: 0.0141\n",
      "Epoch [5/50], Train Loss: 0.0311, Val Loss: 0.0034\n",
      "Epoch [6/50], Train Loss: 0.0267, Val Loss: 0.0255\n",
      "Epoch [7/50], Train Loss: 0.0416, Val Loss: 0.0451\n",
      "Epoch [8/50], Train Loss: 0.0305, Val Loss: 0.0095\n",
      "Epoch [9/50], Train Loss: 0.0241, Val Loss: 0.0032\n",
      "Epoch [10/50], Train Loss: 0.0155, Val Loss: 0.0136\n",
      "Epoch [11/50], Train Loss: 0.0119, Val Loss: 0.0057\n",
      "Epoch [12/50], Train Loss: 0.0155, Val Loss: 0.0041\n",
      "Epoch [13/50], Train Loss: 0.0129, Val Loss: 0.0088\n",
      "Epoch [14/50], Train Loss: 0.0143, Val Loss: 0.0074\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=128, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0251, Val Loss: 0.0602\n",
      "Epoch [2/50], Train Loss: 0.0562, Val Loss: 0.0117\n",
      "Epoch [3/50], Train Loss: 0.0379, Val Loss: 0.0064\n",
      "Epoch [4/50], Train Loss: 0.0258, Val Loss: 0.0046\n",
      "Epoch [5/50], Train Loss: 0.0138, Val Loss: 0.0092\n",
      "Epoch [6/50], Train Loss: 0.0096, Val Loss: 0.0030\n",
      "Epoch [7/50], Train Loss: 0.0095, Val Loss: 0.0013\n",
      "Epoch [8/50], Train Loss: 0.0117, Val Loss: 0.0316\n",
      "Epoch [9/50], Train Loss: 0.0097, Val Loss: 0.0090\n",
      "Epoch [10/50], Train Loss: 0.0105, Val Loss: 0.0160\n",
      "Epoch [11/50], Train Loss: 0.0032, Val Loss: 0.0096\n",
      "Epoch [12/50], Train Loss: 0.0027, Val Loss: 0.0023\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=128, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0249, Val Loss: 0.0513\n",
      "Epoch [2/50], Train Loss: 0.0529, Val Loss: 0.0124\n",
      "Epoch [3/50], Train Loss: 0.0380, Val Loss: 0.0194\n",
      "Epoch [4/50], Train Loss: 0.0387, Val Loss: 0.0336\n",
      "Epoch [5/50], Train Loss: 0.0159, Val Loss: 0.0031\n",
      "Epoch [6/50], Train Loss: 0.0158, Val Loss: 0.0103\n",
      "Epoch [7/50], Train Loss: 0.0094, Val Loss: 0.0096\n",
      "Epoch [8/50], Train Loss: 0.0066, Val Loss: 0.0024\n",
      "Epoch [9/50], Train Loss: 0.0047, Val Loss: 0.0078\n",
      "Epoch [10/50], Train Loss: 0.0030, Val Loss: 0.0015\n",
      "Epoch [11/50], Train Loss: 0.0033, Val Loss: 0.0024\n",
      "Epoch [12/50], Train Loss: 0.0031, Val Loss: 0.0017\n",
      "Epoch [13/50], Train Loss: 0.0030, Val Loss: 0.0034\n",
      "Epoch [14/50], Train Loss: 0.0030, Val Loss: 0.0016\n",
      "Epoch [15/50], Train Loss: 0.0033, Val Loss: 0.0030\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=128, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0279, Val Loss: 0.0568\n",
      "Epoch [2/50], Train Loss: 0.0588, Val Loss: 0.0208\n",
      "Epoch [3/50], Train Loss: 0.0379, Val Loss: 0.0090\n",
      "Epoch [4/50], Train Loss: 0.0317, Val Loss: 0.0048\n",
      "Epoch [5/50], Train Loss: 0.0155, Val Loss: 0.0062\n",
      "Epoch [6/50], Train Loss: 0.0124, Val Loss: 0.0091\n",
      "Epoch [7/50], Train Loss: 0.0093, Val Loss: 0.0040\n",
      "Epoch [8/50], Train Loss: 0.0066, Val Loss: 0.0086\n",
      "Epoch [9/50], Train Loss: 0.0057, Val Loss: 0.0031\n",
      "Epoch [10/50], Train Loss: 0.0052, Val Loss: 0.0020\n",
      "Epoch [11/50], Train Loss: 0.0045, Val Loss: 0.0018\n",
      "Epoch [12/50], Train Loss: 0.0046, Val Loss: 0.0036\n",
      "Epoch [13/50], Train Loss: 0.0042, Val Loss: 0.0018\n",
      "Epoch [14/50], Train Loss: 0.0043, Val Loss: 0.0018\n",
      "Epoch [15/50], Train Loss: 0.0043, Val Loss: 0.0037\n",
      "Epoch [16/50], Train Loss: 0.0048, Val Loss: 0.0035\n",
      "Epoch [17/50], Train Loss: 0.0060, Val Loss: 0.0035\n",
      "Epoch [18/50], Train Loss: 0.0063, Val Loss: 0.0077\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=128, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0204, Val Loss: 0.0542\n",
      "Epoch [2/50], Train Loss: 0.0467, Val Loss: 0.0573\n",
      "Epoch [3/50], Train Loss: 0.0374, Val Loss: 0.0131\n",
      "Epoch [4/50], Train Loss: 0.0346, Val Loss: 0.0118\n",
      "Epoch [5/50], Train Loss: 0.0200, Val Loss: 0.0103\n",
      "Epoch [6/50], Train Loss: 0.0127, Val Loss: 0.0182\n",
      "Epoch [7/50], Train Loss: 0.0108, Val Loss: 0.0144\n",
      "Epoch [8/50], Train Loss: 0.0101, Val Loss: 0.0157\n",
      "Epoch [9/50], Train Loss: 0.0059, Val Loss: 0.0015\n",
      "Epoch [10/50], Train Loss: 0.0059, Val Loss: 0.0124\n",
      "Epoch [11/50], Train Loss: 0.0082, Val Loss: 0.0051\n",
      "Epoch [12/50], Train Loss: 0.0048, Val Loss: 0.0062\n",
      "Epoch [13/50], Train Loss: 0.0039, Val Loss: 0.0085\n",
      "Epoch [14/50], Train Loss: 0.0029, Val Loss: 0.0015\n",
      "Epoch [15/50], Train Loss: 0.0027, Val Loss: 0.0056\n",
      "Epoch [16/50], Train Loss: 0.0027, Val Loss: 0.0053\n",
      "Epoch [17/50], Train Loss: 0.0040, Val Loss: 0.0071\n",
      "Epoch [18/50], Train Loss: 0.0025, Val Loss: 0.0031\n",
      "Epoch [19/50], Train Loss: 0.0026, Val Loss: 0.0068\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=128, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0208, Val Loss: 0.0733\n",
      "Epoch [2/50], Train Loss: 0.0389, Val Loss: 0.0635\n",
      "Epoch [3/50], Train Loss: 0.0381, Val Loss: 0.0407\n",
      "Epoch [4/50], Train Loss: 0.0349, Val Loss: 0.0116\n",
      "Epoch [5/50], Train Loss: 0.0202, Val Loss: 0.0043\n",
      "Epoch [6/50], Train Loss: 0.0089, Val Loss: 0.0059\n",
      "Epoch [7/50], Train Loss: 0.0062, Val Loss: 0.0058\n",
      "Epoch [8/50], Train Loss: 0.0125, Val Loss: 0.0097\n",
      "Epoch [9/50], Train Loss: 0.0128, Val Loss: 0.0109\n",
      "Epoch [10/50], Train Loss: 0.0083, Val Loss: 0.0117\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=128, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0308, Val Loss: 0.0614\n",
      "Epoch [2/50], Train Loss: 0.0496, Val Loss: 0.0813\n",
      "Epoch [3/50], Train Loss: 0.0426, Val Loss: 0.0495\n",
      "Epoch [4/50], Train Loss: 0.0387, Val Loss: 0.0171\n",
      "Epoch [5/50], Train Loss: 0.0226, Val Loss: 0.0063\n",
      "Epoch [6/50], Train Loss: 0.0148, Val Loss: 0.0057\n",
      "Epoch [7/50], Train Loss: 0.0106, Val Loss: 0.0058\n",
      "Epoch [8/50], Train Loss: 0.0096, Val Loss: 0.0075\n",
      "Epoch [9/50], Train Loss: 0.0083, Val Loss: 0.0059\n",
      "Epoch [10/50], Train Loss: 0.0083, Val Loss: 0.0106\n",
      "Epoch [11/50], Train Loss: 0.0093, Val Loss: 0.0063\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=128, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0216, Val Loss: 0.0829\n",
      "Epoch [2/50], Train Loss: 0.0451, Val Loss: 0.0760\n",
      "Epoch [3/50], Train Loss: 0.0495, Val Loss: 0.0657\n",
      "Epoch [4/50], Train Loss: 0.0441, Val Loss: 0.0283\n",
      "Epoch [5/50], Train Loss: 0.0394, Val Loss: 0.0071\n",
      "Epoch [6/50], Train Loss: 0.0267, Val Loss: 0.0090\n",
      "Epoch [7/50], Train Loss: 0.0180, Val Loss: 0.0066\n",
      "Epoch [8/50], Train Loss: 0.0137, Val Loss: 0.0132\n",
      "Epoch [9/50], Train Loss: 0.0085, Val Loss: 0.0095\n",
      "Epoch [10/50], Train Loss: 0.0071, Val Loss: 0.0251\n",
      "Epoch [11/50], Train Loss: 0.0090, Val Loss: 0.0069\n",
      "Epoch [12/50], Train Loss: 0.0142, Val Loss: 0.0234\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=128, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0237, Val Loss: 0.0514\n",
      "Epoch [2/50], Train Loss: 0.0423, Val Loss: 0.0612\n",
      "Epoch [3/50], Train Loss: 0.0439, Val Loss: 0.0346\n",
      "Epoch [4/50], Train Loss: 0.0335, Val Loss: 0.0141\n",
      "Epoch [5/50], Train Loss: 0.0410, Val Loss: 0.0140\n",
      "Epoch [6/50], Train Loss: 0.0276, Val Loss: 0.0294\n",
      "Epoch [7/50], Train Loss: 0.0294, Val Loss: 0.0122\n",
      "Epoch [8/50], Train Loss: 0.0145, Val Loss: 0.0152\n",
      "Epoch [9/50], Train Loss: 0.0093, Val Loss: 0.0035\n",
      "Epoch [10/50], Train Loss: 0.0102, Val Loss: 0.0061\n",
      "Epoch [11/50], Train Loss: 0.0108, Val Loss: 0.0124\n",
      "Epoch [12/50], Train Loss: 0.0311, Val Loss: 0.0360\n",
      "Epoch [13/50], Train Loss: 0.0200, Val Loss: 0.0038\n",
      "Epoch [14/50], Train Loss: 0.0190, Val Loss: 0.0138\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=128, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0338, Val Loss: 0.0460\n",
      "Epoch [2/50], Train Loss: 0.0568, Val Loss: 0.0581\n",
      "Epoch [3/50], Train Loss: 0.0511, Val Loss: 0.0336\n",
      "Epoch [4/50], Train Loss: 0.0402, Val Loss: 0.0102\n",
      "Epoch [5/50], Train Loss: 0.0262, Val Loss: 0.0164\n",
      "Epoch [6/50], Train Loss: 0.0149, Val Loss: 0.0111\n",
      "Epoch [7/50], Train Loss: 0.0220, Val Loss: 0.0099\n",
      "Epoch [8/50], Train Loss: 0.0102, Val Loss: 0.0013\n",
      "Epoch [9/50], Train Loss: 0.0115, Val Loss: 0.0214\n",
      "Epoch [10/50], Train Loss: 0.0151, Val Loss: 0.0111\n",
      "Epoch [11/50], Train Loss: 0.0135, Val Loss: 0.0049\n",
      "Epoch [12/50], Train Loss: 0.0186, Val Loss: 0.0038\n",
      "Epoch [13/50], Train Loss: 0.0284, Val Loss: 0.0054\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=16, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.2139, Val Loss: 0.5114\n",
      "Epoch [2/50], Train Loss: 0.2017, Val Loss: 0.4922\n",
      "Epoch [3/50], Train Loss: 0.1906, Val Loss: 0.4745\n",
      "Epoch [4/50], Train Loss: 0.1804, Val Loss: 0.4581\n",
      "Epoch [5/50], Train Loss: 0.1710, Val Loss: 0.4427\n",
      "Epoch [6/50], Train Loss: 0.1623, Val Loss: 0.4283\n",
      "Epoch [7/50], Train Loss: 0.1543, Val Loss: 0.4147\n",
      "Epoch [8/50], Train Loss: 0.1468, Val Loss: 0.4019\n",
      "Epoch [9/50], Train Loss: 0.1399, Val Loss: 0.3898\n",
      "Epoch [10/50], Train Loss: 0.1334, Val Loss: 0.3783\n",
      "Epoch [11/50], Train Loss: 0.1273, Val Loss: 0.3674\n",
      "Epoch [12/50], Train Loss: 0.1216, Val Loss: 0.3571\n",
      "Epoch [13/50], Train Loss: 0.1163, Val Loss: 0.3471\n",
      "Epoch [14/50], Train Loss: 0.1112, Val Loss: 0.3377\n",
      "Epoch [15/50], Train Loss: 0.1065, Val Loss: 0.3286\n",
      "Epoch [16/50], Train Loss: 0.1021, Val Loss: 0.3199\n",
      "Epoch [17/50], Train Loss: 0.0979, Val Loss: 0.3116\n",
      "Epoch [18/50], Train Loss: 0.0939, Val Loss: 0.3036\n",
      "Epoch [19/50], Train Loss: 0.0902, Val Loss: 0.2959\n",
      "Epoch [20/50], Train Loss: 0.0866, Val Loss: 0.2886\n",
      "Epoch [21/50], Train Loss: 0.0833, Val Loss: 0.2815\n",
      "Epoch [22/50], Train Loss: 0.0801, Val Loss: 0.2746\n",
      "Epoch [23/50], Train Loss: 0.0772, Val Loss: 0.2681\n",
      "Epoch [24/50], Train Loss: 0.0744, Val Loss: 0.2617\n",
      "Epoch [25/50], Train Loss: 0.0717, Val Loss: 0.2556\n",
      "Epoch [26/50], Train Loss: 0.0692, Val Loss: 0.2498\n",
      "Epoch [27/50], Train Loss: 0.0668, Val Loss: 0.2441\n",
      "Epoch [28/50], Train Loss: 0.0646, Val Loss: 0.2387\n",
      "Epoch [29/50], Train Loss: 0.0625, Val Loss: 0.2334\n",
      "Epoch [30/50], Train Loss: 0.0605, Val Loss: 0.2284\n",
      "Epoch [31/50], Train Loss: 0.0587, Val Loss: 0.2235\n",
      "Epoch [32/50], Train Loss: 0.0569, Val Loss: 0.2188\n",
      "Epoch [33/50], Train Loss: 0.0553, Val Loss: 0.2143\n",
      "Epoch [34/50], Train Loss: 0.0537, Val Loss: 0.2100\n",
      "Epoch [35/50], Train Loss: 0.0522, Val Loss: 0.2058\n",
      "Epoch [36/50], Train Loss: 0.0509, Val Loss: 0.2018\n",
      "Epoch [37/50], Train Loss: 0.0496, Val Loss: 0.1979\n",
      "Epoch [38/50], Train Loss: 0.0484, Val Loss: 0.1942\n",
      "Epoch [39/50], Train Loss: 0.0472, Val Loss: 0.1906\n",
      "Epoch [40/50], Train Loss: 0.0461, Val Loss: 0.1871\n",
      "Epoch [41/50], Train Loss: 0.0451, Val Loss: 0.1838\n",
      "Epoch [42/50], Train Loss: 0.0442, Val Loss: 0.1806\n",
      "Epoch [43/50], Train Loss: 0.0433, Val Loss: 0.1776\n",
      "Epoch [44/50], Train Loss: 0.0425, Val Loss: 0.1746\n",
      "Epoch [45/50], Train Loss: 0.0417, Val Loss: 0.1718\n",
      "Epoch [46/50], Train Loss: 0.0410, Val Loss: 0.1690\n",
      "Epoch [47/50], Train Loss: 0.0403, Val Loss: 0.1664\n",
      "Epoch [48/50], Train Loss: 0.0397, Val Loss: 0.1639\n",
      "Epoch [49/50], Train Loss: 0.0391, Val Loss: 0.1615\n",
      "Epoch [50/50], Train Loss: 0.0386, Val Loss: 0.1592\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=16, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1129, Val Loss: 0.3105\n",
      "Epoch [2/50], Train Loss: 0.1069, Val Loss: 0.2972\n",
      "Epoch [3/50], Train Loss: 0.1015, Val Loss: 0.2844\n",
      "Epoch [4/50], Train Loss: 0.0975, Val Loss: 0.2723\n",
      "Epoch [5/50], Train Loss: 0.0918, Val Loss: 0.2610\n",
      "Epoch [6/50], Train Loss: 0.0878, Val Loss: 0.2502\n",
      "Epoch [7/50], Train Loss: 0.0821, Val Loss: 0.2401\n",
      "Epoch [8/50], Train Loss: 0.0815, Val Loss: 0.2306\n",
      "Epoch [9/50], Train Loss: 0.0765, Val Loss: 0.2216\n",
      "Epoch [10/50], Train Loss: 0.0731, Val Loss: 0.2129\n",
      "Epoch [11/50], Train Loss: 0.0696, Val Loss: 0.2048\n",
      "Epoch [12/50], Train Loss: 0.0650, Val Loss: 0.1975\n",
      "Epoch [13/50], Train Loss: 0.0637, Val Loss: 0.1902\n",
      "Epoch [14/50], Train Loss: 0.0623, Val Loss: 0.1835\n",
      "Epoch [15/50], Train Loss: 0.0601, Val Loss: 0.1772\n",
      "Epoch [16/50], Train Loss: 0.0559, Val Loss: 0.1713\n",
      "Epoch [17/50], Train Loss: 0.0558, Val Loss: 0.1658\n",
      "Epoch [18/50], Train Loss: 0.0534, Val Loss: 0.1605\n",
      "Epoch [19/50], Train Loss: 0.0525, Val Loss: 0.1557\n",
      "Epoch [20/50], Train Loss: 0.0510, Val Loss: 0.1511\n",
      "Epoch [21/50], Train Loss: 0.0504, Val Loss: 0.1467\n",
      "Epoch [22/50], Train Loss: 0.0490, Val Loss: 0.1427\n",
      "Epoch [23/50], Train Loss: 0.0484, Val Loss: 0.1389\n",
      "Epoch [24/50], Train Loss: 0.0464, Val Loss: 0.1353\n",
      "Epoch [25/50], Train Loss: 0.0457, Val Loss: 0.1320\n",
      "Epoch [26/50], Train Loss: 0.0456, Val Loss: 0.1288\n",
      "Epoch [27/50], Train Loss: 0.0434, Val Loss: 0.1258\n",
      "Epoch [28/50], Train Loss: 0.0445, Val Loss: 0.1228\n",
      "Epoch [29/50], Train Loss: 0.0436, Val Loss: 0.1201\n",
      "Epoch [30/50], Train Loss: 0.0424, Val Loss: 0.1176\n",
      "Epoch [31/50], Train Loss: 0.0433, Val Loss: 0.1152\n",
      "Epoch [32/50], Train Loss: 0.0425, Val Loss: 0.1130\n",
      "Epoch [33/50], Train Loss: 0.0412, Val Loss: 0.1108\n",
      "Epoch [34/50], Train Loss: 0.0396, Val Loss: 0.1089\n",
      "Epoch [35/50], Train Loss: 0.0400, Val Loss: 0.1070\n",
      "Epoch [36/50], Train Loss: 0.0383, Val Loss: 0.1054\n",
      "Epoch [37/50], Train Loss: 0.0388, Val Loss: 0.1038\n",
      "Epoch [38/50], Train Loss: 0.0382, Val Loss: 0.1023\n",
      "Epoch [39/50], Train Loss: 0.0372, Val Loss: 0.1008\n",
      "Epoch [40/50], Train Loss: 0.0371, Val Loss: 0.0996\n",
      "Epoch [41/50], Train Loss: 0.0397, Val Loss: 0.0982\n",
      "Epoch [42/50], Train Loss: 0.0373, Val Loss: 0.0971\n",
      "Epoch [43/50], Train Loss: 0.0374, Val Loss: 0.0960\n",
      "Epoch [44/50], Train Loss: 0.0362, Val Loss: 0.0949\n",
      "Epoch [45/50], Train Loss: 0.0373, Val Loss: 0.0939\n",
      "Epoch [46/50], Train Loss: 0.0363, Val Loss: 0.0929\n",
      "Epoch [47/50], Train Loss: 0.0369, Val Loss: 0.0920\n",
      "Epoch [48/50], Train Loss: 0.0378, Val Loss: 0.0910\n",
      "Epoch [49/50], Train Loss: 0.0359, Val Loss: 0.0902\n",
      "Epoch [50/50], Train Loss: 0.0374, Val Loss: 0.0892\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=16, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1446, Val Loss: 0.3172\n",
      "Epoch [2/50], Train Loss: 0.1339, Val Loss: 0.3023\n",
      "Epoch [3/50], Train Loss: 0.1219, Val Loss: 0.2891\n",
      "Epoch [4/50], Train Loss: 0.1183, Val Loss: 0.2772\n",
      "Epoch [5/50], Train Loss: 0.1123, Val Loss: 0.2662\n",
      "Epoch [6/50], Train Loss: 0.1060, Val Loss: 0.2562\n",
      "Epoch [7/50], Train Loss: 0.1005, Val Loss: 0.2474\n",
      "Epoch [8/50], Train Loss: 0.0982, Val Loss: 0.2390\n",
      "Epoch [9/50], Train Loss: 0.0944, Val Loss: 0.2312\n",
      "Epoch [10/50], Train Loss: 0.0888, Val Loss: 0.2242\n",
      "Epoch [11/50], Train Loss: 0.0880, Val Loss: 0.2174\n",
      "Epoch [12/50], Train Loss: 0.0837, Val Loss: 0.2111\n",
      "Epoch [13/50], Train Loss: 0.0831, Val Loss: 0.2053\n",
      "Epoch [14/50], Train Loss: 0.0776, Val Loss: 0.1999\n",
      "Epoch [15/50], Train Loss: 0.0757, Val Loss: 0.1951\n",
      "Epoch [16/50], Train Loss: 0.0737, Val Loss: 0.1904\n",
      "Epoch [17/50], Train Loss: 0.0720, Val Loss: 0.1860\n",
      "Epoch [18/50], Train Loss: 0.0712, Val Loss: 0.1817\n",
      "Epoch [19/50], Train Loss: 0.0698, Val Loss: 0.1779\n",
      "Epoch [20/50], Train Loss: 0.0692, Val Loss: 0.1742\n",
      "Epoch [21/50], Train Loss: 0.0667, Val Loss: 0.1706\n",
      "Epoch [22/50], Train Loss: 0.0651, Val Loss: 0.1673\n",
      "Epoch [23/50], Train Loss: 0.0641, Val Loss: 0.1641\n",
      "Epoch [24/50], Train Loss: 0.0625, Val Loss: 0.1611\n",
      "Epoch [25/50], Train Loss: 0.0598, Val Loss: 0.1585\n",
      "Epoch [26/50], Train Loss: 0.0599, Val Loss: 0.1559\n",
      "Epoch [27/50], Train Loss: 0.0594, Val Loss: 0.1532\n",
      "Epoch [28/50], Train Loss: 0.0566, Val Loss: 0.1507\n",
      "Epoch [29/50], Train Loss: 0.0577, Val Loss: 0.1483\n",
      "Epoch [30/50], Train Loss: 0.0578, Val Loss: 0.1460\n",
      "Epoch [31/50], Train Loss: 0.0556, Val Loss: 0.1439\n",
      "Epoch [32/50], Train Loss: 0.0552, Val Loss: 0.1419\n",
      "Epoch [33/50], Train Loss: 0.0552, Val Loss: 0.1398\n",
      "Epoch [34/50], Train Loss: 0.0538, Val Loss: 0.1379\n",
      "Epoch [35/50], Train Loss: 0.0528, Val Loss: 0.1360\n",
      "Epoch [36/50], Train Loss: 0.0513, Val Loss: 0.1344\n",
      "Epoch [37/50], Train Loss: 0.0511, Val Loss: 0.1328\n",
      "Epoch [38/50], Train Loss: 0.0515, Val Loss: 0.1313\n",
      "Epoch [39/50], Train Loss: 0.0498, Val Loss: 0.1299\n",
      "Epoch [40/50], Train Loss: 0.0506, Val Loss: 0.1284\n",
      "Epoch [41/50], Train Loss: 0.0515, Val Loss: 0.1269\n",
      "Epoch [42/50], Train Loss: 0.0501, Val Loss: 0.1254\n",
      "Epoch [43/50], Train Loss: 0.0484, Val Loss: 0.1241\n",
      "Epoch [44/50], Train Loss: 0.0489, Val Loss: 0.1227\n",
      "Epoch [45/50], Train Loss: 0.0470, Val Loss: 0.1216\n",
      "Epoch [46/50], Train Loss: 0.0484, Val Loss: 0.1205\n",
      "Epoch [47/50], Train Loss: 0.0471, Val Loss: 0.1194\n",
      "Epoch [48/50], Train Loss: 0.0453, Val Loss: 0.1183\n",
      "Epoch [49/50], Train Loss: 0.0467, Val Loss: 0.1174\n",
      "Epoch [50/50], Train Loss: 0.0444, Val Loss: 0.1162\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=16, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1058, Val Loss: 0.2960\n",
      "Epoch [2/50], Train Loss: 0.1004, Val Loss: 0.2877\n",
      "Epoch [3/50], Train Loss: 0.0954, Val Loss: 0.2798\n",
      "Epoch [4/50], Train Loss: 0.0908, Val Loss: 0.2723\n",
      "Epoch [5/50], Train Loss: 0.0865, Val Loss: 0.2651\n",
      "Epoch [6/50], Train Loss: 0.0826, Val Loss: 0.2583\n",
      "Epoch [7/50], Train Loss: 0.0789, Val Loss: 0.2517\n",
      "Epoch [8/50], Train Loss: 0.0755, Val Loss: 0.2455\n",
      "Epoch [9/50], Train Loss: 0.0724, Val Loss: 0.2396\n",
      "Epoch [10/50], Train Loss: 0.0695, Val Loss: 0.2340\n",
      "Epoch [11/50], Train Loss: 0.0669, Val Loss: 0.2286\n",
      "Epoch [12/50], Train Loss: 0.0644, Val Loss: 0.2235\n",
      "Epoch [13/50], Train Loss: 0.0622, Val Loss: 0.2186\n",
      "Epoch [14/50], Train Loss: 0.0601, Val Loss: 0.2140\n",
      "Epoch [15/50], Train Loss: 0.0582, Val Loss: 0.2096\n",
      "Epoch [16/50], Train Loss: 0.0565, Val Loss: 0.2054\n",
      "Epoch [17/50], Train Loss: 0.0548, Val Loss: 0.2014\n",
      "Epoch [18/50], Train Loss: 0.0534, Val Loss: 0.1977\n",
      "Epoch [19/50], Train Loss: 0.0520, Val Loss: 0.1941\n",
      "Epoch [20/50], Train Loss: 0.0508, Val Loss: 0.1906\n",
      "Epoch [21/50], Train Loss: 0.0497, Val Loss: 0.1874\n",
      "Epoch [22/50], Train Loss: 0.0487, Val Loss: 0.1843\n",
      "Epoch [23/50], Train Loss: 0.0477, Val Loss: 0.1813\n",
      "Epoch [24/50], Train Loss: 0.0469, Val Loss: 0.1785\n",
      "Epoch [25/50], Train Loss: 0.0461, Val Loss: 0.1759\n",
      "Epoch [26/50], Train Loss: 0.0454, Val Loss: 0.1734\n",
      "Epoch [27/50], Train Loss: 0.0447, Val Loss: 0.1710\n",
      "Epoch [28/50], Train Loss: 0.0441, Val Loss: 0.1687\n",
      "Epoch [29/50], Train Loss: 0.0436, Val Loss: 0.1665\n",
      "Epoch [30/50], Train Loss: 0.0431, Val Loss: 0.1645\n",
      "Epoch [31/50], Train Loss: 0.0426, Val Loss: 0.1625\n",
      "Epoch [32/50], Train Loss: 0.0422, Val Loss: 0.1607\n",
      "Epoch [33/50], Train Loss: 0.0418, Val Loss: 0.1589\n",
      "Epoch [34/50], Train Loss: 0.0415, Val Loss: 0.1572\n",
      "Epoch [35/50], Train Loss: 0.0412, Val Loss: 0.1556\n",
      "Epoch [36/50], Train Loss: 0.0409, Val Loss: 0.1541\n",
      "Epoch [37/50], Train Loss: 0.0406, Val Loss: 0.1527\n",
      "Epoch [38/50], Train Loss: 0.0404, Val Loss: 0.1513\n",
      "Epoch [39/50], Train Loss: 0.0402, Val Loss: 0.1500\n",
      "Epoch [40/50], Train Loss: 0.0400, Val Loss: 0.1488\n",
      "Epoch [41/50], Train Loss: 0.0398, Val Loss: 0.1476\n",
      "Epoch [42/50], Train Loss: 0.0396, Val Loss: 0.1465\n",
      "Epoch [43/50], Train Loss: 0.0395, Val Loss: 0.1454\n",
      "Epoch [44/50], Train Loss: 0.0393, Val Loss: 0.1444\n",
      "Epoch [45/50], Train Loss: 0.0392, Val Loss: 0.1435\n",
      "Epoch [46/50], Train Loss: 0.0390, Val Loss: 0.1425\n",
      "Epoch [47/50], Train Loss: 0.0389, Val Loss: 0.1417\n",
      "Epoch [48/50], Train Loss: 0.0388, Val Loss: 0.1408\n",
      "Epoch [49/50], Train Loss: 0.0387, Val Loss: 0.1400\n",
      "Epoch [50/50], Train Loss: 0.0386, Val Loss: 0.1393\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=16, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.2053, Val Loss: 0.4383\n",
      "Epoch [2/50], Train Loss: 0.1932, Val Loss: 0.4104\n",
      "Epoch [3/50], Train Loss: 0.1772, Val Loss: 0.3852\n",
      "Epoch [4/50], Train Loss: 0.1611, Val Loss: 0.3622\n",
      "Epoch [5/50], Train Loss: 0.1521, Val Loss: 0.3410\n",
      "Epoch [6/50], Train Loss: 0.1401, Val Loss: 0.3215\n",
      "Epoch [7/50], Train Loss: 0.1292, Val Loss: 0.3037\n",
      "Epoch [8/50], Train Loss: 0.1229, Val Loss: 0.2874\n",
      "Epoch [9/50], Train Loss: 0.1147, Val Loss: 0.2722\n",
      "Epoch [10/50], Train Loss: 0.1082, Val Loss: 0.2583\n",
      "Epoch [11/50], Train Loss: 0.1028, Val Loss: 0.2456\n",
      "Epoch [12/50], Train Loss: 0.0961, Val Loss: 0.2339\n",
      "Epoch [13/50], Train Loss: 0.0916, Val Loss: 0.2231\n",
      "Epoch [14/50], Train Loss: 0.0871, Val Loss: 0.2134\n",
      "Epoch [15/50], Train Loss: 0.0848, Val Loss: 0.2042\n",
      "Epoch [16/50], Train Loss: 0.0803, Val Loss: 0.1956\n",
      "Epoch [17/50], Train Loss: 0.0765, Val Loss: 0.1876\n",
      "Epoch [18/50], Train Loss: 0.0729, Val Loss: 0.1805\n",
      "Epoch [19/50], Train Loss: 0.0718, Val Loss: 0.1739\n",
      "Epoch [20/50], Train Loss: 0.0700, Val Loss: 0.1677\n",
      "Epoch [21/50], Train Loss: 0.0670, Val Loss: 0.1622\n",
      "Epoch [22/50], Train Loss: 0.0655, Val Loss: 0.1570\n",
      "Epoch [23/50], Train Loss: 0.0622, Val Loss: 0.1522\n",
      "Epoch [24/50], Train Loss: 0.0632, Val Loss: 0.1480\n",
      "Epoch [25/50], Train Loss: 0.0606, Val Loss: 0.1440\n",
      "Epoch [26/50], Train Loss: 0.0623, Val Loss: 0.1402\n",
      "Epoch [27/50], Train Loss: 0.0591, Val Loss: 0.1367\n",
      "Epoch [28/50], Train Loss: 0.0602, Val Loss: 0.1336\n",
      "Epoch [29/50], Train Loss: 0.0583, Val Loss: 0.1310\n",
      "Epoch [30/50], Train Loss: 0.0570, Val Loss: 0.1281\n",
      "Epoch [31/50], Train Loss: 0.0570, Val Loss: 0.1257\n",
      "Epoch [32/50], Train Loss: 0.0545, Val Loss: 0.1233\n",
      "Epoch [33/50], Train Loss: 0.0558, Val Loss: 0.1213\n",
      "Epoch [34/50], Train Loss: 0.0556, Val Loss: 0.1193\n",
      "Epoch [35/50], Train Loss: 0.0543, Val Loss: 0.1175\n",
      "Epoch [36/50], Train Loss: 0.0551, Val Loss: 0.1156\n",
      "Epoch [37/50], Train Loss: 0.0538, Val Loss: 0.1141\n",
      "Epoch [38/50], Train Loss: 0.0533, Val Loss: 0.1127\n",
      "Epoch [39/50], Train Loss: 0.0532, Val Loss: 0.1112\n",
      "Epoch [40/50], Train Loss: 0.0513, Val Loss: 0.1101\n",
      "Epoch [41/50], Train Loss: 0.0541, Val Loss: 0.1090\n",
      "Epoch [42/50], Train Loss: 0.0520, Val Loss: 0.1079\n",
      "Epoch [43/50], Train Loss: 0.0514, Val Loss: 0.1069\n",
      "Epoch [44/50], Train Loss: 0.0516, Val Loss: 0.1061\n",
      "Epoch [45/50], Train Loss: 0.0519, Val Loss: 0.1052\n",
      "Epoch [46/50], Train Loss: 0.0504, Val Loss: 0.1045\n",
      "Epoch [47/50], Train Loss: 0.0514, Val Loss: 0.1038\n",
      "Epoch [48/50], Train Loss: 0.0505, Val Loss: 0.1031\n",
      "Epoch [49/50], Train Loss: 0.0500, Val Loss: 0.1023\n",
      "Epoch [50/50], Train Loss: 0.0510, Val Loss: 0.1017\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=16, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.2013, Val Loss: 0.4689\n",
      "Epoch [2/50], Train Loss: 0.1919, Val Loss: 0.4533\n",
      "Epoch [3/50], Train Loss: 0.1854, Val Loss: 0.4387\n",
      "Epoch [4/50], Train Loss: 0.1744, Val Loss: 0.4254\n",
      "Epoch [5/50], Train Loss: 0.1690, Val Loss: 0.4125\n",
      "Epoch [6/50], Train Loss: 0.1640, Val Loss: 0.4004\n",
      "Epoch [7/50], Train Loss: 0.1585, Val Loss: 0.3887\n",
      "Epoch [8/50], Train Loss: 0.1538, Val Loss: 0.3779\n",
      "Epoch [9/50], Train Loss: 0.1473, Val Loss: 0.3677\n",
      "Epoch [10/50], Train Loss: 0.1396, Val Loss: 0.3578\n",
      "Epoch [11/50], Train Loss: 0.1385, Val Loss: 0.3484\n",
      "Epoch [12/50], Train Loss: 0.1353, Val Loss: 0.3397\n",
      "Epoch [13/50], Train Loss: 0.1289, Val Loss: 0.3311\n",
      "Epoch [14/50], Train Loss: 0.1254, Val Loss: 0.3230\n",
      "Epoch [15/50], Train Loss: 0.1258, Val Loss: 0.3154\n",
      "Epoch [16/50], Train Loss: 0.1212, Val Loss: 0.3080\n",
      "Epoch [17/50], Train Loss: 0.1176, Val Loss: 0.3010\n",
      "Epoch [18/50], Train Loss: 0.1169, Val Loss: 0.2944\n",
      "Epoch [19/50], Train Loss: 0.1129, Val Loss: 0.2879\n",
      "Epoch [20/50], Train Loss: 0.1094, Val Loss: 0.2817\n",
      "Epoch [21/50], Train Loss: 0.1076, Val Loss: 0.2759\n",
      "Epoch [22/50], Train Loss: 0.1070, Val Loss: 0.2704\n",
      "Epoch [23/50], Train Loss: 0.1052, Val Loss: 0.2651\n",
      "Epoch [24/50], Train Loss: 0.1011, Val Loss: 0.2599\n",
      "Epoch [25/50], Train Loss: 0.0977, Val Loss: 0.2549\n",
      "Epoch [26/50], Train Loss: 0.0957, Val Loss: 0.2500\n",
      "Epoch [27/50], Train Loss: 0.0987, Val Loss: 0.2454\n",
      "Epoch [28/50], Train Loss: 0.0948, Val Loss: 0.2413\n",
      "Epoch [29/50], Train Loss: 0.0927, Val Loss: 0.2373\n",
      "Epoch [30/50], Train Loss: 0.0920, Val Loss: 0.2333\n",
      "Epoch [31/50], Train Loss: 0.0929, Val Loss: 0.2295\n",
      "Epoch [32/50], Train Loss: 0.0886, Val Loss: 0.2257\n",
      "Epoch [33/50], Train Loss: 0.0895, Val Loss: 0.2222\n",
      "Epoch [34/50], Train Loss: 0.0874, Val Loss: 0.2188\n",
      "Epoch [35/50], Train Loss: 0.0871, Val Loss: 0.2156\n",
      "Epoch [36/50], Train Loss: 0.0866, Val Loss: 0.2124\n",
      "Epoch [37/50], Train Loss: 0.0826, Val Loss: 0.2094\n",
      "Epoch [38/50], Train Loss: 0.0857, Val Loss: 0.2066\n",
      "Epoch [39/50], Train Loss: 0.0859, Val Loss: 0.2039\n",
      "Epoch [40/50], Train Loss: 0.0814, Val Loss: 0.2014\n",
      "Epoch [41/50], Train Loss: 0.0823, Val Loss: 0.1988\n",
      "Epoch [42/50], Train Loss: 0.0802, Val Loss: 0.1962\n",
      "Epoch [43/50], Train Loss: 0.0789, Val Loss: 0.1940\n",
      "Epoch [44/50], Train Loss: 0.0800, Val Loss: 0.1916\n",
      "Epoch [45/50], Train Loss: 0.0787, Val Loss: 0.1895\n",
      "Epoch [46/50], Train Loss: 0.0784, Val Loss: 0.1874\n",
      "Epoch [47/50], Train Loss: 0.0780, Val Loss: 0.1853\n",
      "Epoch [48/50], Train Loss: 0.0768, Val Loss: 0.1833\n",
      "Epoch [49/50], Train Loss: 0.0764, Val Loss: 0.1815\n",
      "Epoch [50/50], Train Loss: 0.0740, Val Loss: 0.1798\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=16, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.2481, Val Loss: 0.5117\n",
      "Epoch [2/50], Train Loss: 0.2285, Val Loss: 0.4831\n",
      "Epoch [3/50], Train Loss: 0.2111, Val Loss: 0.4570\n",
      "Epoch [4/50], Train Loss: 0.1954, Val Loss: 0.4331\n",
      "Epoch [5/50], Train Loss: 0.1814, Val Loss: 0.4111\n",
      "Epoch [6/50], Train Loss: 0.1686, Val Loss: 0.3908\n",
      "Epoch [7/50], Train Loss: 0.1571, Val Loss: 0.3720\n",
      "Epoch [8/50], Train Loss: 0.1465, Val Loss: 0.3544\n",
      "Epoch [9/50], Train Loss: 0.1368, Val Loss: 0.3380\n",
      "Epoch [10/50], Train Loss: 0.1280, Val Loss: 0.3226\n",
      "Epoch [11/50], Train Loss: 0.1198, Val Loss: 0.3082\n",
      "Epoch [12/50], Train Loss: 0.1123, Val Loss: 0.2945\n",
      "Epoch [13/50], Train Loss: 0.1054, Val Loss: 0.2818\n",
      "Epoch [14/50], Train Loss: 0.0991, Val Loss: 0.2698\n",
      "Epoch [15/50], Train Loss: 0.0932, Val Loss: 0.2584\n",
      "Epoch [16/50], Train Loss: 0.0879, Val Loss: 0.2478\n",
      "Epoch [17/50], Train Loss: 0.0829, Val Loss: 0.2377\n",
      "Epoch [18/50], Train Loss: 0.0784, Val Loss: 0.2283\n",
      "Epoch [19/50], Train Loss: 0.0743, Val Loss: 0.2194\n",
      "Epoch [20/50], Train Loss: 0.0705, Val Loss: 0.2111\n",
      "Epoch [21/50], Train Loss: 0.0670, Val Loss: 0.2033\n",
      "Epoch [22/50], Train Loss: 0.0638, Val Loss: 0.1959\n",
      "Epoch [23/50], Train Loss: 0.0610, Val Loss: 0.1891\n",
      "Epoch [24/50], Train Loss: 0.0584, Val Loss: 0.1826\n",
      "Epoch [25/50], Train Loss: 0.0560, Val Loss: 0.1766\n",
      "Epoch [26/50], Train Loss: 0.0539, Val Loss: 0.1710\n",
      "Epoch [27/50], Train Loss: 0.0519, Val Loss: 0.1658\n",
      "Epoch [28/50], Train Loss: 0.0502, Val Loss: 0.1609\n",
      "Epoch [29/50], Train Loss: 0.0486, Val Loss: 0.1564\n",
      "Epoch [30/50], Train Loss: 0.0472, Val Loss: 0.1522\n",
      "Epoch [31/50], Train Loss: 0.0460, Val Loss: 0.1483\n",
      "Epoch [32/50], Train Loss: 0.0449, Val Loss: 0.1446\n",
      "Epoch [33/50], Train Loss: 0.0439, Val Loss: 0.1413\n",
      "Epoch [34/50], Train Loss: 0.0430, Val Loss: 0.1381\n",
      "Epoch [35/50], Train Loss: 0.0422, Val Loss: 0.1352\n",
      "Epoch [36/50], Train Loss: 0.0415, Val Loss: 0.1325\n",
      "Epoch [37/50], Train Loss: 0.0408, Val Loss: 0.1301\n",
      "Epoch [38/50], Train Loss: 0.0403, Val Loss: 0.1278\n",
      "Epoch [39/50], Train Loss: 0.0398, Val Loss: 0.1257\n",
      "Epoch [40/50], Train Loss: 0.0393, Val Loss: 0.1237\n",
      "Epoch [41/50], Train Loss: 0.0389, Val Loss: 0.1219\n",
      "Epoch [42/50], Train Loss: 0.0386, Val Loss: 0.1202\n",
      "Epoch [43/50], Train Loss: 0.0383, Val Loss: 0.1187\n",
      "Epoch [44/50], Train Loss: 0.0380, Val Loss: 0.1172\n",
      "Epoch [45/50], Train Loss: 0.0377, Val Loss: 0.1159\n",
      "Epoch [46/50], Train Loss: 0.0375, Val Loss: 0.1147\n",
      "Epoch [47/50], Train Loss: 0.0373, Val Loss: 0.1135\n",
      "Epoch [48/50], Train Loss: 0.0371, Val Loss: 0.1125\n",
      "Epoch [49/50], Train Loss: 0.0370, Val Loss: 0.1116\n",
      "Epoch [50/50], Train Loss: 0.0369, Val Loss: 0.1106\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=16, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.2901, Val Loss: 0.5659\n",
      "Epoch [2/50], Train Loss: 0.2579, Val Loss: 0.5114\n",
      "Epoch [3/50], Train Loss: 0.2268, Val Loss: 0.4661\n",
      "Epoch [4/50], Train Loss: 0.2047, Val Loss: 0.4267\n",
      "Epoch [5/50], Train Loss: 0.1862, Val Loss: 0.3924\n",
      "Epoch [6/50], Train Loss: 0.1706, Val Loss: 0.3627\n",
      "Epoch [7/50], Train Loss: 0.1571, Val Loss: 0.3363\n",
      "Epoch [8/50], Train Loss: 0.1422, Val Loss: 0.3127\n",
      "Epoch [9/50], Train Loss: 0.1315, Val Loss: 0.2917\n",
      "Epoch [10/50], Train Loss: 0.1225, Val Loss: 0.2730\n",
      "Epoch [11/50], Train Loss: 0.1171, Val Loss: 0.2567\n",
      "Epoch [12/50], Train Loss: 0.1096, Val Loss: 0.2421\n",
      "Epoch [13/50], Train Loss: 0.1007, Val Loss: 0.2287\n",
      "Epoch [14/50], Train Loss: 0.0986, Val Loss: 0.2165\n",
      "Epoch [15/50], Train Loss: 0.0925, Val Loss: 0.2057\n",
      "Epoch [16/50], Train Loss: 0.0878, Val Loss: 0.1960\n",
      "Epoch [17/50], Train Loss: 0.0857, Val Loss: 0.1870\n",
      "Epoch [18/50], Train Loss: 0.0815, Val Loss: 0.1788\n",
      "Epoch [19/50], Train Loss: 0.0779, Val Loss: 0.1714\n",
      "Epoch [20/50], Train Loss: 0.0760, Val Loss: 0.1649\n",
      "Epoch [21/50], Train Loss: 0.0725, Val Loss: 0.1585\n",
      "Epoch [22/50], Train Loss: 0.0721, Val Loss: 0.1530\n",
      "Epoch [23/50], Train Loss: 0.0684, Val Loss: 0.1476\n",
      "Epoch [24/50], Train Loss: 0.0685, Val Loss: 0.1431\n",
      "Epoch [25/50], Train Loss: 0.0651, Val Loss: 0.1391\n",
      "Epoch [26/50], Train Loss: 0.0642, Val Loss: 0.1356\n",
      "Epoch [27/50], Train Loss: 0.0641, Val Loss: 0.1324\n",
      "Epoch [28/50], Train Loss: 0.0625, Val Loss: 0.1290\n",
      "Epoch [29/50], Train Loss: 0.0596, Val Loss: 0.1263\n",
      "Epoch [30/50], Train Loss: 0.0597, Val Loss: 0.1235\n",
      "Epoch [31/50], Train Loss: 0.0592, Val Loss: 0.1209\n",
      "Epoch [32/50], Train Loss: 0.0580, Val Loss: 0.1187\n",
      "Epoch [33/50], Train Loss: 0.0590, Val Loss: 0.1168\n",
      "Epoch [34/50], Train Loss: 0.0575, Val Loss: 0.1152\n",
      "Epoch [35/50], Train Loss: 0.0541, Val Loss: 0.1135\n",
      "Epoch [36/50], Train Loss: 0.0551, Val Loss: 0.1111\n",
      "Epoch [37/50], Train Loss: 0.0536, Val Loss: 0.1095\n",
      "Epoch [38/50], Train Loss: 0.0548, Val Loss: 0.1083\n",
      "Epoch [39/50], Train Loss: 0.0552, Val Loss: 0.1068\n",
      "Epoch [40/50], Train Loss: 0.0547, Val Loss: 0.1056\n",
      "Epoch [41/50], Train Loss: 0.0544, Val Loss: 0.1048\n",
      "Epoch [42/50], Train Loss: 0.0519, Val Loss: 0.1037\n",
      "Epoch [43/50], Train Loss: 0.0512, Val Loss: 0.1029\n",
      "Epoch [44/50], Train Loss: 0.0526, Val Loss: 0.1023\n",
      "Epoch [45/50], Train Loss: 0.0538, Val Loss: 0.1016\n",
      "Epoch [46/50], Train Loss: 0.0534, Val Loss: 0.1007\n",
      "Epoch [47/50], Train Loss: 0.0522, Val Loss: 0.0998\n",
      "Epoch [48/50], Train Loss: 0.0525, Val Loss: 0.0990\n",
      "Epoch [49/50], Train Loss: 0.0520, Val Loss: 0.0986\n",
      "Epoch [50/50], Train Loss: 0.0513, Val Loss: 0.0982\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=16, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.2573, Val Loss: 0.4210\n",
      "Epoch [2/50], Train Loss: 0.2367, Val Loss: 0.3997\n",
      "Epoch [3/50], Train Loss: 0.2239, Val Loss: 0.3799\n",
      "Epoch [4/50], Train Loss: 0.2086, Val Loss: 0.3622\n",
      "Epoch [5/50], Train Loss: 0.2000, Val Loss: 0.3460\n",
      "Epoch [6/50], Train Loss: 0.1958, Val Loss: 0.3310\n",
      "Epoch [7/50], Train Loss: 0.1875, Val Loss: 0.3173\n",
      "Epoch [8/50], Train Loss: 0.1814, Val Loss: 0.3042\n",
      "Epoch [9/50], Train Loss: 0.1720, Val Loss: 0.2926\n",
      "Epoch [10/50], Train Loss: 0.1730, Val Loss: 0.2816\n",
      "Epoch [11/50], Train Loss: 0.1581, Val Loss: 0.2715\n",
      "Epoch [12/50], Train Loss: 0.1576, Val Loss: 0.2621\n",
      "Epoch [13/50], Train Loss: 0.1575, Val Loss: 0.2532\n",
      "Epoch [14/50], Train Loss: 0.1452, Val Loss: 0.2452\n",
      "Epoch [15/50], Train Loss: 0.1450, Val Loss: 0.2377\n",
      "Epoch [16/50], Train Loss: 0.1368, Val Loss: 0.2311\n",
      "Epoch [17/50], Train Loss: 0.1363, Val Loss: 0.2244\n",
      "Epoch [18/50], Train Loss: 0.1313, Val Loss: 0.2184\n",
      "Epoch [19/50], Train Loss: 0.1332, Val Loss: 0.2125\n",
      "Epoch [20/50], Train Loss: 0.1280, Val Loss: 0.2070\n",
      "Epoch [21/50], Train Loss: 0.1248, Val Loss: 0.2020\n",
      "Epoch [22/50], Train Loss: 0.1280, Val Loss: 0.1977\n",
      "Epoch [23/50], Train Loss: 0.1181, Val Loss: 0.1932\n",
      "Epoch [24/50], Train Loss: 0.1200, Val Loss: 0.1890\n",
      "Epoch [25/50], Train Loss: 0.1182, Val Loss: 0.1853\n",
      "Epoch [26/50], Train Loss: 0.1122, Val Loss: 0.1818\n",
      "Epoch [27/50], Train Loss: 0.1159, Val Loss: 0.1786\n",
      "Epoch [28/50], Train Loss: 0.1166, Val Loss: 0.1753\n",
      "Epoch [29/50], Train Loss: 0.1120, Val Loss: 0.1725\n",
      "Epoch [30/50], Train Loss: 0.1112, Val Loss: 0.1692\n",
      "Epoch [31/50], Train Loss: 0.1109, Val Loss: 0.1668\n",
      "Epoch [32/50], Train Loss: 0.1092, Val Loss: 0.1642\n",
      "Epoch [33/50], Train Loss: 0.1065, Val Loss: 0.1618\n",
      "Epoch [34/50], Train Loss: 0.1056, Val Loss: 0.1596\n",
      "Epoch [35/50], Train Loss: 0.1063, Val Loss: 0.1575\n",
      "Epoch [36/50], Train Loss: 0.1051, Val Loss: 0.1554\n",
      "Epoch [37/50], Train Loss: 0.1033, Val Loss: 0.1535\n",
      "Epoch [38/50], Train Loss: 0.1015, Val Loss: 0.1516\n",
      "Epoch [39/50], Train Loss: 0.1025, Val Loss: 0.1501\n",
      "Epoch [40/50], Train Loss: 0.1007, Val Loss: 0.1483\n",
      "Epoch [41/50], Train Loss: 0.0982, Val Loss: 0.1466\n",
      "Epoch [42/50], Train Loss: 0.0996, Val Loss: 0.1455\n",
      "Epoch [43/50], Train Loss: 0.0980, Val Loss: 0.1441\n",
      "Epoch [44/50], Train Loss: 0.0947, Val Loss: 0.1430\n",
      "Epoch [45/50], Train Loss: 0.1004, Val Loss: 0.1418\n",
      "Epoch [46/50], Train Loss: 0.0958, Val Loss: 0.1407\n",
      "Epoch [47/50], Train Loss: 0.0968, Val Loss: 0.1399\n",
      "Epoch [48/50], Train Loss: 0.0913, Val Loss: 0.1387\n",
      "Epoch [49/50], Train Loss: 0.0953, Val Loss: 0.1375\n",
      "Epoch [50/50], Train Loss: 0.0972, Val Loss: 0.1367\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=32, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1202, Val Loss: 0.3207\n",
      "Epoch [2/50], Train Loss: 0.1128, Val Loss: 0.3055\n",
      "Epoch [3/50], Train Loss: 0.1062, Val Loss: 0.2914\n",
      "Epoch [4/50], Train Loss: 0.1001, Val Loss: 0.2783\n",
      "Epoch [5/50], Train Loss: 0.0946, Val Loss: 0.2661\n",
      "Epoch [6/50], Train Loss: 0.0895, Val Loss: 0.2548\n",
      "Epoch [7/50], Train Loss: 0.0849, Val Loss: 0.2442\n",
      "Epoch [8/50], Train Loss: 0.0806, Val Loss: 0.2342\n",
      "Epoch [9/50], Train Loss: 0.0767, Val Loss: 0.2248\n",
      "Epoch [10/50], Train Loss: 0.0731, Val Loss: 0.2161\n",
      "Epoch [11/50], Train Loss: 0.0698, Val Loss: 0.2079\n",
      "Epoch [12/50], Train Loss: 0.0667, Val Loss: 0.2001\n",
      "Epoch [13/50], Train Loss: 0.0639, Val Loss: 0.1929\n",
      "Epoch [14/50], Train Loss: 0.0613, Val Loss: 0.1860\n",
      "Epoch [15/50], Train Loss: 0.0589, Val Loss: 0.1796\n",
      "Epoch [16/50], Train Loss: 0.0567, Val Loss: 0.1735\n",
      "Epoch [17/50], Train Loss: 0.0547, Val Loss: 0.1678\n",
      "Epoch [18/50], Train Loss: 0.0529, Val Loss: 0.1625\n",
      "Epoch [19/50], Train Loss: 0.0511, Val Loss: 0.1574\n",
      "Epoch [20/50], Train Loss: 0.0496, Val Loss: 0.1527\n",
      "Epoch [21/50], Train Loss: 0.0481, Val Loss: 0.1482\n",
      "Epoch [22/50], Train Loss: 0.0468, Val Loss: 0.1440\n",
      "Epoch [23/50], Train Loss: 0.0455, Val Loss: 0.1401\n",
      "Epoch [24/50], Train Loss: 0.0444, Val Loss: 0.1364\n",
      "Epoch [25/50], Train Loss: 0.0434, Val Loss: 0.1329\n",
      "Epoch [26/50], Train Loss: 0.0424, Val Loss: 0.1296\n",
      "Epoch [27/50], Train Loss: 0.0416, Val Loss: 0.1265\n",
      "Epoch [28/50], Train Loss: 0.0408, Val Loss: 0.1236\n",
      "Epoch [29/50], Train Loss: 0.0400, Val Loss: 0.1208\n",
      "Epoch [30/50], Train Loss: 0.0393, Val Loss: 0.1183\n",
      "Epoch [31/50], Train Loss: 0.0387, Val Loss: 0.1159\n",
      "Epoch [32/50], Train Loss: 0.0382, Val Loss: 0.1136\n",
      "Epoch [33/50], Train Loss: 0.0376, Val Loss: 0.1114\n",
      "Epoch [34/50], Train Loss: 0.0371, Val Loss: 0.1094\n",
      "Epoch [35/50], Train Loss: 0.0367, Val Loss: 0.1075\n",
      "Epoch [36/50], Train Loss: 0.0363, Val Loss: 0.1057\n",
      "Epoch [37/50], Train Loss: 0.0359, Val Loss: 0.1041\n",
      "Epoch [38/50], Train Loss: 0.0356, Val Loss: 0.1025\n",
      "Epoch [39/50], Train Loss: 0.0352, Val Loss: 0.1010\n",
      "Epoch [40/50], Train Loss: 0.0349, Val Loss: 0.0996\n",
      "Epoch [41/50], Train Loss: 0.0347, Val Loss: 0.0982\n",
      "Epoch [42/50], Train Loss: 0.0344, Val Loss: 0.0970\n",
      "Epoch [43/50], Train Loss: 0.0341, Val Loss: 0.0958\n",
      "Epoch [44/50], Train Loss: 0.0339, Val Loss: 0.0947\n",
      "Epoch [45/50], Train Loss: 0.0337, Val Loss: 0.0936\n",
      "Epoch [46/50], Train Loss: 0.0335, Val Loss: 0.0926\n",
      "Epoch [47/50], Train Loss: 0.0333, Val Loss: 0.0917\n",
      "Epoch [48/50], Train Loss: 0.0332, Val Loss: 0.0908\n",
      "Epoch [49/50], Train Loss: 0.0330, Val Loss: 0.0900\n",
      "Epoch [50/50], Train Loss: 0.0328, Val Loss: 0.0892\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=32, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1051, Val Loss: 0.2731\n",
      "Epoch [2/50], Train Loss: 0.0994, Val Loss: 0.2593\n",
      "Epoch [3/50], Train Loss: 0.0923, Val Loss: 0.2465\n",
      "Epoch [4/50], Train Loss: 0.0889, Val Loss: 0.2347\n",
      "Epoch [5/50], Train Loss: 0.0819, Val Loss: 0.2238\n",
      "Epoch [6/50], Train Loss: 0.0781, Val Loss: 0.2137\n",
      "Epoch [7/50], Train Loss: 0.0736, Val Loss: 0.2043\n",
      "Epoch [8/50], Train Loss: 0.0699, Val Loss: 0.1956\n",
      "Epoch [9/50], Train Loss: 0.0671, Val Loss: 0.1875\n",
      "Epoch [10/50], Train Loss: 0.0636, Val Loss: 0.1799\n",
      "Epoch [11/50], Train Loss: 0.0604, Val Loss: 0.1730\n",
      "Epoch [12/50], Train Loss: 0.0588, Val Loss: 0.1666\n",
      "Epoch [13/50], Train Loss: 0.0580, Val Loss: 0.1606\n",
      "Epoch [14/50], Train Loss: 0.0553, Val Loss: 0.1551\n",
      "Epoch [15/50], Train Loss: 0.0519, Val Loss: 0.1500\n",
      "Epoch [16/50], Train Loss: 0.0520, Val Loss: 0.1452\n",
      "Epoch [17/50], Train Loss: 0.0503, Val Loss: 0.1409\n",
      "Epoch [18/50], Train Loss: 0.0491, Val Loss: 0.1368\n",
      "Epoch [19/50], Train Loss: 0.0483, Val Loss: 0.1330\n",
      "Epoch [20/50], Train Loss: 0.0480, Val Loss: 0.1293\n",
      "Epoch [21/50], Train Loss: 0.0462, Val Loss: 0.1259\n",
      "Epoch [22/50], Train Loss: 0.0456, Val Loss: 0.1229\n",
      "Epoch [23/50], Train Loss: 0.0441, Val Loss: 0.1200\n",
      "Epoch [24/50], Train Loss: 0.0431, Val Loss: 0.1173\n",
      "Epoch [25/50], Train Loss: 0.0424, Val Loss: 0.1147\n",
      "Epoch [26/50], Train Loss: 0.0435, Val Loss: 0.1124\n",
      "Epoch [27/50], Train Loss: 0.0423, Val Loss: 0.1102\n",
      "Epoch [28/50], Train Loss: 0.0431, Val Loss: 0.1082\n",
      "Epoch [29/50], Train Loss: 0.0414, Val Loss: 0.1063\n",
      "Epoch [30/50], Train Loss: 0.0417, Val Loss: 0.1044\n",
      "Epoch [31/50], Train Loss: 0.0415, Val Loss: 0.1028\n",
      "Epoch [32/50], Train Loss: 0.0404, Val Loss: 0.1013\n",
      "Epoch [33/50], Train Loss: 0.0403, Val Loss: 0.0999\n",
      "Epoch [34/50], Train Loss: 0.0394, Val Loss: 0.0986\n",
      "Epoch [35/50], Train Loss: 0.0398, Val Loss: 0.0974\n",
      "Epoch [36/50], Train Loss: 0.0403, Val Loss: 0.0961\n",
      "Epoch [37/50], Train Loss: 0.0390, Val Loss: 0.0951\n",
      "Epoch [38/50], Train Loss: 0.0406, Val Loss: 0.0939\n",
      "Epoch [39/50], Train Loss: 0.0396, Val Loss: 0.0928\n",
      "Epoch [40/50], Train Loss: 0.0384, Val Loss: 0.0919\n",
      "Epoch [41/50], Train Loss: 0.0381, Val Loss: 0.0911\n",
      "Epoch [42/50], Train Loss: 0.0396, Val Loss: 0.0903\n",
      "Epoch [43/50], Train Loss: 0.0386, Val Loss: 0.0895\n",
      "Epoch [44/50], Train Loss: 0.0386, Val Loss: 0.0889\n",
      "Epoch [45/50], Train Loss: 0.0379, Val Loss: 0.0882\n",
      "Epoch [46/50], Train Loss: 0.0373, Val Loss: 0.0875\n",
      "Epoch [47/50], Train Loss: 0.0377, Val Loss: 0.0869\n",
      "Epoch [48/50], Train Loss: 0.0380, Val Loss: 0.0864\n",
      "Epoch [49/50], Train Loss: 0.0378, Val Loss: 0.0858\n",
      "Epoch [50/50], Train Loss: 0.0378, Val Loss: 0.0853\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=32, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.2220, Val Loss: 0.5380\n",
      "Epoch [2/50], Train Loss: 0.2038, Val Loss: 0.5078\n",
      "Epoch [3/50], Train Loss: 0.1885, Val Loss: 0.4810\n",
      "Epoch [4/50], Train Loss: 0.1767, Val Loss: 0.4563\n",
      "Epoch [5/50], Train Loss: 0.1668, Val Loss: 0.4338\n",
      "Epoch [6/50], Train Loss: 0.1579, Val Loss: 0.4133\n",
      "Epoch [7/50], Train Loss: 0.1456, Val Loss: 0.3945\n",
      "Epoch [8/50], Train Loss: 0.1363, Val Loss: 0.3775\n",
      "Epoch [9/50], Train Loss: 0.1301, Val Loss: 0.3614\n",
      "Epoch [10/50], Train Loss: 0.1251, Val Loss: 0.3465\n",
      "Epoch [11/50], Train Loss: 0.1171, Val Loss: 0.3327\n",
      "Epoch [12/50], Train Loss: 0.1127, Val Loss: 0.3199\n",
      "Epoch [13/50], Train Loss: 0.1080, Val Loss: 0.3076\n",
      "Epoch [14/50], Train Loss: 0.1038, Val Loss: 0.2961\n",
      "Epoch [15/50], Train Loss: 0.0996, Val Loss: 0.2855\n",
      "Epoch [16/50], Train Loss: 0.0939, Val Loss: 0.2756\n",
      "Epoch [17/50], Train Loss: 0.0918, Val Loss: 0.2662\n",
      "Epoch [18/50], Train Loss: 0.0885, Val Loss: 0.2572\n",
      "Epoch [19/50], Train Loss: 0.0851, Val Loss: 0.2488\n",
      "Epoch [20/50], Train Loss: 0.0812, Val Loss: 0.2408\n",
      "Epoch [21/50], Train Loss: 0.0800, Val Loss: 0.2333\n",
      "Epoch [22/50], Train Loss: 0.0768, Val Loss: 0.2263\n",
      "Epoch [23/50], Train Loss: 0.0746, Val Loss: 0.2196\n",
      "Epoch [24/50], Train Loss: 0.0721, Val Loss: 0.2134\n",
      "Epoch [25/50], Train Loss: 0.0714, Val Loss: 0.2073\n",
      "Epoch [26/50], Train Loss: 0.0686, Val Loss: 0.2018\n",
      "Epoch [27/50], Train Loss: 0.0665, Val Loss: 0.1965\n",
      "Epoch [28/50], Train Loss: 0.0659, Val Loss: 0.1915\n",
      "Epoch [29/50], Train Loss: 0.0644, Val Loss: 0.1866\n",
      "Epoch [30/50], Train Loss: 0.0621, Val Loss: 0.1820\n",
      "Epoch [31/50], Train Loss: 0.0622, Val Loss: 0.1777\n",
      "Epoch [32/50], Train Loss: 0.0594, Val Loss: 0.1737\n",
      "Epoch [33/50], Train Loss: 0.0584, Val Loss: 0.1698\n",
      "Epoch [34/50], Train Loss: 0.0594, Val Loss: 0.1661\n",
      "Epoch [35/50], Train Loss: 0.0565, Val Loss: 0.1627\n",
      "Epoch [36/50], Train Loss: 0.0571, Val Loss: 0.1596\n",
      "Epoch [37/50], Train Loss: 0.0556, Val Loss: 0.1564\n",
      "Epoch [38/50], Train Loss: 0.0552, Val Loss: 0.1534\n",
      "Epoch [39/50], Train Loss: 0.0550, Val Loss: 0.1504\n",
      "Epoch [40/50], Train Loss: 0.0551, Val Loss: 0.1478\n",
      "Epoch [41/50], Train Loss: 0.0505, Val Loss: 0.1454\n",
      "Epoch [42/50], Train Loss: 0.0530, Val Loss: 0.1429\n",
      "Epoch [43/50], Train Loss: 0.0500, Val Loss: 0.1406\n",
      "Epoch [44/50], Train Loss: 0.0525, Val Loss: 0.1385\n",
      "Epoch [45/50], Train Loss: 0.0526, Val Loss: 0.1364\n",
      "Epoch [46/50], Train Loss: 0.0511, Val Loss: 0.1345\n",
      "Epoch [47/50], Train Loss: 0.0506, Val Loss: 0.1325\n",
      "Epoch [48/50], Train Loss: 0.0510, Val Loss: 0.1307\n",
      "Epoch [49/50], Train Loss: 0.0495, Val Loss: 0.1290\n",
      "Epoch [50/50], Train Loss: 0.0490, Val Loss: 0.1275\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=32, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1206, Val Loss: 0.2917\n",
      "Epoch [2/50], Train Loss: 0.1145, Val Loss: 0.2818\n",
      "Epoch [3/50], Train Loss: 0.1087, Val Loss: 0.2723\n",
      "Epoch [4/50], Train Loss: 0.1034, Val Loss: 0.2632\n",
      "Epoch [5/50], Train Loss: 0.0983, Val Loss: 0.2544\n",
      "Epoch [6/50], Train Loss: 0.0936, Val Loss: 0.2460\n",
      "Epoch [7/50], Train Loss: 0.0891, Val Loss: 0.2380\n",
      "Epoch [8/50], Train Loss: 0.0850, Val Loss: 0.2303\n",
      "Epoch [9/50], Train Loss: 0.0810, Val Loss: 0.2229\n",
      "Epoch [10/50], Train Loss: 0.0774, Val Loss: 0.2157\n",
      "Epoch [11/50], Train Loss: 0.0739, Val Loss: 0.2089\n",
      "Epoch [12/50], Train Loss: 0.0707, Val Loss: 0.2024\n",
      "Epoch [13/50], Train Loss: 0.0676, Val Loss: 0.1961\n",
      "Epoch [14/50], Train Loss: 0.0648, Val Loss: 0.1901\n",
      "Epoch [15/50], Train Loss: 0.0621, Val Loss: 0.1844\n",
      "Epoch [16/50], Train Loss: 0.0596, Val Loss: 0.1789\n",
      "Epoch [17/50], Train Loss: 0.0573, Val Loss: 0.1736\n",
      "Epoch [18/50], Train Loss: 0.0552, Val Loss: 0.1686\n",
      "Epoch [19/50], Train Loss: 0.0532, Val Loss: 0.1638\n",
      "Epoch [20/50], Train Loss: 0.0513, Val Loss: 0.1593\n",
      "Epoch [21/50], Train Loss: 0.0496, Val Loss: 0.1549\n",
      "Epoch [22/50], Train Loss: 0.0480, Val Loss: 0.1508\n",
      "Epoch [23/50], Train Loss: 0.0466, Val Loss: 0.1469\n",
      "Epoch [24/50], Train Loss: 0.0452, Val Loss: 0.1432\n",
      "Epoch [25/50], Train Loss: 0.0440, Val Loss: 0.1396\n",
      "Epoch [26/50], Train Loss: 0.0429, Val Loss: 0.1363\n",
      "Epoch [27/50], Train Loss: 0.0418, Val Loss: 0.1332\n",
      "Epoch [28/50], Train Loss: 0.0409, Val Loss: 0.1301\n",
      "Epoch [29/50], Train Loss: 0.0400, Val Loss: 0.1273\n",
      "Epoch [30/50], Train Loss: 0.0392, Val Loss: 0.1246\n",
      "Epoch [31/50], Train Loss: 0.0384, Val Loss: 0.1221\n",
      "Epoch [32/50], Train Loss: 0.0378, Val Loss: 0.1197\n",
      "Epoch [33/50], Train Loss: 0.0372, Val Loss: 0.1175\n",
      "Epoch [34/50], Train Loss: 0.0366, Val Loss: 0.1153\n",
      "Epoch [35/50], Train Loss: 0.0361, Val Loss: 0.1133\n",
      "Epoch [36/50], Train Loss: 0.0357, Val Loss: 0.1114\n",
      "Epoch [37/50], Train Loss: 0.0352, Val Loss: 0.1096\n",
      "Epoch [38/50], Train Loss: 0.0349, Val Loss: 0.1080\n",
      "Epoch [39/50], Train Loss: 0.0345, Val Loss: 0.1064\n",
      "Epoch [40/50], Train Loss: 0.0342, Val Loss: 0.1049\n",
      "Epoch [41/50], Train Loss: 0.0339, Val Loss: 0.1035\n",
      "Epoch [42/50], Train Loss: 0.0337, Val Loss: 0.1021\n",
      "Epoch [43/50], Train Loss: 0.0334, Val Loss: 0.1009\n",
      "Epoch [44/50], Train Loss: 0.0332, Val Loss: 0.0997\n",
      "Epoch [45/50], Train Loss: 0.0330, Val Loss: 0.0986\n",
      "Epoch [46/50], Train Loss: 0.0328, Val Loss: 0.0975\n",
      "Epoch [47/50], Train Loss: 0.0327, Val Loss: 0.0965\n",
      "Epoch [48/50], Train Loss: 0.0325, Val Loss: 0.0956\n",
      "Epoch [49/50], Train Loss: 0.0324, Val Loss: 0.0947\n",
      "Epoch [50/50], Train Loss: 0.0322, Val Loss: 0.0938\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=32, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1883, Val Loss: 0.4377\n",
      "Epoch [2/50], Train Loss: 0.1714, Val Loss: 0.4123\n",
      "Epoch [3/50], Train Loss: 0.1585, Val Loss: 0.3883\n",
      "Epoch [4/50], Train Loss: 0.1484, Val Loss: 0.3660\n",
      "Epoch [5/50], Train Loss: 0.1357, Val Loss: 0.3447\n",
      "Epoch [6/50], Train Loss: 0.1242, Val Loss: 0.3247\n",
      "Epoch [7/50], Train Loss: 0.1152, Val Loss: 0.3059\n",
      "Epoch [8/50], Train Loss: 0.1078, Val Loss: 0.2883\n",
      "Epoch [9/50], Train Loss: 0.0994, Val Loss: 0.2720\n",
      "Epoch [10/50], Train Loss: 0.0929, Val Loss: 0.2568\n",
      "Epoch [11/50], Train Loss: 0.0872, Val Loss: 0.2426\n",
      "Epoch [12/50], Train Loss: 0.0806, Val Loss: 0.2294\n",
      "Epoch [13/50], Train Loss: 0.0763, Val Loss: 0.2176\n",
      "Epoch [14/50], Train Loss: 0.0720, Val Loss: 0.2066\n",
      "Epoch [15/50], Train Loss: 0.0692, Val Loss: 0.1967\n",
      "Epoch [16/50], Train Loss: 0.0643, Val Loss: 0.1876\n",
      "Epoch [17/50], Train Loss: 0.0629, Val Loss: 0.1792\n",
      "Epoch [18/50], Train Loss: 0.0604, Val Loss: 0.1718\n",
      "Epoch [19/50], Train Loss: 0.0578, Val Loss: 0.1647\n",
      "Epoch [20/50], Train Loss: 0.0567, Val Loss: 0.1584\n",
      "Epoch [21/50], Train Loss: 0.0558, Val Loss: 0.1527\n",
      "Epoch [22/50], Train Loss: 0.0535, Val Loss: 0.1476\n",
      "Epoch [23/50], Train Loss: 0.0543, Val Loss: 0.1428\n",
      "Epoch [24/50], Train Loss: 0.0542, Val Loss: 0.1385\n",
      "Epoch [25/50], Train Loss: 0.0504, Val Loss: 0.1348\n",
      "Epoch [26/50], Train Loss: 0.0507, Val Loss: 0.1314\n",
      "Epoch [27/50], Train Loss: 0.0497, Val Loss: 0.1282\n",
      "Epoch [28/50], Train Loss: 0.0489, Val Loss: 0.1253\n",
      "Epoch [29/50], Train Loss: 0.0484, Val Loss: 0.1226\n",
      "Epoch [30/50], Train Loss: 0.0485, Val Loss: 0.1203\n",
      "Epoch [31/50], Train Loss: 0.0473, Val Loss: 0.1183\n",
      "Epoch [32/50], Train Loss: 0.0478, Val Loss: 0.1165\n",
      "Epoch [33/50], Train Loss: 0.0475, Val Loss: 0.1146\n",
      "Epoch [34/50], Train Loss: 0.0469, Val Loss: 0.1128\n",
      "Epoch [35/50], Train Loss: 0.0455, Val Loss: 0.1113\n",
      "Epoch [36/50], Train Loss: 0.0461, Val Loss: 0.1097\n",
      "Epoch [37/50], Train Loss: 0.0465, Val Loss: 0.1083\n",
      "Epoch [38/50], Train Loss: 0.0459, Val Loss: 0.1073\n",
      "Epoch [39/50], Train Loss: 0.0457, Val Loss: 0.1064\n",
      "Epoch [40/50], Train Loss: 0.0458, Val Loss: 0.1056\n",
      "Epoch [41/50], Train Loss: 0.0447, Val Loss: 0.1046\n",
      "Epoch [42/50], Train Loss: 0.0462, Val Loss: 0.1038\n",
      "Epoch [43/50], Train Loss: 0.0446, Val Loss: 0.1031\n",
      "Epoch [44/50], Train Loss: 0.0456, Val Loss: 0.1024\n",
      "Epoch [45/50], Train Loss: 0.0446, Val Loss: 0.1016\n",
      "Epoch [46/50], Train Loss: 0.0447, Val Loss: 0.1011\n",
      "Epoch [47/50], Train Loss: 0.0451, Val Loss: 0.1004\n",
      "Epoch [48/50], Train Loss: 0.0443, Val Loss: 0.1000\n",
      "Epoch [49/50], Train Loss: 0.0443, Val Loss: 0.0995\n",
      "Epoch [50/50], Train Loss: 0.0450, Val Loss: 0.0993\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=32, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1529, Val Loss: 0.3252\n",
      "Epoch [2/50], Train Loss: 0.1419, Val Loss: 0.3096\n",
      "Epoch [3/50], Train Loss: 0.1343, Val Loss: 0.2959\n",
      "Epoch [4/50], Train Loss: 0.1274, Val Loss: 0.2838\n",
      "Epoch [5/50], Train Loss: 0.1220, Val Loss: 0.2726\n",
      "Epoch [6/50], Train Loss: 0.1130, Val Loss: 0.2625\n",
      "Epoch [7/50], Train Loss: 0.1079, Val Loss: 0.2536\n",
      "Epoch [8/50], Train Loss: 0.1040, Val Loss: 0.2455\n",
      "Epoch [9/50], Train Loss: 0.0997, Val Loss: 0.2382\n",
      "Epoch [10/50], Train Loss: 0.0940, Val Loss: 0.2314\n",
      "Epoch [11/50], Train Loss: 0.0954, Val Loss: 0.2247\n",
      "Epoch [12/50], Train Loss: 0.0894, Val Loss: 0.2185\n",
      "Epoch [13/50], Train Loss: 0.0883, Val Loss: 0.2127\n",
      "Epoch [14/50], Train Loss: 0.0848, Val Loss: 0.2075\n",
      "Epoch [15/50], Train Loss: 0.0815, Val Loss: 0.2027\n",
      "Epoch [16/50], Train Loss: 0.0807, Val Loss: 0.1980\n",
      "Epoch [17/50], Train Loss: 0.0778, Val Loss: 0.1938\n",
      "Epoch [18/50], Train Loss: 0.0775, Val Loss: 0.1899\n",
      "Epoch [19/50], Train Loss: 0.0737, Val Loss: 0.1860\n",
      "Epoch [20/50], Train Loss: 0.0731, Val Loss: 0.1826\n",
      "Epoch [21/50], Train Loss: 0.0716, Val Loss: 0.1792\n",
      "Epoch [22/50], Train Loss: 0.0713, Val Loss: 0.1761\n",
      "Epoch [23/50], Train Loss: 0.0696, Val Loss: 0.1731\n",
      "Epoch [24/50], Train Loss: 0.0679, Val Loss: 0.1702\n",
      "Epoch [25/50], Train Loss: 0.0667, Val Loss: 0.1675\n",
      "Epoch [26/50], Train Loss: 0.0661, Val Loss: 0.1649\n",
      "Epoch [27/50], Train Loss: 0.0624, Val Loss: 0.1627\n",
      "Epoch [28/50], Train Loss: 0.0643, Val Loss: 0.1604\n",
      "Epoch [29/50], Train Loss: 0.0643, Val Loss: 0.1581\n",
      "Epoch [30/50], Train Loss: 0.0642, Val Loss: 0.1560\n",
      "Epoch [31/50], Train Loss: 0.0626, Val Loss: 0.1540\n",
      "Epoch [32/50], Train Loss: 0.0609, Val Loss: 0.1521\n",
      "Epoch [33/50], Train Loss: 0.0625, Val Loss: 0.1500\n",
      "Epoch [34/50], Train Loss: 0.0605, Val Loss: 0.1482\n",
      "Epoch [35/50], Train Loss: 0.0599, Val Loss: 0.1466\n",
      "Epoch [36/50], Train Loss: 0.0608, Val Loss: 0.1451\n",
      "Epoch [37/50], Train Loss: 0.0586, Val Loss: 0.1436\n",
      "Epoch [38/50], Train Loss: 0.0591, Val Loss: 0.1422\n",
      "Epoch [39/50], Train Loss: 0.0570, Val Loss: 0.1409\n",
      "Epoch [40/50], Train Loss: 0.0566, Val Loss: 0.1394\n",
      "Epoch [41/50], Train Loss: 0.0557, Val Loss: 0.1382\n",
      "Epoch [42/50], Train Loss: 0.0557, Val Loss: 0.1371\n",
      "Epoch [43/50], Train Loss: 0.0557, Val Loss: 0.1361\n",
      "Epoch [44/50], Train Loss: 0.0565, Val Loss: 0.1350\n",
      "Epoch [45/50], Train Loss: 0.0546, Val Loss: 0.1339\n",
      "Epoch [46/50], Train Loss: 0.0544, Val Loss: 0.1330\n",
      "Epoch [47/50], Train Loss: 0.0547, Val Loss: 0.1321\n",
      "Epoch [48/50], Train Loss: 0.0558, Val Loss: 0.1312\n",
      "Epoch [49/50], Train Loss: 0.0537, Val Loss: 0.1304\n",
      "Epoch [50/50], Train Loss: 0.0534, Val Loss: 0.1295\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=32, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1493, Val Loss: 0.3773\n",
      "Epoch [2/50], Train Loss: 0.1374, Val Loss: 0.3576\n",
      "Epoch [3/50], Train Loss: 0.1269, Val Loss: 0.3396\n",
      "Epoch [4/50], Train Loss: 0.1175, Val Loss: 0.3229\n",
      "Epoch [5/50], Train Loss: 0.1091, Val Loss: 0.3076\n",
      "Epoch [6/50], Train Loss: 0.1015, Val Loss: 0.2933\n",
      "Epoch [7/50], Train Loss: 0.0947, Val Loss: 0.2800\n",
      "Epoch [8/50], Train Loss: 0.0885, Val Loss: 0.2676\n",
      "Epoch [9/50], Train Loss: 0.0830, Val Loss: 0.2560\n",
      "Epoch [10/50], Train Loss: 0.0780, Val Loss: 0.2452\n",
      "Epoch [11/50], Train Loss: 0.0734, Val Loss: 0.2351\n",
      "Epoch [12/50], Train Loss: 0.0693, Val Loss: 0.2257\n",
      "Epoch [13/50], Train Loss: 0.0656, Val Loss: 0.2169\n",
      "Epoch [14/50], Train Loss: 0.0623, Val Loss: 0.2086\n",
      "Epoch [15/50], Train Loss: 0.0593, Val Loss: 0.2009\n",
      "Epoch [16/50], Train Loss: 0.0566, Val Loss: 0.1937\n",
      "Epoch [17/50], Train Loss: 0.0542, Val Loss: 0.1869\n",
      "Epoch [18/50], Train Loss: 0.0521, Val Loss: 0.1807\n",
      "Epoch [19/50], Train Loss: 0.0501, Val Loss: 0.1748\n",
      "Epoch [20/50], Train Loss: 0.0484, Val Loss: 0.1693\n",
      "Epoch [21/50], Train Loss: 0.0469, Val Loss: 0.1642\n",
      "Epoch [22/50], Train Loss: 0.0455, Val Loss: 0.1595\n",
      "Epoch [23/50], Train Loss: 0.0443, Val Loss: 0.1551\n",
      "Epoch [24/50], Train Loss: 0.0432, Val Loss: 0.1510\n",
      "Epoch [25/50], Train Loss: 0.0423, Val Loss: 0.1472\n",
      "Epoch [26/50], Train Loss: 0.0415, Val Loss: 0.1437\n",
      "Epoch [27/50], Train Loss: 0.0407, Val Loss: 0.1404\n",
      "Epoch [28/50], Train Loss: 0.0401, Val Loss: 0.1374\n",
      "Epoch [29/50], Train Loss: 0.0395, Val Loss: 0.1345\n",
      "Epoch [30/50], Train Loss: 0.0390, Val Loss: 0.1319\n",
      "Epoch [31/50], Train Loss: 0.0385, Val Loss: 0.1295\n",
      "Epoch [32/50], Train Loss: 0.0381, Val Loss: 0.1272\n",
      "Epoch [33/50], Train Loss: 0.0378, Val Loss: 0.1251\n",
      "Epoch [34/50], Train Loss: 0.0375, Val Loss: 0.1232\n",
      "Epoch [35/50], Train Loss: 0.0372, Val Loss: 0.1214\n",
      "Epoch [36/50], Train Loss: 0.0369, Val Loss: 0.1198\n",
      "Epoch [37/50], Train Loss: 0.0367, Val Loss: 0.1182\n",
      "Epoch [38/50], Train Loss: 0.0365, Val Loss: 0.1168\n",
      "Epoch [39/50], Train Loss: 0.0364, Val Loss: 0.1154\n",
      "Epoch [40/50], Train Loss: 0.0362, Val Loss: 0.1142\n",
      "Epoch [41/50], Train Loss: 0.0361, Val Loss: 0.1131\n",
      "Epoch [42/50], Train Loss: 0.0360, Val Loss: 0.1120\n",
      "Epoch [43/50], Train Loss: 0.0359, Val Loss: 0.1110\n",
      "Epoch [44/50], Train Loss: 0.0358, Val Loss: 0.1101\n",
      "Epoch [45/50], Train Loss: 0.0357, Val Loss: 0.1093\n",
      "Epoch [46/50], Train Loss: 0.0356, Val Loss: 0.1085\n",
      "Epoch [47/50], Train Loss: 0.0355, Val Loss: 0.1077\n",
      "Epoch [48/50], Train Loss: 0.0355, Val Loss: 0.1070\n",
      "Epoch [49/50], Train Loss: 0.0354, Val Loss: 0.1064\n",
      "Epoch [50/50], Train Loss: 0.0353, Val Loss: 0.1058\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=32, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0848, Val Loss: 0.2553\n",
      "Epoch [2/50], Train Loss: 0.0805, Val Loss: 0.2460\n",
      "Epoch [3/50], Train Loss: 0.0765, Val Loss: 0.2373\n",
      "Epoch [4/50], Train Loss: 0.0747, Val Loss: 0.2292\n",
      "Epoch [5/50], Train Loss: 0.0701, Val Loss: 0.2216\n",
      "Epoch [6/50], Train Loss: 0.0674, Val Loss: 0.2143\n",
      "Epoch [7/50], Train Loss: 0.0661, Val Loss: 0.2077\n",
      "Epoch [8/50], Train Loss: 0.0628, Val Loss: 0.2014\n",
      "Epoch [9/50], Train Loss: 0.0609, Val Loss: 0.1955\n",
      "Epoch [10/50], Train Loss: 0.0608, Val Loss: 0.1899\n",
      "Epoch [11/50], Train Loss: 0.0599, Val Loss: 0.1848\n",
      "Epoch [12/50], Train Loss: 0.0566, Val Loss: 0.1800\n",
      "Epoch [13/50], Train Loss: 0.0561, Val Loss: 0.1756\n",
      "Epoch [14/50], Train Loss: 0.0553, Val Loss: 0.1714\n",
      "Epoch [15/50], Train Loss: 0.0550, Val Loss: 0.1673\n",
      "Epoch [16/50], Train Loss: 0.0527, Val Loss: 0.1637\n",
      "Epoch [17/50], Train Loss: 0.0518, Val Loss: 0.1603\n",
      "Epoch [18/50], Train Loss: 0.0526, Val Loss: 0.1572\n",
      "Epoch [19/50], Train Loss: 0.0511, Val Loss: 0.1542\n",
      "Epoch [20/50], Train Loss: 0.0491, Val Loss: 0.1513\n",
      "Epoch [21/50], Train Loss: 0.0496, Val Loss: 0.1488\n",
      "Epoch [22/50], Train Loss: 0.0500, Val Loss: 0.1464\n",
      "Epoch [23/50], Train Loss: 0.0501, Val Loss: 0.1442\n",
      "Epoch [24/50], Train Loss: 0.0496, Val Loss: 0.1420\n",
      "Epoch [25/50], Train Loss: 0.0494, Val Loss: 0.1401\n",
      "Epoch [26/50], Train Loss: 0.0486, Val Loss: 0.1384\n",
      "Epoch [27/50], Train Loss: 0.0491, Val Loss: 0.1367\n",
      "Epoch [28/50], Train Loss: 0.0477, Val Loss: 0.1350\n",
      "Epoch [29/50], Train Loss: 0.0483, Val Loss: 0.1335\n",
      "Epoch [30/50], Train Loss: 0.0470, Val Loss: 0.1321\n",
      "Epoch [31/50], Train Loss: 0.0463, Val Loss: 0.1307\n",
      "Epoch [32/50], Train Loss: 0.0478, Val Loss: 0.1296\n",
      "Epoch [33/50], Train Loss: 0.0487, Val Loss: 0.1285\n",
      "Epoch [34/50], Train Loss: 0.0472, Val Loss: 0.1275\n",
      "Epoch [35/50], Train Loss: 0.0458, Val Loss: 0.1266\n",
      "Epoch [36/50], Train Loss: 0.0468, Val Loss: 0.1256\n",
      "Epoch [37/50], Train Loss: 0.0473, Val Loss: 0.1247\n",
      "Epoch [38/50], Train Loss: 0.0459, Val Loss: 0.1240\n",
      "Epoch [39/50], Train Loss: 0.0465, Val Loss: 0.1233\n",
      "Epoch [40/50], Train Loss: 0.0461, Val Loss: 0.1225\n",
      "Epoch [41/50], Train Loss: 0.0470, Val Loss: 0.1217\n",
      "Epoch [42/50], Train Loss: 0.0474, Val Loss: 0.1213\n",
      "Epoch [43/50], Train Loss: 0.0449, Val Loss: 0.1207\n",
      "Epoch [44/50], Train Loss: 0.0465, Val Loss: 0.1201\n",
      "Epoch [45/50], Train Loss: 0.0461, Val Loss: 0.1197\n",
      "Epoch [46/50], Train Loss: 0.0456, Val Loss: 0.1191\n",
      "Epoch [47/50], Train Loss: 0.0459, Val Loss: 0.1188\n",
      "Epoch [48/50], Train Loss: 0.0461, Val Loss: 0.1184\n",
      "Epoch [49/50], Train Loss: 0.0452, Val Loss: 0.1180\n",
      "Epoch [50/50], Train Loss: 0.0446, Val Loss: 0.1176\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=32, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1862, Val Loss: 0.3610\n",
      "Epoch [2/50], Train Loss: 0.1744, Val Loss: 0.3418\n",
      "Epoch [3/50], Train Loss: 0.1727, Val Loss: 0.3234\n",
      "Epoch [4/50], Train Loss: 0.1612, Val Loss: 0.3075\n",
      "Epoch [5/50], Train Loss: 0.1494, Val Loss: 0.2931\n",
      "Epoch [6/50], Train Loss: 0.1419, Val Loss: 0.2797\n",
      "Epoch [7/50], Train Loss: 0.1346, Val Loss: 0.2678\n",
      "Epoch [8/50], Train Loss: 0.1300, Val Loss: 0.2569\n",
      "Epoch [9/50], Train Loss: 0.1242, Val Loss: 0.2465\n",
      "Epoch [10/50], Train Loss: 0.1207, Val Loss: 0.2371\n",
      "Epoch [11/50], Train Loss: 0.1156, Val Loss: 0.2283\n",
      "Epoch [12/50], Train Loss: 0.1092, Val Loss: 0.2205\n",
      "Epoch [13/50], Train Loss: 0.1095, Val Loss: 0.2130\n",
      "Epoch [14/50], Train Loss: 0.1024, Val Loss: 0.2060\n",
      "Epoch [15/50], Train Loss: 0.1012, Val Loss: 0.1997\n",
      "Epoch [16/50], Train Loss: 0.1000, Val Loss: 0.1936\n",
      "Epoch [17/50], Train Loss: 0.0981, Val Loss: 0.1880\n",
      "Epoch [18/50], Train Loss: 0.0916, Val Loss: 0.1831\n",
      "Epoch [19/50], Train Loss: 0.0941, Val Loss: 0.1782\n",
      "Epoch [20/50], Train Loss: 0.0940, Val Loss: 0.1736\n",
      "Epoch [21/50], Train Loss: 0.0881, Val Loss: 0.1696\n",
      "Epoch [22/50], Train Loss: 0.0863, Val Loss: 0.1657\n",
      "Epoch [23/50], Train Loss: 0.0858, Val Loss: 0.1626\n",
      "Epoch [24/50], Train Loss: 0.0873, Val Loss: 0.1592\n",
      "Epoch [25/50], Train Loss: 0.0862, Val Loss: 0.1562\n",
      "Epoch [26/50], Train Loss: 0.0842, Val Loss: 0.1532\n",
      "Epoch [27/50], Train Loss: 0.0831, Val Loss: 0.1504\n",
      "Epoch [28/50], Train Loss: 0.0822, Val Loss: 0.1474\n",
      "Epoch [29/50], Train Loss: 0.0828, Val Loss: 0.1450\n",
      "Epoch [30/50], Train Loss: 0.0787, Val Loss: 0.1427\n",
      "Epoch [31/50], Train Loss: 0.0797, Val Loss: 0.1407\n",
      "Epoch [32/50], Train Loss: 0.0763, Val Loss: 0.1385\n",
      "Epoch [33/50], Train Loss: 0.0765, Val Loss: 0.1367\n",
      "Epoch [34/50], Train Loss: 0.0790, Val Loss: 0.1352\n",
      "Epoch [35/50], Train Loss: 0.0767, Val Loss: 0.1339\n",
      "Epoch [36/50], Train Loss: 0.0751, Val Loss: 0.1322\n",
      "Epoch [37/50], Train Loss: 0.0757, Val Loss: 0.1311\n",
      "Epoch [38/50], Train Loss: 0.0753, Val Loss: 0.1298\n",
      "Epoch [39/50], Train Loss: 0.0766, Val Loss: 0.1284\n",
      "Epoch [40/50], Train Loss: 0.0729, Val Loss: 0.1273\n",
      "Epoch [41/50], Train Loss: 0.0742, Val Loss: 0.1265\n",
      "Epoch [42/50], Train Loss: 0.0754, Val Loss: 0.1257\n",
      "Epoch [43/50], Train Loss: 0.0716, Val Loss: 0.1248\n",
      "Epoch [44/50], Train Loss: 0.0721, Val Loss: 0.1240\n",
      "Epoch [45/50], Train Loss: 0.0699, Val Loss: 0.1229\n",
      "Epoch [46/50], Train Loss: 0.0710, Val Loss: 0.1223\n",
      "Epoch [47/50], Train Loss: 0.0692, Val Loss: 0.1217\n",
      "Epoch [48/50], Train Loss: 0.0702, Val Loss: 0.1210\n",
      "Epoch [49/50], Train Loss: 0.0719, Val Loss: 0.1202\n",
      "Epoch [50/50], Train Loss: 0.0677, Val Loss: 0.1195\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=64, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1707, Val Loss: 0.4262\n",
      "Epoch [2/50], Train Loss: 0.1576, Val Loss: 0.4030\n",
      "Epoch [3/50], Train Loss: 0.1459, Val Loss: 0.3819\n",
      "Epoch [4/50], Train Loss: 0.1355, Val Loss: 0.3625\n",
      "Epoch [5/50], Train Loss: 0.1261, Val Loss: 0.3447\n",
      "Epoch [6/50], Train Loss: 0.1176, Val Loss: 0.3281\n",
      "Epoch [7/50], Train Loss: 0.1099, Val Loss: 0.3127\n",
      "Epoch [8/50], Train Loss: 0.1029, Val Loss: 0.2984\n",
      "Epoch [9/50], Train Loss: 0.0965, Val Loss: 0.2850\n",
      "Epoch [10/50], Train Loss: 0.0907, Val Loss: 0.2724\n",
      "Epoch [11/50], Train Loss: 0.0853, Val Loss: 0.2607\n",
      "Epoch [12/50], Train Loss: 0.0804, Val Loss: 0.2496\n",
      "Epoch [13/50], Train Loss: 0.0760, Val Loss: 0.2392\n",
      "Epoch [14/50], Train Loss: 0.0718, Val Loss: 0.2294\n",
      "Epoch [15/50], Train Loss: 0.0681, Val Loss: 0.2202\n",
      "Epoch [16/50], Train Loss: 0.0646, Val Loss: 0.2115\n",
      "Epoch [17/50], Train Loss: 0.0615, Val Loss: 0.2033\n",
      "Epoch [18/50], Train Loss: 0.0586, Val Loss: 0.1955\n",
      "Epoch [19/50], Train Loss: 0.0559, Val Loss: 0.1882\n",
      "Epoch [20/50], Train Loss: 0.0535, Val Loss: 0.1813\n",
      "Epoch [21/50], Train Loss: 0.0512, Val Loss: 0.1748\n",
      "Epoch [22/50], Train Loss: 0.0492, Val Loss: 0.1686\n",
      "Epoch [23/50], Train Loss: 0.0473, Val Loss: 0.1628\n",
      "Epoch [24/50], Train Loss: 0.0456, Val Loss: 0.1574\n",
      "Epoch [25/50], Train Loss: 0.0441, Val Loss: 0.1522\n",
      "Epoch [26/50], Train Loss: 0.0427, Val Loss: 0.1474\n",
      "Epoch [27/50], Train Loss: 0.0414, Val Loss: 0.1428\n",
      "Epoch [28/50], Train Loss: 0.0402, Val Loss: 0.1385\n",
      "Epoch [29/50], Train Loss: 0.0392, Val Loss: 0.1344\n",
      "Epoch [30/50], Train Loss: 0.0382, Val Loss: 0.1306\n",
      "Epoch [31/50], Train Loss: 0.0374, Val Loss: 0.1269\n",
      "Epoch [32/50], Train Loss: 0.0366, Val Loss: 0.1235\n",
      "Epoch [33/50], Train Loss: 0.0359, Val Loss: 0.1203\n",
      "Epoch [34/50], Train Loss: 0.0352, Val Loss: 0.1173\n",
      "Epoch [35/50], Train Loss: 0.0346, Val Loss: 0.1145\n",
      "Epoch [36/50], Train Loss: 0.0341, Val Loss: 0.1118\n",
      "Epoch [37/50], Train Loss: 0.0336, Val Loss: 0.1093\n",
      "Epoch [38/50], Train Loss: 0.0332, Val Loss: 0.1070\n",
      "Epoch [39/50], Train Loss: 0.0328, Val Loss: 0.1047\n",
      "Epoch [40/50], Train Loss: 0.0324, Val Loss: 0.1027\n",
      "Epoch [41/50], Train Loss: 0.0321, Val Loss: 0.1007\n",
      "Epoch [42/50], Train Loss: 0.0318, Val Loss: 0.0988\n",
      "Epoch [43/50], Train Loss: 0.0315, Val Loss: 0.0971\n",
      "Epoch [44/50], Train Loss: 0.0313, Val Loss: 0.0955\n",
      "Epoch [45/50], Train Loss: 0.0311, Val Loss: 0.0939\n",
      "Epoch [46/50], Train Loss: 0.0309, Val Loss: 0.0925\n",
      "Epoch [47/50], Train Loss: 0.0307, Val Loss: 0.0911\n",
      "Epoch [48/50], Train Loss: 0.0305, Val Loss: 0.0898\n",
      "Epoch [49/50], Train Loss: 0.0304, Val Loss: 0.0886\n",
      "Epoch [50/50], Train Loss: 0.0302, Val Loss: 0.0875\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=64, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1267, Val Loss: 0.3160\n",
      "Epoch [2/50], Train Loss: 0.1178, Val Loss: 0.2988\n",
      "Epoch [3/50], Train Loss: 0.1103, Val Loss: 0.2829\n",
      "Epoch [4/50], Train Loss: 0.1024, Val Loss: 0.2682\n",
      "Epoch [5/50], Train Loss: 0.0955, Val Loss: 0.2545\n",
      "Epoch [6/50], Train Loss: 0.0891, Val Loss: 0.2418\n",
      "Epoch [7/50], Train Loss: 0.0827, Val Loss: 0.2300\n",
      "Epoch [8/50], Train Loss: 0.0785, Val Loss: 0.2190\n",
      "Epoch [9/50], Train Loss: 0.0738, Val Loss: 0.2088\n",
      "Epoch [10/50], Train Loss: 0.0692, Val Loss: 0.1992\n",
      "Epoch [11/50], Train Loss: 0.0657, Val Loss: 0.1904\n",
      "Epoch [12/50], Train Loss: 0.0608, Val Loss: 0.1821\n",
      "Epoch [13/50], Train Loss: 0.0597, Val Loss: 0.1743\n",
      "Epoch [14/50], Train Loss: 0.0553, Val Loss: 0.1671\n",
      "Epoch [15/50], Train Loss: 0.0532, Val Loss: 0.1604\n",
      "Epoch [16/50], Train Loss: 0.0508, Val Loss: 0.1542\n",
      "Epoch [17/50], Train Loss: 0.0485, Val Loss: 0.1484\n",
      "Epoch [18/50], Train Loss: 0.0473, Val Loss: 0.1430\n",
      "Epoch [19/50], Train Loss: 0.0456, Val Loss: 0.1379\n",
      "Epoch [20/50], Train Loss: 0.0440, Val Loss: 0.1332\n",
      "Epoch [21/50], Train Loss: 0.0425, Val Loss: 0.1288\n",
      "Epoch [22/50], Train Loss: 0.0409, Val Loss: 0.1247\n",
      "Epoch [23/50], Train Loss: 0.0403, Val Loss: 0.1210\n",
      "Epoch [24/50], Train Loss: 0.0394, Val Loss: 0.1174\n",
      "Epoch [25/50], Train Loss: 0.0384, Val Loss: 0.1142\n",
      "Epoch [26/50], Train Loss: 0.0373, Val Loss: 0.1111\n",
      "Epoch [27/50], Train Loss: 0.0369, Val Loss: 0.1082\n",
      "Epoch [28/50], Train Loss: 0.0371, Val Loss: 0.1055\n",
      "Epoch [29/50], Train Loss: 0.0361, Val Loss: 0.1030\n",
      "Epoch [30/50], Train Loss: 0.0352, Val Loss: 0.1007\n",
      "Epoch [31/50], Train Loss: 0.0346, Val Loss: 0.0986\n",
      "Epoch [32/50], Train Loss: 0.0338, Val Loss: 0.0966\n",
      "Epoch [33/50], Train Loss: 0.0342, Val Loss: 0.0947\n",
      "Epoch [34/50], Train Loss: 0.0334, Val Loss: 0.0929\n",
      "Epoch [35/50], Train Loss: 0.0325, Val Loss: 0.0913\n",
      "Epoch [36/50], Train Loss: 0.0324, Val Loss: 0.0899\n",
      "Epoch [37/50], Train Loss: 0.0335, Val Loss: 0.0885\n",
      "Epoch [38/50], Train Loss: 0.0327, Val Loss: 0.0871\n",
      "Epoch [39/50], Train Loss: 0.0330, Val Loss: 0.0858\n",
      "Epoch [40/50], Train Loss: 0.0325, Val Loss: 0.0847\n",
      "Epoch [41/50], Train Loss: 0.0321, Val Loss: 0.0836\n",
      "Epoch [42/50], Train Loss: 0.0322, Val Loss: 0.0826\n",
      "Epoch [43/50], Train Loss: 0.0314, Val Loss: 0.0816\n",
      "Epoch [44/50], Train Loss: 0.0317, Val Loss: 0.0806\n",
      "Epoch [45/50], Train Loss: 0.0314, Val Loss: 0.0797\n",
      "Epoch [46/50], Train Loss: 0.0313, Val Loss: 0.0789\n",
      "Epoch [47/50], Train Loss: 0.0313, Val Loss: 0.0782\n",
      "Epoch [48/50], Train Loss: 0.0312, Val Loss: 0.0775\n",
      "Epoch [49/50], Train Loss: 0.0313, Val Loss: 0.0768\n",
      "Epoch [50/50], Train Loss: 0.0309, Val Loss: 0.0761\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=64, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1903, Val Loss: 0.4346\n",
      "Epoch [2/50], Train Loss: 0.1789, Val Loss: 0.4100\n",
      "Epoch [3/50], Train Loss: 0.1625, Val Loss: 0.3879\n",
      "Epoch [4/50], Train Loss: 0.1520, Val Loss: 0.3679\n",
      "Epoch [5/50], Train Loss: 0.1401, Val Loss: 0.3496\n",
      "Epoch [6/50], Train Loss: 0.1325, Val Loss: 0.3328\n",
      "Epoch [7/50], Train Loss: 0.1251, Val Loss: 0.3170\n",
      "Epoch [8/50], Train Loss: 0.1174, Val Loss: 0.3024\n",
      "Epoch [9/50], Train Loss: 0.1113, Val Loss: 0.2888\n",
      "Epoch [10/50], Train Loss: 0.1042, Val Loss: 0.2760\n",
      "Epoch [11/50], Train Loss: 0.0999, Val Loss: 0.2642\n",
      "Epoch [12/50], Train Loss: 0.0957, Val Loss: 0.2533\n",
      "Epoch [13/50], Train Loss: 0.0910, Val Loss: 0.2429\n",
      "Epoch [14/50], Train Loss: 0.0866, Val Loss: 0.2332\n",
      "Epoch [15/50], Train Loss: 0.0836, Val Loss: 0.2241\n",
      "Epoch [16/50], Train Loss: 0.0804, Val Loss: 0.2157\n",
      "Epoch [17/50], Train Loss: 0.0767, Val Loss: 0.2078\n",
      "Epoch [18/50], Train Loss: 0.0729, Val Loss: 0.2005\n",
      "Epoch [19/50], Train Loss: 0.0704, Val Loss: 0.1936\n",
      "Epoch [20/50], Train Loss: 0.0687, Val Loss: 0.1872\n",
      "Epoch [21/50], Train Loss: 0.0675, Val Loss: 0.1811\n",
      "Epoch [22/50], Train Loss: 0.0634, Val Loss: 0.1755\n",
      "Epoch [23/50], Train Loss: 0.0633, Val Loss: 0.1703\n",
      "Epoch [24/50], Train Loss: 0.0616, Val Loss: 0.1653\n",
      "Epoch [25/50], Train Loss: 0.0619, Val Loss: 0.1608\n",
      "Epoch [26/50], Train Loss: 0.0608, Val Loss: 0.1565\n",
      "Epoch [27/50], Train Loss: 0.0580, Val Loss: 0.1526\n",
      "Epoch [28/50], Train Loss: 0.0573, Val Loss: 0.1488\n",
      "Epoch [29/50], Train Loss: 0.0547, Val Loss: 0.1454\n",
      "Epoch [30/50], Train Loss: 0.0548, Val Loss: 0.1422\n",
      "Epoch [31/50], Train Loss: 0.0550, Val Loss: 0.1392\n",
      "Epoch [32/50], Train Loss: 0.0549, Val Loss: 0.1364\n",
      "Epoch [33/50], Train Loss: 0.0524, Val Loss: 0.1339\n",
      "Epoch [34/50], Train Loss: 0.0539, Val Loss: 0.1314\n",
      "Epoch [35/50], Train Loss: 0.0509, Val Loss: 0.1291\n",
      "Epoch [36/50], Train Loss: 0.0514, Val Loss: 0.1271\n",
      "Epoch [37/50], Train Loss: 0.0514, Val Loss: 0.1251\n",
      "Epoch [38/50], Train Loss: 0.0505, Val Loss: 0.1232\n",
      "Epoch [39/50], Train Loss: 0.0510, Val Loss: 0.1213\n",
      "Epoch [40/50], Train Loss: 0.0506, Val Loss: 0.1195\n",
      "Epoch [41/50], Train Loss: 0.0491, Val Loss: 0.1179\n",
      "Epoch [42/50], Train Loss: 0.0511, Val Loss: 0.1163\n",
      "Epoch [43/50], Train Loss: 0.0504, Val Loss: 0.1150\n",
      "Epoch [44/50], Train Loss: 0.0486, Val Loss: 0.1138\n",
      "Epoch [45/50], Train Loss: 0.0500, Val Loss: 0.1125\n",
      "Epoch [46/50], Train Loss: 0.0485, Val Loss: 0.1115\n",
      "Epoch [47/50], Train Loss: 0.0502, Val Loss: 0.1103\n",
      "Epoch [48/50], Train Loss: 0.0497, Val Loss: 0.1093\n",
      "Epoch [49/50], Train Loss: 0.0476, Val Loss: 0.1084\n",
      "Epoch [50/50], Train Loss: 0.0468, Val Loss: 0.1076\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=64, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1510, Val Loss: 0.3947\n",
      "Epoch [2/50], Train Loss: 0.1378, Val Loss: 0.3713\n",
      "Epoch [3/50], Train Loss: 0.1261, Val Loss: 0.3499\n",
      "Epoch [4/50], Train Loss: 0.1158, Val Loss: 0.3303\n",
      "Epoch [5/50], Train Loss: 0.1066, Val Loss: 0.3123\n",
      "Epoch [6/50], Train Loss: 0.0985, Val Loss: 0.2957\n",
      "Epoch [7/50], Train Loss: 0.0911, Val Loss: 0.2803\n",
      "Epoch [8/50], Train Loss: 0.0846, Val Loss: 0.2660\n",
      "Epoch [9/50], Train Loss: 0.0787, Val Loss: 0.2527\n",
      "Epoch [10/50], Train Loss: 0.0735, Val Loss: 0.2404\n",
      "Epoch [11/50], Train Loss: 0.0688, Val Loss: 0.2289\n",
      "Epoch [12/50], Train Loss: 0.0647, Val Loss: 0.2183\n",
      "Epoch [13/50], Train Loss: 0.0610, Val Loss: 0.2084\n",
      "Epoch [14/50], Train Loss: 0.0577, Val Loss: 0.1993\n",
      "Epoch [15/50], Train Loss: 0.0548, Val Loss: 0.1908\n",
      "Epoch [16/50], Train Loss: 0.0522, Val Loss: 0.1830\n",
      "Epoch [17/50], Train Loss: 0.0499, Val Loss: 0.1757\n",
      "Epoch [18/50], Train Loss: 0.0479, Val Loss: 0.1690\n",
      "Epoch [19/50], Train Loss: 0.0462, Val Loss: 0.1629\n",
      "Epoch [20/50], Train Loss: 0.0446, Val Loss: 0.1572\n",
      "Epoch [21/50], Train Loss: 0.0433, Val Loss: 0.1520\n",
      "Epoch [22/50], Train Loss: 0.0421, Val Loss: 0.1472\n",
      "Epoch [23/50], Train Loss: 0.0411, Val Loss: 0.1428\n",
      "Epoch [24/50], Train Loss: 0.0402, Val Loss: 0.1387\n",
      "Epoch [25/50], Train Loss: 0.0394, Val Loss: 0.1350\n",
      "Epoch [26/50], Train Loss: 0.0387, Val Loss: 0.1316\n",
      "Epoch [27/50], Train Loss: 0.0381, Val Loss: 0.1285\n",
      "Epoch [28/50], Train Loss: 0.0376, Val Loss: 0.1256\n",
      "Epoch [29/50], Train Loss: 0.0371, Val Loss: 0.1230\n",
      "Epoch [30/50], Train Loss: 0.0367, Val Loss: 0.1205\n",
      "Epoch [31/50], Train Loss: 0.0363, Val Loss: 0.1183\n",
      "Epoch [32/50], Train Loss: 0.0360, Val Loss: 0.1162\n",
      "Epoch [33/50], Train Loss: 0.0357, Val Loss: 0.1144\n",
      "Epoch [34/50], Train Loss: 0.0354, Val Loss: 0.1126\n",
      "Epoch [35/50], Train Loss: 0.0352, Val Loss: 0.1110\n",
      "Epoch [36/50], Train Loss: 0.0350, Val Loss: 0.1095\n",
      "Epoch [37/50], Train Loss: 0.0348, Val Loss: 0.1082\n",
      "Epoch [38/50], Train Loss: 0.0346, Val Loss: 0.1069\n",
      "Epoch [39/50], Train Loss: 0.0344, Val Loss: 0.1057\n",
      "Epoch [40/50], Train Loss: 0.0343, Val Loss: 0.1046\n",
      "Epoch [41/50], Train Loss: 0.0342, Val Loss: 0.1036\n",
      "Epoch [42/50], Train Loss: 0.0340, Val Loss: 0.1027\n",
      "Epoch [43/50], Train Loss: 0.0339, Val Loss: 0.1018\n",
      "Epoch [44/50], Train Loss: 0.0338, Val Loss: 0.1010\n",
      "Epoch [45/50], Train Loss: 0.0337, Val Loss: 0.1002\n",
      "Epoch [46/50], Train Loss: 0.0336, Val Loss: 0.0995\n",
      "Epoch [47/50], Train Loss: 0.0335, Val Loss: 0.0988\n",
      "Epoch [48/50], Train Loss: 0.0334, Val Loss: 0.0982\n",
      "Epoch [49/50], Train Loss: 0.0333, Val Loss: 0.0976\n",
      "Epoch [50/50], Train Loss: 0.0332, Val Loss: 0.0970\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=64, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1542, Val Loss: 0.3647\n",
      "Epoch [2/50], Train Loss: 0.1404, Val Loss: 0.3402\n",
      "Epoch [3/50], Train Loss: 0.1284, Val Loss: 0.3183\n",
      "Epoch [4/50], Train Loss: 0.1185, Val Loss: 0.2984\n",
      "Epoch [5/50], Train Loss: 0.1090, Val Loss: 0.2803\n",
      "Epoch [6/50], Train Loss: 0.1003, Val Loss: 0.2637\n",
      "Epoch [7/50], Train Loss: 0.0931, Val Loss: 0.2484\n",
      "Epoch [8/50], Train Loss: 0.0868, Val Loss: 0.2344\n",
      "Epoch [9/50], Train Loss: 0.0806, Val Loss: 0.2213\n",
      "Epoch [10/50], Train Loss: 0.0751, Val Loss: 0.2093\n",
      "Epoch [11/50], Train Loss: 0.0705, Val Loss: 0.1982\n",
      "Epoch [12/50], Train Loss: 0.0664, Val Loss: 0.1881\n",
      "Epoch [13/50], Train Loss: 0.0630, Val Loss: 0.1785\n",
      "Epoch [14/50], Train Loss: 0.0597, Val Loss: 0.1697\n",
      "Epoch [15/50], Train Loss: 0.0560, Val Loss: 0.1616\n",
      "Epoch [16/50], Train Loss: 0.0533, Val Loss: 0.1540\n",
      "Epoch [17/50], Train Loss: 0.0507, Val Loss: 0.1470\n",
      "Epoch [18/50], Train Loss: 0.0487, Val Loss: 0.1407\n",
      "Epoch [19/50], Train Loss: 0.0473, Val Loss: 0.1347\n",
      "Epoch [20/50], Train Loss: 0.0447, Val Loss: 0.1293\n",
      "Epoch [21/50], Train Loss: 0.0441, Val Loss: 0.1243\n",
      "Epoch [22/50], Train Loss: 0.0429, Val Loss: 0.1196\n",
      "Epoch [23/50], Train Loss: 0.0414, Val Loss: 0.1155\n",
      "Epoch [24/50], Train Loss: 0.0401, Val Loss: 0.1116\n",
      "Epoch [25/50], Train Loss: 0.0394, Val Loss: 0.1081\n",
      "Epoch [26/50], Train Loss: 0.0393, Val Loss: 0.1048\n",
      "Epoch [27/50], Train Loss: 0.0376, Val Loss: 0.1019\n",
      "Epoch [28/50], Train Loss: 0.0372, Val Loss: 0.0992\n",
      "Epoch [29/50], Train Loss: 0.0372, Val Loss: 0.0967\n",
      "Epoch [30/50], Train Loss: 0.0360, Val Loss: 0.0942\n",
      "Epoch [31/50], Train Loss: 0.0365, Val Loss: 0.0922\n",
      "Epoch [32/50], Train Loss: 0.0356, Val Loss: 0.0903\n",
      "Epoch [33/50], Train Loss: 0.0348, Val Loss: 0.0885\n",
      "Epoch [34/50], Train Loss: 0.0354, Val Loss: 0.0869\n",
      "Epoch [35/50], Train Loss: 0.0339, Val Loss: 0.0855\n",
      "Epoch [36/50], Train Loss: 0.0348, Val Loss: 0.0841\n",
      "Epoch [37/50], Train Loss: 0.0345, Val Loss: 0.0828\n",
      "Epoch [38/50], Train Loss: 0.0343, Val Loss: 0.0816\n",
      "Epoch [39/50], Train Loss: 0.0333, Val Loss: 0.0806\n",
      "Epoch [40/50], Train Loss: 0.0340, Val Loss: 0.0796\n",
      "Epoch [41/50], Train Loss: 0.0337, Val Loss: 0.0787\n",
      "Epoch [42/50], Train Loss: 0.0330, Val Loss: 0.0779\n",
      "Epoch [43/50], Train Loss: 0.0333, Val Loss: 0.0772\n",
      "Epoch [44/50], Train Loss: 0.0331, Val Loss: 0.0763\n",
      "Epoch [45/50], Train Loss: 0.0331, Val Loss: 0.0756\n",
      "Epoch [46/50], Train Loss: 0.0334, Val Loss: 0.0751\n",
      "Epoch [47/50], Train Loss: 0.0333, Val Loss: 0.0745\n",
      "Epoch [48/50], Train Loss: 0.0326, Val Loss: 0.0739\n",
      "Epoch [49/50], Train Loss: 0.0330, Val Loss: 0.0734\n",
      "Epoch [50/50], Train Loss: 0.0326, Val Loss: 0.0728\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=64, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1272, Val Loss: 0.3097\n",
      "Epoch [2/50], Train Loss: 0.1171, Val Loss: 0.2858\n",
      "Epoch [3/50], Train Loss: 0.1082, Val Loss: 0.2646\n",
      "Epoch [4/50], Train Loss: 0.1010, Val Loss: 0.2455\n",
      "Epoch [5/50], Train Loss: 0.0930, Val Loss: 0.2288\n",
      "Epoch [6/50], Train Loss: 0.0833, Val Loss: 0.2138\n",
      "Epoch [7/50], Train Loss: 0.0785, Val Loss: 0.2006\n",
      "Epoch [8/50], Train Loss: 0.0748, Val Loss: 0.1888\n",
      "Epoch [9/50], Train Loss: 0.0703, Val Loss: 0.1782\n",
      "Epoch [10/50], Train Loss: 0.0691, Val Loss: 0.1685\n",
      "Epoch [11/50], Train Loss: 0.0657, Val Loss: 0.1604\n",
      "Epoch [12/50], Train Loss: 0.0638, Val Loss: 0.1527\n",
      "Epoch [13/50], Train Loss: 0.0629, Val Loss: 0.1456\n",
      "Epoch [14/50], Train Loss: 0.0579, Val Loss: 0.1398\n",
      "Epoch [15/50], Train Loss: 0.0558, Val Loss: 0.1346\n",
      "Epoch [16/50], Train Loss: 0.0552, Val Loss: 0.1302\n",
      "Epoch [17/50], Train Loss: 0.0579, Val Loss: 0.1258\n",
      "Epoch [18/50], Train Loss: 0.0561, Val Loss: 0.1217\n",
      "Epoch [19/50], Train Loss: 0.0529, Val Loss: 0.1182\n",
      "Epoch [20/50], Train Loss: 0.0536, Val Loss: 0.1152\n",
      "Epoch [21/50], Train Loss: 0.0530, Val Loss: 0.1127\n",
      "Epoch [22/50], Train Loss: 0.0517, Val Loss: 0.1105\n",
      "Epoch [23/50], Train Loss: 0.0520, Val Loss: 0.1082\n",
      "Epoch [24/50], Train Loss: 0.0489, Val Loss: 0.1064\n",
      "Epoch [25/50], Train Loss: 0.0496, Val Loss: 0.1045\n",
      "Epoch [26/50], Train Loss: 0.0512, Val Loss: 0.1029\n",
      "Epoch [27/50], Train Loss: 0.0493, Val Loss: 0.1013\n",
      "Epoch [28/50], Train Loss: 0.0491, Val Loss: 0.1000\n",
      "Epoch [29/50], Train Loss: 0.0495, Val Loss: 0.0988\n",
      "Epoch [30/50], Train Loss: 0.0492, Val Loss: 0.0975\n",
      "Epoch [31/50], Train Loss: 0.0492, Val Loss: 0.0964\n",
      "Epoch [32/50], Train Loss: 0.0483, Val Loss: 0.0955\n",
      "Epoch [33/50], Train Loss: 0.0477, Val Loss: 0.0946\n",
      "Epoch [34/50], Train Loss: 0.0478, Val Loss: 0.0938\n",
      "Epoch [35/50], Train Loss: 0.0499, Val Loss: 0.0928\n",
      "Epoch [36/50], Train Loss: 0.0482, Val Loss: 0.0921\n",
      "Epoch [37/50], Train Loss: 0.0476, Val Loss: 0.0912\n",
      "Epoch [38/50], Train Loss: 0.0484, Val Loss: 0.0908\n",
      "Epoch [39/50], Train Loss: 0.0475, Val Loss: 0.0904\n",
      "Epoch [40/50], Train Loss: 0.0470, Val Loss: 0.0899\n",
      "Epoch [41/50], Train Loss: 0.0473, Val Loss: 0.0895\n",
      "Epoch [42/50], Train Loss: 0.0478, Val Loss: 0.0889\n",
      "Epoch [43/50], Train Loss: 0.0467, Val Loss: 0.0885\n",
      "Epoch [44/50], Train Loss: 0.0464, Val Loss: 0.0880\n",
      "Epoch [45/50], Train Loss: 0.0459, Val Loss: 0.0874\n",
      "Epoch [46/50], Train Loss: 0.0460, Val Loss: 0.0867\n",
      "Epoch [47/50], Train Loss: 0.0463, Val Loss: 0.0861\n",
      "Epoch [48/50], Train Loss: 0.0446, Val Loss: 0.0855\n",
      "Epoch [49/50], Train Loss: 0.0463, Val Loss: 0.0849\n",
      "Epoch [50/50], Train Loss: 0.0449, Val Loss: 0.0844\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=64, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1118, Val Loss: 0.3101\n",
      "Epoch [2/50], Train Loss: 0.1040, Val Loss: 0.2965\n",
      "Epoch [3/50], Train Loss: 0.0970, Val Loss: 0.2839\n",
      "Epoch [4/50], Train Loss: 0.0906, Val Loss: 0.2721\n",
      "Epoch [5/50], Train Loss: 0.0849, Val Loss: 0.2611\n",
      "Epoch [6/50], Train Loss: 0.0798, Val Loss: 0.2508\n",
      "Epoch [7/50], Train Loss: 0.0751, Val Loss: 0.2410\n",
      "Epoch [8/50], Train Loss: 0.0708, Val Loss: 0.2318\n",
      "Epoch [9/50], Train Loss: 0.0669, Val Loss: 0.2231\n",
      "Epoch [10/50], Train Loss: 0.0634, Val Loss: 0.2148\n",
      "Epoch [11/50], Train Loss: 0.0603, Val Loss: 0.2071\n",
      "Epoch [12/50], Train Loss: 0.0574, Val Loss: 0.1997\n",
      "Epoch [13/50], Train Loss: 0.0548, Val Loss: 0.1928\n",
      "Epoch [14/50], Train Loss: 0.0525, Val Loss: 0.1862\n",
      "Epoch [15/50], Train Loss: 0.0504, Val Loss: 0.1801\n",
      "Epoch [16/50], Train Loss: 0.0485, Val Loss: 0.1743\n",
      "Epoch [17/50], Train Loss: 0.0469, Val Loss: 0.1688\n",
      "Epoch [18/50], Train Loss: 0.0454, Val Loss: 0.1637\n",
      "Epoch [19/50], Train Loss: 0.0441, Val Loss: 0.1590\n",
      "Epoch [20/50], Train Loss: 0.0430, Val Loss: 0.1545\n",
      "Epoch [21/50], Train Loss: 0.0419, Val Loss: 0.1504\n",
      "Epoch [22/50], Train Loss: 0.0411, Val Loss: 0.1465\n",
      "Epoch [23/50], Train Loss: 0.0403, Val Loss: 0.1429\n",
      "Epoch [24/50], Train Loss: 0.0396, Val Loss: 0.1395\n",
      "Epoch [25/50], Train Loss: 0.0390, Val Loss: 0.1364\n",
      "Epoch [26/50], Train Loss: 0.0385, Val Loss: 0.1335\n",
      "Epoch [27/50], Train Loss: 0.0380, Val Loss: 0.1308\n",
      "Epoch [28/50], Train Loss: 0.0376, Val Loss: 0.1284\n",
      "Epoch [29/50], Train Loss: 0.0373, Val Loss: 0.1260\n",
      "Epoch [30/50], Train Loss: 0.0370, Val Loss: 0.1239\n",
      "Epoch [31/50], Train Loss: 0.0367, Val Loss: 0.1219\n",
      "Epoch [32/50], Train Loss: 0.0365, Val Loss: 0.1201\n",
      "Epoch [33/50], Train Loss: 0.0363, Val Loss: 0.1184\n",
      "Epoch [34/50], Train Loss: 0.0361, Val Loss: 0.1168\n",
      "Epoch [35/50], Train Loss: 0.0360, Val Loss: 0.1154\n",
      "Epoch [36/50], Train Loss: 0.0358, Val Loss: 0.1140\n",
      "Epoch [37/50], Train Loss: 0.0357, Val Loss: 0.1128\n",
      "Epoch [38/50], Train Loss: 0.0356, Val Loss: 0.1116\n",
      "Epoch [39/50], Train Loss: 0.0355, Val Loss: 0.1106\n",
      "Epoch [40/50], Train Loss: 0.0354, Val Loss: 0.1096\n",
      "Epoch [41/50], Train Loss: 0.0353, Val Loss: 0.1087\n",
      "Epoch [42/50], Train Loss: 0.0353, Val Loss: 0.1078\n",
      "Epoch [43/50], Train Loss: 0.0352, Val Loss: 0.1070\n",
      "Epoch [44/50], Train Loss: 0.0351, Val Loss: 0.1063\n",
      "Epoch [45/50], Train Loss: 0.0351, Val Loss: 0.1056\n",
      "Epoch [46/50], Train Loss: 0.0350, Val Loss: 0.1049\n",
      "Epoch [47/50], Train Loss: 0.0350, Val Loss: 0.1043\n",
      "Epoch [48/50], Train Loss: 0.0349, Val Loss: 0.1038\n",
      "Epoch [49/50], Train Loss: 0.0349, Val Loss: 0.1032\n",
      "Epoch [50/50], Train Loss: 0.0348, Val Loss: 0.1027\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=64, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1629, Val Loss: 0.4231\n",
      "Epoch [2/50], Train Loss: 0.1478, Val Loss: 0.3957\n",
      "Epoch [3/50], Train Loss: 0.1344, Val Loss: 0.3710\n",
      "Epoch [4/50], Train Loss: 0.1237, Val Loss: 0.3484\n",
      "Epoch [5/50], Train Loss: 0.1141, Val Loss: 0.3279\n",
      "Epoch [6/50], Train Loss: 0.1043, Val Loss: 0.3087\n",
      "Epoch [7/50], Train Loss: 0.0965, Val Loss: 0.2915\n",
      "Epoch [8/50], Train Loss: 0.0905, Val Loss: 0.2756\n",
      "Epoch [9/50], Train Loss: 0.0831, Val Loss: 0.2607\n",
      "Epoch [10/50], Train Loss: 0.0784, Val Loss: 0.2471\n",
      "Epoch [11/50], Train Loss: 0.0732, Val Loss: 0.2343\n",
      "Epoch [12/50], Train Loss: 0.0693, Val Loss: 0.2226\n",
      "Epoch [13/50], Train Loss: 0.0640, Val Loss: 0.2119\n",
      "Epoch [14/50], Train Loss: 0.0620, Val Loss: 0.2017\n",
      "Epoch [15/50], Train Loss: 0.0595, Val Loss: 0.1925\n",
      "Epoch [16/50], Train Loss: 0.0558, Val Loss: 0.1842\n",
      "Epoch [17/50], Train Loss: 0.0546, Val Loss: 0.1766\n",
      "Epoch [18/50], Train Loss: 0.0526, Val Loss: 0.1697\n",
      "Epoch [19/50], Train Loss: 0.0511, Val Loss: 0.1634\n",
      "Epoch [20/50], Train Loss: 0.0488, Val Loss: 0.1579\n",
      "Epoch [21/50], Train Loss: 0.0487, Val Loss: 0.1525\n",
      "Epoch [22/50], Train Loss: 0.0479, Val Loss: 0.1479\n",
      "Epoch [23/50], Train Loss: 0.0469, Val Loss: 0.1436\n",
      "Epoch [24/50], Train Loss: 0.0464, Val Loss: 0.1397\n",
      "Epoch [25/50], Train Loss: 0.0457, Val Loss: 0.1361\n",
      "Epoch [26/50], Train Loss: 0.0455, Val Loss: 0.1331\n",
      "Epoch [27/50], Train Loss: 0.0451, Val Loss: 0.1303\n",
      "Epoch [28/50], Train Loss: 0.0450, Val Loss: 0.1279\n",
      "Epoch [29/50], Train Loss: 0.0438, Val Loss: 0.1257\n",
      "Epoch [30/50], Train Loss: 0.0436, Val Loss: 0.1235\n",
      "Epoch [31/50], Train Loss: 0.0435, Val Loss: 0.1215\n",
      "Epoch [32/50], Train Loss: 0.0438, Val Loss: 0.1201\n",
      "Epoch [33/50], Train Loss: 0.0436, Val Loss: 0.1187\n",
      "Epoch [34/50], Train Loss: 0.0438, Val Loss: 0.1174\n",
      "Epoch [35/50], Train Loss: 0.0431, Val Loss: 0.1162\n",
      "Epoch [36/50], Train Loss: 0.0441, Val Loss: 0.1150\n",
      "Epoch [37/50], Train Loss: 0.0428, Val Loss: 0.1140\n",
      "Epoch [38/50], Train Loss: 0.0428, Val Loss: 0.1131\n",
      "Epoch [39/50], Train Loss: 0.0437, Val Loss: 0.1124\n",
      "Epoch [40/50], Train Loss: 0.0429, Val Loss: 0.1115\n",
      "Epoch [41/50], Train Loss: 0.0425, Val Loss: 0.1106\n",
      "Epoch [42/50], Train Loss: 0.0427, Val Loss: 0.1099\n",
      "Epoch [43/50], Train Loss: 0.0441, Val Loss: 0.1093\n",
      "Epoch [44/50], Train Loss: 0.0420, Val Loss: 0.1086\n",
      "Epoch [45/50], Train Loss: 0.0422, Val Loss: 0.1081\n",
      "Epoch [46/50], Train Loss: 0.0416, Val Loss: 0.1077\n",
      "Epoch [47/50], Train Loss: 0.0424, Val Loss: 0.1073\n",
      "Epoch [48/50], Train Loss: 0.0412, Val Loss: 0.1068\n",
      "Epoch [49/50], Train Loss: 0.0420, Val Loss: 0.1064\n",
      "Epoch [50/50], Train Loss: 0.0418, Val Loss: 0.1060\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=64, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1373, Val Loss: 0.3257\n",
      "Epoch [2/50], Train Loss: 0.1269, Val Loss: 0.3058\n",
      "Epoch [3/50], Train Loss: 0.1186, Val Loss: 0.2889\n",
      "Epoch [4/50], Train Loss: 0.1090, Val Loss: 0.2737\n",
      "Epoch [5/50], Train Loss: 0.1042, Val Loss: 0.2597\n",
      "Epoch [6/50], Train Loss: 0.0985, Val Loss: 0.2472\n",
      "Epoch [7/50], Train Loss: 0.0934, Val Loss: 0.2357\n",
      "Epoch [8/50], Train Loss: 0.0865, Val Loss: 0.2252\n",
      "Epoch [9/50], Train Loss: 0.0831, Val Loss: 0.2158\n",
      "Epoch [10/50], Train Loss: 0.0821, Val Loss: 0.2068\n",
      "Epoch [11/50], Train Loss: 0.0779, Val Loss: 0.1985\n",
      "Epoch [12/50], Train Loss: 0.0743, Val Loss: 0.1913\n",
      "Epoch [13/50], Train Loss: 0.0710, Val Loss: 0.1850\n",
      "Epoch [14/50], Train Loss: 0.0715, Val Loss: 0.1787\n",
      "Epoch [15/50], Train Loss: 0.0691, Val Loss: 0.1729\n",
      "Epoch [16/50], Train Loss: 0.0670, Val Loss: 0.1677\n",
      "Epoch [17/50], Train Loss: 0.0656, Val Loss: 0.1627\n",
      "Epoch [18/50], Train Loss: 0.0646, Val Loss: 0.1582\n",
      "Epoch [19/50], Train Loss: 0.0623, Val Loss: 0.1542\n",
      "Epoch [20/50], Train Loss: 0.0640, Val Loss: 0.1504\n",
      "Epoch [21/50], Train Loss: 0.0609, Val Loss: 0.1468\n",
      "Epoch [22/50], Train Loss: 0.0591, Val Loss: 0.1434\n",
      "Epoch [23/50], Train Loss: 0.0605, Val Loss: 0.1405\n",
      "Epoch [24/50], Train Loss: 0.0592, Val Loss: 0.1376\n",
      "Epoch [25/50], Train Loss: 0.0571, Val Loss: 0.1348\n",
      "Epoch [26/50], Train Loss: 0.0557, Val Loss: 0.1321\n",
      "Epoch [27/50], Train Loss: 0.0568, Val Loss: 0.1300\n",
      "Epoch [28/50], Train Loss: 0.0565, Val Loss: 0.1280\n",
      "Epoch [29/50], Train Loss: 0.0552, Val Loss: 0.1260\n",
      "Epoch [30/50], Train Loss: 0.0560, Val Loss: 0.1242\n",
      "Epoch [31/50], Train Loss: 0.0563, Val Loss: 0.1226\n",
      "Epoch [32/50], Train Loss: 0.0537, Val Loss: 0.1213\n",
      "Epoch [33/50], Train Loss: 0.0553, Val Loss: 0.1198\n",
      "Epoch [34/50], Train Loss: 0.0563, Val Loss: 0.1183\n",
      "Epoch [35/50], Train Loss: 0.0529, Val Loss: 0.1171\n",
      "Epoch [36/50], Train Loss: 0.0541, Val Loss: 0.1158\n",
      "Epoch [37/50], Train Loss: 0.0556, Val Loss: 0.1145\n",
      "Epoch [38/50], Train Loss: 0.0533, Val Loss: 0.1137\n",
      "Epoch [39/50], Train Loss: 0.0537, Val Loss: 0.1128\n",
      "Epoch [40/50], Train Loss: 0.0539, Val Loss: 0.1116\n",
      "Epoch [41/50], Train Loss: 0.0528, Val Loss: 0.1106\n",
      "Epoch [42/50], Train Loss: 0.0514, Val Loss: 0.1097\n",
      "Epoch [43/50], Train Loss: 0.0515, Val Loss: 0.1091\n",
      "Epoch [44/50], Train Loss: 0.0530, Val Loss: 0.1083\n",
      "Epoch [45/50], Train Loss: 0.0534, Val Loss: 0.1078\n",
      "Epoch [46/50], Train Loss: 0.0517, Val Loss: 0.1070\n",
      "Epoch [47/50], Train Loss: 0.0542, Val Loss: 0.1062\n",
      "Epoch [48/50], Train Loss: 0.0518, Val Loss: 0.1056\n",
      "Epoch [49/50], Train Loss: 0.0514, Val Loss: 0.1051\n",
      "Epoch [50/50], Train Loss: 0.0519, Val Loss: 0.1046\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=128, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1294, Val Loss: 0.3350\n",
      "Epoch [2/50], Train Loss: 0.1203, Val Loss: 0.3171\n",
      "Epoch [3/50], Train Loss: 0.1121, Val Loss: 0.3005\n",
      "Epoch [4/50], Train Loss: 0.1045, Val Loss: 0.2849\n",
      "Epoch [5/50], Train Loss: 0.0976, Val Loss: 0.2703\n",
      "Epoch [6/50], Train Loss: 0.0912, Val Loss: 0.2566\n",
      "Epoch [7/50], Train Loss: 0.0853, Val Loss: 0.2437\n",
      "Epoch [8/50], Train Loss: 0.0799, Val Loss: 0.2316\n",
      "Epoch [9/50], Train Loss: 0.0749, Val Loss: 0.2203\n",
      "Epoch [10/50], Train Loss: 0.0703, Val Loss: 0.2095\n",
      "Epoch [11/50], Train Loss: 0.0661, Val Loss: 0.1995\n",
      "Epoch [12/50], Train Loss: 0.0623, Val Loss: 0.1900\n",
      "Epoch [13/50], Train Loss: 0.0587, Val Loss: 0.1811\n",
      "Epoch [14/50], Train Loss: 0.0555, Val Loss: 0.1728\n",
      "Epoch [15/50], Train Loss: 0.0526, Val Loss: 0.1649\n",
      "Epoch [16/50], Train Loss: 0.0499, Val Loss: 0.1576\n",
      "Epoch [17/50], Train Loss: 0.0475, Val Loss: 0.1507\n",
      "Epoch [18/50], Train Loss: 0.0453, Val Loss: 0.1442\n",
      "Epoch [19/50], Train Loss: 0.0433, Val Loss: 0.1382\n",
      "Epoch [20/50], Train Loss: 0.0415, Val Loss: 0.1325\n",
      "Epoch [21/50], Train Loss: 0.0399, Val Loss: 0.1273\n",
      "Epoch [22/50], Train Loss: 0.0385, Val Loss: 0.1224\n",
      "Epoch [23/50], Train Loss: 0.0372, Val Loss: 0.1178\n",
      "Epoch [24/50], Train Loss: 0.0360, Val Loss: 0.1135\n",
      "Epoch [25/50], Train Loss: 0.0350, Val Loss: 0.1096\n",
      "Epoch [26/50], Train Loss: 0.0340, Val Loss: 0.1059\n",
      "Epoch [27/50], Train Loss: 0.0332, Val Loss: 0.1024\n",
      "Epoch [28/50], Train Loss: 0.0325, Val Loss: 0.0993\n",
      "Epoch [29/50], Train Loss: 0.0319, Val Loss: 0.0963\n",
      "Epoch [30/50], Train Loss: 0.0313, Val Loss: 0.0936\n",
      "Epoch [31/50], Train Loss: 0.0308, Val Loss: 0.0911\n",
      "Epoch [32/50], Train Loss: 0.0303, Val Loss: 0.0887\n",
      "Epoch [33/50], Train Loss: 0.0299, Val Loss: 0.0865\n",
      "Epoch [34/50], Train Loss: 0.0296, Val Loss: 0.0845\n",
      "Epoch [35/50], Train Loss: 0.0293, Val Loss: 0.0826\n",
      "Epoch [36/50], Train Loss: 0.0290, Val Loss: 0.0809\n",
      "Epoch [37/50], Train Loss: 0.0288, Val Loss: 0.0793\n",
      "Epoch [38/50], Train Loss: 0.0285, Val Loss: 0.0778\n",
      "Epoch [39/50], Train Loss: 0.0283, Val Loss: 0.0764\n",
      "Epoch [40/50], Train Loss: 0.0282, Val Loss: 0.0751\n",
      "Epoch [41/50], Train Loss: 0.0280, Val Loss: 0.0739\n",
      "Epoch [42/50], Train Loss: 0.0279, Val Loss: 0.0727\n",
      "Epoch [43/50], Train Loss: 0.0277, Val Loss: 0.0717\n",
      "Epoch [44/50], Train Loss: 0.0276, Val Loss: 0.0707\n",
      "Epoch [45/50], Train Loss: 0.0275, Val Loss: 0.0698\n",
      "Epoch [46/50], Train Loss: 0.0274, Val Loss: 0.0690\n",
      "Epoch [47/50], Train Loss: 0.0273, Val Loss: 0.0682\n",
      "Epoch [48/50], Train Loss: 0.0272, Val Loss: 0.0674\n",
      "Epoch [49/50], Train Loss: 0.0271, Val Loss: 0.0667\n",
      "Epoch [50/50], Train Loss: 0.0270, Val Loss: 0.0661\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=128, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1547, Val Loss: 0.3893\n",
      "Epoch [2/50], Train Loss: 0.1442, Val Loss: 0.3696\n",
      "Epoch [3/50], Train Loss: 0.1359, Val Loss: 0.3515\n",
      "Epoch [4/50], Train Loss: 0.1274, Val Loss: 0.3346\n",
      "Epoch [5/50], Train Loss: 0.1198, Val Loss: 0.3190\n",
      "Epoch [6/50], Train Loss: 0.1128, Val Loss: 0.3043\n",
      "Epoch [7/50], Train Loss: 0.1063, Val Loss: 0.2905\n",
      "Epoch [8/50], Train Loss: 0.1004, Val Loss: 0.2775\n",
      "Epoch [9/50], Train Loss: 0.0955, Val Loss: 0.2653\n",
      "Epoch [10/50], Train Loss: 0.0897, Val Loss: 0.2538\n",
      "Epoch [11/50], Train Loss: 0.0839, Val Loss: 0.2429\n",
      "Epoch [12/50], Train Loss: 0.0799, Val Loss: 0.2326\n",
      "Epoch [13/50], Train Loss: 0.0759, Val Loss: 0.2229\n",
      "Epoch [14/50], Train Loss: 0.0725, Val Loss: 0.2137\n",
      "Epoch [15/50], Train Loss: 0.0693, Val Loss: 0.2049\n",
      "Epoch [16/50], Train Loss: 0.0659, Val Loss: 0.1967\n",
      "Epoch [17/50], Train Loss: 0.0640, Val Loss: 0.1889\n",
      "Epoch [18/50], Train Loss: 0.0605, Val Loss: 0.1815\n",
      "Epoch [19/50], Train Loss: 0.0575, Val Loss: 0.1745\n",
      "Epoch [20/50], Train Loss: 0.0550, Val Loss: 0.1678\n",
      "Epoch [21/50], Train Loss: 0.0534, Val Loss: 0.1616\n",
      "Epoch [22/50], Train Loss: 0.0515, Val Loss: 0.1556\n",
      "Epoch [23/50], Train Loss: 0.0499, Val Loss: 0.1500\n",
      "Epoch [24/50], Train Loss: 0.0474, Val Loss: 0.1447\n",
      "Epoch [25/50], Train Loss: 0.0460, Val Loss: 0.1397\n",
      "Epoch [26/50], Train Loss: 0.0448, Val Loss: 0.1350\n",
      "Epoch [27/50], Train Loss: 0.0434, Val Loss: 0.1306\n",
      "Epoch [28/50], Train Loss: 0.0422, Val Loss: 0.1264\n",
      "Epoch [29/50], Train Loss: 0.0408, Val Loss: 0.1225\n",
      "Epoch [30/50], Train Loss: 0.0397, Val Loss: 0.1188\n",
      "Epoch [31/50], Train Loss: 0.0389, Val Loss: 0.1154\n",
      "Epoch [32/50], Train Loss: 0.0390, Val Loss: 0.1121\n",
      "Epoch [33/50], Train Loss: 0.0381, Val Loss: 0.1090\n",
      "Epoch [34/50], Train Loss: 0.0366, Val Loss: 0.1061\n",
      "Epoch [35/50], Train Loss: 0.0356, Val Loss: 0.1034\n",
      "Epoch [36/50], Train Loss: 0.0355, Val Loss: 0.1009\n",
      "Epoch [37/50], Train Loss: 0.0348, Val Loss: 0.0985\n",
      "Epoch [38/50], Train Loss: 0.0343, Val Loss: 0.0963\n",
      "Epoch [39/50], Train Loss: 0.0340, Val Loss: 0.0942\n",
      "Epoch [40/50], Train Loss: 0.0339, Val Loss: 0.0922\n",
      "Epoch [41/50], Train Loss: 0.0335, Val Loss: 0.0903\n",
      "Epoch [42/50], Train Loss: 0.0333, Val Loss: 0.0885\n",
      "Epoch [43/50], Train Loss: 0.0326, Val Loss: 0.0869\n",
      "Epoch [44/50], Train Loss: 0.0323, Val Loss: 0.0854\n",
      "Epoch [45/50], Train Loss: 0.0324, Val Loss: 0.0840\n",
      "Epoch [46/50], Train Loss: 0.0317, Val Loss: 0.0825\n",
      "Epoch [47/50], Train Loss: 0.0311, Val Loss: 0.0813\n",
      "Epoch [48/50], Train Loss: 0.0315, Val Loss: 0.0801\n",
      "Epoch [49/50], Train Loss: 0.0308, Val Loss: 0.0790\n",
      "Epoch [50/50], Train Loss: 0.0304, Val Loss: 0.0779\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=128, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1273, Val Loss: 0.3107\n",
      "Epoch [2/50], Train Loss: 0.1187, Val Loss: 0.2972\n",
      "Epoch [3/50], Train Loss: 0.1143, Val Loss: 0.2846\n",
      "Epoch [4/50], Train Loss: 0.1072, Val Loss: 0.2728\n",
      "Epoch [5/50], Train Loss: 0.1032, Val Loss: 0.2617\n",
      "Epoch [6/50], Train Loss: 0.0971, Val Loss: 0.2513\n",
      "Epoch [7/50], Train Loss: 0.0922, Val Loss: 0.2416\n",
      "Epoch [8/50], Train Loss: 0.0885, Val Loss: 0.2326\n",
      "Epoch [9/50], Train Loss: 0.0851, Val Loss: 0.2240\n",
      "Epoch [10/50], Train Loss: 0.0813, Val Loss: 0.2158\n",
      "Epoch [11/50], Train Loss: 0.0777, Val Loss: 0.2081\n",
      "Epoch [12/50], Train Loss: 0.0746, Val Loss: 0.2009\n",
      "Epoch [13/50], Train Loss: 0.0717, Val Loss: 0.1941\n",
      "Epoch [14/50], Train Loss: 0.0685, Val Loss: 0.1877\n",
      "Epoch [15/50], Train Loss: 0.0666, Val Loss: 0.1816\n",
      "Epoch [16/50], Train Loss: 0.0650, Val Loss: 0.1757\n",
      "Epoch [17/50], Train Loss: 0.0619, Val Loss: 0.1703\n",
      "Epoch [18/50], Train Loss: 0.0606, Val Loss: 0.1651\n",
      "Epoch [19/50], Train Loss: 0.0587, Val Loss: 0.1602\n",
      "Epoch [20/50], Train Loss: 0.0569, Val Loss: 0.1555\n",
      "Epoch [21/50], Train Loss: 0.0555, Val Loss: 0.1511\n",
      "Epoch [22/50], Train Loss: 0.0544, Val Loss: 0.1469\n",
      "Epoch [23/50], Train Loss: 0.0525, Val Loss: 0.1429\n",
      "Epoch [24/50], Train Loss: 0.0513, Val Loss: 0.1391\n",
      "Epoch [25/50], Train Loss: 0.0506, Val Loss: 0.1356\n",
      "Epoch [26/50], Train Loss: 0.0493, Val Loss: 0.1322\n",
      "Epoch [27/50], Train Loss: 0.0478, Val Loss: 0.1291\n",
      "Epoch [28/50], Train Loss: 0.0473, Val Loss: 0.1260\n",
      "Epoch [29/50], Train Loss: 0.0460, Val Loss: 0.1231\n",
      "Epoch [30/50], Train Loss: 0.0461, Val Loss: 0.1204\n",
      "Epoch [31/50], Train Loss: 0.0454, Val Loss: 0.1178\n",
      "Epoch [32/50], Train Loss: 0.0441, Val Loss: 0.1154\n",
      "Epoch [33/50], Train Loss: 0.0445, Val Loss: 0.1130\n",
      "Epoch [34/50], Train Loss: 0.0428, Val Loss: 0.1108\n",
      "Epoch [35/50], Train Loss: 0.0428, Val Loss: 0.1088\n",
      "Epoch [36/50], Train Loss: 0.0422, Val Loss: 0.1068\n",
      "Epoch [37/50], Train Loss: 0.0425, Val Loss: 0.1050\n",
      "Epoch [38/50], Train Loss: 0.0412, Val Loss: 0.1033\n",
      "Epoch [39/50], Train Loss: 0.0411, Val Loss: 0.1015\n",
      "Epoch [40/50], Train Loss: 0.0410, Val Loss: 0.1000\n",
      "Epoch [41/50], Train Loss: 0.0401, Val Loss: 0.0985\n",
      "Epoch [42/50], Train Loss: 0.0407, Val Loss: 0.0972\n",
      "Epoch [43/50], Train Loss: 0.0397, Val Loss: 0.0959\n",
      "Epoch [44/50], Train Loss: 0.0398, Val Loss: 0.0946\n",
      "Epoch [45/50], Train Loss: 0.0385, Val Loss: 0.0934\n",
      "Epoch [46/50], Train Loss: 0.0388, Val Loss: 0.0922\n",
      "Epoch [47/50], Train Loss: 0.0390, Val Loss: 0.0912\n",
      "Epoch [48/50], Train Loss: 0.0394, Val Loss: 0.0901\n",
      "Epoch [49/50], Train Loss: 0.0387, Val Loss: 0.0890\n",
      "Epoch [50/50], Train Loss: 0.0391, Val Loss: 0.0882\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=128, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1376, Val Loss: 0.3399\n",
      "Epoch [2/50], Train Loss: 0.1270, Val Loss: 0.3224\n",
      "Epoch [3/50], Train Loss: 0.1175, Val Loss: 0.3063\n",
      "Epoch [4/50], Train Loss: 0.1091, Val Loss: 0.2915\n",
      "Epoch [5/50], Train Loss: 0.1015, Val Loss: 0.2777\n",
      "Epoch [6/50], Train Loss: 0.0947, Val Loss: 0.2650\n",
      "Epoch [7/50], Train Loss: 0.0885, Val Loss: 0.2530\n",
      "Epoch [8/50], Train Loss: 0.0829, Val Loss: 0.2419\n",
      "Epoch [9/50], Train Loss: 0.0777, Val Loss: 0.2314\n",
      "Epoch [10/50], Train Loss: 0.0731, Val Loss: 0.2216\n",
      "Epoch [11/50], Train Loss: 0.0689, Val Loss: 0.2123\n",
      "Epoch [12/50], Train Loss: 0.0650, Val Loss: 0.2036\n",
      "Epoch [13/50], Train Loss: 0.0615, Val Loss: 0.1954\n",
      "Epoch [14/50], Train Loss: 0.0583, Val Loss: 0.1878\n",
      "Epoch [15/50], Train Loss: 0.0554, Val Loss: 0.1805\n",
      "Epoch [16/50], Train Loss: 0.0528, Val Loss: 0.1737\n",
      "Epoch [17/50], Train Loss: 0.0504, Val Loss: 0.1673\n",
      "Epoch [18/50], Train Loss: 0.0483, Val Loss: 0.1614\n",
      "Epoch [19/50], Train Loss: 0.0464, Val Loss: 0.1557\n",
      "Epoch [20/50], Train Loss: 0.0447, Val Loss: 0.1505\n",
      "Epoch [21/50], Train Loss: 0.0431, Val Loss: 0.1456\n",
      "Epoch [22/50], Train Loss: 0.0417, Val Loss: 0.1410\n",
      "Epoch [23/50], Train Loss: 0.0405, Val Loss: 0.1367\n",
      "Epoch [24/50], Train Loss: 0.0394, Val Loss: 0.1327\n",
      "Epoch [25/50], Train Loss: 0.0384, Val Loss: 0.1289\n",
      "Epoch [26/50], Train Loss: 0.0375, Val Loss: 0.1255\n",
      "Epoch [27/50], Train Loss: 0.0367, Val Loss: 0.1222\n",
      "Epoch [28/50], Train Loss: 0.0360, Val Loss: 0.1192\n",
      "Epoch [29/50], Train Loss: 0.0354, Val Loss: 0.1164\n",
      "Epoch [30/50], Train Loss: 0.0349, Val Loss: 0.1138\n",
      "Epoch [31/50], Train Loss: 0.0344, Val Loss: 0.1114\n",
      "Epoch [32/50], Train Loss: 0.0339, Val Loss: 0.1091\n",
      "Epoch [33/50], Train Loss: 0.0335, Val Loss: 0.1070\n",
      "Epoch [34/50], Train Loss: 0.0332, Val Loss: 0.1051\n",
      "Epoch [35/50], Train Loss: 0.0329, Val Loss: 0.1032\n",
      "Epoch [36/50], Train Loss: 0.0326, Val Loss: 0.1016\n",
      "Epoch [37/50], Train Loss: 0.0324, Val Loss: 0.1000\n",
      "Epoch [38/50], Train Loss: 0.0321, Val Loss: 0.0985\n",
      "Epoch [39/50], Train Loss: 0.0319, Val Loss: 0.0971\n",
      "Epoch [40/50], Train Loss: 0.0317, Val Loss: 0.0959\n",
      "Epoch [41/50], Train Loss: 0.0316, Val Loss: 0.0947\n",
      "Epoch [42/50], Train Loss: 0.0314, Val Loss: 0.0935\n",
      "Epoch [43/50], Train Loss: 0.0313, Val Loss: 0.0925\n",
      "Epoch [44/50], Train Loss: 0.0311, Val Loss: 0.0915\n",
      "Epoch [45/50], Train Loss: 0.0310, Val Loss: 0.0906\n",
      "Epoch [46/50], Train Loss: 0.0309, Val Loss: 0.0897\n",
      "Epoch [47/50], Train Loss: 0.0307, Val Loss: 0.0888\n",
      "Epoch [48/50], Train Loss: 0.0306, Val Loss: 0.0880\n",
      "Epoch [49/50], Train Loss: 0.0305, Val Loss: 0.0873\n",
      "Epoch [50/50], Train Loss: 0.0304, Val Loss: 0.0866\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=128, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1400, Val Loss: 0.3771\n",
      "Epoch [2/50], Train Loss: 0.1288, Val Loss: 0.3564\n",
      "Epoch [3/50], Train Loss: 0.1199, Val Loss: 0.3372\n",
      "Epoch [4/50], Train Loss: 0.1098, Val Loss: 0.3194\n",
      "Epoch [5/50], Train Loss: 0.1025, Val Loss: 0.3027\n",
      "Epoch [6/50], Train Loss: 0.0952, Val Loss: 0.2870\n",
      "Epoch [7/50], Train Loss: 0.0887, Val Loss: 0.2723\n",
      "Epoch [8/50], Train Loss: 0.0828, Val Loss: 0.2585\n",
      "Epoch [9/50], Train Loss: 0.0767, Val Loss: 0.2457\n",
      "Epoch [10/50], Train Loss: 0.0726, Val Loss: 0.2336\n",
      "Epoch [11/50], Train Loss: 0.0683, Val Loss: 0.2222\n",
      "Epoch [12/50], Train Loss: 0.0631, Val Loss: 0.2115\n",
      "Epoch [13/50], Train Loss: 0.0600, Val Loss: 0.2016\n",
      "Epoch [14/50], Train Loss: 0.0574, Val Loss: 0.1923\n",
      "Epoch [15/50], Train Loss: 0.0548, Val Loss: 0.1836\n",
      "Epoch [16/50], Train Loss: 0.0519, Val Loss: 0.1756\n",
      "Epoch [17/50], Train Loss: 0.0496, Val Loss: 0.1682\n",
      "Epoch [18/50], Train Loss: 0.0479, Val Loss: 0.1613\n",
      "Epoch [19/50], Train Loss: 0.0452, Val Loss: 0.1549\n",
      "Epoch [20/50], Train Loss: 0.0441, Val Loss: 0.1491\n",
      "Epoch [21/50], Train Loss: 0.0428, Val Loss: 0.1438\n",
      "Epoch [22/50], Train Loss: 0.0412, Val Loss: 0.1387\n",
      "Epoch [23/50], Train Loss: 0.0407, Val Loss: 0.1342\n",
      "Epoch [24/50], Train Loss: 0.0397, Val Loss: 0.1301\n",
      "Epoch [25/50], Train Loss: 0.0393, Val Loss: 0.1263\n",
      "Epoch [26/50], Train Loss: 0.0387, Val Loss: 0.1228\n",
      "Epoch [27/50], Train Loss: 0.0381, Val Loss: 0.1197\n",
      "Epoch [28/50], Train Loss: 0.0376, Val Loss: 0.1168\n",
      "Epoch [29/50], Train Loss: 0.0372, Val Loss: 0.1142\n",
      "Epoch [30/50], Train Loss: 0.0368, Val Loss: 0.1118\n",
      "Epoch [31/50], Train Loss: 0.0367, Val Loss: 0.1096\n",
      "Epoch [32/50], Train Loss: 0.0355, Val Loss: 0.1076\n",
      "Epoch [33/50], Train Loss: 0.0364, Val Loss: 0.1058\n",
      "Epoch [34/50], Train Loss: 0.0359, Val Loss: 0.1042\n",
      "Epoch [35/50], Train Loss: 0.0360, Val Loss: 0.1027\n",
      "Epoch [36/50], Train Loss: 0.0349, Val Loss: 0.1013\n",
      "Epoch [37/50], Train Loss: 0.0354, Val Loss: 0.1000\n",
      "Epoch [38/50], Train Loss: 0.0353, Val Loss: 0.0988\n",
      "Epoch [39/50], Train Loss: 0.0347, Val Loss: 0.0978\n",
      "Epoch [40/50], Train Loss: 0.0352, Val Loss: 0.0968\n",
      "Epoch [41/50], Train Loss: 0.0349, Val Loss: 0.0959\n",
      "Epoch [42/50], Train Loss: 0.0350, Val Loss: 0.0951\n",
      "Epoch [43/50], Train Loss: 0.0348, Val Loss: 0.0943\n",
      "Epoch [44/50], Train Loss: 0.0345, Val Loss: 0.0935\n",
      "Epoch [45/50], Train Loss: 0.0349, Val Loss: 0.0929\n",
      "Epoch [46/50], Train Loss: 0.0347, Val Loss: 0.0921\n",
      "Epoch [47/50], Train Loss: 0.0346, Val Loss: 0.0914\n",
      "Epoch [48/50], Train Loss: 0.0339, Val Loss: 0.0908\n",
      "Epoch [49/50], Train Loss: 0.0339, Val Loss: 0.0903\n",
      "Epoch [50/50], Train Loss: 0.0336, Val Loss: 0.0898\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=128, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1839, Val Loss: 0.4430\n",
      "Epoch [2/50], Train Loss: 0.1655, Val Loss: 0.4121\n",
      "Epoch [3/50], Train Loss: 0.1503, Val Loss: 0.3842\n",
      "Epoch [4/50], Train Loss: 0.1377, Val Loss: 0.3586\n",
      "Epoch [5/50], Train Loss: 0.1269, Val Loss: 0.3352\n",
      "Epoch [6/50], Train Loss: 0.1175, Val Loss: 0.3136\n",
      "Epoch [7/50], Train Loss: 0.1061, Val Loss: 0.2939\n",
      "Epoch [8/50], Train Loss: 0.0999, Val Loss: 0.2756\n",
      "Epoch [9/50], Train Loss: 0.0909, Val Loss: 0.2587\n",
      "Epoch [10/50], Train Loss: 0.0857, Val Loss: 0.2431\n",
      "Epoch [11/50], Train Loss: 0.0789, Val Loss: 0.2287\n",
      "Epoch [12/50], Train Loss: 0.0744, Val Loss: 0.2153\n",
      "Epoch [13/50], Train Loss: 0.0698, Val Loss: 0.2034\n",
      "Epoch [14/50], Train Loss: 0.0684, Val Loss: 0.1925\n",
      "Epoch [15/50], Train Loss: 0.0633, Val Loss: 0.1828\n",
      "Epoch [16/50], Train Loss: 0.0603, Val Loss: 0.1741\n",
      "Epoch [17/50], Train Loss: 0.0588, Val Loss: 0.1664\n",
      "Epoch [18/50], Train Loss: 0.0559, Val Loss: 0.1592\n",
      "Epoch [19/50], Train Loss: 0.0549, Val Loss: 0.1525\n",
      "Epoch [20/50], Train Loss: 0.0531, Val Loss: 0.1466\n",
      "Epoch [21/50], Train Loss: 0.0520, Val Loss: 0.1412\n",
      "Epoch [22/50], Train Loss: 0.0539, Val Loss: 0.1362\n",
      "Epoch [23/50], Train Loss: 0.0518, Val Loss: 0.1320\n",
      "Epoch [24/50], Train Loss: 0.0498, Val Loss: 0.1280\n",
      "Epoch [25/50], Train Loss: 0.0490, Val Loss: 0.1250\n",
      "Epoch [26/50], Train Loss: 0.0504, Val Loss: 0.1219\n",
      "Epoch [27/50], Train Loss: 0.0484, Val Loss: 0.1194\n",
      "Epoch [28/50], Train Loss: 0.0491, Val Loss: 0.1168\n",
      "Epoch [29/50], Train Loss: 0.0478, Val Loss: 0.1145\n",
      "Epoch [30/50], Train Loss: 0.0483, Val Loss: 0.1125\n",
      "Epoch [31/50], Train Loss: 0.0486, Val Loss: 0.1105\n",
      "Epoch [32/50], Train Loss: 0.0461, Val Loss: 0.1092\n",
      "Epoch [33/50], Train Loss: 0.0484, Val Loss: 0.1077\n",
      "Epoch [34/50], Train Loss: 0.0458, Val Loss: 0.1065\n",
      "Epoch [35/50], Train Loss: 0.0467, Val Loss: 0.1053\n",
      "Epoch [36/50], Train Loss: 0.0480, Val Loss: 0.1042\n",
      "Epoch [37/50], Train Loss: 0.0470, Val Loss: 0.1032\n",
      "Epoch [38/50], Train Loss: 0.0463, Val Loss: 0.1022\n",
      "Epoch [39/50], Train Loss: 0.0455, Val Loss: 0.1014\n",
      "Epoch [40/50], Train Loss: 0.0475, Val Loss: 0.1005\n",
      "Epoch [41/50], Train Loss: 0.0458, Val Loss: 0.0999\n",
      "Epoch [42/50], Train Loss: 0.0468, Val Loss: 0.0994\n",
      "Epoch [43/50], Train Loss: 0.0450, Val Loss: 0.0988\n",
      "Epoch [44/50], Train Loss: 0.0453, Val Loss: 0.0984\n",
      "Epoch [45/50], Train Loss: 0.0451, Val Loss: 0.0981\n",
      "Epoch [46/50], Train Loss: 0.0459, Val Loss: 0.0976\n",
      "Epoch [47/50], Train Loss: 0.0446, Val Loss: 0.0969\n",
      "Epoch [48/50], Train Loss: 0.0436, Val Loss: 0.0965\n",
      "Epoch [49/50], Train Loss: 0.0460, Val Loss: 0.0962\n",
      "Epoch [50/50], Train Loss: 0.0451, Val Loss: 0.0958\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=128, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1795, Val Loss: 0.4015\n",
      "Epoch [2/50], Train Loss: 0.1620, Val Loss: 0.3732\n",
      "Epoch [3/50], Train Loss: 0.1466, Val Loss: 0.3474\n",
      "Epoch [4/50], Train Loss: 0.1328, Val Loss: 0.3236\n",
      "Epoch [5/50], Train Loss: 0.1205, Val Loss: 0.3017\n",
      "Epoch [6/50], Train Loss: 0.1095, Val Loss: 0.2813\n",
      "Epoch [7/50], Train Loss: 0.0995, Val Loss: 0.2623\n",
      "Epoch [8/50], Train Loss: 0.0906, Val Loss: 0.2447\n",
      "Epoch [9/50], Train Loss: 0.0825, Val Loss: 0.2283\n",
      "Epoch [10/50], Train Loss: 0.0754, Val Loss: 0.2131\n",
      "Epoch [11/50], Train Loss: 0.0690, Val Loss: 0.1991\n",
      "Epoch [12/50], Train Loss: 0.0634, Val Loss: 0.1862\n",
      "Epoch [13/50], Train Loss: 0.0585, Val Loss: 0.1745\n",
      "Epoch [14/50], Train Loss: 0.0543, Val Loss: 0.1638\n",
      "Epoch [15/50], Train Loss: 0.0507, Val Loss: 0.1542\n",
      "Epoch [16/50], Train Loss: 0.0476, Val Loss: 0.1455\n",
      "Epoch [17/50], Train Loss: 0.0450, Val Loss: 0.1378\n",
      "Epoch [18/50], Train Loss: 0.0428, Val Loss: 0.1309\n",
      "Epoch [19/50], Train Loss: 0.0410, Val Loss: 0.1248\n",
      "Epoch [20/50], Train Loss: 0.0395, Val Loss: 0.1195\n",
      "Epoch [21/50], Train Loss: 0.0383, Val Loss: 0.1148\n",
      "Epoch [22/50], Train Loss: 0.0373, Val Loss: 0.1107\n",
      "Epoch [23/50], Train Loss: 0.0365, Val Loss: 0.1072\n",
      "Epoch [24/50], Train Loss: 0.0359, Val Loss: 0.1041\n",
      "Epoch [25/50], Train Loss: 0.0353, Val Loss: 0.1014\n",
      "Epoch [26/50], Train Loss: 0.0349, Val Loss: 0.0991\n",
      "Epoch [27/50], Train Loss: 0.0346, Val Loss: 0.0971\n",
      "Epoch [28/50], Train Loss: 0.0343, Val Loss: 0.0954\n",
      "Epoch [29/50], Train Loss: 0.0341, Val Loss: 0.0939\n",
      "Epoch [30/50], Train Loss: 0.0339, Val Loss: 0.0926\n",
      "Epoch [31/50], Train Loss: 0.0338, Val Loss: 0.0914\n",
      "Epoch [32/50], Train Loss: 0.0336, Val Loss: 0.0905\n",
      "Epoch [33/50], Train Loss: 0.0335, Val Loss: 0.0896\n",
      "Epoch [34/50], Train Loss: 0.0334, Val Loss: 0.0888\n",
      "Epoch [35/50], Train Loss: 0.0333, Val Loss: 0.0881\n",
      "Epoch [36/50], Train Loss: 0.0332, Val Loss: 0.0875\n",
      "Epoch [37/50], Train Loss: 0.0331, Val Loss: 0.0870\n",
      "Epoch [38/50], Train Loss: 0.0331, Val Loss: 0.0865\n",
      "Epoch [39/50], Train Loss: 0.0330, Val Loss: 0.0860\n",
      "Epoch [40/50], Train Loss: 0.0329, Val Loss: 0.0856\n",
      "Epoch [41/50], Train Loss: 0.0329, Val Loss: 0.0853\n",
      "Epoch [42/50], Train Loss: 0.0328, Val Loss: 0.0849\n",
      "Epoch [43/50], Train Loss: 0.0327, Val Loss: 0.0846\n",
      "Epoch [44/50], Train Loss: 0.0327, Val Loss: 0.0842\n",
      "Epoch [45/50], Train Loss: 0.0326, Val Loss: 0.0839\n",
      "Epoch [46/50], Train Loss: 0.0325, Val Loss: 0.0836\n",
      "Epoch [47/50], Train Loss: 0.0325, Val Loss: 0.0834\n",
      "Epoch [48/50], Train Loss: 0.0324, Val Loss: 0.0831\n",
      "Epoch [49/50], Train Loss: 0.0323, Val Loss: 0.0828\n",
      "Epoch [50/50], Train Loss: 0.0323, Val Loss: 0.0825\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=128, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1380, Val Loss: 0.3610\n",
      "Epoch [2/50], Train Loss: 0.1213, Val Loss: 0.3317\n",
      "Epoch [3/50], Train Loss: 0.1070, Val Loss: 0.3066\n",
      "Epoch [4/50], Train Loss: 0.0962, Val Loss: 0.2843\n",
      "Epoch [5/50], Train Loss: 0.0877, Val Loss: 0.2646\n",
      "Epoch [6/50], Train Loss: 0.0796, Val Loss: 0.2471\n",
      "Epoch [7/50], Train Loss: 0.0738, Val Loss: 0.2311\n",
      "Epoch [8/50], Train Loss: 0.0675, Val Loss: 0.2170\n",
      "Epoch [9/50], Train Loss: 0.0630, Val Loss: 0.2041\n",
      "Epoch [10/50], Train Loss: 0.0588, Val Loss: 0.1926\n",
      "Epoch [11/50], Train Loss: 0.0543, Val Loss: 0.1823\n",
      "Epoch [12/50], Train Loss: 0.0522, Val Loss: 0.1729\n",
      "Epoch [13/50], Train Loss: 0.0491, Val Loss: 0.1644\n",
      "Epoch [14/50], Train Loss: 0.0472, Val Loss: 0.1567\n",
      "Epoch [15/50], Train Loss: 0.0445, Val Loss: 0.1499\n",
      "Epoch [16/50], Train Loss: 0.0439, Val Loss: 0.1437\n",
      "Epoch [17/50], Train Loss: 0.0428, Val Loss: 0.1380\n",
      "Epoch [18/50], Train Loss: 0.0413, Val Loss: 0.1330\n",
      "Epoch [19/50], Train Loss: 0.0402, Val Loss: 0.1285\n",
      "Epoch [20/50], Train Loss: 0.0405, Val Loss: 0.1246\n",
      "Epoch [21/50], Train Loss: 0.0394, Val Loss: 0.1211\n",
      "Epoch [22/50], Train Loss: 0.0392, Val Loss: 0.1178\n",
      "Epoch [23/50], Train Loss: 0.0378, Val Loss: 0.1149\n",
      "Epoch [24/50], Train Loss: 0.0379, Val Loss: 0.1124\n",
      "Epoch [25/50], Train Loss: 0.0375, Val Loss: 0.1101\n",
      "Epoch [26/50], Train Loss: 0.0378, Val Loss: 0.1080\n",
      "Epoch [27/50], Train Loss: 0.0374, Val Loss: 0.1062\n",
      "Epoch [28/50], Train Loss: 0.0370, Val Loss: 0.1044\n",
      "Epoch [29/50], Train Loss: 0.0365, Val Loss: 0.1030\n",
      "Epoch [30/50], Train Loss: 0.0364, Val Loss: 0.1015\n",
      "Epoch [31/50], Train Loss: 0.0363, Val Loss: 0.1003\n",
      "Epoch [32/50], Train Loss: 0.0365, Val Loss: 0.0991\n",
      "Epoch [33/50], Train Loss: 0.0364, Val Loss: 0.0980\n",
      "Epoch [34/50], Train Loss: 0.0364, Val Loss: 0.0971\n",
      "Epoch [35/50], Train Loss: 0.0367, Val Loss: 0.0963\n",
      "Epoch [36/50], Train Loss: 0.0357, Val Loss: 0.0956\n",
      "Epoch [37/50], Train Loss: 0.0362, Val Loss: 0.0949\n",
      "Epoch [38/50], Train Loss: 0.0359, Val Loss: 0.0942\n",
      "Epoch [39/50], Train Loss: 0.0352, Val Loss: 0.0936\n",
      "Epoch [40/50], Train Loss: 0.0358, Val Loss: 0.0932\n",
      "Epoch [41/50], Train Loss: 0.0356, Val Loss: 0.0926\n",
      "Epoch [42/50], Train Loss: 0.0358, Val Loss: 0.0922\n",
      "Epoch [43/50], Train Loss: 0.0356, Val Loss: 0.0918\n",
      "Epoch [44/50], Train Loss: 0.0356, Val Loss: 0.0913\n",
      "Epoch [45/50], Train Loss: 0.0345, Val Loss: 0.0909\n",
      "Epoch [46/50], Train Loss: 0.0354, Val Loss: 0.0905\n",
      "Epoch [47/50], Train Loss: 0.0353, Val Loss: 0.0901\n",
      "Epoch [48/50], Train Loss: 0.0356, Val Loss: 0.0899\n",
      "Epoch [49/50], Train Loss: 0.0355, Val Loss: 0.0895\n",
      "Epoch [50/50], Train Loss: 0.0351, Val Loss: 0.0892\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=128, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1591, Val Loss: 0.3750\n",
      "Epoch [2/50], Train Loss: 0.1435, Val Loss: 0.3480\n",
      "Epoch [3/50], Train Loss: 0.1285, Val Loss: 0.3248\n",
      "Epoch [4/50], Train Loss: 0.1169, Val Loss: 0.3049\n",
      "Epoch [5/50], Train Loss: 0.1076, Val Loss: 0.2869\n",
      "Epoch [6/50], Train Loss: 0.1000, Val Loss: 0.2707\n",
      "Epoch [7/50], Train Loss: 0.0923, Val Loss: 0.2562\n",
      "Epoch [8/50], Train Loss: 0.0853, Val Loss: 0.2428\n",
      "Epoch [9/50], Train Loss: 0.0795, Val Loss: 0.2308\n",
      "Epoch [10/50], Train Loss: 0.0768, Val Loss: 0.2197\n",
      "Epoch [11/50], Train Loss: 0.0722, Val Loss: 0.2094\n",
      "Epoch [12/50], Train Loss: 0.0678, Val Loss: 0.2000\n",
      "Epoch [13/50], Train Loss: 0.0643, Val Loss: 0.1915\n",
      "Epoch [14/50], Train Loss: 0.0614, Val Loss: 0.1836\n",
      "Epoch [15/50], Train Loss: 0.0615, Val Loss: 0.1763\n",
      "Epoch [16/50], Train Loss: 0.0571, Val Loss: 0.1698\n",
      "Epoch [17/50], Train Loss: 0.0563, Val Loss: 0.1635\n",
      "Epoch [18/50], Train Loss: 0.0558, Val Loss: 0.1580\n",
      "Epoch [19/50], Train Loss: 0.0541, Val Loss: 0.1530\n",
      "Epoch [20/50], Train Loss: 0.0515, Val Loss: 0.1485\n",
      "Epoch [21/50], Train Loss: 0.0512, Val Loss: 0.1443\n",
      "Epoch [22/50], Train Loss: 0.0522, Val Loss: 0.1403\n",
      "Epoch [23/50], Train Loss: 0.0504, Val Loss: 0.1368\n",
      "Epoch [24/50], Train Loss: 0.0489, Val Loss: 0.1335\n",
      "Epoch [25/50], Train Loss: 0.0505, Val Loss: 0.1305\n",
      "Epoch [26/50], Train Loss: 0.0491, Val Loss: 0.1278\n",
      "Epoch [27/50], Train Loss: 0.0494, Val Loss: 0.1252\n",
      "Epoch [28/50], Train Loss: 0.0470, Val Loss: 0.1233\n",
      "Epoch [29/50], Train Loss: 0.0485, Val Loss: 0.1214\n",
      "Epoch [30/50], Train Loss: 0.0469, Val Loss: 0.1196\n",
      "Epoch [31/50], Train Loss: 0.0471, Val Loss: 0.1180\n",
      "Epoch [32/50], Train Loss: 0.0469, Val Loss: 0.1163\n",
      "Epoch [33/50], Train Loss: 0.0454, Val Loss: 0.1148\n",
      "Epoch [34/50], Train Loss: 0.0467, Val Loss: 0.1134\n",
      "Epoch [35/50], Train Loss: 0.0450, Val Loss: 0.1121\n",
      "Epoch [36/50], Train Loss: 0.0470, Val Loss: 0.1110\n",
      "Epoch [37/50], Train Loss: 0.0465, Val Loss: 0.1098\n",
      "Epoch [38/50], Train Loss: 0.0458, Val Loss: 0.1089\n",
      "Epoch [39/50], Train Loss: 0.0475, Val Loss: 0.1083\n",
      "Epoch [40/50], Train Loss: 0.0452, Val Loss: 0.1075\n",
      "Epoch [41/50], Train Loss: 0.0466, Val Loss: 0.1066\n",
      "Epoch [42/50], Train Loss: 0.0456, Val Loss: 0.1061\n",
      "Epoch [43/50], Train Loss: 0.0442, Val Loss: 0.1055\n",
      "Epoch [44/50], Train Loss: 0.0463, Val Loss: 0.1050\n",
      "Epoch [45/50], Train Loss: 0.0461, Val Loss: 0.1044\n",
      "Epoch [46/50], Train Loss: 0.0453, Val Loss: 0.1037\n",
      "Epoch [47/50], Train Loss: 0.0445, Val Loss: 0.1032\n",
      "Epoch [48/50], Train Loss: 0.0440, Val Loss: 0.1029\n",
      "Epoch [49/50], Train Loss: 0.0459, Val Loss: 0.1025\n",
      "Epoch [50/50], Train Loss: 0.0455, Val Loss: 0.1020\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=16, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0936, Val Loss: 0.1838\n",
      "Epoch [2/50], Train Loss: 0.0338, Val Loss: 0.0758\n",
      "Epoch [3/50], Train Loss: 0.0234, Val Loss: 0.0531\n",
      "Epoch [4/50], Train Loss: 0.0201, Val Loss: 0.0444\n",
      "Epoch [5/50], Train Loss: 0.0168, Val Loss: 0.0316\n",
      "Epoch [6/50], Train Loss: 0.0137, Val Loss: 0.0254\n",
      "Epoch [7/50], Train Loss: 0.0125, Val Loss: 0.0256\n",
      "Epoch [8/50], Train Loss: 0.0108, Val Loss: 0.0218\n",
      "Epoch [9/50], Train Loss: 0.0098, Val Loss: 0.0170\n",
      "Epoch [10/50], Train Loss: 0.0084, Val Loss: 0.0139\n",
      "Epoch [11/50], Train Loss: 0.0067, Val Loss: 0.0118\n",
      "Epoch [12/50], Train Loss: 0.0050, Val Loss: 0.0099\n",
      "Epoch [13/50], Train Loss: 0.0037, Val Loss: 0.0094\n",
      "Epoch [14/50], Train Loss: 0.0034, Val Loss: 0.0112\n",
      "Epoch [15/50], Train Loss: 0.0032, Val Loss: 0.0118\n",
      "Epoch [16/50], Train Loss: 0.0026, Val Loss: 0.0095\n",
      "Epoch [17/50], Train Loss: 0.0023, Val Loss: 0.0080\n",
      "Epoch [18/50], Train Loss: 0.0024, Val Loss: 0.0092\n",
      "Epoch [19/50], Train Loss: 0.0022, Val Loss: 0.0089\n",
      "Epoch [20/50], Train Loss: 0.0022, Val Loss: 0.0083\n",
      "Epoch [21/50], Train Loss: 0.0022, Val Loss: 0.0074\n",
      "Epoch [22/50], Train Loss: 0.0022, Val Loss: 0.0089\n",
      "Epoch [23/50], Train Loss: 0.0021, Val Loss: 0.0089\n",
      "Epoch [24/50], Train Loss: 0.0020, Val Loss: 0.0071\n",
      "Epoch [25/50], Train Loss: 0.0021, Val Loss: 0.0064\n",
      "Epoch [26/50], Train Loss: 0.0021, Val Loss: 0.0087\n",
      "Epoch [27/50], Train Loss: 0.0022, Val Loss: 0.0096\n",
      "Epoch [28/50], Train Loss: 0.0021, Val Loss: 0.0061\n",
      "Epoch [29/50], Train Loss: 0.0024, Val Loss: 0.0049\n",
      "Epoch [30/50], Train Loss: 0.0023, Val Loss: 0.0079\n",
      "Epoch [31/50], Train Loss: 0.0028, Val Loss: 0.0115\n",
      "Epoch [32/50], Train Loss: 0.0025, Val Loss: 0.0060\n",
      "Epoch [33/50], Train Loss: 0.0033, Val Loss: 0.0038\n",
      "Epoch [34/50], Train Loss: 0.0024, Val Loss: 0.0058\n",
      "Epoch [35/50], Train Loss: 0.0037, Val Loss: 0.0126\n",
      "Epoch [36/50], Train Loss: 0.0030, Val Loss: 0.0060\n",
      "Epoch [37/50], Train Loss: 0.0039, Val Loss: 0.0039\n",
      "Epoch [38/50], Train Loss: 0.0023, Val Loss: 0.0065\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=16, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1398, Val Loss: 0.2453\n",
      "Epoch [2/50], Train Loss: 0.0484, Val Loss: 0.0857\n",
      "Epoch [3/50], Train Loss: 0.0301, Val Loss: 0.0434\n",
      "Epoch [4/50], Train Loss: 0.0288, Val Loss: 0.0341\n",
      "Epoch [5/50], Train Loss: 0.0244, Val Loss: 0.0283\n",
      "Epoch [6/50], Train Loss: 0.0227, Val Loss: 0.0208\n",
      "Epoch [7/50], Train Loss: 0.0193, Val Loss: 0.0172\n",
      "Epoch [8/50], Train Loss: 0.0183, Val Loss: 0.0145\n",
      "Epoch [9/50], Train Loss: 0.0164, Val Loss: 0.0124\n",
      "Epoch [10/50], Train Loss: 0.0139, Val Loss: 0.0084\n",
      "Epoch [11/50], Train Loss: 0.0125, Val Loss: 0.0071\n",
      "Epoch [12/50], Train Loss: 0.0102, Val Loss: 0.0055\n",
      "Epoch [13/50], Train Loss: 0.0091, Val Loss: 0.0075\n",
      "Epoch [14/50], Train Loss: 0.0093, Val Loss: 0.0038\n",
      "Epoch [15/50], Train Loss: 0.0091, Val Loss: 0.0041\n",
      "Epoch [16/50], Train Loss: 0.0074, Val Loss: 0.0057\n",
      "Epoch [17/50], Train Loss: 0.0077, Val Loss: 0.0042\n",
      "Epoch [18/50], Train Loss: 0.0079, Val Loss: 0.0032\n",
      "Epoch [19/50], Train Loss: 0.0075, Val Loss: 0.0030\n",
      "Epoch [20/50], Train Loss: 0.0072, Val Loss: 0.0049\n",
      "Epoch [21/50], Train Loss: 0.0070, Val Loss: 0.0058\n",
      "Epoch [22/50], Train Loss: 0.0070, Val Loss: 0.0027\n",
      "Epoch [23/50], Train Loss: 0.0073, Val Loss: 0.0027\n",
      "Epoch [24/50], Train Loss: 0.0066, Val Loss: 0.0032\n",
      "Epoch [25/50], Train Loss: 0.0067, Val Loss: 0.0054\n",
      "Epoch [26/50], Train Loss: 0.0064, Val Loss: 0.0031\n",
      "Epoch [27/50], Train Loss: 0.0067, Val Loss: 0.0022\n",
      "Epoch [28/50], Train Loss: 0.0063, Val Loss: 0.0032\n",
      "Epoch [29/50], Train Loss: 0.0063, Val Loss: 0.0055\n",
      "Epoch [30/50], Train Loss: 0.0062, Val Loss: 0.0030\n",
      "Epoch [31/50], Train Loss: 0.0059, Val Loss: 0.0018\n",
      "Epoch [32/50], Train Loss: 0.0062, Val Loss: 0.0020\n",
      "Epoch [33/50], Train Loss: 0.0060, Val Loss: 0.0038\n",
      "Epoch [34/50], Train Loss: 0.0058, Val Loss: 0.0030\n",
      "Epoch [35/50], Train Loss: 0.0057, Val Loss: 0.0023\n",
      "Epoch [36/50], Train Loss: 0.0052, Val Loss: 0.0030\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=16, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1113, Val Loss: 0.2182\n",
      "Epoch [2/50], Train Loss: 0.0689, Val Loss: 0.1378\n",
      "Epoch [3/50], Train Loss: 0.0532, Val Loss: 0.0927\n",
      "Epoch [4/50], Train Loss: 0.0455, Val Loss: 0.0696\n",
      "Epoch [5/50], Train Loss: 0.0358, Val Loss: 0.0481\n",
      "Epoch [6/50], Train Loss: 0.0300, Val Loss: 0.0372\n",
      "Epoch [7/50], Train Loss: 0.0265, Val Loss: 0.0213\n",
      "Epoch [8/50], Train Loss: 0.0241, Val Loss: 0.0177\n",
      "Epoch [9/50], Train Loss: 0.0226, Val Loss: 0.0134\n",
      "Epoch [10/50], Train Loss: 0.0206, Val Loss: 0.0199\n",
      "Epoch [11/50], Train Loss: 0.0186, Val Loss: 0.0131\n",
      "Epoch [12/50], Train Loss: 0.0181, Val Loss: 0.0084\n",
      "Epoch [13/50], Train Loss: 0.0174, Val Loss: 0.0064\n",
      "Epoch [14/50], Train Loss: 0.0171, Val Loss: 0.0187\n",
      "Epoch [15/50], Train Loss: 0.0162, Val Loss: 0.0128\n",
      "Epoch [16/50], Train Loss: 0.0162, Val Loss: 0.0078\n",
      "Epoch [17/50], Train Loss: 0.0152, Val Loss: 0.0056\n",
      "Epoch [18/50], Train Loss: 0.0143, Val Loss: 0.0144\n",
      "Epoch [19/50], Train Loss: 0.0137, Val Loss: 0.0066\n",
      "Epoch [20/50], Train Loss: 0.0133, Val Loss: 0.0043\n",
      "Epoch [21/50], Train Loss: 0.0123, Val Loss: 0.0065\n",
      "Epoch [22/50], Train Loss: 0.0134, Val Loss: 0.0111\n",
      "Epoch [23/50], Train Loss: 0.0116, Val Loss: 0.0060\n",
      "Epoch [24/50], Train Loss: 0.0117, Val Loss: 0.0061\n",
      "Epoch [25/50], Train Loss: 0.0114, Val Loss: 0.0063\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=16, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1134, Val Loss: 0.1170\n",
      "Epoch [2/50], Train Loss: 0.0501, Val Loss: 0.0599\n",
      "Epoch [3/50], Train Loss: 0.0404, Val Loss: 0.0530\n",
      "Epoch [4/50], Train Loss: 0.0345, Val Loss: 0.0443\n",
      "Epoch [5/50], Train Loss: 0.0303, Val Loss: 0.0338\n",
      "Epoch [6/50], Train Loss: 0.0253, Val Loss: 0.0225\n",
      "Epoch [7/50], Train Loss: 0.0200, Val Loss: 0.0161\n",
      "Epoch [8/50], Train Loss: 0.0166, Val Loss: 0.0165\n",
      "Epoch [9/50], Train Loss: 0.0146, Val Loss: 0.0167\n",
      "Epoch [10/50], Train Loss: 0.0129, Val Loss: 0.0172\n",
      "Epoch [11/50], Train Loss: 0.0113, Val Loss: 0.0189\n",
      "Epoch [12/50], Train Loss: 0.0092, Val Loss: 0.0190\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=16, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1072, Val Loss: 0.0959\n",
      "Epoch [2/50], Train Loss: 0.0647, Val Loss: 0.0809\n",
      "Epoch [3/50], Train Loss: 0.0542, Val Loss: 0.0720\n",
      "Epoch [4/50], Train Loss: 0.0420, Val Loss: 0.0541\n",
      "Epoch [5/50], Train Loss: 0.0361, Val Loss: 0.0373\n",
      "Epoch [6/50], Train Loss: 0.0278, Val Loss: 0.0303\n",
      "Epoch [7/50], Train Loss: 0.0237, Val Loss: 0.0300\n",
      "Epoch [8/50], Train Loss: 0.0176, Val Loss: 0.0182\n",
      "Epoch [9/50], Train Loss: 0.0154, Val Loss: 0.0174\n",
      "Epoch [10/50], Train Loss: 0.0135, Val Loss: 0.0159\n",
      "Epoch [11/50], Train Loss: 0.0139, Val Loss: 0.0140\n",
      "Epoch [12/50], Train Loss: 0.0137, Val Loss: 0.0108\n",
      "Epoch [13/50], Train Loss: 0.0119, Val Loss: 0.0115\n",
      "Epoch [14/50], Train Loss: 0.0122, Val Loss: 0.0122\n",
      "Epoch [15/50], Train Loss: 0.0111, Val Loss: 0.0096\n",
      "Epoch [16/50], Train Loss: 0.0111, Val Loss: 0.0077\n",
      "Epoch [17/50], Train Loss: 0.0105, Val Loss: 0.0086\n",
      "Epoch [18/50], Train Loss: 0.0114, Val Loss: 0.0194\n",
      "Epoch [19/50], Train Loss: 0.0101, Val Loss: 0.0032\n",
      "Epoch [20/50], Train Loss: 0.0121, Val Loss: 0.0078\n",
      "Epoch [21/50], Train Loss: 0.0117, Val Loss: 0.0194\n",
      "Epoch [22/50], Train Loss: 0.0088, Val Loss: 0.0092\n",
      "Epoch [23/50], Train Loss: 0.0095, Val Loss: 0.0033\n",
      "Epoch [24/50], Train Loss: 0.0092, Val Loss: 0.0060\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=16, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0864, Val Loss: 0.1242\n",
      "Epoch [2/50], Train Loss: 0.0654, Val Loss: 0.0953\n",
      "Epoch [3/50], Train Loss: 0.0527, Val Loss: 0.0768\n",
      "Epoch [4/50], Train Loss: 0.0442, Val Loss: 0.0473\n",
      "Epoch [5/50], Train Loss: 0.0360, Val Loss: 0.0329\n",
      "Epoch [6/50], Train Loss: 0.0316, Val Loss: 0.0347\n",
      "Epoch [7/50], Train Loss: 0.0284, Val Loss: 0.0273\n",
      "Epoch [8/50], Train Loss: 0.0250, Val Loss: 0.0144\n",
      "Epoch [9/50], Train Loss: 0.0218, Val Loss: 0.0085\n",
      "Epoch [10/50], Train Loss: 0.0218, Val Loss: 0.0243\n",
      "Epoch [11/50], Train Loss: 0.0190, Val Loss: 0.0077\n",
      "Epoch [12/50], Train Loss: 0.0175, Val Loss: 0.0074\n",
      "Epoch [13/50], Train Loss: 0.0169, Val Loss: 0.0283\n",
      "Epoch [14/50], Train Loss: 0.0172, Val Loss: 0.0101\n",
      "Epoch [15/50], Train Loss: 0.0171, Val Loss: 0.0073\n",
      "Epoch [16/50], Train Loss: 0.0171, Val Loss: 0.0257\n",
      "Epoch [17/50], Train Loss: 0.0230, Val Loss: 0.0250\n",
      "Epoch [18/50], Train Loss: 0.0154, Val Loss: 0.0157\n",
      "Epoch [19/50], Train Loss: 0.0135, Val Loss: 0.0201\n",
      "Epoch [20/50], Train Loss: 0.0139, Val Loss: 0.0125\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=16, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0517, Val Loss: 0.0755\n",
      "Epoch [2/50], Train Loss: 0.0557, Val Loss: 0.0865\n",
      "Epoch [3/50], Train Loss: 0.0456, Val Loss: 0.0835\n",
      "Epoch [4/50], Train Loss: 0.0403, Val Loss: 0.0709\n",
      "Epoch [5/50], Train Loss: 0.0347, Val Loss: 0.0474\n",
      "Epoch [6/50], Train Loss: 0.0228, Val Loss: 0.0343\n",
      "Epoch [7/50], Train Loss: 0.0181, Val Loss: 0.0301\n",
      "Epoch [8/50], Train Loss: 0.0158, Val Loss: 0.0402\n",
      "Epoch [9/50], Train Loss: 0.0158, Val Loss: 0.0309\n",
      "Epoch [10/50], Train Loss: 0.0094, Val Loss: 0.0204\n",
      "Epoch [11/50], Train Loss: 0.0074, Val Loss: 0.0140\n",
      "Epoch [12/50], Train Loss: 0.0107, Val Loss: 0.0373\n",
      "Epoch [13/50], Train Loss: 0.0106, Val Loss: 0.0176\n",
      "Epoch [14/50], Train Loss: 0.0092, Val Loss: 0.0165\n",
      "Epoch [15/50], Train Loss: 0.0083, Val Loss: 0.0203\n",
      "Epoch [16/50], Train Loss: 0.0062, Val Loss: 0.0186\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=16, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0762, Val Loss: 0.0948\n",
      "Epoch [2/50], Train Loss: 0.0659, Val Loss: 0.1070\n",
      "Epoch [3/50], Train Loss: 0.0506, Val Loss: 0.0862\n",
      "Epoch [4/50], Train Loss: 0.0399, Val Loss: 0.0559\n",
      "Epoch [5/50], Train Loss: 0.0326, Val Loss: 0.0385\n",
      "Epoch [6/50], Train Loss: 0.0272, Val Loss: 0.0330\n",
      "Epoch [7/50], Train Loss: 0.0262, Val Loss: 0.0314\n",
      "Epoch [8/50], Train Loss: 0.0241, Val Loss: 0.0270\n",
      "Epoch [9/50], Train Loss: 0.0206, Val Loss: 0.0276\n",
      "Epoch [10/50], Train Loss: 0.0164, Val Loss: 0.0132\n",
      "Epoch [11/50], Train Loss: 0.0149, Val Loss: 0.0094\n",
      "Epoch [12/50], Train Loss: 0.0137, Val Loss: 0.0227\n",
      "Epoch [13/50], Train Loss: 0.0165, Val Loss: 0.0080\n",
      "Epoch [14/50], Train Loss: 0.0133, Val Loss: 0.0167\n",
      "Epoch [15/50], Train Loss: 0.0121, Val Loss: 0.0168\n",
      "Epoch [16/50], Train Loss: 0.0098, Val Loss: 0.0078\n",
      "Epoch [17/50], Train Loss: 0.0102, Val Loss: 0.0111\n",
      "Epoch [18/50], Train Loss: 0.0112, Val Loss: 0.0085\n",
      "Epoch [19/50], Train Loss: 0.0097, Val Loss: 0.0127\n",
      "Epoch [20/50], Train Loss: 0.0099, Val Loss: 0.0104\n",
      "Epoch [21/50], Train Loss: 0.0168, Val Loss: 0.0068\n",
      "Epoch [22/50], Train Loss: 0.0108, Val Loss: 0.0092\n",
      "Epoch [23/50], Train Loss: 0.0109, Val Loss: 0.0193\n",
      "Epoch [24/50], Train Loss: 0.0185, Val Loss: 0.0154\n",
      "Epoch [25/50], Train Loss: 0.0098, Val Loss: 0.0106\n",
      "Epoch [26/50], Train Loss: 0.0088, Val Loss: 0.0139\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=16, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0944, Val Loss: 0.1126\n",
      "Epoch [2/50], Train Loss: 0.0766, Val Loss: 0.1172\n",
      "Epoch [3/50], Train Loss: 0.0684, Val Loss: 0.1179\n",
      "Epoch [4/50], Train Loss: 0.0562, Val Loss: 0.0983\n",
      "Epoch [5/50], Train Loss: 0.0483, Val Loss: 0.0685\n",
      "Epoch [6/50], Train Loss: 0.0442, Val Loss: 0.0450\n",
      "Epoch [7/50], Train Loss: 0.0366, Val Loss: 0.0380\n",
      "Epoch [8/50], Train Loss: 0.0341, Val Loss: 0.0294\n",
      "Epoch [9/50], Train Loss: 0.0314, Val Loss: 0.0256\n",
      "Epoch [10/50], Train Loss: 0.0277, Val Loss: 0.0285\n",
      "Epoch [11/50], Train Loss: 0.0240, Val Loss: 0.0200\n",
      "Epoch [12/50], Train Loss: 0.0238, Val Loss: 0.0117\n",
      "Epoch [13/50], Train Loss: 0.0215, Val Loss: 0.0140\n",
      "Epoch [14/50], Train Loss: 0.0208, Val Loss: 0.0180\n",
      "Epoch [15/50], Train Loss: 0.0273, Val Loss: 0.0103\n",
      "Epoch [16/50], Train Loss: 0.0229, Val Loss: 0.0152\n",
      "Epoch [17/50], Train Loss: 0.0228, Val Loss: 0.0353\n",
      "Epoch [18/50], Train Loss: 0.0287, Val Loss: 0.0433\n",
      "Epoch [19/50], Train Loss: 0.0172, Val Loss: 0.0170\n",
      "Epoch [20/50], Train Loss: 0.0161, Val Loss: 0.0260\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=32, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0445, Val Loss: 0.0472\n",
      "Epoch [2/50], Train Loss: 0.0351, Val Loss: 0.0420\n",
      "Epoch [3/50], Train Loss: 0.0236, Val Loss: 0.0235\n",
      "Epoch [4/50], Train Loss: 0.0161, Val Loss: 0.0095\n",
      "Epoch [5/50], Train Loss: 0.0103, Val Loss: 0.0088\n",
      "Epoch [6/50], Train Loss: 0.0062, Val Loss: 0.0103\n",
      "Epoch [7/50], Train Loss: 0.0042, Val Loss: 0.0061\n",
      "Epoch [8/50], Train Loss: 0.0049, Val Loss: 0.0098\n",
      "Epoch [9/50], Train Loss: 0.0041, Val Loss: 0.0089\n",
      "Epoch [10/50], Train Loss: 0.0043, Val Loss: 0.0087\n",
      "Epoch [11/50], Train Loss: 0.0030, Val Loss: 0.0035\n",
      "Epoch [12/50], Train Loss: 0.0030, Val Loss: 0.0065\n",
      "Epoch [13/50], Train Loss: 0.0033, Val Loss: 0.0085\n",
      "Epoch [14/50], Train Loss: 0.0043, Val Loss: 0.0032\n",
      "Epoch [15/50], Train Loss: 0.0033, Val Loss: 0.0046\n",
      "Epoch [16/50], Train Loss: 0.0040, Val Loss: 0.0092\n",
      "Epoch [17/50], Train Loss: 0.0045, Val Loss: 0.0036\n",
      "Epoch [18/50], Train Loss: 0.0039, Val Loss: 0.0048\n",
      "Epoch [19/50], Train Loss: 0.0034, Val Loss: 0.0074\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=32, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0426, Val Loss: 0.0458\n",
      "Epoch [2/50], Train Loss: 0.0514, Val Loss: 0.0663\n",
      "Epoch [3/50], Train Loss: 0.0303, Val Loss: 0.0417\n",
      "Epoch [4/50], Train Loss: 0.0244, Val Loss: 0.0186\n",
      "Epoch [5/50], Train Loss: 0.0166, Val Loss: 0.0042\n",
      "Epoch [6/50], Train Loss: 0.0108, Val Loss: 0.0058\n",
      "Epoch [7/50], Train Loss: 0.0081, Val Loss: 0.0124\n",
      "Epoch [8/50], Train Loss: 0.0068, Val Loss: 0.0048\n",
      "Epoch [9/50], Train Loss: 0.0075, Val Loss: 0.0037\n",
      "Epoch [10/50], Train Loss: 0.0059, Val Loss: 0.0073\n",
      "Epoch [11/50], Train Loss: 0.0055, Val Loss: 0.0065\n",
      "Epoch [12/50], Train Loss: 0.0063, Val Loss: 0.0027\n",
      "Epoch [13/50], Train Loss: 0.0050, Val Loss: 0.0031\n",
      "Epoch [14/50], Train Loss: 0.0052, Val Loss: 0.0089\n",
      "Epoch [15/50], Train Loss: 0.0054, Val Loss: 0.0028\n",
      "Epoch [16/50], Train Loss: 0.0049, Val Loss: 0.0021\n",
      "Epoch [17/50], Train Loss: 0.0045, Val Loss: 0.0066\n",
      "Epoch [18/50], Train Loss: 0.0040, Val Loss: 0.0048\n",
      "Epoch [19/50], Train Loss: 0.0042, Val Loss: 0.0022\n",
      "Epoch [20/50], Train Loss: 0.0040, Val Loss: 0.0027\n",
      "Epoch [21/50], Train Loss: 0.0041, Val Loss: 0.0074\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=32, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0876, Val Loss: 0.0767\n",
      "Epoch [2/50], Train Loss: 0.0622, Val Loss: 0.0671\n",
      "Epoch [3/50], Train Loss: 0.0405, Val Loss: 0.0366\n",
      "Epoch [4/50], Train Loss: 0.0348, Val Loss: 0.0173\n",
      "Epoch [5/50], Train Loss: 0.0267, Val Loss: 0.0109\n",
      "Epoch [6/50], Train Loss: 0.0225, Val Loss: 0.0139\n",
      "Epoch [7/50], Train Loss: 0.0194, Val Loss: 0.0086\n",
      "Epoch [8/50], Train Loss: 0.0163, Val Loss: 0.0069\n",
      "Epoch [9/50], Train Loss: 0.0138, Val Loss: 0.0028\n",
      "Epoch [10/50], Train Loss: 0.0128, Val Loss: 0.0057\n",
      "Epoch [11/50], Train Loss: 0.0109, Val Loss: 0.0066\n",
      "Epoch [12/50], Train Loss: 0.0113, Val Loss: 0.0028\n",
      "Epoch [13/50], Train Loss: 0.0117, Val Loss: 0.0037\n",
      "Epoch [14/50], Train Loss: 0.0102, Val Loss: 0.0124\n",
      "Epoch [15/50], Train Loss: 0.0098, Val Loss: 0.0053\n",
      "Epoch [16/50], Train Loss: 0.0089, Val Loss: 0.0018\n",
      "Epoch [17/50], Train Loss: 0.0084, Val Loss: 0.0026\n",
      "Epoch [18/50], Train Loss: 0.0086, Val Loss: 0.0036\n",
      "Epoch [19/50], Train Loss: 0.0085, Val Loss: 0.0018\n",
      "Epoch [20/50], Train Loss: 0.0075, Val Loss: 0.0041\n",
      "Epoch [21/50], Train Loss: 0.0081, Val Loss: 0.0098\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=32, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0358, Val Loss: 0.0366\n",
      "Epoch [2/50], Train Loss: 0.0758, Val Loss: 0.1062\n",
      "Epoch [3/50], Train Loss: 0.0419, Val Loss: 0.0602\n",
      "Epoch [4/50], Train Loss: 0.0361, Val Loss: 0.0380\n",
      "Epoch [5/50], Train Loss: 0.0222, Val Loss: 0.0093\n",
      "Epoch [6/50], Train Loss: 0.0097, Val Loss: 0.0042\n",
      "Epoch [7/50], Train Loss: 0.0079, Val Loss: 0.0132\n",
      "Epoch [8/50], Train Loss: 0.0110, Val Loss: 0.0069\n",
      "Epoch [9/50], Train Loss: 0.0031, Val Loss: 0.0042\n",
      "Epoch [10/50], Train Loss: 0.0032, Val Loss: 0.0095\n",
      "Epoch [11/50], Train Loss: 0.0023, Val Loss: 0.0036\n",
      "Epoch [12/50], Train Loss: 0.0028, Val Loss: 0.0038\n",
      "Epoch [13/50], Train Loss: 0.0025, Val Loss: 0.0074\n",
      "Epoch [14/50], Train Loss: 0.0024, Val Loss: 0.0061\n",
      "Epoch [15/50], Train Loss: 0.0032, Val Loss: 0.0042\n",
      "Epoch [16/50], Train Loss: 0.0026, Val Loss: 0.0032\n",
      "Epoch [17/50], Train Loss: 0.0036, Val Loss: 0.0123\n",
      "Epoch [18/50], Train Loss: 0.0043, Val Loss: 0.0062\n",
      "Epoch [19/50], Train Loss: 0.0040, Val Loss: 0.0018\n",
      "Epoch [20/50], Train Loss: 0.0067, Val Loss: 0.0157\n",
      "Epoch [21/50], Train Loss: 0.0070, Val Loss: 0.0027\n",
      "Epoch [22/50], Train Loss: 0.0079, Val Loss: 0.0079\n",
      "Epoch [23/50], Train Loss: 0.0065, Val Loss: 0.0133\n",
      "Epoch [24/50], Train Loss: 0.0092, Val Loss: 0.0083\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=32, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0540, Val Loss: 0.0443\n",
      "Epoch [2/50], Train Loss: 0.0603, Val Loss: 0.0558\n",
      "Epoch [3/50], Train Loss: 0.0403, Val Loss: 0.0273\n",
      "Epoch [4/50], Train Loss: 0.0241, Val Loss: 0.0119\n",
      "Epoch [5/50], Train Loss: 0.0213, Val Loss: 0.0214\n",
      "Epoch [6/50], Train Loss: 0.0164, Val Loss: 0.0065\n",
      "Epoch [7/50], Train Loss: 0.0141, Val Loss: 0.0160\n",
      "Epoch [8/50], Train Loss: 0.0116, Val Loss: 0.0125\n",
      "Epoch [9/50], Train Loss: 0.0104, Val Loss: 0.0029\n",
      "Epoch [10/50], Train Loss: 0.0090, Val Loss: 0.0037\n",
      "Epoch [11/50], Train Loss: 0.0089, Val Loss: 0.0198\n",
      "Epoch [12/50], Train Loss: 0.0084, Val Loss: 0.0026\n",
      "Epoch [13/50], Train Loss: 0.0115, Val Loss: 0.0080\n",
      "Epoch [14/50], Train Loss: 0.0100, Val Loss: 0.0170\n",
      "Epoch [15/50], Train Loss: 0.0134, Val Loss: 0.0082\n",
      "Epoch [16/50], Train Loss: 0.0105, Val Loss: 0.0082\n",
      "Epoch [17/50], Train Loss: 0.0090, Val Loss: 0.0180\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=32, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0936, Val Loss: 0.0569\n",
      "Epoch [2/50], Train Loss: 0.0956, Val Loss: 0.0976\n",
      "Epoch [3/50], Train Loss: 0.0561, Val Loss: 0.0526\n",
      "Epoch [4/50], Train Loss: 0.0400, Val Loss: 0.0286\n",
      "Epoch [5/50], Train Loss: 0.0321, Val Loss: 0.0217\n",
      "Epoch [6/50], Train Loss: 0.0311, Val Loss: 0.0038\n",
      "Epoch [7/50], Train Loss: 0.0279, Val Loss: 0.0082\n",
      "Epoch [8/50], Train Loss: 0.0222, Val Loss: 0.0171\n",
      "Epoch [9/50], Train Loss: 0.0199, Val Loss: 0.0017\n",
      "Epoch [10/50], Train Loss: 0.0202, Val Loss: 0.0037\n",
      "Epoch [11/50], Train Loss: 0.0189, Val Loss: 0.0108\n",
      "Epoch [12/50], Train Loss: 0.0158, Val Loss: 0.0100\n",
      "Epoch [13/50], Train Loss: 0.0152, Val Loss: 0.0016\n",
      "Epoch [14/50], Train Loss: 0.0170, Val Loss: 0.0024\n",
      "Epoch [15/50], Train Loss: 0.0148, Val Loss: 0.0127\n",
      "Epoch [16/50], Train Loss: 0.0165, Val Loss: 0.0025\n",
      "Epoch [17/50], Train Loss: 0.0158, Val Loss: 0.0049\n",
      "Epoch [18/50], Train Loss: 0.0151, Val Loss: 0.0273\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=32, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0357, Val Loss: 0.0744\n",
      "Epoch [2/50], Train Loss: 0.0758, Val Loss: 0.0919\n",
      "Epoch [3/50], Train Loss: 0.0523, Val Loss: 0.0673\n",
      "Epoch [4/50], Train Loss: 0.0454, Val Loss: 0.0663\n",
      "Epoch [5/50], Train Loss: 0.0366, Val Loss: 0.0414\n",
      "Epoch [6/50], Train Loss: 0.0216, Val Loss: 0.0433\n",
      "Epoch [7/50], Train Loss: 0.0200, Val Loss: 0.0321\n",
      "Epoch [8/50], Train Loss: 0.0100, Val Loss: 0.0148\n",
      "Epoch [9/50], Train Loss: 0.0092, Val Loss: 0.0083\n",
      "Epoch [10/50], Train Loss: 0.0109, Val Loss: 0.0580\n",
      "Epoch [11/50], Train Loss: 0.0136, Val Loss: 0.0086\n",
      "Epoch [12/50], Train Loss: 0.0112, Val Loss: 0.0166\n",
      "Epoch [13/50], Train Loss: 0.0271, Val Loss: 0.0512\n",
      "Epoch [14/50], Train Loss: 0.0351, Val Loss: 0.0627\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=32, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0782, Val Loss: 0.0771\n",
      "Epoch [2/50], Train Loss: 0.0784, Val Loss: 0.0478\n",
      "Epoch [3/50], Train Loss: 0.0548, Val Loss: 0.0408\n",
      "Epoch [4/50], Train Loss: 0.0364, Val Loss: 0.0196\n",
      "Epoch [5/50], Train Loss: 0.0286, Val Loss: 0.0186\n",
      "Epoch [6/50], Train Loss: 0.0233, Val Loss: 0.0095\n",
      "Epoch [7/50], Train Loss: 0.0211, Val Loss: 0.0275\n",
      "Epoch [8/50], Train Loss: 0.0155, Val Loss: 0.0058\n",
      "Epoch [9/50], Train Loss: 0.0234, Val Loss: 0.0126\n",
      "Epoch [10/50], Train Loss: 0.0164, Val Loss: 0.0300\n",
      "Epoch [11/50], Train Loss: 0.0158, Val Loss: 0.0070\n",
      "Epoch [12/50], Train Loss: 0.0154, Val Loss: 0.0038\n",
      "Epoch [13/50], Train Loss: 0.0148, Val Loss: 0.0264\n",
      "Epoch [14/50], Train Loss: 0.0163, Val Loss: 0.0168\n",
      "Epoch [15/50], Train Loss: 0.0173, Val Loss: 0.0169\n",
      "Epoch [16/50], Train Loss: 0.0151, Val Loss: 0.0207\n",
      "Epoch [17/50], Train Loss: 0.0141, Val Loss: 0.0058\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=32, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0728, Val Loss: 0.0739\n",
      "Epoch [2/50], Train Loss: 0.0666, Val Loss: 0.0317\n",
      "Epoch [3/50], Train Loss: 0.0611, Val Loss: 0.0205\n",
      "Epoch [4/50], Train Loss: 0.0398, Val Loss: 0.0094\n",
      "Epoch [5/50], Train Loss: 0.0402, Val Loss: 0.0444\n",
      "Epoch [6/50], Train Loss: 0.0419, Val Loss: 0.0262\n",
      "Epoch [7/50], Train Loss: 0.0291, Val Loss: 0.0121\n",
      "Epoch [8/50], Train Loss: 0.0270, Val Loss: 0.0384\n",
      "Epoch [9/50], Train Loss: 0.0251, Val Loss: 0.0055\n",
      "Epoch [10/50], Train Loss: 0.0230, Val Loss: 0.0056\n",
      "Epoch [11/50], Train Loss: 0.0211, Val Loss: 0.0235\n",
      "Epoch [12/50], Train Loss: 0.0335, Val Loss: 0.0291\n",
      "Epoch [13/50], Train Loss: 0.0273, Val Loss: 0.0211\n",
      "Epoch [14/50], Train Loss: 0.0194, Val Loss: 0.0074\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=64, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0377, Val Loss: 0.0493\n",
      "Epoch [2/50], Train Loss: 0.0486, Val Loss: 0.0342\n",
      "Epoch [3/50], Train Loss: 0.0323, Val Loss: 0.0153\n",
      "Epoch [4/50], Train Loss: 0.0214, Val Loss: 0.0034\n",
      "Epoch [5/50], Train Loss: 0.0140, Val Loss: 0.0058\n",
      "Epoch [6/50], Train Loss: 0.0131, Val Loss: 0.0061\n",
      "Epoch [7/50], Train Loss: 0.0107, Val Loss: 0.0042\n",
      "Epoch [8/50], Train Loss: 0.0044, Val Loss: 0.0021\n",
      "Epoch [9/50], Train Loss: 0.0036, Val Loss: 0.0049\n",
      "Epoch [10/50], Train Loss: 0.0030, Val Loss: 0.0061\n",
      "Epoch [11/50], Train Loss: 0.0032, Val Loss: 0.0039\n",
      "Epoch [12/50], Train Loss: 0.0030, Val Loss: 0.0055\n",
      "Epoch [13/50], Train Loss: 0.0020, Val Loss: 0.0020\n",
      "Epoch [14/50], Train Loss: 0.0020, Val Loss: 0.0033\n",
      "Epoch [15/50], Train Loss: 0.0016, Val Loss: 0.0013\n",
      "Epoch [16/50], Train Loss: 0.0016, Val Loss: 0.0020\n",
      "Epoch [17/50], Train Loss: 0.0015, Val Loss: 0.0016\n",
      "Epoch [18/50], Train Loss: 0.0015, Val Loss: 0.0013\n",
      "Epoch [19/50], Train Loss: 0.0016, Val Loss: 0.0019\n",
      "Epoch [20/50], Train Loss: 0.0015, Val Loss: 0.0014\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=64, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0309, Val Loss: 0.0465\n",
      "Epoch [2/50], Train Loss: 0.0514, Val Loss: 0.0249\n",
      "Epoch [3/50], Train Loss: 0.0296, Val Loss: 0.0076\n",
      "Epoch [4/50], Train Loss: 0.0182, Val Loss: 0.0061\n",
      "Epoch [5/50], Train Loss: 0.0104, Val Loss: 0.0042\n",
      "Epoch [6/50], Train Loss: 0.0072, Val Loss: 0.0098\n",
      "Epoch [7/50], Train Loss: 0.0063, Val Loss: 0.0068\n",
      "Epoch [8/50], Train Loss: 0.0044, Val Loss: 0.0035\n",
      "Epoch [9/50], Train Loss: 0.0040, Val Loss: 0.0017\n",
      "Epoch [10/50], Train Loss: 0.0038, Val Loss: 0.0031\n",
      "Epoch [11/50], Train Loss: 0.0038, Val Loss: 0.0018\n",
      "Epoch [12/50], Train Loss: 0.0034, Val Loss: 0.0022\n",
      "Epoch [13/50], Train Loss: 0.0035, Val Loss: 0.0022\n",
      "Epoch [14/50], Train Loss: 0.0035, Val Loss: 0.0014\n",
      "Epoch [15/50], Train Loss: 0.0036, Val Loss: 0.0030\n",
      "Epoch [16/50], Train Loss: 0.0033, Val Loss: 0.0017\n",
      "Epoch [17/50], Train Loss: 0.0035, Val Loss: 0.0018\n",
      "Epoch [18/50], Train Loss: 0.0033, Val Loss: 0.0025\n",
      "Epoch [19/50], Train Loss: 0.0032, Val Loss: 0.0017\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=64, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0494, Val Loss: 0.0700\n",
      "Epoch [2/50], Train Loss: 0.0561, Val Loss: 0.0302\n",
      "Epoch [3/50], Train Loss: 0.0345, Val Loss: 0.0070\n",
      "Epoch [4/50], Train Loss: 0.0223, Val Loss: 0.0062\n",
      "Epoch [5/50], Train Loss: 0.0182, Val Loss: 0.0085\n",
      "Epoch [6/50], Train Loss: 0.0129, Val Loss: 0.0020\n",
      "Epoch [7/50], Train Loss: 0.0124, Val Loss: 0.0072\n",
      "Epoch [8/50], Train Loss: 0.0093, Val Loss: 0.0033\n",
      "Epoch [9/50], Train Loss: 0.0081, Val Loss: 0.0028\n",
      "Epoch [10/50], Train Loss: 0.0079, Val Loss: 0.0033\n",
      "Epoch [11/50], Train Loss: 0.0071, Val Loss: 0.0040\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=64, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0258, Val Loss: 0.1007\n",
      "Epoch [2/50], Train Loss: 0.0610, Val Loss: 0.0106\n",
      "Epoch [3/50], Train Loss: 0.0490, Val Loss: 0.0192\n",
      "Epoch [4/50], Train Loss: 0.0399, Val Loss: 0.0074\n",
      "Epoch [5/50], Train Loss: 0.0130, Val Loss: 0.0048\n",
      "Epoch [6/50], Train Loss: 0.0201, Val Loss: 0.0278\n",
      "Epoch [7/50], Train Loss: 0.0190, Val Loss: 0.0248\n",
      "Epoch [8/50], Train Loss: 0.0069, Val Loss: 0.0019\n",
      "Epoch [9/50], Train Loss: 0.0033, Val Loss: 0.0053\n",
      "Epoch [10/50], Train Loss: 0.0024, Val Loss: 0.0030\n",
      "Epoch [11/50], Train Loss: 0.0031, Val Loss: 0.0021\n",
      "Epoch [12/50], Train Loss: 0.0023, Val Loss: 0.0028\n",
      "Epoch [13/50], Train Loss: 0.0025, Val Loss: 0.0057\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=64, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0307, Val Loss: 0.0874\n",
      "Epoch [2/50], Train Loss: 0.0591, Val Loss: 0.0334\n",
      "Epoch [3/50], Train Loss: 0.0520, Val Loss: 0.0204\n",
      "Epoch [4/50], Train Loss: 0.0393, Val Loss: 0.0160\n",
      "Epoch [5/50], Train Loss: 0.0230, Val Loss: 0.0286\n",
      "Epoch [6/50], Train Loss: 0.0099, Val Loss: 0.0054\n",
      "Epoch [7/50], Train Loss: 0.0123, Val Loss: 0.0036\n",
      "Epoch [8/50], Train Loss: 0.0103, Val Loss: 0.0151\n",
      "Epoch [9/50], Train Loss: 0.0109, Val Loss: 0.0041\n",
      "Epoch [10/50], Train Loss: 0.0083, Val Loss: 0.0081\n",
      "Epoch [11/50], Train Loss: 0.0126, Val Loss: 0.0068\n",
      "Epoch [12/50], Train Loss: 0.0092, Val Loss: 0.0015\n",
      "Epoch [13/50], Train Loss: 0.0065, Val Loss: 0.0071\n",
      "Epoch [14/50], Train Loss: 0.0079, Val Loss: 0.0031\n",
      "Epoch [15/50], Train Loss: 0.0062, Val Loss: 0.0024\n",
      "Epoch [16/50], Train Loss: 0.0067, Val Loss: 0.0099\n",
      "Epoch [17/50], Train Loss: 0.0099, Val Loss: 0.0035\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=64, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0433, Val Loss: 0.1042\n",
      "Epoch [2/50], Train Loss: 0.0596, Val Loss: 0.0266\n",
      "Epoch [3/50], Train Loss: 0.0514, Val Loss: 0.0205\n",
      "Epoch [4/50], Train Loss: 0.0315, Val Loss: 0.0029\n",
      "Epoch [5/50], Train Loss: 0.0161, Val Loss: 0.0024\n",
      "Epoch [6/50], Train Loss: 0.0185, Val Loss: 0.0061\n",
      "Epoch [7/50], Train Loss: 0.0166, Val Loss: 0.0163\n",
      "Epoch [8/50], Train Loss: 0.0271, Val Loss: 0.0134\n",
      "Epoch [9/50], Train Loss: 0.0178, Val Loss: 0.0179\n",
      "Epoch [10/50], Train Loss: 0.0183, Val Loss: 0.0289\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=64, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0268, Val Loss: 0.0894\n",
      "Epoch [2/50], Train Loss: 0.0673, Val Loss: 0.0654\n",
      "Epoch [3/50], Train Loss: 0.0546, Val Loss: 0.0423\n",
      "Epoch [4/50], Train Loss: 0.0513, Val Loss: 0.0401\n",
      "Epoch [5/50], Train Loss: 0.0384, Val Loss: 0.0748\n",
      "Epoch [6/50], Train Loss: 0.0387, Val Loss: 0.0067\n",
      "Epoch [7/50], Train Loss: 0.0215, Val Loss: 0.0078\n",
      "Epoch [8/50], Train Loss: 0.0177, Val Loss: 0.0050\n",
      "Epoch [9/50], Train Loss: 0.0091, Val Loss: 0.0101\n",
      "Epoch [10/50], Train Loss: 0.0096, Val Loss: 0.0162\n",
      "Epoch [11/50], Train Loss: 0.0086, Val Loss: 0.0161\n",
      "Epoch [12/50], Train Loss: 0.0081, Val Loss: 0.0068\n",
      "Epoch [13/50], Train Loss: 0.0084, Val Loss: 0.0148\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=64, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0316, Val Loss: 0.0790\n",
      "Epoch [2/50], Train Loss: 0.0571, Val Loss: 0.0948\n",
      "Epoch [3/50], Train Loss: 0.0485, Val Loss: 0.0151\n",
      "Epoch [4/50], Train Loss: 0.0575, Val Loss: 0.0111\n",
      "Epoch [5/50], Train Loss: 0.0376, Val Loss: 0.0036\n",
      "Epoch [6/50], Train Loss: 0.0200, Val Loss: 0.0034\n",
      "Epoch [7/50], Train Loss: 0.0144, Val Loss: 0.0039\n",
      "Epoch [8/50], Train Loss: 0.0174, Val Loss: 0.0164\n",
      "Epoch [9/50], Train Loss: 0.0179, Val Loss: 0.0242\n",
      "Epoch [10/50], Train Loss: 0.0195, Val Loss: 0.0045\n",
      "Epoch [11/50], Train Loss: 0.0092, Val Loss: 0.0048\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=64, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0465, Val Loss: 0.0799\n",
      "Epoch [2/50], Train Loss: 0.0629, Val Loss: 0.0989\n",
      "Epoch [3/50], Train Loss: 0.0559, Val Loss: 0.0401\n",
      "Epoch [4/50], Train Loss: 0.0427, Val Loss: 0.0058\n",
      "Epoch [5/50], Train Loss: 0.0304, Val Loss: 0.0061\n",
      "Epoch [6/50], Train Loss: 0.0198, Val Loss: 0.0043\n",
      "Epoch [7/50], Train Loss: 0.0181, Val Loss: 0.0044\n",
      "Epoch [8/50], Train Loss: 0.0140, Val Loss: 0.0070\n",
      "Epoch [9/50], Train Loss: 0.0341, Val Loss: 0.0247\n",
      "Epoch [10/50], Train Loss: 0.0221, Val Loss: 0.0116\n",
      "Epoch [11/50], Train Loss: 0.0148, Val Loss: 0.0082\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=128, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0193, Val Loss: 0.0629\n",
      "Epoch [2/50], Train Loss: 0.0459, Val Loss: 0.0295\n",
      "Epoch [3/50], Train Loss: 0.0263, Val Loss: 0.0137\n",
      "Epoch [4/50], Train Loss: 0.0410, Val Loss: 0.0594\n",
      "Epoch [5/50], Train Loss: 0.0366, Val Loss: 0.0386\n",
      "Epoch [6/50], Train Loss: 0.0170, Val Loss: 0.0223\n",
      "Epoch [7/50], Train Loss: 0.0191, Val Loss: 0.0043\n",
      "Epoch [8/50], Train Loss: 0.0089, Val Loss: 0.0053\n",
      "Epoch [9/50], Train Loss: 0.0103, Val Loss: 0.0054\n",
      "Epoch [10/50], Train Loss: 0.0075, Val Loss: 0.0025\n",
      "Epoch [11/50], Train Loss: 0.0041, Val Loss: 0.0085\n",
      "Epoch [12/50], Train Loss: 0.0021, Val Loss: 0.0015\n",
      "Epoch [13/50], Train Loss: 0.0023, Val Loss: 0.0013\n",
      "Epoch [14/50], Train Loss: 0.0021, Val Loss: 0.0046\n",
      "Epoch [15/50], Train Loss: 0.0017, Val Loss: 0.0017\n",
      "Epoch [16/50], Train Loss: 0.0022, Val Loss: 0.0014\n",
      "Epoch [17/50], Train Loss: 0.0020, Val Loss: 0.0049\n",
      "Epoch [18/50], Train Loss: 0.0017, Val Loss: 0.0015\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=128, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0221, Val Loss: 0.0555\n",
      "Epoch [2/50], Train Loss: 0.0458, Val Loss: 0.0314\n",
      "Epoch [3/50], Train Loss: 0.0319, Val Loss: 0.0124\n",
      "Epoch [4/50], Train Loss: 0.0196, Val Loss: 0.0030\n",
      "Epoch [5/50], Train Loss: 0.0187, Val Loss: 0.0115\n",
      "Epoch [6/50], Train Loss: 0.0112, Val Loss: 0.0011\n",
      "Epoch [7/50], Train Loss: 0.0069, Val Loss: 0.0089\n",
      "Epoch [8/50], Train Loss: 0.0058, Val Loss: 0.0025\n",
      "Epoch [9/50], Train Loss: 0.0045, Val Loss: 0.0089\n",
      "Epoch [10/50], Train Loss: 0.0038, Val Loss: 0.0023\n",
      "Epoch [11/50], Train Loss: 0.0032, Val Loss: 0.0021\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=128, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0343, Val Loss: 0.0675\n",
      "Epoch [2/50], Train Loss: 0.0521, Val Loss: 0.0277\n",
      "Epoch [3/50], Train Loss: 0.0402, Val Loss: 0.0067\n",
      "Epoch [4/50], Train Loss: 0.0271, Val Loss: 0.0071\n",
      "Epoch [5/50], Train Loss: 0.0178, Val Loss: 0.0102\n",
      "Epoch [6/50], Train Loss: 0.0115, Val Loss: 0.0033\n",
      "Epoch [7/50], Train Loss: 0.0099, Val Loss: 0.0049\n",
      "Epoch [8/50], Train Loss: 0.0105, Val Loss: 0.0024\n",
      "Epoch [9/50], Train Loss: 0.0065, Val Loss: 0.0041\n",
      "Epoch [10/50], Train Loss: 0.0065, Val Loss: 0.0098\n",
      "Epoch [11/50], Train Loss: 0.0052, Val Loss: 0.0020\n",
      "Epoch [12/50], Train Loss: 0.0064, Val Loss: 0.0043\n",
      "Epoch [13/50], Train Loss: 0.0055, Val Loss: 0.0029\n",
      "Epoch [14/50], Train Loss: 0.0058, Val Loss: 0.0076\n",
      "Epoch [15/50], Train Loss: 0.0055, Val Loss: 0.0014\n",
      "Epoch [16/50], Train Loss: 0.0069, Val Loss: 0.0034\n",
      "Epoch [17/50], Train Loss: 0.0062, Val Loss: 0.0037\n",
      "Epoch [18/50], Train Loss: 0.0054, Val Loss: 0.0029\n",
      "Epoch [19/50], Train Loss: 0.0067, Val Loss: 0.0043\n",
      "Epoch [20/50], Train Loss: 0.0080, Val Loss: 0.0067\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=128, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0205, Val Loss: 0.0719\n",
      "Epoch [2/50], Train Loss: 0.0389, Val Loss: 0.0788\n",
      "Epoch [3/50], Train Loss: 0.0393, Val Loss: 0.0558\n",
      "Epoch [4/50], Train Loss: 0.0299, Val Loss: 0.0122\n",
      "Epoch [5/50], Train Loss: 0.0290, Val Loss: 0.0089\n",
      "Epoch [6/50], Train Loss: 0.0203, Val Loss: 0.0083\n",
      "Epoch [7/50], Train Loss: 0.0207, Val Loss: 0.0144\n",
      "Epoch [8/50], Train Loss: 0.0072, Val Loss: 0.0020\n",
      "Epoch [9/50], Train Loss: 0.0134, Val Loss: 0.0124\n",
      "Epoch [10/50], Train Loss: 0.0096, Val Loss: 0.0053\n",
      "Epoch [11/50], Train Loss: 0.0083, Val Loss: 0.0070\n",
      "Epoch [12/50], Train Loss: 0.0043, Val Loss: 0.0098\n",
      "Epoch [13/50], Train Loss: 0.0029, Val Loss: 0.0073\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=128, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0251, Val Loss: 0.0571\n",
      "Epoch [2/50], Train Loss: 0.0445, Val Loss: 0.0782\n",
      "Epoch [3/50], Train Loss: 0.0389, Val Loss: 0.0513\n",
      "Epoch [4/50], Train Loss: 0.0509, Val Loss: 0.0174\n",
      "Epoch [5/50], Train Loss: 0.0274, Val Loss: 0.0100\n",
      "Epoch [6/50], Train Loss: 0.0222, Val Loss: 0.0110\n",
      "Epoch [7/50], Train Loss: 0.0081, Val Loss: 0.0032\n",
      "Epoch [8/50], Train Loss: 0.0061, Val Loss: 0.0109\n",
      "Epoch [9/50], Train Loss: 0.0063, Val Loss: 0.0038\n",
      "Epoch [10/50], Train Loss: 0.0053, Val Loss: 0.0048\n",
      "Epoch [11/50], Train Loss: 0.0043, Val Loss: 0.0060\n",
      "Epoch [12/50], Train Loss: 0.0047, Val Loss: 0.0037\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=128, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0280, Val Loss: 0.0538\n",
      "Epoch [2/50], Train Loss: 0.0491, Val Loss: 0.0723\n",
      "Epoch [3/50], Train Loss: 0.0405, Val Loss: 0.0268\n",
      "Epoch [4/50], Train Loss: 0.0389, Val Loss: 0.0167\n",
      "Epoch [5/50], Train Loss: 0.0188, Val Loss: 0.0060\n",
      "Epoch [6/50], Train Loss: 0.0203, Val Loss: 0.0019\n",
      "Epoch [7/50], Train Loss: 0.0111, Val Loss: 0.0015\n",
      "Epoch [8/50], Train Loss: 0.0118, Val Loss: 0.0040\n",
      "Epoch [9/50], Train Loss: 0.0086, Val Loss: 0.0066\n",
      "Epoch [10/50], Train Loss: 0.0193, Val Loss: 0.0045\n",
      "Epoch [11/50], Train Loss: 0.0182, Val Loss: 0.0099\n",
      "Epoch [12/50], Train Loss: 0.0159, Val Loss: 0.0106\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=128, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0219, Val Loss: 0.0668\n",
      "Epoch [2/50], Train Loss: 0.0464, Val Loss: 0.0712\n",
      "Epoch [3/50], Train Loss: 0.0398, Val Loss: 0.0396\n",
      "Epoch [4/50], Train Loss: 0.0433, Val Loss: 0.0223\n",
      "Epoch [5/50], Train Loss: 0.0382, Val Loss: 0.0101\n",
      "Epoch [6/50], Train Loss: 0.0321, Val Loss: 0.0202\n",
      "Epoch [7/50], Train Loss: 0.0272, Val Loss: 0.0126\n",
      "Epoch [8/50], Train Loss: 0.0187, Val Loss: 0.0167\n",
      "Epoch [9/50], Train Loss: 0.0139, Val Loss: 0.0036\n",
      "Epoch [10/50], Train Loss: 0.0095, Val Loss: 0.0292\n",
      "Epoch [11/50], Train Loss: 0.0157, Val Loss: 0.0147\n",
      "Epoch [12/50], Train Loss: 0.0222, Val Loss: 0.0217\n",
      "Epoch [13/50], Train Loss: 0.0081, Val Loss: 0.0229\n",
      "Epoch [14/50], Train Loss: 0.0187, Val Loss: 0.0094\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=128, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0285, Val Loss: 0.0710\n",
      "Epoch [2/50], Train Loss: 0.0500, Val Loss: 0.0864\n",
      "Epoch [3/50], Train Loss: 0.0499, Val Loss: 0.0413\n",
      "Epoch [4/50], Train Loss: 0.0732, Val Loss: 0.0113\n",
      "Epoch [5/50], Train Loss: 0.0331, Val Loss: 0.0111\n",
      "Epoch [6/50], Train Loss: 0.0307, Val Loss: 0.0433\n",
      "Epoch [7/50], Train Loss: 0.0208, Val Loss: 0.0066\n",
      "Epoch [8/50], Train Loss: 0.0217, Val Loss: 0.0091\n",
      "Epoch [9/50], Train Loss: 0.0096, Val Loss: 0.0026\n",
      "Epoch [10/50], Train Loss: 0.0323, Val Loss: 0.0250\n",
      "Epoch [11/50], Train Loss: 0.0180, Val Loss: 0.0194\n",
      "Epoch [12/50], Train Loss: 0.0184, Val Loss: 0.0197\n",
      "Epoch [13/50], Train Loss: 0.0082, Val Loss: 0.0097\n",
      "Epoch [14/50], Train Loss: 0.0159, Val Loss: 0.0167\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=128, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0304, Val Loss: 0.0484\n",
      "Epoch [2/50], Train Loss: 0.0586, Val Loss: 0.0734\n",
      "Epoch [3/50], Train Loss: 0.0520, Val Loss: 0.0525\n",
      "Epoch [4/50], Train Loss: 0.0381, Val Loss: 0.0254\n",
      "Epoch [5/50], Train Loss: 0.0276, Val Loss: 0.0169\n",
      "Epoch [6/50], Train Loss: 0.0122, Val Loss: 0.0144\n",
      "Epoch [7/50], Train Loss: 0.0132, Val Loss: 0.0096\n",
      "Epoch [8/50], Train Loss: 0.0138, Val Loss: 0.0082\n",
      "Epoch [9/50], Train Loss: 0.0253, Val Loss: 0.0220\n",
      "Epoch [10/50], Train Loss: 0.0234, Val Loss: 0.0318\n",
      "Epoch [11/50], Train Loss: 0.0438, Val Loss: 0.0115\n",
      "Epoch [12/50], Train Loss: 0.0183, Val Loss: 0.0034\n",
      "Epoch [13/50], Train Loss: 0.0144, Val Loss: 0.0119\n",
      "Epoch [14/50], Train Loss: 0.0259, Val Loss: 0.0065\n",
      "Epoch [15/50], Train Loss: 0.0160, Val Loss: 0.0139\n",
      "Epoch [16/50], Train Loss: 0.0162, Val Loss: 0.0091\n",
      "Epoch [17/50], Train Loss: 0.0295, Val Loss: 0.0052\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=16, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1130, Val Loss: 0.1643\n",
      "Epoch [2/50], Train Loss: 0.0634, Val Loss: 0.0929\n",
      "Epoch [3/50], Train Loss: 0.0436, Val Loss: 0.0661\n",
      "Epoch [4/50], Train Loss: 0.0372, Val Loss: 0.0618\n",
      "Epoch [5/50], Train Loss: 0.0341, Val Loss: 0.0603\n",
      "Epoch [6/50], Train Loss: 0.0317, Val Loss: 0.0568\n",
      "Epoch [7/50], Train Loss: 0.0297, Val Loss: 0.0518\n",
      "Epoch [8/50], Train Loss: 0.0277, Val Loss: 0.0460\n",
      "Epoch [9/50], Train Loss: 0.0257, Val Loss: 0.0398\n",
      "Epoch [10/50], Train Loss: 0.0236, Val Loss: 0.0332\n",
      "Epoch [11/50], Train Loss: 0.0213, Val Loss: 0.0266\n",
      "Epoch [12/50], Train Loss: 0.0190, Val Loss: 0.0203\n",
      "Epoch [13/50], Train Loss: 0.0167, Val Loss: 0.0147\n",
      "Epoch [14/50], Train Loss: 0.0145, Val Loss: 0.0106\n",
      "Epoch [15/50], Train Loss: 0.0127, Val Loss: 0.0080\n",
      "Epoch [16/50], Train Loss: 0.0114, Val Loss: 0.0067\n",
      "Epoch [17/50], Train Loss: 0.0105, Val Loss: 0.0060\n",
      "Epoch [18/50], Train Loss: 0.0097, Val Loss: 0.0056\n",
      "Epoch [19/50], Train Loss: 0.0089, Val Loss: 0.0053\n",
      "Epoch [20/50], Train Loss: 0.0081, Val Loss: 0.0051\n",
      "Epoch [21/50], Train Loss: 0.0072, Val Loss: 0.0049\n",
      "Epoch [22/50], Train Loss: 0.0064, Val Loss: 0.0047\n",
      "Epoch [23/50], Train Loss: 0.0056, Val Loss: 0.0045\n",
      "Epoch [24/50], Train Loss: 0.0048, Val Loss: 0.0042\n",
      "Epoch [25/50], Train Loss: 0.0041, Val Loss: 0.0040\n",
      "Epoch [26/50], Train Loss: 0.0035, Val Loss: 0.0039\n",
      "Epoch [27/50], Train Loss: 0.0031, Val Loss: 0.0039\n",
      "Epoch [28/50], Train Loss: 0.0028, Val Loss: 0.0039\n",
      "Epoch [29/50], Train Loss: 0.0027, Val Loss: 0.0039\n",
      "Epoch [30/50], Train Loss: 0.0025, Val Loss: 0.0038\n",
      "Epoch [31/50], Train Loss: 0.0024, Val Loss: 0.0038\n",
      "Epoch [32/50], Train Loss: 0.0023, Val Loss: 0.0037\n",
      "Epoch [33/50], Train Loss: 0.0023, Val Loss: 0.0037\n",
      "Epoch [34/50], Train Loss: 0.0022, Val Loss: 0.0036\n",
      "Epoch [35/50], Train Loss: 0.0022, Val Loss: 0.0036\n",
      "Epoch [36/50], Train Loss: 0.0021, Val Loss: 0.0036\n",
      "Epoch [37/50], Train Loss: 0.0021, Val Loss: 0.0035\n",
      "Epoch [38/50], Train Loss: 0.0021, Val Loss: 0.0035\n",
      "Epoch [39/50], Train Loss: 0.0020, Val Loss: 0.0035\n",
      "Epoch [40/50], Train Loss: 0.0020, Val Loss: 0.0034\n",
      "Epoch [41/50], Train Loss: 0.0020, Val Loss: 0.0034\n",
      "Epoch [42/50], Train Loss: 0.0020, Val Loss: 0.0034\n",
      "Epoch [43/50], Train Loss: 0.0020, Val Loss: 0.0034\n",
      "Epoch [44/50], Train Loss: 0.0019, Val Loss: 0.0033\n",
      "Epoch [45/50], Train Loss: 0.0019, Val Loss: 0.0033\n",
      "Epoch [46/50], Train Loss: 0.0019, Val Loss: 0.0033\n",
      "Epoch [47/50], Train Loss: 0.0019, Val Loss: 0.0033\n",
      "Epoch [48/50], Train Loss: 0.0019, Val Loss: 0.0032\n",
      "Epoch [49/50], Train Loss: 0.0019, Val Loss: 0.0032\n",
      "Epoch [50/50], Train Loss: 0.0019, Val Loss: 0.0032\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=16, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1734, Val Loss: 0.3439\n",
      "Epoch [2/50], Train Loss: 0.1078, Val Loss: 0.2348\n",
      "Epoch [3/50], Train Loss: 0.0722, Val Loss: 0.1623\n",
      "Epoch [4/50], Train Loss: 0.0539, Val Loss: 0.1181\n",
      "Epoch [5/50], Train Loss: 0.0464, Val Loss: 0.0961\n",
      "Epoch [6/50], Train Loss: 0.0428, Val Loss: 0.0862\n",
      "Epoch [7/50], Train Loss: 0.0407, Val Loss: 0.0813\n",
      "Epoch [8/50], Train Loss: 0.0393, Val Loss: 0.0763\n",
      "Epoch [9/50], Train Loss: 0.0370, Val Loss: 0.0721\n",
      "Epoch [10/50], Train Loss: 0.0350, Val Loss: 0.0672\n",
      "Epoch [11/50], Train Loss: 0.0325, Val Loss: 0.0606\n",
      "Epoch [12/50], Train Loss: 0.0316, Val Loss: 0.0543\n",
      "Epoch [13/50], Train Loss: 0.0280, Val Loss: 0.0484\n",
      "Epoch [14/50], Train Loss: 0.0265, Val Loss: 0.0421\n",
      "Epoch [15/50], Train Loss: 0.0229, Val Loss: 0.0358\n",
      "Epoch [16/50], Train Loss: 0.0223, Val Loss: 0.0293\n",
      "Epoch [17/50], Train Loss: 0.0200, Val Loss: 0.0239\n",
      "Epoch [18/50], Train Loss: 0.0182, Val Loss: 0.0222\n",
      "Epoch [19/50], Train Loss: 0.0173, Val Loss: 0.0199\n",
      "Epoch [20/50], Train Loss: 0.0174, Val Loss: 0.0176\n",
      "Epoch [21/50], Train Loss: 0.0157, Val Loss: 0.0182\n",
      "Epoch [22/50], Train Loss: 0.0165, Val Loss: 0.0165\n",
      "Epoch [23/50], Train Loss: 0.0150, Val Loss: 0.0166\n",
      "Epoch [24/50], Train Loss: 0.0144, Val Loss: 0.0163\n",
      "Epoch [25/50], Train Loss: 0.0142, Val Loss: 0.0149\n",
      "Epoch [26/50], Train Loss: 0.0144, Val Loss: 0.0158\n",
      "Epoch [27/50], Train Loss: 0.0126, Val Loss: 0.0161\n",
      "Epoch [28/50], Train Loss: 0.0125, Val Loss: 0.0144\n",
      "Epoch [29/50], Train Loss: 0.0120, Val Loss: 0.0143\n",
      "Epoch [30/50], Train Loss: 0.0117, Val Loss: 0.0147\n",
      "Epoch [31/50], Train Loss: 0.0113, Val Loss: 0.0128\n",
      "Epoch [32/50], Train Loss: 0.0111, Val Loss: 0.0122\n",
      "Epoch [33/50], Train Loss: 0.0110, Val Loss: 0.0124\n",
      "Epoch [34/50], Train Loss: 0.0103, Val Loss: 0.0115\n",
      "Epoch [35/50], Train Loss: 0.0100, Val Loss: 0.0118\n",
      "Epoch [36/50], Train Loss: 0.0097, Val Loss: 0.0121\n",
      "Epoch [37/50], Train Loss: 0.0091, Val Loss: 0.0115\n",
      "Epoch [38/50], Train Loss: 0.0087, Val Loss: 0.0109\n",
      "Epoch [39/50], Train Loss: 0.0084, Val Loss: 0.0102\n",
      "Epoch [40/50], Train Loss: 0.0084, Val Loss: 0.0102\n",
      "Epoch [41/50], Train Loss: 0.0082, Val Loss: 0.0111\n",
      "Epoch [42/50], Train Loss: 0.0078, Val Loss: 0.0102\n",
      "Epoch [43/50], Train Loss: 0.0079, Val Loss: 0.0100\n",
      "Epoch [44/50], Train Loss: 0.0076, Val Loss: 0.0086\n",
      "Epoch [45/50], Train Loss: 0.0075, Val Loss: 0.0104\n",
      "Epoch [46/50], Train Loss: 0.0072, Val Loss: 0.0106\n",
      "Epoch [47/50], Train Loss: 0.0072, Val Loss: 0.0086\n",
      "Epoch [48/50], Train Loss: 0.0074, Val Loss: 0.0081\n",
      "Epoch [49/50], Train Loss: 0.0070, Val Loss: 0.0094\n",
      "Epoch [50/50], Train Loss: 0.0070, Val Loss: 0.0083\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=16, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1557, Val Loss: 0.2731\n",
      "Epoch [2/50], Train Loss: 0.1019, Val Loss: 0.1786\n",
      "Epoch [3/50], Train Loss: 0.0772, Val Loss: 0.1255\n",
      "Epoch [4/50], Train Loss: 0.0662, Val Loss: 0.1049\n",
      "Epoch [5/50], Train Loss: 0.0632, Val Loss: 0.0907\n",
      "Epoch [6/50], Train Loss: 0.0564, Val Loss: 0.0838\n",
      "Epoch [7/50], Train Loss: 0.0499, Val Loss: 0.0740\n",
      "Epoch [8/50], Train Loss: 0.0464, Val Loss: 0.0659\n",
      "Epoch [9/50], Train Loss: 0.0452, Val Loss: 0.0578\n",
      "Epoch [10/50], Train Loss: 0.0389, Val Loss: 0.0519\n",
      "Epoch [11/50], Train Loss: 0.0394, Val Loss: 0.0444\n",
      "Epoch [12/50], Train Loss: 0.0359, Val Loss: 0.0352\n",
      "Epoch [13/50], Train Loss: 0.0347, Val Loss: 0.0314\n",
      "Epoch [14/50], Train Loss: 0.0330, Val Loss: 0.0261\n",
      "Epoch [15/50], Train Loss: 0.0317, Val Loss: 0.0238\n",
      "Epoch [16/50], Train Loss: 0.0301, Val Loss: 0.0217\n",
      "Epoch [17/50], Train Loss: 0.0285, Val Loss: 0.0244\n",
      "Epoch [18/50], Train Loss: 0.0261, Val Loss: 0.0202\n",
      "Epoch [19/50], Train Loss: 0.0251, Val Loss: 0.0207\n",
      "Epoch [20/50], Train Loss: 0.0252, Val Loss: 0.0167\n",
      "Epoch [21/50], Train Loss: 0.0247, Val Loss: 0.0207\n",
      "Epoch [22/50], Train Loss: 0.0239, Val Loss: 0.0166\n",
      "Epoch [23/50], Train Loss: 0.0223, Val Loss: 0.0157\n",
      "Epoch [24/50], Train Loss: 0.0217, Val Loss: 0.0156\n",
      "Epoch [25/50], Train Loss: 0.0214, Val Loss: 0.0135\n",
      "Epoch [26/50], Train Loss: 0.0198, Val Loss: 0.0149\n",
      "Epoch [27/50], Train Loss: 0.0187, Val Loss: 0.0148\n",
      "Epoch [28/50], Train Loss: 0.0187, Val Loss: 0.0150\n",
      "Epoch [29/50], Train Loss: 0.0175, Val Loss: 0.0122\n",
      "Epoch [30/50], Train Loss: 0.0173, Val Loss: 0.0138\n",
      "Epoch [31/50], Train Loss: 0.0175, Val Loss: 0.0137\n",
      "Epoch [32/50], Train Loss: 0.0183, Val Loss: 0.0105\n",
      "Epoch [33/50], Train Loss: 0.0170, Val Loss: 0.0132\n",
      "Epoch [34/50], Train Loss: 0.0178, Val Loss: 0.0114\n",
      "Epoch [35/50], Train Loss: 0.0162, Val Loss: 0.0110\n",
      "Epoch [36/50], Train Loss: 0.0161, Val Loss: 0.0122\n",
      "Epoch [37/50], Train Loss: 0.0145, Val Loss: 0.0115\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=16, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1162, Val Loss: 0.2416\n",
      "Epoch [2/50], Train Loss: 0.0632, Val Loss: 0.1597\n",
      "Epoch [3/50], Train Loss: 0.0409, Val Loss: 0.1097\n",
      "Epoch [4/50], Train Loss: 0.0353, Val Loss: 0.0862\n",
      "Epoch [5/50], Train Loss: 0.0329, Val Loss: 0.0732\n",
      "Epoch [6/50], Train Loss: 0.0303, Val Loss: 0.0629\n",
      "Epoch [7/50], Train Loss: 0.0274, Val Loss: 0.0527\n",
      "Epoch [8/50], Train Loss: 0.0240, Val Loss: 0.0421\n",
      "Epoch [9/50], Train Loss: 0.0202, Val Loss: 0.0319\n",
      "Epoch [10/50], Train Loss: 0.0166, Val Loss: 0.0241\n",
      "Epoch [11/50], Train Loss: 0.0142, Val Loss: 0.0201\n",
      "Epoch [12/50], Train Loss: 0.0127, Val Loss: 0.0185\n",
      "Epoch [13/50], Train Loss: 0.0113, Val Loss: 0.0174\n",
      "Epoch [14/50], Train Loss: 0.0098, Val Loss: 0.0162\n",
      "Epoch [15/50], Train Loss: 0.0081, Val Loss: 0.0147\n",
      "Epoch [16/50], Train Loss: 0.0062, Val Loss: 0.0123\n",
      "Epoch [17/50], Train Loss: 0.0043, Val Loss: 0.0098\n",
      "Epoch [18/50], Train Loss: 0.0031, Val Loss: 0.0080\n",
      "Epoch [19/50], Train Loss: 0.0026, Val Loss: 0.0077\n",
      "Epoch [20/50], Train Loss: 0.0023, Val Loss: 0.0081\n",
      "Epoch [21/50], Train Loss: 0.0022, Val Loss: 0.0073\n",
      "Epoch [22/50], Train Loss: 0.0021, Val Loss: 0.0067\n",
      "Epoch [23/50], Train Loss: 0.0021, Val Loss: 0.0071\n",
      "Epoch [24/50], Train Loss: 0.0020, Val Loss: 0.0071\n",
      "Epoch [25/50], Train Loss: 0.0020, Val Loss: 0.0064\n",
      "Epoch [26/50], Train Loss: 0.0020, Val Loss: 0.0063\n",
      "Epoch [27/50], Train Loss: 0.0020, Val Loss: 0.0067\n",
      "Epoch [28/50], Train Loss: 0.0019, Val Loss: 0.0064\n",
      "Epoch [29/50], Train Loss: 0.0019, Val Loss: 0.0059\n",
      "Epoch [30/50], Train Loss: 0.0019, Val Loss: 0.0061\n",
      "Epoch [31/50], Train Loss: 0.0019, Val Loss: 0.0063\n",
      "Epoch [32/50], Train Loss: 0.0019, Val Loss: 0.0059\n",
      "Epoch [33/50], Train Loss: 0.0019, Val Loss: 0.0057\n",
      "Epoch [34/50], Train Loss: 0.0019, Val Loss: 0.0058\n",
      "Epoch [35/50], Train Loss: 0.0019, Val Loss: 0.0058\n",
      "Epoch [36/50], Train Loss: 0.0019, Val Loss: 0.0057\n",
      "Epoch [37/50], Train Loss: 0.0019, Val Loss: 0.0056\n",
      "Epoch [38/50], Train Loss: 0.0018, Val Loss: 0.0054\n",
      "Epoch [39/50], Train Loss: 0.0019, Val Loss: 0.0055\n",
      "Epoch [40/50], Train Loss: 0.0019, Val Loss: 0.0058\n",
      "Epoch [41/50], Train Loss: 0.0018, Val Loss: 0.0052\n",
      "Epoch [42/50], Train Loss: 0.0019, Val Loss: 0.0048\n",
      "Epoch [43/50], Train Loss: 0.0020, Val Loss: 0.0057\n",
      "Epoch [44/50], Train Loss: 0.0019, Val Loss: 0.0059\n",
      "Epoch [45/50], Train Loss: 0.0020, Val Loss: 0.0045\n",
      "Epoch [46/50], Train Loss: 0.0020, Val Loss: 0.0046\n",
      "Epoch [47/50], Train Loss: 0.0025, Val Loss: 0.0072\n",
      "Epoch [48/50], Train Loss: 0.0022, Val Loss: 0.0052\n",
      "Epoch [49/50], Train Loss: 0.0025, Val Loss: 0.0039\n",
      "Epoch [50/50], Train Loss: 0.0030, Val Loss: 0.0075\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=16, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0955, Val Loss: 0.1961\n",
      "Epoch [2/50], Train Loss: 0.0597, Val Loss: 0.1167\n",
      "Epoch [3/50], Train Loss: 0.0486, Val Loss: 0.0827\n",
      "Epoch [4/50], Train Loss: 0.0429, Val Loss: 0.0689\n",
      "Epoch [5/50], Train Loss: 0.0390, Val Loss: 0.0545\n",
      "Epoch [6/50], Train Loss: 0.0329, Val Loss: 0.0431\n",
      "Epoch [7/50], Train Loss: 0.0277, Val Loss: 0.0310\n",
      "Epoch [8/50], Train Loss: 0.0236, Val Loss: 0.0239\n",
      "Epoch [9/50], Train Loss: 0.0212, Val Loss: 0.0208\n",
      "Epoch [10/50], Train Loss: 0.0184, Val Loss: 0.0197\n",
      "Epoch [11/50], Train Loss: 0.0164, Val Loss: 0.0175\n",
      "Epoch [12/50], Train Loss: 0.0145, Val Loss: 0.0127\n",
      "Epoch [13/50], Train Loss: 0.0126, Val Loss: 0.0103\n",
      "Epoch [14/50], Train Loss: 0.0115, Val Loss: 0.0125\n",
      "Epoch [15/50], Train Loss: 0.0115, Val Loss: 0.0123\n",
      "Epoch [16/50], Train Loss: 0.0103, Val Loss: 0.0087\n",
      "Epoch [17/50], Train Loss: 0.0096, Val Loss: 0.0096\n",
      "Epoch [18/50], Train Loss: 0.0108, Val Loss: 0.0132\n",
      "Epoch [19/50], Train Loss: 0.0101, Val Loss: 0.0076\n",
      "Epoch [20/50], Train Loss: 0.0102, Val Loss: 0.0060\n",
      "Epoch [21/50], Train Loss: 0.0105, Val Loss: 0.0157\n",
      "Epoch [22/50], Train Loss: 0.0096, Val Loss: 0.0076\n",
      "Epoch [23/50], Train Loss: 0.0112, Val Loss: 0.0067\n",
      "Epoch [24/50], Train Loss: 0.0099, Val Loss: 0.0151\n",
      "Epoch [25/50], Train Loss: 0.0088, Val Loss: 0.0088\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=16, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1606, Val Loss: 0.2847\n",
      "Epoch [2/50], Train Loss: 0.0955, Val Loss: 0.1769\n",
      "Epoch [3/50], Train Loss: 0.0822, Val Loss: 0.1378\n",
      "Epoch [4/50], Train Loss: 0.0719, Val Loss: 0.1244\n",
      "Epoch [5/50], Train Loss: 0.0651, Val Loss: 0.1152\n",
      "Epoch [6/50], Train Loss: 0.0602, Val Loss: 0.1071\n",
      "Epoch [7/50], Train Loss: 0.0566, Val Loss: 0.0986\n",
      "Epoch [8/50], Train Loss: 0.0511, Val Loss: 0.0864\n",
      "Epoch [9/50], Train Loss: 0.0504, Val Loss: 0.0782\n",
      "Epoch [10/50], Train Loss: 0.0445, Val Loss: 0.0666\n",
      "Epoch [11/50], Train Loss: 0.0420, Val Loss: 0.0590\n",
      "Epoch [12/50], Train Loss: 0.0393, Val Loss: 0.0515\n",
      "Epoch [13/50], Train Loss: 0.0365, Val Loss: 0.0402\n",
      "Epoch [14/50], Train Loss: 0.0328, Val Loss: 0.0322\n",
      "Epoch [15/50], Train Loss: 0.0298, Val Loss: 0.0289\n",
      "Epoch [16/50], Train Loss: 0.0292, Val Loss: 0.0270\n",
      "Epoch [17/50], Train Loss: 0.0281, Val Loss: 0.0235\n",
      "Epoch [18/50], Train Loss: 0.0245, Val Loss: 0.0234\n",
      "Epoch [19/50], Train Loss: 0.0235, Val Loss: 0.0197\n",
      "Epoch [20/50], Train Loss: 0.0244, Val Loss: 0.0203\n",
      "Epoch [21/50], Train Loss: 0.0243, Val Loss: 0.0201\n",
      "Epoch [22/50], Train Loss: 0.0216, Val Loss: 0.0165\n",
      "Epoch [23/50], Train Loss: 0.0231, Val Loss: 0.0207\n",
      "Epoch [24/50], Train Loss: 0.0204, Val Loss: 0.0188\n",
      "Epoch [25/50], Train Loss: 0.0206, Val Loss: 0.0176\n",
      "Epoch [26/50], Train Loss: 0.0203, Val Loss: 0.0187\n",
      "Epoch [27/50], Train Loss: 0.0201, Val Loss: 0.0179\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=16, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1105, Val Loss: 0.2421\n",
      "Epoch [2/50], Train Loss: 0.0467, Val Loss: 0.1381\n",
      "Epoch [3/50], Train Loss: 0.0354, Val Loss: 0.1001\n",
      "Epoch [4/50], Train Loss: 0.0377, Val Loss: 0.0937\n",
      "Epoch [5/50], Train Loss: 0.0371, Val Loss: 0.0906\n",
      "Epoch [6/50], Train Loss: 0.0355, Val Loss: 0.0854\n",
      "Epoch [7/50], Train Loss: 0.0334, Val Loss: 0.0773\n",
      "Epoch [8/50], Train Loss: 0.0298, Val Loss: 0.0637\n",
      "Epoch [9/50], Train Loss: 0.0215, Val Loss: 0.0447\n",
      "Epoch [10/50], Train Loss: 0.0159, Val Loss: 0.0356\n",
      "Epoch [11/50], Train Loss: 0.0128, Val Loss: 0.0367\n",
      "Epoch [12/50], Train Loss: 0.0090, Val Loss: 0.0241\n",
      "Epoch [13/50], Train Loss: 0.0098, Val Loss: 0.0252\n",
      "Epoch [14/50], Train Loss: 0.0068, Val Loss: 0.0219\n",
      "Epoch [15/50], Train Loss: 0.0099, Val Loss: 0.0201\n",
      "Epoch [16/50], Train Loss: 0.0056, Val Loss: 0.0239\n",
      "Epoch [17/50], Train Loss: 0.0053, Val Loss: 0.0192\n",
      "Epoch [18/50], Train Loss: 0.0035, Val Loss: 0.0204\n",
      "Epoch [19/50], Train Loss: 0.0030, Val Loss: 0.0184\n",
      "Epoch [20/50], Train Loss: 0.0028, Val Loss: 0.0179\n",
      "Epoch [21/50], Train Loss: 0.0027, Val Loss: 0.0182\n",
      "Epoch [22/50], Train Loss: 0.0026, Val Loss: 0.0177\n",
      "Epoch [23/50], Train Loss: 0.0025, Val Loss: 0.0175\n",
      "Epoch [24/50], Train Loss: 0.0025, Val Loss: 0.0176\n",
      "Epoch [25/50], Train Loss: 0.0024, Val Loss: 0.0173\n",
      "Epoch [26/50], Train Loss: 0.0024, Val Loss: 0.0171\n",
      "Epoch [27/50], Train Loss: 0.0025, Val Loss: 0.0172\n",
      "Epoch [28/50], Train Loss: 0.0024, Val Loss: 0.0166\n",
      "Epoch [29/50], Train Loss: 0.0026, Val Loss: 0.0172\n",
      "Epoch [30/50], Train Loss: 0.0024, Val Loss: 0.0165\n",
      "Epoch [31/50], Train Loss: 0.0024, Val Loss: 0.0169\n",
      "Epoch [32/50], Train Loss: 0.0023, Val Loss: 0.0166\n",
      "Epoch [33/50], Train Loss: 0.0022, Val Loss: 0.0163\n",
      "Epoch [34/50], Train Loss: 0.0023, Val Loss: 0.0167\n",
      "Epoch [35/50], Train Loss: 0.0023, Val Loss: 0.0159\n",
      "Epoch [36/50], Train Loss: 0.0023, Val Loss: 0.0164\n",
      "Epoch [37/50], Train Loss: 0.0023, Val Loss: 0.0160\n",
      "Epoch [38/50], Train Loss: 0.0022, Val Loss: 0.0159\n",
      "Epoch [39/50], Train Loss: 0.0022, Val Loss: 0.0159\n",
      "Epoch [40/50], Train Loss: 0.0022, Val Loss: 0.0157\n",
      "Epoch [41/50], Train Loss: 0.0022, Val Loss: 0.0156\n",
      "Epoch [42/50], Train Loss: 0.0022, Val Loss: 0.0154\n",
      "Epoch [43/50], Train Loss: 0.0022, Val Loss: 0.0155\n",
      "Epoch [44/50], Train Loss: 0.0022, Val Loss: 0.0150\n",
      "Epoch [45/50], Train Loss: 0.0022, Val Loss: 0.0153\n",
      "Epoch [46/50], Train Loss: 0.0022, Val Loss: 0.0146\n",
      "Epoch [47/50], Train Loss: 0.0022, Val Loss: 0.0149\n",
      "Epoch [48/50], Train Loss: 0.0021, Val Loss: 0.0141\n",
      "Epoch [49/50], Train Loss: 0.0022, Val Loss: 0.0145\n",
      "Epoch [50/50], Train Loss: 0.0021, Val Loss: 0.0135\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=16, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0869, Val Loss: 0.1357\n",
      "Epoch [2/50], Train Loss: 0.0583, Val Loss: 0.0913\n",
      "Epoch [3/50], Train Loss: 0.0541, Val Loss: 0.0803\n",
      "Epoch [4/50], Train Loss: 0.0503, Val Loss: 0.0697\n",
      "Epoch [5/50], Train Loss: 0.0452, Val Loss: 0.0552\n",
      "Epoch [6/50], Train Loss: 0.0386, Val Loss: 0.0377\n",
      "Epoch [7/50], Train Loss: 0.0324, Val Loss: 0.0280\n",
      "Epoch [8/50], Train Loss: 0.0284, Val Loss: 0.0316\n",
      "Epoch [9/50], Train Loss: 0.0256, Val Loss: 0.0259\n",
      "Epoch [10/50], Train Loss: 0.0237, Val Loss: 0.0204\n",
      "Epoch [11/50], Train Loss: 0.0202, Val Loss: 0.0183\n",
      "Epoch [12/50], Train Loss: 0.0191, Val Loss: 0.0171\n",
      "Epoch [13/50], Train Loss: 0.0178, Val Loss: 0.0145\n",
      "Epoch [14/50], Train Loss: 0.0167, Val Loss: 0.0127\n",
      "Epoch [15/50], Train Loss: 0.0156, Val Loss: 0.0122\n",
      "Epoch [16/50], Train Loss: 0.0159, Val Loss: 0.0145\n",
      "Epoch [17/50], Train Loss: 0.0141, Val Loss: 0.0121\n",
      "Epoch [18/50], Train Loss: 0.0144, Val Loss: 0.0111\n",
      "Epoch [19/50], Train Loss: 0.0137, Val Loss: 0.0096\n",
      "Epoch [20/50], Train Loss: 0.0131, Val Loss: 0.0133\n",
      "Epoch [21/50], Train Loss: 0.0128, Val Loss: 0.0112\n",
      "Epoch [22/50], Train Loss: 0.0126, Val Loss: 0.0099\n",
      "Epoch [23/50], Train Loss: 0.0123, Val Loss: 0.0067\n",
      "Epoch [24/50], Train Loss: 0.0121, Val Loss: 0.0086\n",
      "Epoch [25/50], Train Loss: 0.0115, Val Loss: 0.0135\n",
      "Epoch [26/50], Train Loss: 0.0124, Val Loss: 0.0060\n",
      "Epoch [27/50], Train Loss: 0.0111, Val Loss: 0.0054\n",
      "Epoch [28/50], Train Loss: 0.0120, Val Loss: 0.0143\n",
      "Epoch [29/50], Train Loss: 0.0109, Val Loss: 0.0071\n",
      "Epoch [30/50], Train Loss: 0.0108, Val Loss: 0.0053\n",
      "Epoch [31/50], Train Loss: 0.0111, Val Loss: 0.0140\n",
      "Epoch [32/50], Train Loss: 0.0111, Val Loss: 0.0080\n",
      "Epoch [33/50], Train Loss: 0.0109, Val Loss: 0.0051\n",
      "Epoch [34/50], Train Loss: 0.0111, Val Loss: 0.0151\n",
      "Epoch [35/50], Train Loss: 0.0098, Val Loss: 0.0077\n",
      "Epoch [36/50], Train Loss: 0.0115, Val Loss: 0.0043\n",
      "Epoch [37/50], Train Loss: 0.0108, Val Loss: 0.0166\n",
      "Epoch [38/50], Train Loss: 0.0097, Val Loss: 0.0084\n",
      "Epoch [39/50], Train Loss: 0.0104, Val Loss: 0.0049\n",
      "Epoch [40/50], Train Loss: 0.0101, Val Loss: 0.0142\n",
      "Epoch [41/50], Train Loss: 0.0095, Val Loss: 0.0127\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=16, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1593, Val Loss: 0.2300\n",
      "Epoch [2/50], Train Loss: 0.1106, Val Loss: 0.1778\n",
      "Epoch [3/50], Train Loss: 0.0915, Val Loss: 0.1561\n",
      "Epoch [4/50], Train Loss: 0.0802, Val Loss: 0.1435\n",
      "Epoch [5/50], Train Loss: 0.0726, Val Loss: 0.1273\n",
      "Epoch [6/50], Train Loss: 0.0647, Val Loss: 0.1135\n",
      "Epoch [7/50], Train Loss: 0.0597, Val Loss: 0.0989\n",
      "Epoch [8/50], Train Loss: 0.0545, Val Loss: 0.0797\n",
      "Epoch [9/50], Train Loss: 0.0501, Val Loss: 0.0725\n",
      "Epoch [10/50], Train Loss: 0.0457, Val Loss: 0.0626\n",
      "Epoch [11/50], Train Loss: 0.0441, Val Loss: 0.0592\n",
      "Epoch [12/50], Train Loss: 0.0400, Val Loss: 0.0543\n",
      "Epoch [13/50], Train Loss: 0.0373, Val Loss: 0.0513\n",
      "Epoch [14/50], Train Loss: 0.0344, Val Loss: 0.0392\n",
      "Epoch [15/50], Train Loss: 0.0337, Val Loss: 0.0355\n",
      "Epoch [16/50], Train Loss: 0.0341, Val Loss: 0.0387\n",
      "Epoch [17/50], Train Loss: 0.0301, Val Loss: 0.0337\n",
      "Epoch [18/50], Train Loss: 0.0269, Val Loss: 0.0271\n",
      "Epoch [19/50], Train Loss: 0.0252, Val Loss: 0.0278\n",
      "Epoch [20/50], Train Loss: 0.0267, Val Loss: 0.0235\n",
      "Epoch [21/50], Train Loss: 0.0238, Val Loss: 0.0258\n",
      "Epoch [22/50], Train Loss: 0.0226, Val Loss: 0.0222\n",
      "Epoch [23/50], Train Loss: 0.0245, Val Loss: 0.0200\n",
      "Epoch [24/50], Train Loss: 0.0211, Val Loss: 0.0192\n",
      "Epoch [25/50], Train Loss: 0.0208, Val Loss: 0.0165\n",
      "Epoch [26/50], Train Loss: 0.0201, Val Loss: 0.0174\n",
      "Epoch [27/50], Train Loss: 0.0196, Val Loss: 0.0218\n",
      "Epoch [28/50], Train Loss: 0.0195, Val Loss: 0.0129\n",
      "Epoch [29/50], Train Loss: 0.0204, Val Loss: 0.0156\n",
      "Epoch [30/50], Train Loss: 0.0205, Val Loss: 0.0316\n",
      "Epoch [31/50], Train Loss: 0.0211, Val Loss: 0.0165\n",
      "Epoch [32/50], Train Loss: 0.0201, Val Loss: 0.0168\n",
      "Epoch [33/50], Train Loss: 0.0206, Val Loss: 0.0291\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=32, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1120, Val Loss: 0.1887\n",
      "Epoch [2/50], Train Loss: 0.0424, Val Loss: 0.0817\n",
      "Epoch [3/50], Train Loss: 0.0350, Val Loss: 0.0705\n",
      "Epoch [4/50], Train Loss: 0.0309, Val Loss: 0.0626\n",
      "Epoch [5/50], Train Loss: 0.0276, Val Loss: 0.0542\n",
      "Epoch [6/50], Train Loss: 0.0243, Val Loss: 0.0446\n",
      "Epoch [7/50], Train Loss: 0.0204, Val Loss: 0.0330\n",
      "Epoch [8/50], Train Loss: 0.0152, Val Loss: 0.0185\n",
      "Epoch [9/50], Train Loss: 0.0090, Val Loss: 0.0122\n",
      "Epoch [10/50], Train Loss: 0.0100, Val Loss: 0.0128\n",
      "Epoch [11/50], Train Loss: 0.0060, Val Loss: 0.0084\n",
      "Epoch [12/50], Train Loss: 0.0049, Val Loss: 0.0081\n",
      "Epoch [13/50], Train Loss: 0.0039, Val Loss: 0.0080\n",
      "Epoch [14/50], Train Loss: 0.0030, Val Loss: 0.0070\n",
      "Epoch [15/50], Train Loss: 0.0026, Val Loss: 0.0063\n",
      "Epoch [16/50], Train Loss: 0.0024, Val Loss: 0.0065\n",
      "Epoch [17/50], Train Loss: 0.0022, Val Loss: 0.0068\n",
      "Epoch [18/50], Train Loss: 0.0020, Val Loss: 0.0063\n",
      "Epoch [19/50], Train Loss: 0.0019, Val Loss: 0.0056\n",
      "Epoch [20/50], Train Loss: 0.0020, Val Loss: 0.0059\n",
      "Epoch [21/50], Train Loss: 0.0020, Val Loss: 0.0064\n",
      "Epoch [22/50], Train Loss: 0.0019, Val Loss: 0.0059\n",
      "Epoch [23/50], Train Loss: 0.0018, Val Loss: 0.0051\n",
      "Epoch [24/50], Train Loss: 0.0019, Val Loss: 0.0053\n",
      "Epoch [25/50], Train Loss: 0.0019, Val Loss: 0.0061\n",
      "Epoch [26/50], Train Loss: 0.0018, Val Loss: 0.0056\n",
      "Epoch [27/50], Train Loss: 0.0018, Val Loss: 0.0045\n",
      "Epoch [28/50], Train Loss: 0.0019, Val Loss: 0.0046\n",
      "Epoch [29/50], Train Loss: 0.0020, Val Loss: 0.0058\n",
      "Epoch [30/50], Train Loss: 0.0018, Val Loss: 0.0057\n",
      "Epoch [31/50], Train Loss: 0.0017, Val Loss: 0.0040\n",
      "Epoch [32/50], Train Loss: 0.0018, Val Loss: 0.0038\n",
      "Epoch [33/50], Train Loss: 0.0021, Val Loss: 0.0054\n",
      "Epoch [34/50], Train Loss: 0.0018, Val Loss: 0.0061\n",
      "Epoch [35/50], Train Loss: 0.0017, Val Loss: 0.0040\n",
      "Epoch [36/50], Train Loss: 0.0017, Val Loss: 0.0030\n",
      "Epoch [37/50], Train Loss: 0.0022, Val Loss: 0.0046\n",
      "Epoch [38/50], Train Loss: 0.0019, Val Loss: 0.0069\n",
      "Epoch [39/50], Train Loss: 0.0017, Val Loss: 0.0044\n",
      "Epoch [40/50], Train Loss: 0.0018, Val Loss: 0.0027\n",
      "Epoch [41/50], Train Loss: 0.0022, Val Loss: 0.0033\n",
      "Epoch [42/50], Train Loss: 0.0021, Val Loss: 0.0076\n",
      "Epoch [43/50], Train Loss: 0.0017, Val Loss: 0.0050\n",
      "Epoch [44/50], Train Loss: 0.0020, Val Loss: 0.0029\n",
      "Epoch [45/50], Train Loss: 0.0020, Val Loss: 0.0022\n",
      "Epoch [46/50], Train Loss: 0.0026, Val Loss: 0.0069\n",
      "Epoch [47/50], Train Loss: 0.0018, Val Loss: 0.0061\n",
      "Epoch [48/50], Train Loss: 0.0021, Val Loss: 0.0031\n",
      "Epoch [49/50], Train Loss: 0.0019, Val Loss: 0.0022\n",
      "Epoch [50/50], Train Loss: 0.0035, Val Loss: 0.0066\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=32, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1187, Val Loss: 0.1375\n",
      "Epoch [2/50], Train Loss: 0.0462, Val Loss: 0.0384\n",
      "Epoch [3/50], Train Loss: 0.0312, Val Loss: 0.0211\n",
      "Epoch [4/50], Train Loss: 0.0268, Val Loss: 0.0152\n",
      "Epoch [5/50], Train Loss: 0.0244, Val Loss: 0.0130\n",
      "Epoch [6/50], Train Loss: 0.0215, Val Loss: 0.0127\n",
      "Epoch [7/50], Train Loss: 0.0204, Val Loss: 0.0114\n",
      "Epoch [8/50], Train Loss: 0.0180, Val Loss: 0.0090\n",
      "Epoch [9/50], Train Loss: 0.0164, Val Loss: 0.0077\n",
      "Epoch [10/50], Train Loss: 0.0146, Val Loss: 0.0066\n",
      "Epoch [11/50], Train Loss: 0.0129, Val Loss: 0.0065\n",
      "Epoch [12/50], Train Loss: 0.0115, Val Loss: 0.0090\n",
      "Epoch [13/50], Train Loss: 0.0104, Val Loss: 0.0067\n",
      "Epoch [14/50], Train Loss: 0.0084, Val Loss: 0.0039\n",
      "Epoch [15/50], Train Loss: 0.0071, Val Loss: 0.0042\n",
      "Epoch [16/50], Train Loss: 0.0067, Val Loss: 0.0043\n",
      "Epoch [17/50], Train Loss: 0.0067, Val Loss: 0.0044\n",
      "Epoch [18/50], Train Loss: 0.0064, Val Loss: 0.0031\n",
      "Epoch [19/50], Train Loss: 0.0063, Val Loss: 0.0057\n",
      "Epoch [20/50], Train Loss: 0.0061, Val Loss: 0.0055\n",
      "Epoch [21/50], Train Loss: 0.0057, Val Loss: 0.0042\n",
      "Epoch [22/50], Train Loss: 0.0060, Val Loss: 0.0026\n",
      "Epoch [23/50], Train Loss: 0.0058, Val Loss: 0.0029\n",
      "Epoch [24/50], Train Loss: 0.0059, Val Loss: 0.0094\n",
      "Epoch [25/50], Train Loss: 0.0057, Val Loss: 0.0043\n",
      "Epoch [26/50], Train Loss: 0.0066, Val Loss: 0.0016\n",
      "Epoch [27/50], Train Loss: 0.0063, Val Loss: 0.0017\n",
      "Epoch [28/50], Train Loss: 0.0068, Val Loss: 0.0126\n",
      "Epoch [29/50], Train Loss: 0.0056, Val Loss: 0.0016\n",
      "Epoch [30/50], Train Loss: 0.0075, Val Loss: 0.0025\n",
      "Epoch [31/50], Train Loss: 0.0054, Val Loss: 0.0066\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=32, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0660, Val Loss: 0.1245\n",
      "Epoch [2/50], Train Loss: 0.0442, Val Loss: 0.0765\n",
      "Epoch [3/50], Train Loss: 0.0366, Val Loss: 0.0514\n",
      "Epoch [4/50], Train Loss: 0.0317, Val Loss: 0.0333\n",
      "Epoch [5/50], Train Loss: 0.0247, Val Loss: 0.0205\n",
      "Epoch [6/50], Train Loss: 0.0210, Val Loss: 0.0187\n",
      "Epoch [7/50], Train Loss: 0.0189, Val Loss: 0.0145\n",
      "Epoch [8/50], Train Loss: 0.0161, Val Loss: 0.0098\n",
      "Epoch [9/50], Train Loss: 0.0137, Val Loss: 0.0131\n",
      "Epoch [10/50], Train Loss: 0.0116, Val Loss: 0.0068\n",
      "Epoch [11/50], Train Loss: 0.0134, Val Loss: 0.0056\n",
      "Epoch [12/50], Train Loss: 0.0110, Val Loss: 0.0125\n",
      "Epoch [13/50], Train Loss: 0.0120, Val Loss: 0.0092\n",
      "Epoch [14/50], Train Loss: 0.0124, Val Loss: 0.0070\n",
      "Epoch [15/50], Train Loss: 0.0107, Val Loss: 0.0096\n",
      "Epoch [16/50], Train Loss: 0.0108, Val Loss: 0.0090\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=32, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0586, Val Loss: 0.0884\n",
      "Epoch [2/50], Train Loss: 0.0465, Val Loss: 0.0781\n",
      "Epoch [3/50], Train Loss: 0.0372, Val Loss: 0.0636\n",
      "Epoch [4/50], Train Loss: 0.0328, Val Loss: 0.0502\n",
      "Epoch [5/50], Train Loss: 0.0271, Val Loss: 0.0331\n",
      "Epoch [6/50], Train Loss: 0.0197, Val Loss: 0.0172\n",
      "Epoch [7/50], Train Loss: 0.0147, Val Loss: 0.0139\n",
      "Epoch [8/50], Train Loss: 0.0129, Val Loss: 0.0209\n",
      "Epoch [9/50], Train Loss: 0.0091, Val Loss: 0.0140\n",
      "Epoch [10/50], Train Loss: 0.0065, Val Loss: 0.0108\n",
      "Epoch [11/50], Train Loss: 0.0041, Val Loss: 0.0143\n",
      "Epoch [12/50], Train Loss: 0.0030, Val Loss: 0.0108\n",
      "Epoch [13/50], Train Loss: 0.0024, Val Loss: 0.0072\n",
      "Epoch [14/50], Train Loss: 0.0028, Val Loss: 0.0081\n",
      "Epoch [15/50], Train Loss: 0.0032, Val Loss: 0.0141\n",
      "Epoch [16/50], Train Loss: 0.0024, Val Loss: 0.0075\n",
      "Epoch [17/50], Train Loss: 0.0029, Val Loss: 0.0055\n",
      "Epoch [18/50], Train Loss: 0.0034, Val Loss: 0.0112\n",
      "Epoch [19/50], Train Loss: 0.0029, Val Loss: 0.0129\n",
      "Epoch [20/50], Train Loss: 0.0028, Val Loss: 0.0058\n",
      "Epoch [21/50], Train Loss: 0.0032, Val Loss: 0.0050\n",
      "Epoch [22/50], Train Loss: 0.0045, Val Loss: 0.0158\n",
      "Epoch [23/50], Train Loss: 0.0028, Val Loss: 0.0072\n",
      "Epoch [24/50], Train Loss: 0.0034, Val Loss: 0.0050\n",
      "Epoch [25/50], Train Loss: 0.0032, Val Loss: 0.0086\n",
      "Epoch [26/50], Train Loss: 0.0036, Val Loss: 0.0141\n",
      "Epoch [27/50], Train Loss: 0.0030, Val Loss: 0.0048\n",
      "Epoch [28/50], Train Loss: 0.0033, Val Loss: 0.0047\n",
      "Epoch [29/50], Train Loss: 0.0040, Val Loss: 0.0137\n",
      "Epoch [30/50], Train Loss: 0.0025, Val Loss: 0.0065\n",
      "Epoch [31/50], Train Loss: 0.0030, Val Loss: 0.0043\n",
      "Epoch [32/50], Train Loss: 0.0027, Val Loss: 0.0084\n",
      "Epoch [33/50], Train Loss: 0.0027, Val Loss: 0.0110\n",
      "Epoch [34/50], Train Loss: 0.0027, Val Loss: 0.0047\n",
      "Epoch [35/50], Train Loss: 0.0026, Val Loss: 0.0052\n",
      "Epoch [36/50], Train Loss: 0.0033, Val Loss: 0.0127\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=32, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0750, Val Loss: 0.0829\n",
      "Epoch [2/50], Train Loss: 0.0518, Val Loss: 0.0728\n",
      "Epoch [3/50], Train Loss: 0.0413, Val Loss: 0.0565\n",
      "Epoch [4/50], Train Loss: 0.0341, Val Loss: 0.0405\n",
      "Epoch [5/50], Train Loss: 0.0267, Val Loss: 0.0220\n",
      "Epoch [6/50], Train Loss: 0.0176, Val Loss: 0.0172\n",
      "Epoch [7/50], Train Loss: 0.0154, Val Loss: 0.0179\n",
      "Epoch [8/50], Train Loss: 0.0131, Val Loss: 0.0151\n",
      "Epoch [9/50], Train Loss: 0.0118, Val Loss: 0.0059\n",
      "Epoch [10/50], Train Loss: 0.0110, Val Loss: 0.0160\n",
      "Epoch [11/50], Train Loss: 0.0099, Val Loss: 0.0120\n",
      "Epoch [12/50], Train Loss: 0.0115, Val Loss: 0.0047\n",
      "Epoch [13/50], Train Loss: 0.0092, Val Loss: 0.0070\n",
      "Epoch [14/50], Train Loss: 0.0094, Val Loss: 0.0156\n",
      "Epoch [15/50], Train Loss: 0.0091, Val Loss: 0.0045\n",
      "Epoch [16/50], Train Loss: 0.0101, Val Loss: 0.0056\n",
      "Epoch [17/50], Train Loss: 0.0090, Val Loss: 0.0142\n",
      "Epoch [18/50], Train Loss: 0.0087, Val Loss: 0.0124\n",
      "Epoch [19/50], Train Loss: 0.0099, Val Loss: 0.0037\n",
      "Epoch [20/50], Train Loss: 0.0084, Val Loss: 0.0129\n",
      "Epoch [21/50], Train Loss: 0.0099, Val Loss: 0.0139\n",
      "Epoch [22/50], Train Loss: 0.0110, Val Loss: 0.0036\n",
      "Epoch [23/50], Train Loss: 0.0079, Val Loss: 0.0071\n",
      "Epoch [24/50], Train Loss: 0.0086, Val Loss: 0.0138\n",
      "Epoch [25/50], Train Loss: 0.0088, Val Loss: 0.0056\n",
      "Epoch [26/50], Train Loss: 0.0074, Val Loss: 0.0050\n",
      "Epoch [27/50], Train Loss: 0.0076, Val Loss: 0.0106\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=32, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1172, Val Loss: 0.0770\n",
      "Epoch [2/50], Train Loss: 0.0941, Val Loss: 0.1019\n",
      "Epoch [3/50], Train Loss: 0.0647, Val Loss: 0.0711\n",
      "Epoch [4/50], Train Loss: 0.0560, Val Loss: 0.0489\n",
      "Epoch [5/50], Train Loss: 0.0478, Val Loss: 0.0342\n",
      "Epoch [6/50], Train Loss: 0.0396, Val Loss: 0.0186\n",
      "Epoch [7/50], Train Loss: 0.0344, Val Loss: 0.0221\n",
      "Epoch [8/50], Train Loss: 0.0289, Val Loss: 0.0230\n",
      "Epoch [9/50], Train Loss: 0.0282, Val Loss: 0.0071\n",
      "Epoch [10/50], Train Loss: 0.0272, Val Loss: 0.0083\n",
      "Epoch [11/50], Train Loss: 0.0260, Val Loss: 0.0297\n",
      "Epoch [12/50], Train Loss: 0.0257, Val Loss: 0.0152\n",
      "Epoch [13/50], Train Loss: 0.0258, Val Loss: 0.0069\n",
      "Epoch [14/50], Train Loss: 0.0226, Val Loss: 0.0101\n",
      "Epoch [15/50], Train Loss: 0.0219, Val Loss: 0.0219\n",
      "Epoch [16/50], Train Loss: 0.0194, Val Loss: 0.0094\n",
      "Epoch [17/50], Train Loss: 0.0196, Val Loss: 0.0064\n",
      "Epoch [18/50], Train Loss: 0.0182, Val Loss: 0.0148\n",
      "Epoch [19/50], Train Loss: 0.0167, Val Loss: 0.0135\n",
      "Epoch [20/50], Train Loss: 0.0185, Val Loss: 0.0059\n",
      "Epoch [21/50], Train Loss: 0.0181, Val Loss: 0.0160\n",
      "Epoch [22/50], Train Loss: 0.0168, Val Loss: 0.0169\n",
      "Epoch [23/50], Train Loss: 0.0173, Val Loss: 0.0059\n",
      "Epoch [24/50], Train Loss: 0.0166, Val Loss: 0.0057\n",
      "Epoch [25/50], Train Loss: 0.0164, Val Loss: 0.0164\n",
      "Epoch [26/50], Train Loss: 0.0154, Val Loss: 0.0089\n",
      "Epoch [27/50], Train Loss: 0.0152, Val Loss: 0.0038\n",
      "Epoch [28/50], Train Loss: 0.0147, Val Loss: 0.0092\n",
      "Epoch [29/50], Train Loss: 0.0125, Val Loss: 0.0090\n",
      "Epoch [30/50], Train Loss: 0.0127, Val Loss: 0.0031\n",
      "Epoch [31/50], Train Loss: 0.0128, Val Loss: 0.0058\n",
      "Epoch [32/50], Train Loss: 0.0134, Val Loss: 0.0157\n",
      "Epoch [33/50], Train Loss: 0.0143, Val Loss: 0.0024\n",
      "Epoch [34/50], Train Loss: 0.0144, Val Loss: 0.0064\n",
      "Epoch [35/50], Train Loss: 0.0141, Val Loss: 0.0261\n",
      "Epoch [36/50], Train Loss: 0.0149, Val Loss: 0.0034\n",
      "Epoch [37/50], Train Loss: 0.0154, Val Loss: 0.0032\n",
      "Epoch [38/50], Train Loss: 0.0117, Val Loss: 0.0118\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=32, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0539, Val Loss: 0.0676\n",
      "Epoch [2/50], Train Loss: 0.0547, Val Loss: 0.0683\n",
      "Epoch [3/50], Train Loss: 0.0402, Val Loss: 0.0574\n",
      "Epoch [4/50], Train Loss: 0.0283, Val Loss: 0.0272\n",
      "Epoch [5/50], Train Loss: 0.0215, Val Loss: 0.0154\n",
      "Epoch [6/50], Train Loss: 0.0187, Val Loss: 0.0158\n",
      "Epoch [7/50], Train Loss: 0.0145, Val Loss: 0.0097\n",
      "Epoch [8/50], Train Loss: 0.0115, Val Loss: 0.0235\n",
      "Epoch [9/50], Train Loss: 0.0132, Val Loss: 0.0213\n",
      "Epoch [10/50], Train Loss: 0.0096, Val Loss: 0.0064\n",
      "Epoch [11/50], Train Loss: 0.0064, Val Loss: 0.0197\n",
      "Epoch [12/50], Train Loss: 0.0050, Val Loss: 0.0117\n",
      "Epoch [13/50], Train Loss: 0.0115, Val Loss: 0.0066\n",
      "Epoch [14/50], Train Loss: 0.0061, Val Loss: 0.0167\n",
      "Epoch [15/50], Train Loss: 0.0040, Val Loss: 0.0075\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=32, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0611, Val Loss: 0.0646\n",
      "Epoch [2/50], Train Loss: 0.0633, Val Loss: 0.1011\n",
      "Epoch [3/50], Train Loss: 0.0464, Val Loss: 0.0857\n",
      "Epoch [4/50], Train Loss: 0.0440, Val Loss: 0.0757\n",
      "Epoch [5/50], Train Loss: 0.0393, Val Loss: 0.0547\n",
      "Epoch [6/50], Train Loss: 0.0299, Val Loss: 0.0246\n",
      "Epoch [7/50], Train Loss: 0.0216, Val Loss: 0.0166\n",
      "Epoch [8/50], Train Loss: 0.0222, Val Loss: 0.0287\n",
      "Epoch [9/50], Train Loss: 0.0165, Val Loss: 0.0087\n",
      "Epoch [10/50], Train Loss: 0.0131, Val Loss: 0.0133\n",
      "Epoch [11/50], Train Loss: 0.0165, Val Loss: 0.0329\n",
      "Epoch [12/50], Train Loss: 0.0193, Val Loss: 0.0192\n",
      "Epoch [13/50], Train Loss: 0.0128, Val Loss: 0.0246\n",
      "Epoch [14/50], Train Loss: 0.0122, Val Loss: 0.0219\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=32, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1159, Val Loss: 0.1352\n",
      "Epoch [2/50], Train Loss: 0.0883, Val Loss: 0.1314\n",
      "Epoch [3/50], Train Loss: 0.0681, Val Loss: 0.1018\n",
      "Epoch [4/50], Train Loss: 0.0618, Val Loss: 0.0780\n",
      "Epoch [5/50], Train Loss: 0.0508, Val Loss: 0.0490\n",
      "Epoch [6/50], Train Loss: 0.0428, Val Loss: 0.0241\n",
      "Epoch [7/50], Train Loss: 0.0397, Val Loss: 0.0336\n",
      "Epoch [8/50], Train Loss: 0.0356, Val Loss: 0.0319\n",
      "Epoch [9/50], Train Loss: 0.0321, Val Loss: 0.0330\n",
      "Epoch [10/50], Train Loss: 0.0289, Val Loss: 0.0210\n",
      "Epoch [11/50], Train Loss: 0.0293, Val Loss: 0.0101\n",
      "Epoch [12/50], Train Loss: 0.0253, Val Loss: 0.0193\n",
      "Epoch [13/50], Train Loss: 0.0222, Val Loss: 0.0120\n",
      "Epoch [14/50], Train Loss: 0.0237, Val Loss: 0.0025\n",
      "Epoch [15/50], Train Loss: 0.0229, Val Loss: 0.0202\n",
      "Epoch [16/50], Train Loss: 0.0198, Val Loss: 0.0058\n",
      "Epoch [17/50], Train Loss: 0.0182, Val Loss: 0.0056\n",
      "Epoch [18/50], Train Loss: 0.0201, Val Loss: 0.0303\n",
      "Epoch [19/50], Train Loss: 0.0203, Val Loss: 0.0073\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=64, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0490, Val Loss: 0.0407\n",
      "Epoch [2/50], Train Loss: 0.0469, Val Loss: 0.0679\n",
      "Epoch [3/50], Train Loss: 0.0296, Val Loss: 0.0422\n",
      "Epoch [4/50], Train Loss: 0.0265, Val Loss: 0.0315\n",
      "Epoch [5/50], Train Loss: 0.0205, Val Loss: 0.0178\n",
      "Epoch [6/50], Train Loss: 0.0122, Val Loss: 0.0063\n",
      "Epoch [7/50], Train Loss: 0.0115, Val Loss: 0.0094\n",
      "Epoch [8/50], Train Loss: 0.0052, Val Loss: 0.0050\n",
      "Epoch [9/50], Train Loss: 0.0064, Val Loss: 0.0087\n",
      "Epoch [10/50], Train Loss: 0.0029, Val Loss: 0.0098\n",
      "Epoch [11/50], Train Loss: 0.0022, Val Loss: 0.0033\n",
      "Epoch [12/50], Train Loss: 0.0026, Val Loss: 0.0017\n",
      "Epoch [13/50], Train Loss: 0.0030, Val Loss: 0.0081\n",
      "Epoch [14/50], Train Loss: 0.0027, Val Loss: 0.0038\n",
      "Epoch [15/50], Train Loss: 0.0028, Val Loss: 0.0017\n",
      "Epoch [16/50], Train Loss: 0.0033, Val Loss: 0.0099\n",
      "Epoch [17/50], Train Loss: 0.0032, Val Loss: 0.0039\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=64, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0554, Val Loss: 0.0426\n",
      "Epoch [2/50], Train Loss: 0.0634, Val Loss: 0.0852\n",
      "Epoch [3/50], Train Loss: 0.0343, Val Loss: 0.0460\n",
      "Epoch [4/50], Train Loss: 0.0312, Val Loss: 0.0324\n",
      "Epoch [5/50], Train Loss: 0.0238, Val Loss: 0.0156\n",
      "Epoch [6/50], Train Loss: 0.0161, Val Loss: 0.0110\n",
      "Epoch [7/50], Train Loss: 0.0132, Val Loss: 0.0060\n",
      "Epoch [8/50], Train Loss: 0.0083, Val Loss: 0.0055\n",
      "Epoch [9/50], Train Loss: 0.0083, Val Loss: 0.0071\n",
      "Epoch [10/50], Train Loss: 0.0067, Val Loss: 0.0096\n",
      "Epoch [11/50], Train Loss: 0.0060, Val Loss: 0.0021\n",
      "Epoch [12/50], Train Loss: 0.0064, Val Loss: 0.0023\n",
      "Epoch [13/50], Train Loss: 0.0060, Val Loss: 0.0086\n",
      "Epoch [14/50], Train Loss: 0.0052, Val Loss: 0.0021\n",
      "Epoch [15/50], Train Loss: 0.0057, Val Loss: 0.0017\n",
      "Epoch [16/50], Train Loss: 0.0054, Val Loss: 0.0083\n",
      "Epoch [17/50], Train Loss: 0.0046, Val Loss: 0.0025\n",
      "Epoch [18/50], Train Loss: 0.0053, Val Loss: 0.0016\n",
      "Epoch [19/50], Train Loss: 0.0044, Val Loss: 0.0033\n",
      "Epoch [20/50], Train Loss: 0.0044, Val Loss: 0.0039\n",
      "Epoch [21/50], Train Loss: 0.0046, Val Loss: 0.0033\n",
      "Epoch [22/50], Train Loss: 0.0047, Val Loss: 0.0018\n",
      "Epoch [23/50], Train Loss: 0.0047, Val Loss: 0.0055\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=64, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0884, Val Loss: 0.0798\n",
      "Epoch [2/50], Train Loss: 0.0593, Val Loss: 0.0673\n",
      "Epoch [3/50], Train Loss: 0.0424, Val Loss: 0.0440\n",
      "Epoch [4/50], Train Loss: 0.0362, Val Loss: 0.0277\n",
      "Epoch [5/50], Train Loss: 0.0296, Val Loss: 0.0141\n",
      "Epoch [6/50], Train Loss: 0.0273, Val Loss: 0.0119\n",
      "Epoch [7/50], Train Loss: 0.0221, Val Loss: 0.0079\n",
      "Epoch [8/50], Train Loss: 0.0202, Val Loss: 0.0094\n",
      "Epoch [9/50], Train Loss: 0.0180, Val Loss: 0.0122\n",
      "Epoch [10/50], Train Loss: 0.0146, Val Loss: 0.0068\n",
      "Epoch [11/50], Train Loss: 0.0127, Val Loss: 0.0029\n",
      "Epoch [12/50], Train Loss: 0.0123, Val Loss: 0.0065\n",
      "Epoch [13/50], Train Loss: 0.0099, Val Loss: 0.0051\n",
      "Epoch [14/50], Train Loss: 0.0094, Val Loss: 0.0033\n",
      "Epoch [15/50], Train Loss: 0.0096, Val Loss: 0.0032\n",
      "Epoch [16/50], Train Loss: 0.0084, Val Loss: 0.0043\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=64, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0342, Val Loss: 0.0295\n",
      "Epoch [2/50], Train Loss: 0.0699, Val Loss: 0.0919\n",
      "Epoch [3/50], Train Loss: 0.0308, Val Loss: 0.0382\n",
      "Epoch [4/50], Train Loss: 0.0293, Val Loss: 0.0156\n",
      "Epoch [5/50], Train Loss: 0.0115, Val Loss: 0.0105\n",
      "Epoch [6/50], Train Loss: 0.0147, Val Loss: 0.0240\n",
      "Epoch [7/50], Train Loss: 0.0157, Val Loss: 0.0160\n",
      "Epoch [8/50], Train Loss: 0.0060, Val Loss: 0.0036\n",
      "Epoch [9/50], Train Loss: 0.0030, Val Loss: 0.0088\n",
      "Epoch [10/50], Train Loss: 0.0025, Val Loss: 0.0020\n",
      "Epoch [11/50], Train Loss: 0.0042, Val Loss: 0.0019\n",
      "Epoch [12/50], Train Loss: 0.0029, Val Loss: 0.0090\n",
      "Epoch [13/50], Train Loss: 0.0026, Val Loss: 0.0032\n",
      "Epoch [14/50], Train Loss: 0.0043, Val Loss: 0.0015\n",
      "Epoch [15/50], Train Loss: 0.0029, Val Loss: 0.0059\n",
      "Epoch [16/50], Train Loss: 0.0038, Val Loss: 0.0085\n",
      "Epoch [17/50], Train Loss: 0.0052, Val Loss: 0.0013\n",
      "Epoch [18/50], Train Loss: 0.0047, Val Loss: 0.0053\n",
      "Epoch [19/50], Train Loss: 0.0062, Val Loss: 0.0134\n",
      "Epoch [20/50], Train Loss: 0.0082, Val Loss: 0.0047\n",
      "Epoch [21/50], Train Loss: 0.0060, Val Loss: 0.0091\n",
      "Epoch [22/50], Train Loss: 0.0046, Val Loss: 0.0106\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=64, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0361, Val Loss: 0.0382\n",
      "Epoch [2/50], Train Loss: 0.0684, Val Loss: 0.0709\n",
      "Epoch [3/50], Train Loss: 0.0377, Val Loss: 0.0397\n",
      "Epoch [4/50], Train Loss: 0.0244, Val Loss: 0.0145\n",
      "Epoch [5/50], Train Loss: 0.0166, Val Loss: 0.0125\n",
      "Epoch [6/50], Train Loss: 0.0104, Val Loss: 0.0067\n",
      "Epoch [7/50], Train Loss: 0.0109, Val Loss: 0.0118\n",
      "Epoch [8/50], Train Loss: 0.0085, Val Loss: 0.0117\n",
      "Epoch [9/50], Train Loss: 0.0085, Val Loss: 0.0052\n",
      "Epoch [10/50], Train Loss: 0.0077, Val Loss: 0.0086\n",
      "Epoch [11/50], Train Loss: 0.0079, Val Loss: 0.0026\n",
      "Epoch [12/50], Train Loss: 0.0076, Val Loss: 0.0029\n",
      "Epoch [13/50], Train Loss: 0.0076, Val Loss: 0.0100\n",
      "Epoch [14/50], Train Loss: 0.0095, Val Loss: 0.0031\n",
      "Epoch [15/50], Train Loss: 0.0071, Val Loss: 0.0031\n",
      "Epoch [16/50], Train Loss: 0.0079, Val Loss: 0.0108\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=64, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0626, Val Loss: 0.0418\n",
      "Epoch [2/50], Train Loss: 0.0788, Val Loss: 0.0979\n",
      "Epoch [3/50], Train Loss: 0.0446, Val Loss: 0.0445\n",
      "Epoch [4/50], Train Loss: 0.0342, Val Loss: 0.0084\n",
      "Epoch [5/50], Train Loss: 0.0244, Val Loss: 0.0040\n",
      "Epoch [6/50], Train Loss: 0.0195, Val Loss: 0.0193\n",
      "Epoch [7/50], Train Loss: 0.0192, Val Loss: 0.0034\n",
      "Epoch [8/50], Train Loss: 0.0171, Val Loss: 0.0048\n",
      "Epoch [9/50], Train Loss: 0.0150, Val Loss: 0.0120\n",
      "Epoch [10/50], Train Loss: 0.0131, Val Loss: 0.0093\n",
      "Epoch [11/50], Train Loss: 0.0151, Val Loss: 0.0027\n",
      "Epoch [12/50], Train Loss: 0.0124, Val Loss: 0.0037\n",
      "Epoch [13/50], Train Loss: 0.0124, Val Loss: 0.0131\n",
      "Epoch [14/50], Train Loss: 0.0133, Val Loss: 0.0045\n",
      "Epoch [15/50], Train Loss: 0.0122, Val Loss: 0.0032\n",
      "Epoch [16/50], Train Loss: 0.0112, Val Loss: 0.0121\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=64, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0253, Val Loss: 0.0737\n",
      "Epoch [2/50], Train Loss: 0.0613, Val Loss: 0.0433\n",
      "Epoch [3/50], Train Loss: 0.0449, Val Loss: 0.0162\n",
      "Epoch [4/50], Train Loss: 0.0298, Val Loss: 0.0058\n",
      "Epoch [5/50], Train Loss: 0.0187, Val Loss: 0.0162\n",
      "Epoch [6/50], Train Loss: 0.0097, Val Loss: 0.0047\n",
      "Epoch [7/50], Train Loss: 0.0093, Val Loss: 0.0125\n",
      "Epoch [8/50], Train Loss: 0.0185, Val Loss: 0.0099\n",
      "Epoch [9/50], Train Loss: 0.0070, Val Loss: 0.0059\n",
      "Epoch [10/50], Train Loss: 0.0058, Val Loss: 0.0127\n",
      "Epoch [11/50], Train Loss: 0.0116, Val Loss: 0.0113\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=64, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0390, Val Loss: 0.1087\n",
      "Epoch [2/50], Train Loss: 0.0621, Val Loss: 0.0583\n",
      "Epoch [3/50], Train Loss: 0.0528, Val Loss: 0.0394\n",
      "Epoch [4/50], Train Loss: 0.0399, Val Loss: 0.0097\n",
      "Epoch [5/50], Train Loss: 0.0240, Val Loss: 0.0030\n",
      "Epoch [6/50], Train Loss: 0.0178, Val Loss: 0.0257\n",
      "Epoch [7/50], Train Loss: 0.0202, Val Loss: 0.0033\n",
      "Epoch [8/50], Train Loss: 0.0110, Val Loss: 0.0117\n",
      "Epoch [9/50], Train Loss: 0.0098, Val Loss: 0.0128\n",
      "Epoch [10/50], Train Loss: 0.0159, Val Loss: 0.0101\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=64, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0680, Val Loss: 0.0497\n",
      "Epoch [2/50], Train Loss: 0.0757, Val Loss: 0.0522\n",
      "Epoch [3/50], Train Loss: 0.0533, Val Loss: 0.0249\n",
      "Epoch [4/50], Train Loss: 0.0356, Val Loss: 0.0045\n",
      "Epoch [5/50], Train Loss: 0.0290, Val Loss: 0.0321\n",
      "Epoch [6/50], Train Loss: 0.0302, Val Loss: 0.0040\n",
      "Epoch [7/50], Train Loss: 0.0203, Val Loss: 0.0031\n",
      "Epoch [8/50], Train Loss: 0.0193, Val Loss: 0.0172\n",
      "Epoch [9/50], Train Loss: 0.0208, Val Loss: 0.0043\n",
      "Epoch [10/50], Train Loss: 0.0197, Val Loss: 0.0072\n",
      "Epoch [11/50], Train Loss: 0.0202, Val Loss: 0.0236\n",
      "Epoch [12/50], Train Loss: 0.0321, Val Loss: 0.0320\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=128, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0345, Val Loss: 0.0761\n",
      "Epoch [2/50], Train Loss: 0.0519, Val Loss: 0.0174\n",
      "Epoch [3/50], Train Loss: 0.0304, Val Loss: 0.0059\n",
      "Epoch [4/50], Train Loss: 0.0194, Val Loss: 0.0077\n",
      "Epoch [5/50], Train Loss: 0.0140, Val Loss: 0.0029\n",
      "Epoch [6/50], Train Loss: 0.0102, Val Loss: 0.0132\n",
      "Epoch [7/50], Train Loss: 0.0126, Val Loss: 0.0058\n",
      "Epoch [8/50], Train Loss: 0.0109, Val Loss: 0.0035\n",
      "Epoch [9/50], Train Loss: 0.0060, Val Loss: 0.0038\n",
      "Epoch [10/50], Train Loss: 0.0056, Val Loss: 0.0162\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=128, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0345, Val Loss: 0.0534\n",
      "Epoch [2/50], Train Loss: 0.0509, Val Loss: 0.0500\n",
      "Epoch [3/50], Train Loss: 0.0288, Val Loss: 0.0158\n",
      "Epoch [4/50], Train Loss: 0.0201, Val Loss: 0.0025\n",
      "Epoch [5/50], Train Loss: 0.0138, Val Loss: 0.0027\n",
      "Epoch [6/50], Train Loss: 0.0100, Val Loss: 0.0076\n",
      "Epoch [7/50], Train Loss: 0.0147, Val Loss: 0.0177\n",
      "Epoch [8/50], Train Loss: 0.0133, Val Loss: 0.0235\n",
      "Epoch [9/50], Train Loss: 0.0066, Val Loss: 0.0050\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=128, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0460, Val Loss: 0.0840\n",
      "Epoch [2/50], Train Loss: 0.0562, Val Loss: 0.0418\n",
      "Epoch [3/50], Train Loss: 0.0377, Val Loss: 0.0256\n",
      "Epoch [4/50], Train Loss: 0.0269, Val Loss: 0.0087\n",
      "Epoch [5/50], Train Loss: 0.0169, Val Loss: 0.0069\n",
      "Epoch [6/50], Train Loss: 0.0179, Val Loss: 0.0087\n",
      "Epoch [7/50], Train Loss: 0.0167, Val Loss: 0.0115\n",
      "Epoch [8/50], Train Loss: 0.0088, Val Loss: 0.0104\n",
      "Epoch [9/50], Train Loss: 0.0090, Val Loss: 0.0065\n",
      "Epoch [10/50], Train Loss: 0.0074, Val Loss: 0.0031\n",
      "Epoch [11/50], Train Loss: 0.0065, Val Loss: 0.0025\n",
      "Epoch [12/50], Train Loss: 0.0067, Val Loss: 0.0094\n",
      "Epoch [13/50], Train Loss: 0.0061, Val Loss: 0.0015\n",
      "Epoch [14/50], Train Loss: 0.0067, Val Loss: 0.0023\n",
      "Epoch [15/50], Train Loss: 0.0062, Val Loss: 0.0052\n",
      "Epoch [16/50], Train Loss: 0.0061, Val Loss: 0.0070\n",
      "Epoch [17/50], Train Loss: 0.0064, Val Loss: 0.0020\n",
      "Epoch [18/50], Train Loss: 0.0066, Val Loss: 0.0022\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=128, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0248, Val Loss: 0.1076\n",
      "Epoch [2/50], Train Loss: 0.0690, Val Loss: 0.0237\n",
      "Epoch [3/50], Train Loss: 0.0520, Val Loss: 0.0163\n",
      "Epoch [4/50], Train Loss: 0.0289, Val Loss: 0.0318\n",
      "Epoch [5/50], Train Loss: 0.0192, Val Loss: 0.0028\n",
      "Epoch [6/50], Train Loss: 0.0179, Val Loss: 0.0069\n",
      "Epoch [7/50], Train Loss: 0.0040, Val Loss: 0.0053\n",
      "Epoch [8/50], Train Loss: 0.0055, Val Loss: 0.0093\n",
      "Epoch [9/50], Train Loss: 0.0080, Val Loss: 0.0025\n",
      "Epoch [10/50], Train Loss: 0.0075, Val Loss: 0.0078\n",
      "Epoch [11/50], Train Loss: 0.0060, Val Loss: 0.0087\n",
      "Epoch [12/50], Train Loss: 0.0165, Val Loss: 0.0158\n",
      "Epoch [13/50], Train Loss: 0.0105, Val Loss: 0.0093\n",
      "Epoch [14/50], Train Loss: 0.0030, Val Loss: 0.0069\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=128, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0246, Val Loss: 0.0902\n",
      "Epoch [2/50], Train Loss: 0.0639, Val Loss: 0.0181\n",
      "Epoch [3/50], Train Loss: 0.0489, Val Loss: 0.0178\n",
      "Epoch [4/50], Train Loss: 0.0262, Val Loss: 0.0074\n",
      "Epoch [5/50], Train Loss: 0.0130, Val Loss: 0.0124\n",
      "Epoch [6/50], Train Loss: 0.0083, Val Loss: 0.0053\n",
      "Epoch [7/50], Train Loss: 0.0075, Val Loss: 0.0088\n",
      "Epoch [8/50], Train Loss: 0.0054, Val Loss: 0.0035\n",
      "Epoch [9/50], Train Loss: 0.0051, Val Loss: 0.0020\n",
      "Epoch [10/50], Train Loss: 0.0048, Val Loss: 0.0063\n",
      "Epoch [11/50], Train Loss: 0.0046, Val Loss: 0.0039\n",
      "Epoch [12/50], Train Loss: 0.0042, Val Loss: 0.0033\n",
      "Epoch [13/50], Train Loss: 0.0046, Val Loss: 0.0039\n",
      "Epoch [14/50], Train Loss: 0.0046, Val Loss: 0.0019\n",
      "Epoch [15/50], Train Loss: 0.0077, Val Loss: 0.0076\n",
      "Epoch [16/50], Train Loss: 0.0059, Val Loss: 0.0026\n",
      "Epoch [17/50], Train Loss: 0.0097, Val Loss: 0.0133\n",
      "Epoch [18/50], Train Loss: 0.0061, Val Loss: 0.0021\n",
      "Epoch [19/50], Train Loss: 0.0079, Val Loss: 0.0148\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=128, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0376, Val Loss: 0.0978\n",
      "Epoch [2/50], Train Loss: 0.0554, Val Loss: 0.0189\n",
      "Epoch [3/50], Train Loss: 0.0405, Val Loss: 0.0028\n",
      "Epoch [4/50], Train Loss: 0.0234, Val Loss: 0.0070\n",
      "Epoch [5/50], Train Loss: 0.0118, Val Loss: 0.0048\n",
      "Epoch [6/50], Train Loss: 0.0146, Val Loss: 0.0063\n",
      "Epoch [7/50], Train Loss: 0.0121, Val Loss: 0.0154\n",
      "Epoch [8/50], Train Loss: 0.0129, Val Loss: 0.0118\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=128, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0207, Val Loss: 0.0767\n",
      "Epoch [2/50], Train Loss: 0.0552, Val Loss: 0.0784\n",
      "Epoch [3/50], Train Loss: 0.0434, Val Loss: 0.0397\n",
      "Epoch [4/50], Train Loss: 0.0394, Val Loss: 0.0235\n",
      "Epoch [5/50], Train Loss: 0.0267, Val Loss: 0.0248\n",
      "Epoch [6/50], Train Loss: 0.0200, Val Loss: 0.0043\n",
      "Epoch [7/50], Train Loss: 0.0074, Val Loss: 0.0107\n",
      "Epoch [8/50], Train Loss: 0.0266, Val Loss: 0.0094\n",
      "Epoch [9/50], Train Loss: 0.0105, Val Loss: 0.0148\n",
      "Epoch [10/50], Train Loss: 0.0055, Val Loss: 0.0031\n",
      "Epoch [11/50], Train Loss: 0.0058, Val Loss: 0.0134\n",
      "Epoch [12/50], Train Loss: 0.0057, Val Loss: 0.0023\n",
      "Epoch [13/50], Train Loss: 0.0090, Val Loss: 0.0088\n",
      "Epoch [14/50], Train Loss: 0.0055, Val Loss: 0.0115\n",
      "Epoch [15/50], Train Loss: 0.0058, Val Loss: 0.0036\n",
      "Epoch [16/50], Train Loss: 0.0057, Val Loss: 0.0039\n",
      "Epoch [17/50], Train Loss: 0.0138, Val Loss: 0.0091\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=128, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0239, Val Loss: 0.1054\n",
      "Epoch [2/50], Train Loss: 0.0545, Val Loss: 0.0969\n",
      "Epoch [3/50], Train Loss: 0.0509, Val Loss: 0.0444\n",
      "Epoch [4/50], Train Loss: 0.0436, Val Loss: 0.0069\n",
      "Epoch [5/50], Train Loss: 0.0335, Val Loss: 0.0047\n",
      "Epoch [6/50], Train Loss: 0.0290, Val Loss: 0.0146\n",
      "Epoch [7/50], Train Loss: 0.0098, Val Loss: 0.0061\n",
      "Epoch [8/50], Train Loss: 0.0137, Val Loss: 0.0287\n",
      "Epoch [9/50], Train Loss: 0.0153, Val Loss: 0.0241\n",
      "Epoch [10/50], Train Loss: 0.0131, Val Loss: 0.0042\n",
      "Epoch [11/50], Train Loss: 0.0071, Val Loss: 0.0066\n",
      "Epoch [12/50], Train Loss: 0.0134, Val Loss: 0.0048\n",
      "Epoch [13/50], Train Loss: 0.0068, Val Loss: 0.0054\n",
      "Epoch [14/50], Train Loss: 0.0068, Val Loss: 0.0044\n",
      "Epoch [15/50], Train Loss: 0.0160, Val Loss: 0.0068\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=128, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0382, Val Loss: 0.1107\n",
      "Epoch [2/50], Train Loss: 0.0580, Val Loss: 0.0634\n",
      "Epoch [3/50], Train Loss: 0.0492, Val Loss: 0.0560\n",
      "Epoch [4/50], Train Loss: 0.0380, Val Loss: 0.0209\n",
      "Epoch [5/50], Train Loss: 0.0248, Val Loss: 0.0511\n",
      "Epoch [6/50], Train Loss: 0.0239, Val Loss: 0.0137\n",
      "Epoch [7/50], Train Loss: 0.0172, Val Loss: 0.0020\n",
      "Epoch [8/50], Train Loss: 0.0154, Val Loss: 0.0112\n",
      "Epoch [9/50], Train Loss: 0.0241, Val Loss: 0.0031\n",
      "Epoch [10/50], Train Loss: 0.0280, Val Loss: 0.1028\n",
      "Epoch [11/50], Train Loss: 0.0230, Val Loss: 0.0298\n",
      "Epoch [12/50], Train Loss: 0.0268, Val Loss: 0.0274\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=16, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0752, Val Loss: 0.2045\n",
      "Epoch [2/50], Train Loss: 0.0736, Val Loss: 0.2008\n",
      "Epoch [3/50], Train Loss: 0.0721, Val Loss: 0.1973\n",
      "Epoch [4/50], Train Loss: 0.0707, Val Loss: 0.1938\n",
      "Epoch [5/50], Train Loss: 0.0694, Val Loss: 0.1905\n",
      "Epoch [6/50], Train Loss: 0.0680, Val Loss: 0.1872\n",
      "Epoch [7/50], Train Loss: 0.0667, Val Loss: 0.1840\n",
      "Epoch [8/50], Train Loss: 0.0655, Val Loss: 0.1809\n",
      "Epoch [9/50], Train Loss: 0.0643, Val Loss: 0.1779\n",
      "Epoch [10/50], Train Loss: 0.0632, Val Loss: 0.1750\n",
      "Epoch [11/50], Train Loss: 0.0621, Val Loss: 0.1722\n",
      "Epoch [12/50], Train Loss: 0.0610, Val Loss: 0.1694\n",
      "Epoch [13/50], Train Loss: 0.0599, Val Loss: 0.1667\n",
      "Epoch [14/50], Train Loss: 0.0589, Val Loss: 0.1641\n",
      "Epoch [15/50], Train Loss: 0.0580, Val Loss: 0.1615\n",
      "Epoch [16/50], Train Loss: 0.0570, Val Loss: 0.1590\n",
      "Epoch [17/50], Train Loss: 0.0561, Val Loss: 0.1566\n",
      "Epoch [18/50], Train Loss: 0.0552, Val Loss: 0.1542\n",
      "Epoch [19/50], Train Loss: 0.0544, Val Loss: 0.1519\n",
      "Epoch [20/50], Train Loss: 0.0536, Val Loss: 0.1497\n",
      "Epoch [21/50], Train Loss: 0.0528, Val Loss: 0.1475\n",
      "Epoch [22/50], Train Loss: 0.0520, Val Loss: 0.1453\n",
      "Epoch [23/50], Train Loss: 0.0513, Val Loss: 0.1433\n",
      "Epoch [24/50], Train Loss: 0.0506, Val Loss: 0.1412\n",
      "Epoch [25/50], Train Loss: 0.0499, Val Loss: 0.1393\n",
      "Epoch [26/50], Train Loss: 0.0492, Val Loss: 0.1373\n",
      "Epoch [27/50], Train Loss: 0.0485, Val Loss: 0.1355\n",
      "Epoch [28/50], Train Loss: 0.0479, Val Loss: 0.1336\n",
      "Epoch [29/50], Train Loss: 0.0473, Val Loss: 0.1319\n",
      "Epoch [30/50], Train Loss: 0.0467, Val Loss: 0.1301\n",
      "Epoch [31/50], Train Loss: 0.0461, Val Loss: 0.1284\n",
      "Epoch [32/50], Train Loss: 0.0456, Val Loss: 0.1267\n",
      "Epoch [33/50], Train Loss: 0.0450, Val Loss: 0.1251\n",
      "Epoch [34/50], Train Loss: 0.0445, Val Loss: 0.1236\n",
      "Epoch [35/50], Train Loss: 0.0440, Val Loss: 0.1220\n",
      "Epoch [36/50], Train Loss: 0.0435, Val Loss: 0.1205\n",
      "Epoch [37/50], Train Loss: 0.0430, Val Loss: 0.1190\n",
      "Epoch [38/50], Train Loss: 0.0426, Val Loss: 0.1176\n",
      "Epoch [39/50], Train Loss: 0.0421, Val Loss: 0.1162\n",
      "Epoch [40/50], Train Loss: 0.0417, Val Loss: 0.1149\n",
      "Epoch [41/50], Train Loss: 0.0413, Val Loss: 0.1135\n",
      "Epoch [42/50], Train Loss: 0.0409, Val Loss: 0.1122\n",
      "Epoch [43/50], Train Loss: 0.0405, Val Loss: 0.1110\n",
      "Epoch [44/50], Train Loss: 0.0401, Val Loss: 0.1097\n",
      "Epoch [45/50], Train Loss: 0.0397, Val Loss: 0.1086\n",
      "Epoch [46/50], Train Loss: 0.0394, Val Loss: 0.1074\n",
      "Epoch [47/50], Train Loss: 0.0390, Val Loss: 0.1062\n",
      "Epoch [48/50], Train Loss: 0.0387, Val Loss: 0.1051\n",
      "Epoch [49/50], Train Loss: 0.0384, Val Loss: 0.1040\n",
      "Epoch [50/50], Train Loss: 0.0381, Val Loss: 0.1029\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=16, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1686, Val Loss: 0.4062\n",
      "Epoch [2/50], Train Loss: 0.1653, Val Loss: 0.4002\n",
      "Epoch [3/50], Train Loss: 0.1620, Val Loss: 0.3944\n",
      "Epoch [4/50], Train Loss: 0.1595, Val Loss: 0.3887\n",
      "Epoch [5/50], Train Loss: 0.1567, Val Loss: 0.3832\n",
      "Epoch [6/50], Train Loss: 0.1531, Val Loss: 0.3779\n",
      "Epoch [7/50], Train Loss: 0.1501, Val Loss: 0.3727\n",
      "Epoch [8/50], Train Loss: 0.1470, Val Loss: 0.3677\n",
      "Epoch [9/50], Train Loss: 0.1448, Val Loss: 0.3628\n",
      "Epoch [10/50], Train Loss: 0.1414, Val Loss: 0.3580\n",
      "Epoch [11/50], Train Loss: 0.1389, Val Loss: 0.3533\n",
      "Epoch [12/50], Train Loss: 0.1369, Val Loss: 0.3488\n",
      "Epoch [13/50], Train Loss: 0.1342, Val Loss: 0.3445\n",
      "Epoch [14/50], Train Loss: 0.1317, Val Loss: 0.3402\n",
      "Epoch [15/50], Train Loss: 0.1290, Val Loss: 0.3359\n",
      "Epoch [16/50], Train Loss: 0.1276, Val Loss: 0.3318\n",
      "Epoch [17/50], Train Loss: 0.1250, Val Loss: 0.3278\n",
      "Epoch [18/50], Train Loss: 0.1235, Val Loss: 0.3239\n",
      "Epoch [19/50], Train Loss: 0.1212, Val Loss: 0.3201\n",
      "Epoch [20/50], Train Loss: 0.1190, Val Loss: 0.3164\n",
      "Epoch [21/50], Train Loss: 0.1171, Val Loss: 0.3127\n",
      "Epoch [22/50], Train Loss: 0.1153, Val Loss: 0.3091\n",
      "Epoch [23/50], Train Loss: 0.1137, Val Loss: 0.3056\n",
      "Epoch [24/50], Train Loss: 0.1114, Val Loss: 0.3022\n",
      "Epoch [25/50], Train Loss: 0.1104, Val Loss: 0.2989\n",
      "Epoch [26/50], Train Loss: 0.1086, Val Loss: 0.2956\n",
      "Epoch [27/50], Train Loss: 0.1066, Val Loss: 0.2923\n",
      "Epoch [28/50], Train Loss: 0.1053, Val Loss: 0.2892\n",
      "Epoch [29/50], Train Loss: 0.1031, Val Loss: 0.2861\n",
      "Epoch [30/50], Train Loss: 0.1022, Val Loss: 0.2831\n",
      "Epoch [31/50], Train Loss: 0.1010, Val Loss: 0.2801\n",
      "Epoch [32/50], Train Loss: 0.0993, Val Loss: 0.2772\n",
      "Epoch [33/50], Train Loss: 0.0979, Val Loss: 0.2744\n",
      "Epoch [34/50], Train Loss: 0.0966, Val Loss: 0.2716\n",
      "Epoch [35/50], Train Loss: 0.0956, Val Loss: 0.2688\n",
      "Epoch [36/50], Train Loss: 0.0925, Val Loss: 0.2661\n",
      "Epoch [37/50], Train Loss: 0.0934, Val Loss: 0.2635\n",
      "Epoch [38/50], Train Loss: 0.0916, Val Loss: 0.2608\n",
      "Epoch [39/50], Train Loss: 0.0897, Val Loss: 0.2583\n",
      "Epoch [40/50], Train Loss: 0.0893, Val Loss: 0.2558\n",
      "Epoch [41/50], Train Loss: 0.0880, Val Loss: 0.2533\n",
      "Epoch [42/50], Train Loss: 0.0864, Val Loss: 0.2509\n",
      "Epoch [43/50], Train Loss: 0.0860, Val Loss: 0.2485\n",
      "Epoch [44/50], Train Loss: 0.0840, Val Loss: 0.2461\n",
      "Epoch [45/50], Train Loss: 0.0822, Val Loss: 0.2438\n",
      "Epoch [46/50], Train Loss: 0.0820, Val Loss: 0.2416\n",
      "Epoch [47/50], Train Loss: 0.0811, Val Loss: 0.2393\n",
      "Epoch [48/50], Train Loss: 0.0811, Val Loss: 0.2372\n",
      "Epoch [49/50], Train Loss: 0.0781, Val Loss: 0.2350\n",
      "Epoch [50/50], Train Loss: 0.0779, Val Loss: 0.2329\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=16, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1525, Val Loss: 0.3488\n",
      "Epoch [2/50], Train Loss: 0.1481, Val Loss: 0.3425\n",
      "Epoch [3/50], Train Loss: 0.1452, Val Loss: 0.3364\n",
      "Epoch [4/50], Train Loss: 0.1401, Val Loss: 0.3307\n",
      "Epoch [5/50], Train Loss: 0.1357, Val Loss: 0.3253\n",
      "Epoch [6/50], Train Loss: 0.1323, Val Loss: 0.3202\n",
      "Epoch [7/50], Train Loss: 0.1280, Val Loss: 0.3152\n",
      "Epoch [8/50], Train Loss: 0.1259, Val Loss: 0.3105\n",
      "Epoch [9/50], Train Loss: 0.1231, Val Loss: 0.3059\n",
      "Epoch [10/50], Train Loss: 0.1216, Val Loss: 0.3016\n",
      "Epoch [11/50], Train Loss: 0.1184, Val Loss: 0.2975\n",
      "Epoch [12/50], Train Loss: 0.1141, Val Loss: 0.2935\n",
      "Epoch [13/50], Train Loss: 0.1126, Val Loss: 0.2897\n",
      "Epoch [14/50], Train Loss: 0.1096, Val Loss: 0.2861\n",
      "Epoch [15/50], Train Loss: 0.1081, Val Loss: 0.2826\n",
      "Epoch [16/50], Train Loss: 0.1068, Val Loss: 0.2792\n",
      "Epoch [17/50], Train Loss: 0.1038, Val Loss: 0.2758\n",
      "Epoch [18/50], Train Loss: 0.1010, Val Loss: 0.2728\n",
      "Epoch [19/50], Train Loss: 0.1012, Val Loss: 0.2696\n",
      "Epoch [20/50], Train Loss: 0.0994, Val Loss: 0.2667\n",
      "Epoch [21/50], Train Loss: 0.0957, Val Loss: 0.2638\n",
      "Epoch [22/50], Train Loss: 0.0953, Val Loss: 0.2611\n",
      "Epoch [23/50], Train Loss: 0.0930, Val Loss: 0.2584\n",
      "Epoch [24/50], Train Loss: 0.0919, Val Loss: 0.2559\n",
      "Epoch [25/50], Train Loss: 0.0884, Val Loss: 0.2535\n",
      "Epoch [26/50], Train Loss: 0.0903, Val Loss: 0.2510\n",
      "Epoch [27/50], Train Loss: 0.0886, Val Loss: 0.2487\n",
      "Epoch [28/50], Train Loss: 0.0888, Val Loss: 0.2463\n",
      "Epoch [29/50], Train Loss: 0.0850, Val Loss: 0.2441\n",
      "Epoch [30/50], Train Loss: 0.0864, Val Loss: 0.2419\n",
      "Epoch [31/50], Train Loss: 0.0818, Val Loss: 0.2398\n",
      "Epoch [32/50], Train Loss: 0.0804, Val Loss: 0.2378\n",
      "Epoch [33/50], Train Loss: 0.0834, Val Loss: 0.2357\n",
      "Epoch [34/50], Train Loss: 0.0797, Val Loss: 0.2338\n",
      "Epoch [35/50], Train Loss: 0.0779, Val Loss: 0.2320\n",
      "Epoch [36/50], Train Loss: 0.0778, Val Loss: 0.2301\n",
      "Epoch [37/50], Train Loss: 0.0790, Val Loss: 0.2282\n",
      "Epoch [38/50], Train Loss: 0.0773, Val Loss: 0.2264\n",
      "Epoch [39/50], Train Loss: 0.0761, Val Loss: 0.2247\n",
      "Epoch [40/50], Train Loss: 0.0755, Val Loss: 0.2230\n",
      "Epoch [41/50], Train Loss: 0.0752, Val Loss: 0.2214\n",
      "Epoch [42/50], Train Loss: 0.0730, Val Loss: 0.2198\n",
      "Epoch [43/50], Train Loss: 0.0746, Val Loss: 0.2182\n",
      "Epoch [44/50], Train Loss: 0.0711, Val Loss: 0.2167\n",
      "Epoch [45/50], Train Loss: 0.0717, Val Loss: 0.2151\n",
      "Epoch [46/50], Train Loss: 0.0723, Val Loss: 0.2136\n",
      "Epoch [47/50], Train Loss: 0.0709, Val Loss: 0.2122\n",
      "Epoch [48/50], Train Loss: 0.0700, Val Loss: 0.2108\n",
      "Epoch [49/50], Train Loss: 0.0700, Val Loss: 0.2094\n",
      "Epoch [50/50], Train Loss: 0.0702, Val Loss: 0.2081\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=16, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.2413, Val Loss: 0.4427\n",
      "Epoch [2/50], Train Loss: 0.2265, Val Loss: 0.4226\n",
      "Epoch [3/50], Train Loss: 0.2131, Val Loss: 0.4042\n",
      "Epoch [4/50], Train Loss: 0.2009, Val Loss: 0.3872\n",
      "Epoch [5/50], Train Loss: 0.1899, Val Loss: 0.3717\n",
      "Epoch [6/50], Train Loss: 0.1799, Val Loss: 0.3574\n",
      "Epoch [7/50], Train Loss: 0.1707, Val Loss: 0.3441\n",
      "Epoch [8/50], Train Loss: 0.1623, Val Loss: 0.3318\n",
      "Epoch [9/50], Train Loss: 0.1546, Val Loss: 0.3204\n",
      "Epoch [10/50], Train Loss: 0.1475, Val Loss: 0.3097\n",
      "Epoch [11/50], Train Loss: 0.1410, Val Loss: 0.2998\n",
      "Epoch [12/50], Train Loss: 0.1350, Val Loss: 0.2905\n",
      "Epoch [13/50], Train Loss: 0.1294, Val Loss: 0.2819\n",
      "Epoch [14/50], Train Loss: 0.1242, Val Loss: 0.2737\n",
      "Epoch [15/50], Train Loss: 0.1194, Val Loss: 0.2660\n",
      "Epoch [16/50], Train Loss: 0.1149, Val Loss: 0.2588\n",
      "Epoch [17/50], Train Loss: 0.1107, Val Loss: 0.2520\n",
      "Epoch [18/50], Train Loss: 0.1068, Val Loss: 0.2456\n",
      "Epoch [19/50], Train Loss: 0.1031, Val Loss: 0.2396\n",
      "Epoch [20/50], Train Loss: 0.0997, Val Loss: 0.2339\n",
      "Epoch [21/50], Train Loss: 0.0965, Val Loss: 0.2284\n",
      "Epoch [22/50], Train Loss: 0.0935, Val Loss: 0.2233\n",
      "Epoch [23/50], Train Loss: 0.0907, Val Loss: 0.2184\n",
      "Epoch [24/50], Train Loss: 0.0880, Val Loss: 0.2138\n",
      "Epoch [25/50], Train Loss: 0.0855, Val Loss: 0.2094\n",
      "Epoch [26/50], Train Loss: 0.0831, Val Loss: 0.2052\n",
      "Epoch [27/50], Train Loss: 0.0809, Val Loss: 0.2012\n",
      "Epoch [28/50], Train Loss: 0.0788, Val Loss: 0.1974\n",
      "Epoch [29/50], Train Loss: 0.0768, Val Loss: 0.1938\n",
      "Epoch [30/50], Train Loss: 0.0749, Val Loss: 0.1903\n",
      "Epoch [31/50], Train Loss: 0.0731, Val Loss: 0.1871\n",
      "Epoch [32/50], Train Loss: 0.0714, Val Loss: 0.1839\n",
      "Epoch [33/50], Train Loss: 0.0698, Val Loss: 0.1809\n",
      "Epoch [34/50], Train Loss: 0.0683, Val Loss: 0.1780\n",
      "Epoch [35/50], Train Loss: 0.0669, Val Loss: 0.1752\n",
      "Epoch [36/50], Train Loss: 0.0655, Val Loss: 0.1726\n",
      "Epoch [37/50], Train Loss: 0.0642, Val Loss: 0.1700\n",
      "Epoch [38/50], Train Loss: 0.0630, Val Loss: 0.1676\n",
      "Epoch [39/50], Train Loss: 0.0618, Val Loss: 0.1653\n",
      "Epoch [40/50], Train Loss: 0.0607, Val Loss: 0.1630\n",
      "Epoch [41/50], Train Loss: 0.0596, Val Loss: 0.1609\n",
      "Epoch [42/50], Train Loss: 0.0586, Val Loss: 0.1588\n",
      "Epoch [43/50], Train Loss: 0.0577, Val Loss: 0.1569\n",
      "Epoch [44/50], Train Loss: 0.0567, Val Loss: 0.1550\n",
      "Epoch [45/50], Train Loss: 0.0559, Val Loss: 0.1531\n",
      "Epoch [46/50], Train Loss: 0.0550, Val Loss: 0.1514\n",
      "Epoch [47/50], Train Loss: 0.0542, Val Loss: 0.1497\n",
      "Epoch [48/50], Train Loss: 0.0535, Val Loss: 0.1481\n",
      "Epoch [49/50], Train Loss: 0.0527, Val Loss: 0.1465\n",
      "Epoch [50/50], Train Loss: 0.0520, Val Loss: 0.1450\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=16, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.2879, Val Loss: 0.6246\n",
      "Epoch [2/50], Train Loss: 0.2729, Val Loss: 0.6052\n",
      "Epoch [3/50], Train Loss: 0.2644, Val Loss: 0.5863\n",
      "Epoch [4/50], Train Loss: 0.2535, Val Loss: 0.5683\n",
      "Epoch [5/50], Train Loss: 0.2365, Val Loss: 0.5516\n",
      "Epoch [6/50], Train Loss: 0.2304, Val Loss: 0.5354\n",
      "Epoch [7/50], Train Loss: 0.2221, Val Loss: 0.5194\n",
      "Epoch [8/50], Train Loss: 0.2108, Val Loss: 0.5044\n",
      "Epoch [9/50], Train Loss: 0.2018, Val Loss: 0.4897\n",
      "Epoch [10/50], Train Loss: 0.1945, Val Loss: 0.4758\n",
      "Epoch [11/50], Train Loss: 0.1893, Val Loss: 0.4621\n",
      "Epoch [12/50], Train Loss: 0.1807, Val Loss: 0.4489\n",
      "Epoch [13/50], Train Loss: 0.1747, Val Loss: 0.4361\n",
      "Epoch [14/50], Train Loss: 0.1675, Val Loss: 0.4239\n",
      "Epoch [15/50], Train Loss: 0.1597, Val Loss: 0.4120\n",
      "Epoch [16/50], Train Loss: 0.1542, Val Loss: 0.4006\n",
      "Epoch [17/50], Train Loss: 0.1507, Val Loss: 0.3894\n",
      "Epoch [18/50], Train Loss: 0.1432, Val Loss: 0.3787\n",
      "Epoch [19/50], Train Loss: 0.1387, Val Loss: 0.3684\n",
      "Epoch [20/50], Train Loss: 0.1323, Val Loss: 0.3585\n",
      "Epoch [21/50], Train Loss: 0.1277, Val Loss: 0.3488\n",
      "Epoch [22/50], Train Loss: 0.1218, Val Loss: 0.3395\n",
      "Epoch [23/50], Train Loss: 0.1191, Val Loss: 0.3304\n",
      "Epoch [24/50], Train Loss: 0.1161, Val Loss: 0.3216\n",
      "Epoch [25/50], Train Loss: 0.1112, Val Loss: 0.3132\n",
      "Epoch [26/50], Train Loss: 0.1067, Val Loss: 0.3050\n",
      "Epoch [27/50], Train Loss: 0.1025, Val Loss: 0.2971\n",
      "Epoch [28/50], Train Loss: 0.1001, Val Loss: 0.2896\n",
      "Epoch [29/50], Train Loss: 0.0972, Val Loss: 0.2822\n",
      "Epoch [30/50], Train Loss: 0.0947, Val Loss: 0.2751\n",
      "Epoch [31/50], Train Loss: 0.0907, Val Loss: 0.2682\n",
      "Epoch [32/50], Train Loss: 0.0876, Val Loss: 0.2616\n",
      "Epoch [33/50], Train Loss: 0.0874, Val Loss: 0.2552\n",
      "Epoch [34/50], Train Loss: 0.0832, Val Loss: 0.2491\n",
      "Epoch [35/50], Train Loss: 0.0809, Val Loss: 0.2432\n",
      "Epoch [36/50], Train Loss: 0.0780, Val Loss: 0.2376\n",
      "Epoch [37/50], Train Loss: 0.0765, Val Loss: 0.2321\n",
      "Epoch [38/50], Train Loss: 0.0745, Val Loss: 0.2268\n",
      "Epoch [39/50], Train Loss: 0.0754, Val Loss: 0.2217\n",
      "Epoch [40/50], Train Loss: 0.0713, Val Loss: 0.2168\n",
      "Epoch [41/50], Train Loss: 0.0694, Val Loss: 0.2121\n",
      "Epoch [42/50], Train Loss: 0.0676, Val Loss: 0.2076\n",
      "Epoch [43/50], Train Loss: 0.0660, Val Loss: 0.2033\n",
      "Epoch [44/50], Train Loss: 0.0650, Val Loss: 0.1991\n",
      "Epoch [45/50], Train Loss: 0.0623, Val Loss: 0.1951\n",
      "Epoch [46/50], Train Loss: 0.0632, Val Loss: 0.1912\n",
      "Epoch [47/50], Train Loss: 0.0614, Val Loss: 0.1874\n",
      "Epoch [48/50], Train Loss: 0.0600, Val Loss: 0.1838\n",
      "Epoch [49/50], Train Loss: 0.0587, Val Loss: 0.1804\n",
      "Epoch [50/50], Train Loss: 0.0570, Val Loss: 0.1771\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=16, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.2350, Val Loss: 0.3664\n",
      "Epoch [2/50], Train Loss: 0.2240, Val Loss: 0.3530\n",
      "Epoch [3/50], Train Loss: 0.2067, Val Loss: 0.3413\n",
      "Epoch [4/50], Train Loss: 0.1914, Val Loss: 0.3309\n",
      "Epoch [5/50], Train Loss: 0.1844, Val Loss: 0.3218\n",
      "Epoch [6/50], Train Loss: 0.1773, Val Loss: 0.3135\n",
      "Epoch [7/50], Train Loss: 0.1734, Val Loss: 0.3061\n",
      "Epoch [8/50], Train Loss: 0.1701, Val Loss: 0.2992\n",
      "Epoch [9/50], Train Loss: 0.1565, Val Loss: 0.2934\n",
      "Epoch [10/50], Train Loss: 0.1528, Val Loss: 0.2880\n",
      "Epoch [11/50], Train Loss: 0.1513, Val Loss: 0.2832\n",
      "Epoch [12/50], Train Loss: 0.1439, Val Loss: 0.2790\n",
      "Epoch [13/50], Train Loss: 0.1452, Val Loss: 0.2750\n",
      "Epoch [14/50], Train Loss: 0.1375, Val Loss: 0.2713\n",
      "Epoch [15/50], Train Loss: 0.1397, Val Loss: 0.2678\n",
      "Epoch [16/50], Train Loss: 0.1350, Val Loss: 0.2644\n",
      "Epoch [17/50], Train Loss: 0.1344, Val Loss: 0.2612\n",
      "Epoch [18/50], Train Loss: 0.1312, Val Loss: 0.2582\n",
      "Epoch [19/50], Train Loss: 0.1269, Val Loss: 0.2553\n",
      "Epoch [20/50], Train Loss: 0.1271, Val Loss: 0.2525\n",
      "Epoch [21/50], Train Loss: 0.1225, Val Loss: 0.2498\n",
      "Epoch [22/50], Train Loss: 0.1185, Val Loss: 0.2472\n",
      "Epoch [23/50], Train Loss: 0.1210, Val Loss: 0.2446\n",
      "Epoch [24/50], Train Loss: 0.1218, Val Loss: 0.2422\n",
      "Epoch [25/50], Train Loss: 0.1164, Val Loss: 0.2397\n",
      "Epoch [26/50], Train Loss: 0.1160, Val Loss: 0.2374\n",
      "Epoch [27/50], Train Loss: 0.1154, Val Loss: 0.2351\n",
      "Epoch [28/50], Train Loss: 0.1124, Val Loss: 0.2329\n",
      "Epoch [29/50], Train Loss: 0.1131, Val Loss: 0.2307\n",
      "Epoch [30/50], Train Loss: 0.1098, Val Loss: 0.2287\n",
      "Epoch [31/50], Train Loss: 0.1103, Val Loss: 0.2267\n",
      "Epoch [32/50], Train Loss: 0.1078, Val Loss: 0.2249\n",
      "Epoch [33/50], Train Loss: 0.1037, Val Loss: 0.2230\n",
      "Epoch [34/50], Train Loss: 0.1040, Val Loss: 0.2210\n",
      "Epoch [35/50], Train Loss: 0.1054, Val Loss: 0.2192\n",
      "Epoch [36/50], Train Loss: 0.1055, Val Loss: 0.2173\n",
      "Epoch [37/50], Train Loss: 0.1012, Val Loss: 0.2156\n",
      "Epoch [38/50], Train Loss: 0.1006, Val Loss: 0.2138\n",
      "Epoch [39/50], Train Loss: 0.1023, Val Loss: 0.2121\n",
      "Epoch [40/50], Train Loss: 0.0974, Val Loss: 0.2104\n",
      "Epoch [41/50], Train Loss: 0.0999, Val Loss: 0.2086\n",
      "Epoch [42/50], Train Loss: 0.0966, Val Loss: 0.2071\n",
      "Epoch [43/50], Train Loss: 0.0985, Val Loss: 0.2055\n",
      "Epoch [44/50], Train Loss: 0.0985, Val Loss: 0.2039\n",
      "Epoch [45/50], Train Loss: 0.0949, Val Loss: 0.2024\n",
      "Epoch [46/50], Train Loss: 0.0969, Val Loss: 0.2009\n",
      "Epoch [47/50], Train Loss: 0.0944, Val Loss: 0.1994\n",
      "Epoch [48/50], Train Loss: 0.0944, Val Loss: 0.1980\n",
      "Epoch [49/50], Train Loss: 0.0908, Val Loss: 0.1966\n",
      "Epoch [50/50], Train Loss: 0.0910, Val Loss: 0.1953\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=16, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1381, Val Loss: 0.3389\n",
      "Epoch [2/50], Train Loss: 0.1332, Val Loss: 0.3306\n",
      "Epoch [3/50], Train Loss: 0.1285, Val Loss: 0.3226\n",
      "Epoch [4/50], Train Loss: 0.1241, Val Loss: 0.3150\n",
      "Epoch [5/50], Train Loss: 0.1199, Val Loss: 0.3076\n",
      "Epoch [6/50], Train Loss: 0.1159, Val Loss: 0.3006\n",
      "Epoch [7/50], Train Loss: 0.1121, Val Loss: 0.2938\n",
      "Epoch [8/50], Train Loss: 0.1086, Val Loss: 0.2872\n",
      "Epoch [9/50], Train Loss: 0.1051, Val Loss: 0.2809\n",
      "Epoch [10/50], Train Loss: 0.1019, Val Loss: 0.2748\n",
      "Epoch [11/50], Train Loss: 0.0988, Val Loss: 0.2689\n",
      "Epoch [12/50], Train Loss: 0.0959, Val Loss: 0.2632\n",
      "Epoch [13/50], Train Loss: 0.0931, Val Loss: 0.2577\n",
      "Epoch [14/50], Train Loss: 0.0904, Val Loss: 0.2524\n",
      "Epoch [15/50], Train Loss: 0.0878, Val Loss: 0.2473\n",
      "Epoch [16/50], Train Loss: 0.0854, Val Loss: 0.2423\n",
      "Epoch [17/50], Train Loss: 0.0830, Val Loss: 0.2375\n",
      "Epoch [18/50], Train Loss: 0.0808, Val Loss: 0.2328\n",
      "Epoch [19/50], Train Loss: 0.0786, Val Loss: 0.2283\n",
      "Epoch [20/50], Train Loss: 0.0766, Val Loss: 0.2239\n",
      "Epoch [21/50], Train Loss: 0.0746, Val Loss: 0.2196\n",
      "Epoch [22/50], Train Loss: 0.0728, Val Loss: 0.2155\n",
      "Epoch [23/50], Train Loss: 0.0710, Val Loss: 0.2115\n",
      "Epoch [24/50], Train Loss: 0.0692, Val Loss: 0.2077\n",
      "Epoch [25/50], Train Loss: 0.0676, Val Loss: 0.2040\n",
      "Epoch [26/50], Train Loss: 0.0660, Val Loss: 0.2003\n",
      "Epoch [27/50], Train Loss: 0.0645, Val Loss: 0.1968\n",
      "Epoch [28/50], Train Loss: 0.0631, Val Loss: 0.1934\n",
      "Epoch [29/50], Train Loss: 0.0617, Val Loss: 0.1901\n",
      "Epoch [30/50], Train Loss: 0.0604, Val Loss: 0.1869\n",
      "Epoch [31/50], Train Loss: 0.0592, Val Loss: 0.1838\n",
      "Epoch [32/50], Train Loss: 0.0580, Val Loss: 0.1808\n",
      "Epoch [33/50], Train Loss: 0.0568, Val Loss: 0.1779\n",
      "Epoch [34/50], Train Loss: 0.0557, Val Loss: 0.1751\n",
      "Epoch [35/50], Train Loss: 0.0547, Val Loss: 0.1724\n",
      "Epoch [36/50], Train Loss: 0.0537, Val Loss: 0.1697\n",
      "Epoch [37/50], Train Loss: 0.0527, Val Loss: 0.1672\n",
      "Epoch [38/50], Train Loss: 0.0518, Val Loss: 0.1648\n",
      "Epoch [39/50], Train Loss: 0.0510, Val Loss: 0.1623\n",
      "Epoch [40/50], Train Loss: 0.0501, Val Loss: 0.1600\n",
      "Epoch [41/50], Train Loss: 0.0493, Val Loss: 0.1578\n",
      "Epoch [42/50], Train Loss: 0.0486, Val Loss: 0.1556\n",
      "Epoch [43/50], Train Loss: 0.0479, Val Loss: 0.1536\n",
      "Epoch [44/50], Train Loss: 0.0472, Val Loss: 0.1515\n",
      "Epoch [45/50], Train Loss: 0.0465, Val Loss: 0.1496\n",
      "Epoch [46/50], Train Loss: 0.0459, Val Loss: 0.1477\n",
      "Epoch [47/50], Train Loss: 0.0453, Val Loss: 0.1459\n",
      "Epoch [48/50], Train Loss: 0.0448, Val Loss: 0.1441\n",
      "Epoch [49/50], Train Loss: 0.0442, Val Loss: 0.1424\n",
      "Epoch [50/50], Train Loss: 0.0437, Val Loss: 0.1408\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=16, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1978, Val Loss: 0.4578\n",
      "Epoch [2/50], Train Loss: 0.1865, Val Loss: 0.4428\n",
      "Epoch [3/50], Train Loss: 0.1754, Val Loss: 0.4293\n",
      "Epoch [4/50], Train Loss: 0.1689, Val Loss: 0.4166\n",
      "Epoch [5/50], Train Loss: 0.1594, Val Loss: 0.4051\n",
      "Epoch [6/50], Train Loss: 0.1534, Val Loss: 0.3942\n",
      "Epoch [7/50], Train Loss: 0.1479, Val Loss: 0.3841\n",
      "Epoch [8/50], Train Loss: 0.1421, Val Loss: 0.3745\n",
      "Epoch [9/50], Train Loss: 0.1371, Val Loss: 0.3653\n",
      "Epoch [10/50], Train Loss: 0.1310, Val Loss: 0.3568\n",
      "Epoch [11/50], Train Loss: 0.1269, Val Loss: 0.3487\n",
      "Epoch [12/50], Train Loss: 0.1239, Val Loss: 0.3411\n",
      "Epoch [13/50], Train Loss: 0.1199, Val Loss: 0.3338\n",
      "Epoch [14/50], Train Loss: 0.1158, Val Loss: 0.3268\n",
      "Epoch [15/50], Train Loss: 0.1107, Val Loss: 0.3201\n",
      "Epoch [16/50], Train Loss: 0.1090, Val Loss: 0.3136\n",
      "Epoch [17/50], Train Loss: 0.1051, Val Loss: 0.3074\n",
      "Epoch [18/50], Train Loss: 0.1038, Val Loss: 0.3014\n",
      "Epoch [19/50], Train Loss: 0.1015, Val Loss: 0.2957\n",
      "Epoch [20/50], Train Loss: 0.0991, Val Loss: 0.2901\n",
      "Epoch [21/50], Train Loss: 0.0966, Val Loss: 0.2847\n",
      "Epoch [22/50], Train Loss: 0.0945, Val Loss: 0.2796\n",
      "Epoch [23/50], Train Loss: 0.0946, Val Loss: 0.2745\n",
      "Epoch [24/50], Train Loss: 0.0901, Val Loss: 0.2696\n",
      "Epoch [25/50], Train Loss: 0.0897, Val Loss: 0.2649\n",
      "Epoch [26/50], Train Loss: 0.0866, Val Loss: 0.2604\n",
      "Epoch [27/50], Train Loss: 0.0853, Val Loss: 0.2560\n",
      "Epoch [28/50], Train Loss: 0.0846, Val Loss: 0.2519\n",
      "Epoch [29/50], Train Loss: 0.0826, Val Loss: 0.2478\n",
      "Epoch [30/50], Train Loss: 0.0826, Val Loss: 0.2437\n",
      "Epoch [31/50], Train Loss: 0.0802, Val Loss: 0.2400\n",
      "Epoch [32/50], Train Loss: 0.0782, Val Loss: 0.2363\n",
      "Epoch [33/50], Train Loss: 0.0784, Val Loss: 0.2327\n",
      "Epoch [34/50], Train Loss: 0.0757, Val Loss: 0.2293\n",
      "Epoch [35/50], Train Loss: 0.0748, Val Loss: 0.2260\n",
      "Epoch [36/50], Train Loss: 0.0758, Val Loss: 0.2228\n",
      "Epoch [37/50], Train Loss: 0.0744, Val Loss: 0.2196\n",
      "Epoch [38/50], Train Loss: 0.0720, Val Loss: 0.2165\n",
      "Epoch [39/50], Train Loss: 0.0713, Val Loss: 0.2136\n",
      "Epoch [40/50], Train Loss: 0.0710, Val Loss: 0.2108\n",
      "Epoch [41/50], Train Loss: 0.0707, Val Loss: 0.2080\n",
      "Epoch [42/50], Train Loss: 0.0682, Val Loss: 0.2055\n",
      "Epoch [43/50], Train Loss: 0.0678, Val Loss: 0.2029\n",
      "Epoch [44/50], Train Loss: 0.0682, Val Loss: 0.2005\n",
      "Epoch [45/50], Train Loss: 0.0656, Val Loss: 0.1981\n",
      "Epoch [46/50], Train Loss: 0.0658, Val Loss: 0.1958\n",
      "Epoch [47/50], Train Loss: 0.0643, Val Loss: 0.1936\n",
      "Epoch [48/50], Train Loss: 0.0636, Val Loss: 0.1913\n",
      "Epoch [49/50], Train Loss: 0.0636, Val Loss: 0.1891\n",
      "Epoch [50/50], Train Loss: 0.0626, Val Loss: 0.1871\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=16, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1927, Val Loss: 0.3727\n",
      "Epoch [2/50], Train Loss: 0.1701, Val Loss: 0.3625\n",
      "Epoch [3/50], Train Loss: 0.1699, Val Loss: 0.3527\n",
      "Epoch [4/50], Train Loss: 0.1663, Val Loss: 0.3427\n",
      "Epoch [5/50], Train Loss: 0.1555, Val Loss: 0.3343\n",
      "Epoch [6/50], Train Loss: 0.1582, Val Loss: 0.3255\n",
      "Epoch [7/50], Train Loss: 0.1549, Val Loss: 0.3173\n",
      "Epoch [8/50], Train Loss: 0.1483, Val Loss: 0.3094\n",
      "Epoch [9/50], Train Loss: 0.1485, Val Loss: 0.3019\n",
      "Epoch [10/50], Train Loss: 0.1360, Val Loss: 0.2951\n",
      "Epoch [11/50], Train Loss: 0.1360, Val Loss: 0.2883\n",
      "Epoch [12/50], Train Loss: 0.1395, Val Loss: 0.2817\n",
      "Epoch [13/50], Train Loss: 0.1343, Val Loss: 0.2754\n",
      "Epoch [14/50], Train Loss: 0.1294, Val Loss: 0.2695\n",
      "Epoch [15/50], Train Loss: 0.1315, Val Loss: 0.2636\n",
      "Epoch [16/50], Train Loss: 0.1267, Val Loss: 0.2583\n",
      "Epoch [17/50], Train Loss: 0.1285, Val Loss: 0.2530\n",
      "Epoch [18/50], Train Loss: 0.1198, Val Loss: 0.2481\n",
      "Epoch [19/50], Train Loss: 0.1211, Val Loss: 0.2435\n",
      "Epoch [20/50], Train Loss: 0.1190, Val Loss: 0.2393\n",
      "Epoch [21/50], Train Loss: 0.1222, Val Loss: 0.2348\n",
      "Epoch [22/50], Train Loss: 0.1157, Val Loss: 0.2306\n",
      "Epoch [23/50], Train Loss: 0.1102, Val Loss: 0.2264\n",
      "Epoch [24/50], Train Loss: 0.1124, Val Loss: 0.2223\n",
      "Epoch [25/50], Train Loss: 0.1116, Val Loss: 0.2187\n",
      "Epoch [26/50], Train Loss: 0.1129, Val Loss: 0.2153\n",
      "Epoch [27/50], Train Loss: 0.1093, Val Loss: 0.2122\n",
      "Epoch [28/50], Train Loss: 0.1087, Val Loss: 0.2088\n",
      "Epoch [29/50], Train Loss: 0.1077, Val Loss: 0.2057\n",
      "Epoch [30/50], Train Loss: 0.1050, Val Loss: 0.2026\n",
      "Epoch [31/50], Train Loss: 0.1005, Val Loss: 0.2001\n",
      "Epoch [32/50], Train Loss: 0.1049, Val Loss: 0.1975\n",
      "Epoch [33/50], Train Loss: 0.0989, Val Loss: 0.1950\n",
      "Epoch [34/50], Train Loss: 0.1019, Val Loss: 0.1924\n",
      "Epoch [35/50], Train Loss: 0.1012, Val Loss: 0.1901\n",
      "Epoch [36/50], Train Loss: 0.1003, Val Loss: 0.1880\n",
      "Epoch [37/50], Train Loss: 0.1004, Val Loss: 0.1859\n",
      "Epoch [38/50], Train Loss: 0.0965, Val Loss: 0.1840\n",
      "Epoch [39/50], Train Loss: 0.1040, Val Loss: 0.1817\n",
      "Epoch [40/50], Train Loss: 0.0931, Val Loss: 0.1800\n",
      "Epoch [41/50], Train Loss: 0.0950, Val Loss: 0.1784\n",
      "Epoch [42/50], Train Loss: 0.0954, Val Loss: 0.1767\n",
      "Epoch [43/50], Train Loss: 0.0963, Val Loss: 0.1747\n",
      "Epoch [44/50], Train Loss: 0.0924, Val Loss: 0.1730\n",
      "Epoch [45/50], Train Loss: 0.0922, Val Loss: 0.1713\n",
      "Epoch [46/50], Train Loss: 0.0915, Val Loss: 0.1700\n",
      "Epoch [47/50], Train Loss: 0.0868, Val Loss: 0.1688\n",
      "Epoch [48/50], Train Loss: 0.0894, Val Loss: 0.1676\n",
      "Epoch [49/50], Train Loss: 0.0889, Val Loss: 0.1664\n",
      "Epoch [50/50], Train Loss: 0.0922, Val Loss: 0.1652\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=32, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1909, Val Loss: 0.4181\n",
      "Epoch [2/50], Train Loss: 0.1819, Val Loss: 0.4028\n",
      "Epoch [3/50], Train Loss: 0.1736, Val Loss: 0.3883\n",
      "Epoch [4/50], Train Loss: 0.1657, Val Loss: 0.3746\n",
      "Epoch [5/50], Train Loss: 0.1584, Val Loss: 0.3616\n",
      "Epoch [6/50], Train Loss: 0.1514, Val Loss: 0.3493\n",
      "Epoch [7/50], Train Loss: 0.1448, Val Loss: 0.3376\n",
      "Epoch [8/50], Train Loss: 0.1386, Val Loss: 0.3264\n",
      "Epoch [9/50], Train Loss: 0.1328, Val Loss: 0.3157\n",
      "Epoch [10/50], Train Loss: 0.1272, Val Loss: 0.3055\n",
      "Epoch [11/50], Train Loss: 0.1219, Val Loss: 0.2958\n",
      "Epoch [12/50], Train Loss: 0.1169, Val Loss: 0.2865\n",
      "Epoch [13/50], Train Loss: 0.1122, Val Loss: 0.2776\n",
      "Epoch [14/50], Train Loss: 0.1077, Val Loss: 0.2691\n",
      "Epoch [15/50], Train Loss: 0.1034, Val Loss: 0.2610\n",
      "Epoch [16/50], Train Loss: 0.0994, Val Loss: 0.2532\n",
      "Epoch [17/50], Train Loss: 0.0955, Val Loss: 0.2457\n",
      "Epoch [18/50], Train Loss: 0.0918, Val Loss: 0.2385\n",
      "Epoch [19/50], Train Loss: 0.0883, Val Loss: 0.2316\n",
      "Epoch [20/50], Train Loss: 0.0850, Val Loss: 0.2250\n",
      "Epoch [21/50], Train Loss: 0.0818, Val Loss: 0.2186\n",
      "Epoch [22/50], Train Loss: 0.0788, Val Loss: 0.2126\n",
      "Epoch [23/50], Train Loss: 0.0760, Val Loss: 0.2067\n",
      "Epoch [24/50], Train Loss: 0.0732, Val Loss: 0.2011\n",
      "Epoch [25/50], Train Loss: 0.0706, Val Loss: 0.1957\n",
      "Epoch [26/50], Train Loss: 0.0682, Val Loss: 0.1905\n",
      "Epoch [27/50], Train Loss: 0.0658, Val Loss: 0.1855\n",
      "Epoch [28/50], Train Loss: 0.0636, Val Loss: 0.1807\n",
      "Epoch [29/50], Train Loss: 0.0614, Val Loss: 0.1761\n",
      "Epoch [30/50], Train Loss: 0.0594, Val Loss: 0.1716\n",
      "Epoch [31/50], Train Loss: 0.0575, Val Loss: 0.1674\n",
      "Epoch [32/50], Train Loss: 0.0556, Val Loss: 0.1633\n",
      "Epoch [33/50], Train Loss: 0.0539, Val Loss: 0.1593\n",
      "Epoch [34/50], Train Loss: 0.0522, Val Loss: 0.1555\n",
      "Epoch [35/50], Train Loss: 0.0507, Val Loss: 0.1519\n",
      "Epoch [36/50], Train Loss: 0.0492, Val Loss: 0.1484\n",
      "Epoch [37/50], Train Loss: 0.0477, Val Loss: 0.1450\n",
      "Epoch [38/50], Train Loss: 0.0464, Val Loss: 0.1418\n",
      "Epoch [39/50], Train Loss: 0.0451, Val Loss: 0.1386\n",
      "Epoch [40/50], Train Loss: 0.0439, Val Loss: 0.1356\n",
      "Epoch [41/50], Train Loss: 0.0427, Val Loss: 0.1327\n",
      "Epoch [42/50], Train Loss: 0.0416, Val Loss: 0.1299\n",
      "Epoch [43/50], Train Loss: 0.0406, Val Loss: 0.1273\n",
      "Epoch [44/50], Train Loss: 0.0396, Val Loss: 0.1247\n",
      "Epoch [45/50], Train Loss: 0.0386, Val Loss: 0.1222\n",
      "Epoch [46/50], Train Loss: 0.0377, Val Loss: 0.1199\n",
      "Epoch [47/50], Train Loss: 0.0369, Val Loss: 0.1176\n",
      "Epoch [48/50], Train Loss: 0.0361, Val Loss: 0.1153\n",
      "Epoch [49/50], Train Loss: 0.0353, Val Loss: 0.1132\n",
      "Epoch [50/50], Train Loss: 0.0346, Val Loss: 0.1112\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=32, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1532, Val Loss: 0.4445\n",
      "Epoch [2/50], Train Loss: 0.1491, Val Loss: 0.4355\n",
      "Epoch [3/50], Train Loss: 0.1464, Val Loss: 0.4267\n",
      "Epoch [4/50], Train Loss: 0.1415, Val Loss: 0.4181\n",
      "Epoch [5/50], Train Loss: 0.1389, Val Loss: 0.4098\n",
      "Epoch [6/50], Train Loss: 0.1336, Val Loss: 0.4018\n",
      "Epoch [7/50], Train Loss: 0.1301, Val Loss: 0.3939\n",
      "Epoch [8/50], Train Loss: 0.1278, Val Loss: 0.3862\n",
      "Epoch [9/50], Train Loss: 0.1239, Val Loss: 0.3787\n",
      "Epoch [10/50], Train Loss: 0.1213, Val Loss: 0.3715\n",
      "Epoch [11/50], Train Loss: 0.1173, Val Loss: 0.3644\n",
      "Epoch [12/50], Train Loss: 0.1150, Val Loss: 0.3575\n",
      "Epoch [13/50], Train Loss: 0.1116, Val Loss: 0.3506\n",
      "Epoch [14/50], Train Loss: 0.1093, Val Loss: 0.3440\n",
      "Epoch [15/50], Train Loss: 0.1062, Val Loss: 0.3376\n",
      "Epoch [16/50], Train Loss: 0.1045, Val Loss: 0.3313\n",
      "Epoch [17/50], Train Loss: 0.1015, Val Loss: 0.3251\n",
      "Epoch [18/50], Train Loss: 0.0999, Val Loss: 0.3190\n",
      "Epoch [19/50], Train Loss: 0.0966, Val Loss: 0.3131\n",
      "Epoch [20/50], Train Loss: 0.0949, Val Loss: 0.3073\n",
      "Epoch [21/50], Train Loss: 0.0925, Val Loss: 0.3017\n",
      "Epoch [22/50], Train Loss: 0.0902, Val Loss: 0.2962\n",
      "Epoch [23/50], Train Loss: 0.0885, Val Loss: 0.2907\n",
      "Epoch [24/50], Train Loss: 0.0864, Val Loss: 0.2855\n",
      "Epoch [25/50], Train Loss: 0.0838, Val Loss: 0.2803\n",
      "Epoch [26/50], Train Loss: 0.0825, Val Loss: 0.2752\n",
      "Epoch [27/50], Train Loss: 0.0815, Val Loss: 0.2702\n",
      "Epoch [28/50], Train Loss: 0.0789, Val Loss: 0.2654\n",
      "Epoch [29/50], Train Loss: 0.0765, Val Loss: 0.2606\n",
      "Epoch [30/50], Train Loss: 0.0748, Val Loss: 0.2560\n",
      "Epoch [31/50], Train Loss: 0.0750, Val Loss: 0.2514\n",
      "Epoch [32/50], Train Loss: 0.0730, Val Loss: 0.2470\n",
      "Epoch [33/50], Train Loss: 0.0708, Val Loss: 0.2426\n",
      "Epoch [34/50], Train Loss: 0.0695, Val Loss: 0.2383\n",
      "Epoch [35/50], Train Loss: 0.0680, Val Loss: 0.2341\n",
      "Epoch [36/50], Train Loss: 0.0664, Val Loss: 0.2301\n",
      "Epoch [37/50], Train Loss: 0.0653, Val Loss: 0.2261\n",
      "Epoch [38/50], Train Loss: 0.0643, Val Loss: 0.2222\n",
      "Epoch [39/50], Train Loss: 0.0634, Val Loss: 0.2184\n",
      "Epoch [40/50], Train Loss: 0.0616, Val Loss: 0.2146\n",
      "Epoch [41/50], Train Loss: 0.0604, Val Loss: 0.2110\n",
      "Epoch [42/50], Train Loss: 0.0590, Val Loss: 0.2074\n",
      "Epoch [43/50], Train Loss: 0.0583, Val Loss: 0.2039\n",
      "Epoch [44/50], Train Loss: 0.0573, Val Loss: 0.2005\n",
      "Epoch [45/50], Train Loss: 0.0558, Val Loss: 0.1972\n",
      "Epoch [46/50], Train Loss: 0.0550, Val Loss: 0.1940\n",
      "Epoch [47/50], Train Loss: 0.0548, Val Loss: 0.1909\n",
      "Epoch [48/50], Train Loss: 0.0537, Val Loss: 0.1878\n",
      "Epoch [49/50], Train Loss: 0.0523, Val Loss: 0.1848\n",
      "Epoch [50/50], Train Loss: 0.0520, Val Loss: 0.1818\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=32, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1542, Val Loss: 0.3676\n",
      "Epoch [2/50], Train Loss: 0.1483, Val Loss: 0.3584\n",
      "Epoch [3/50], Train Loss: 0.1434, Val Loss: 0.3496\n",
      "Epoch [4/50], Train Loss: 0.1388, Val Loss: 0.3411\n",
      "Epoch [5/50], Train Loss: 0.1340, Val Loss: 0.3329\n",
      "Epoch [6/50], Train Loss: 0.1289, Val Loss: 0.3250\n",
      "Epoch [7/50], Train Loss: 0.1267, Val Loss: 0.3176\n",
      "Epoch [8/50], Train Loss: 0.1229, Val Loss: 0.3104\n",
      "Epoch [9/50], Train Loss: 0.1195, Val Loss: 0.3033\n",
      "Epoch [10/50], Train Loss: 0.1149, Val Loss: 0.2966\n",
      "Epoch [11/50], Train Loss: 0.1108, Val Loss: 0.2901\n",
      "Epoch [12/50], Train Loss: 0.1101, Val Loss: 0.2839\n",
      "Epoch [13/50], Train Loss: 0.1073, Val Loss: 0.2779\n",
      "Epoch [14/50], Train Loss: 0.1071, Val Loss: 0.2721\n",
      "Epoch [15/50], Train Loss: 0.1038, Val Loss: 0.2666\n",
      "Epoch [16/50], Train Loss: 0.0999, Val Loss: 0.2612\n",
      "Epoch [17/50], Train Loss: 0.0968, Val Loss: 0.2559\n",
      "Epoch [18/50], Train Loss: 0.0943, Val Loss: 0.2509\n",
      "Epoch [19/50], Train Loss: 0.0915, Val Loss: 0.2459\n",
      "Epoch [20/50], Train Loss: 0.0893, Val Loss: 0.2412\n",
      "Epoch [21/50], Train Loss: 0.0874, Val Loss: 0.2366\n",
      "Epoch [22/50], Train Loss: 0.0870, Val Loss: 0.2322\n",
      "Epoch [23/50], Train Loss: 0.0828, Val Loss: 0.2279\n",
      "Epoch [24/50], Train Loss: 0.0833, Val Loss: 0.2238\n",
      "Epoch [25/50], Train Loss: 0.0797, Val Loss: 0.2197\n",
      "Epoch [26/50], Train Loss: 0.0784, Val Loss: 0.2158\n",
      "Epoch [27/50], Train Loss: 0.0794, Val Loss: 0.2120\n",
      "Epoch [28/50], Train Loss: 0.0775, Val Loss: 0.2084\n",
      "Epoch [29/50], Train Loss: 0.0747, Val Loss: 0.2049\n",
      "Epoch [30/50], Train Loss: 0.0746, Val Loss: 0.2015\n",
      "Epoch [31/50], Train Loss: 0.0711, Val Loss: 0.1982\n",
      "Epoch [32/50], Train Loss: 0.0711, Val Loss: 0.1950\n",
      "Epoch [33/50], Train Loss: 0.0708, Val Loss: 0.1919\n",
      "Epoch [34/50], Train Loss: 0.0695, Val Loss: 0.1889\n",
      "Epoch [35/50], Train Loss: 0.0675, Val Loss: 0.1860\n",
      "Epoch [36/50], Train Loss: 0.0673, Val Loss: 0.1832\n",
      "Epoch [37/50], Train Loss: 0.0663, Val Loss: 0.1805\n",
      "Epoch [38/50], Train Loss: 0.0637, Val Loss: 0.1778\n",
      "Epoch [39/50], Train Loss: 0.0629, Val Loss: 0.1753\n",
      "Epoch [40/50], Train Loss: 0.0652, Val Loss: 0.1729\n",
      "Epoch [41/50], Train Loss: 0.0647, Val Loss: 0.1705\n",
      "Epoch [42/50], Train Loss: 0.0628, Val Loss: 0.1682\n",
      "Epoch [43/50], Train Loss: 0.0631, Val Loss: 0.1659\n",
      "Epoch [44/50], Train Loss: 0.0621, Val Loss: 0.1637\n",
      "Epoch [45/50], Train Loss: 0.0624, Val Loss: 0.1616\n",
      "Epoch [46/50], Train Loss: 0.0590, Val Loss: 0.1595\n",
      "Epoch [47/50], Train Loss: 0.0585, Val Loss: 0.1576\n",
      "Epoch [48/50], Train Loss: 0.0595, Val Loss: 0.1557\n",
      "Epoch [49/50], Train Loss: 0.0604, Val Loss: 0.1538\n",
      "Epoch [50/50], Train Loss: 0.0589, Val Loss: 0.1519\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=32, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1348, Val Loss: 0.3461\n",
      "Epoch [2/50], Train Loss: 0.1293, Val Loss: 0.3367\n",
      "Epoch [3/50], Train Loss: 0.1241, Val Loss: 0.3276\n",
      "Epoch [4/50], Train Loss: 0.1192, Val Loss: 0.3189\n",
      "Epoch [5/50], Train Loss: 0.1145, Val Loss: 0.3106\n",
      "Epoch [6/50], Train Loss: 0.1101, Val Loss: 0.3025\n",
      "Epoch [7/50], Train Loss: 0.1059, Val Loss: 0.2948\n",
      "Epoch [8/50], Train Loss: 0.1019, Val Loss: 0.2873\n",
      "Epoch [9/50], Train Loss: 0.0982, Val Loss: 0.2801\n",
      "Epoch [10/50], Train Loss: 0.0946, Val Loss: 0.2732\n",
      "Epoch [11/50], Train Loss: 0.0912, Val Loss: 0.2665\n",
      "Epoch [12/50], Train Loss: 0.0880, Val Loss: 0.2601\n",
      "Epoch [13/50], Train Loss: 0.0850, Val Loss: 0.2539\n",
      "Epoch [14/50], Train Loss: 0.0821, Val Loss: 0.2479\n",
      "Epoch [15/50], Train Loss: 0.0794, Val Loss: 0.2421\n",
      "Epoch [16/50], Train Loss: 0.0767, Val Loss: 0.2365\n",
      "Epoch [17/50], Train Loss: 0.0743, Val Loss: 0.2312\n",
      "Epoch [18/50], Train Loss: 0.0719, Val Loss: 0.2260\n",
      "Epoch [19/50], Train Loss: 0.0697, Val Loss: 0.2210\n",
      "Epoch [20/50], Train Loss: 0.0676, Val Loss: 0.2161\n",
      "Epoch [21/50], Train Loss: 0.0656, Val Loss: 0.2114\n",
      "Epoch [22/50], Train Loss: 0.0637, Val Loss: 0.2069\n",
      "Epoch [23/50], Train Loss: 0.0619, Val Loss: 0.2026\n",
      "Epoch [24/50], Train Loss: 0.0602, Val Loss: 0.1984\n",
      "Epoch [25/50], Train Loss: 0.0586, Val Loss: 0.1943\n",
      "Epoch [26/50], Train Loss: 0.0570, Val Loss: 0.1904\n",
      "Epoch [27/50], Train Loss: 0.0556, Val Loss: 0.1867\n",
      "Epoch [28/50], Train Loss: 0.0542, Val Loss: 0.1830\n",
      "Epoch [29/50], Train Loss: 0.0529, Val Loss: 0.1795\n",
      "Epoch [30/50], Train Loss: 0.0517, Val Loss: 0.1761\n",
      "Epoch [31/50], Train Loss: 0.0505, Val Loss: 0.1728\n",
      "Epoch [32/50], Train Loss: 0.0494, Val Loss: 0.1697\n",
      "Epoch [33/50], Train Loss: 0.0483, Val Loss: 0.1667\n",
      "Epoch [34/50], Train Loss: 0.0474, Val Loss: 0.1637\n",
      "Epoch [35/50], Train Loss: 0.0464, Val Loss: 0.1609\n",
      "Epoch [36/50], Train Loss: 0.0455, Val Loss: 0.1582\n",
      "Epoch [37/50], Train Loss: 0.0447, Val Loss: 0.1556\n",
      "Epoch [38/50], Train Loss: 0.0439, Val Loss: 0.1530\n",
      "Epoch [39/50], Train Loss: 0.0432, Val Loss: 0.1506\n",
      "Epoch [40/50], Train Loss: 0.0425, Val Loss: 0.1482\n",
      "Epoch [41/50], Train Loss: 0.0418, Val Loss: 0.1460\n",
      "Epoch [42/50], Train Loss: 0.0412, Val Loss: 0.1438\n",
      "Epoch [43/50], Train Loss: 0.0406, Val Loss: 0.1417\n",
      "Epoch [44/50], Train Loss: 0.0400, Val Loss: 0.1396\n",
      "Epoch [45/50], Train Loss: 0.0395, Val Loss: 0.1377\n",
      "Epoch [46/50], Train Loss: 0.0390, Val Loss: 0.1358\n",
      "Epoch [47/50], Train Loss: 0.0385, Val Loss: 0.1339\n",
      "Epoch [48/50], Train Loss: 0.0381, Val Loss: 0.1322\n",
      "Epoch [49/50], Train Loss: 0.0376, Val Loss: 0.1305\n",
      "Epoch [50/50], Train Loss: 0.0373, Val Loss: 0.1289\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=32, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1709, Val Loss: 0.4603\n",
      "Epoch [2/50], Train Loss: 0.1647, Val Loss: 0.4498\n",
      "Epoch [3/50], Train Loss: 0.1582, Val Loss: 0.4398\n",
      "Epoch [4/50], Train Loss: 0.1554, Val Loss: 0.4302\n",
      "Epoch [5/50], Train Loss: 0.1503, Val Loss: 0.4210\n",
      "Epoch [6/50], Train Loss: 0.1457, Val Loss: 0.4121\n",
      "Epoch [7/50], Train Loss: 0.1419, Val Loss: 0.4035\n",
      "Epoch [8/50], Train Loss: 0.1366, Val Loss: 0.3953\n",
      "Epoch [9/50], Train Loss: 0.1358, Val Loss: 0.3875\n",
      "Epoch [10/50], Train Loss: 0.1304, Val Loss: 0.3799\n",
      "Epoch [11/50], Train Loss: 0.1271, Val Loss: 0.3727\n",
      "Epoch [12/50], Train Loss: 0.1239, Val Loss: 0.3657\n",
      "Epoch [13/50], Train Loss: 0.1207, Val Loss: 0.3589\n",
      "Epoch [14/50], Train Loss: 0.1174, Val Loss: 0.3523\n",
      "Epoch [15/50], Train Loss: 0.1154, Val Loss: 0.3460\n",
      "Epoch [16/50], Train Loss: 0.1120, Val Loss: 0.3398\n",
      "Epoch [17/50], Train Loss: 0.1095, Val Loss: 0.3339\n",
      "Epoch [18/50], Train Loss: 0.1069, Val Loss: 0.3280\n",
      "Epoch [19/50], Train Loss: 0.1037, Val Loss: 0.3224\n",
      "Epoch [20/50], Train Loss: 0.1019, Val Loss: 0.3170\n",
      "Epoch [21/50], Train Loss: 0.0991, Val Loss: 0.3117\n",
      "Epoch [22/50], Train Loss: 0.0976, Val Loss: 0.3065\n",
      "Epoch [23/50], Train Loss: 0.0949, Val Loss: 0.3015\n",
      "Epoch [24/50], Train Loss: 0.0922, Val Loss: 0.2967\n",
      "Epoch [25/50], Train Loss: 0.0915, Val Loss: 0.2920\n",
      "Epoch [26/50], Train Loss: 0.0896, Val Loss: 0.2873\n",
      "Epoch [27/50], Train Loss: 0.0885, Val Loss: 0.2828\n",
      "Epoch [28/50], Train Loss: 0.0848, Val Loss: 0.2784\n",
      "Epoch [29/50], Train Loss: 0.0853, Val Loss: 0.2741\n",
      "Epoch [30/50], Train Loss: 0.0831, Val Loss: 0.2699\n",
      "Epoch [31/50], Train Loss: 0.0812, Val Loss: 0.2659\n",
      "Epoch [32/50], Train Loss: 0.0803, Val Loss: 0.2619\n",
      "Epoch [33/50], Train Loss: 0.0779, Val Loss: 0.2580\n",
      "Epoch [34/50], Train Loss: 0.0769, Val Loss: 0.2542\n",
      "Epoch [35/50], Train Loss: 0.0754, Val Loss: 0.2506\n",
      "Epoch [36/50], Train Loss: 0.0745, Val Loss: 0.2470\n",
      "Epoch [37/50], Train Loss: 0.0728, Val Loss: 0.2435\n",
      "Epoch [38/50], Train Loss: 0.0718, Val Loss: 0.2401\n",
      "Epoch [39/50], Train Loss: 0.0706, Val Loss: 0.2367\n",
      "Epoch [40/50], Train Loss: 0.0694, Val Loss: 0.2335\n",
      "Epoch [41/50], Train Loss: 0.0688, Val Loss: 0.2304\n",
      "Epoch [42/50], Train Loss: 0.0681, Val Loss: 0.2273\n",
      "Epoch [43/50], Train Loss: 0.0662, Val Loss: 0.2243\n",
      "Epoch [44/50], Train Loss: 0.0658, Val Loss: 0.2213\n",
      "Epoch [45/50], Train Loss: 0.0638, Val Loss: 0.2184\n",
      "Epoch [46/50], Train Loss: 0.0634, Val Loss: 0.2156\n",
      "Epoch [47/50], Train Loss: 0.0616, Val Loss: 0.2129\n",
      "Epoch [48/50], Train Loss: 0.0617, Val Loss: 0.2102\n",
      "Epoch [49/50], Train Loss: 0.0611, Val Loss: 0.2076\n",
      "Epoch [50/50], Train Loss: 0.0598, Val Loss: 0.2050\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=32, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1818, Val Loss: 0.3450\n",
      "Epoch [2/50], Train Loss: 0.1716, Val Loss: 0.3352\n",
      "Epoch [3/50], Train Loss: 0.1677, Val Loss: 0.3260\n",
      "Epoch [4/50], Train Loss: 0.1593, Val Loss: 0.3176\n",
      "Epoch [5/50], Train Loss: 0.1534, Val Loss: 0.3095\n",
      "Epoch [6/50], Train Loss: 0.1496, Val Loss: 0.3018\n",
      "Epoch [7/50], Train Loss: 0.1449, Val Loss: 0.2946\n",
      "Epoch [8/50], Train Loss: 0.1427, Val Loss: 0.2877\n",
      "Epoch [9/50], Train Loss: 0.1362, Val Loss: 0.2813\n",
      "Epoch [10/50], Train Loss: 0.1315, Val Loss: 0.2751\n",
      "Epoch [11/50], Train Loss: 0.1313, Val Loss: 0.2692\n",
      "Epoch [12/50], Train Loss: 0.1245, Val Loss: 0.2636\n",
      "Epoch [13/50], Train Loss: 0.1242, Val Loss: 0.2583\n",
      "Epoch [14/50], Train Loss: 0.1188, Val Loss: 0.2533\n",
      "Epoch [15/50], Train Loss: 0.1153, Val Loss: 0.2485\n",
      "Epoch [16/50], Train Loss: 0.1129, Val Loss: 0.2438\n",
      "Epoch [17/50], Train Loss: 0.1098, Val Loss: 0.2395\n",
      "Epoch [18/50], Train Loss: 0.1096, Val Loss: 0.2353\n",
      "Epoch [19/50], Train Loss: 0.1043, Val Loss: 0.2314\n",
      "Epoch [20/50], Train Loss: 0.1055, Val Loss: 0.2276\n",
      "Epoch [21/50], Train Loss: 0.1029, Val Loss: 0.2240\n",
      "Epoch [22/50], Train Loss: 0.0999, Val Loss: 0.2203\n",
      "Epoch [23/50], Train Loss: 0.0989, Val Loss: 0.2171\n",
      "Epoch [24/50], Train Loss: 0.0979, Val Loss: 0.2138\n",
      "Epoch [25/50], Train Loss: 0.0957, Val Loss: 0.2108\n",
      "Epoch [26/50], Train Loss: 0.0951, Val Loss: 0.2077\n",
      "Epoch [27/50], Train Loss: 0.0934, Val Loss: 0.2049\n",
      "Epoch [28/50], Train Loss: 0.0928, Val Loss: 0.2020\n",
      "Epoch [29/50], Train Loss: 0.0900, Val Loss: 0.1993\n",
      "Epoch [30/50], Train Loss: 0.0926, Val Loss: 0.1967\n",
      "Epoch [31/50], Train Loss: 0.0897, Val Loss: 0.1942\n",
      "Epoch [32/50], Train Loss: 0.0874, Val Loss: 0.1919\n",
      "Epoch [33/50], Train Loss: 0.0881, Val Loss: 0.1896\n",
      "Epoch [34/50], Train Loss: 0.0863, Val Loss: 0.1874\n",
      "Epoch [35/50], Train Loss: 0.0817, Val Loss: 0.1853\n",
      "Epoch [36/50], Train Loss: 0.0846, Val Loss: 0.1832\n",
      "Epoch [37/50], Train Loss: 0.0842, Val Loss: 0.1811\n",
      "Epoch [38/50], Train Loss: 0.0835, Val Loss: 0.1794\n",
      "Epoch [39/50], Train Loss: 0.0822, Val Loss: 0.1775\n",
      "Epoch [40/50], Train Loss: 0.0812, Val Loss: 0.1756\n",
      "Epoch [41/50], Train Loss: 0.0789, Val Loss: 0.1740\n",
      "Epoch [42/50], Train Loss: 0.0785, Val Loss: 0.1724\n",
      "Epoch [43/50], Train Loss: 0.0783, Val Loss: 0.1708\n",
      "Epoch [44/50], Train Loss: 0.0750, Val Loss: 0.1692\n",
      "Epoch [45/50], Train Loss: 0.0771, Val Loss: 0.1677\n",
      "Epoch [46/50], Train Loss: 0.0758, Val Loss: 0.1662\n",
      "Epoch [47/50], Train Loss: 0.0763, Val Loss: 0.1648\n",
      "Epoch [48/50], Train Loss: 0.0771, Val Loss: 0.1635\n",
      "Epoch [49/50], Train Loss: 0.0759, Val Loss: 0.1621\n",
      "Epoch [50/50], Train Loss: 0.0736, Val Loss: 0.1608\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=32, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.2047, Val Loss: 0.4871\n",
      "Epoch [2/50], Train Loss: 0.1965, Val Loss: 0.4725\n",
      "Epoch [3/50], Train Loss: 0.1889, Val Loss: 0.4586\n",
      "Epoch [4/50], Train Loss: 0.1817, Val Loss: 0.4454\n",
      "Epoch [5/50], Train Loss: 0.1749, Val Loss: 0.4329\n",
      "Epoch [6/50], Train Loss: 0.1685, Val Loss: 0.4209\n",
      "Epoch [7/50], Train Loss: 0.1624, Val Loss: 0.4095\n",
      "Epoch [8/50], Train Loss: 0.1567, Val Loss: 0.3985\n",
      "Epoch [9/50], Train Loss: 0.1513, Val Loss: 0.3881\n",
      "Epoch [10/50], Train Loss: 0.1461, Val Loss: 0.3780\n",
      "Epoch [11/50], Train Loss: 0.1412, Val Loss: 0.3683\n",
      "Epoch [12/50], Train Loss: 0.1366, Val Loss: 0.3590\n",
      "Epoch [13/50], Train Loss: 0.1321, Val Loss: 0.3501\n",
      "Epoch [14/50], Train Loss: 0.1279, Val Loss: 0.3415\n",
      "Epoch [15/50], Train Loss: 0.1238, Val Loss: 0.3333\n",
      "Epoch [16/50], Train Loss: 0.1200, Val Loss: 0.3253\n",
      "Epoch [17/50], Train Loss: 0.1163, Val Loss: 0.3176\n",
      "Epoch [18/50], Train Loss: 0.1128, Val Loss: 0.3102\n",
      "Epoch [19/50], Train Loss: 0.1094, Val Loss: 0.3030\n",
      "Epoch [20/50], Train Loss: 0.1062, Val Loss: 0.2960\n",
      "Epoch [21/50], Train Loss: 0.1031, Val Loss: 0.2893\n",
      "Epoch [22/50], Train Loss: 0.1001, Val Loss: 0.2828\n",
      "Epoch [23/50], Train Loss: 0.0973, Val Loss: 0.2766\n",
      "Epoch [24/50], Train Loss: 0.0946, Val Loss: 0.2705\n",
      "Epoch [25/50], Train Loss: 0.0920, Val Loss: 0.2647\n",
      "Epoch [26/50], Train Loss: 0.0895, Val Loss: 0.2590\n",
      "Epoch [27/50], Train Loss: 0.0871, Val Loss: 0.2535\n",
      "Epoch [28/50], Train Loss: 0.0848, Val Loss: 0.2482\n",
      "Epoch [29/50], Train Loss: 0.0826, Val Loss: 0.2430\n",
      "Epoch [30/50], Train Loss: 0.0805, Val Loss: 0.2380\n",
      "Epoch [31/50], Train Loss: 0.0785, Val Loss: 0.2333\n",
      "Epoch [32/50], Train Loss: 0.0766, Val Loss: 0.2286\n",
      "Epoch [33/50], Train Loss: 0.0747, Val Loss: 0.2241\n",
      "Epoch [34/50], Train Loss: 0.0730, Val Loss: 0.2197\n",
      "Epoch [35/50], Train Loss: 0.0713, Val Loss: 0.2155\n",
      "Epoch [36/50], Train Loss: 0.0697, Val Loss: 0.2114\n",
      "Epoch [37/50], Train Loss: 0.0681, Val Loss: 0.2074\n",
      "Epoch [38/50], Train Loss: 0.0666, Val Loss: 0.2036\n",
      "Epoch [39/50], Train Loss: 0.0652, Val Loss: 0.1999\n",
      "Epoch [40/50], Train Loss: 0.0638, Val Loss: 0.1963\n",
      "Epoch [41/50], Train Loss: 0.0625, Val Loss: 0.1929\n",
      "Epoch [42/50], Train Loss: 0.0613, Val Loss: 0.1895\n",
      "Epoch [43/50], Train Loss: 0.0601, Val Loss: 0.1863\n",
      "Epoch [44/50], Train Loss: 0.0589, Val Loss: 0.1831\n",
      "Epoch [45/50], Train Loss: 0.0578, Val Loss: 0.1801\n",
      "Epoch [46/50], Train Loss: 0.0568, Val Loss: 0.1772\n",
      "Epoch [47/50], Train Loss: 0.0558, Val Loss: 0.1744\n",
      "Epoch [48/50], Train Loss: 0.0548, Val Loss: 0.1716\n",
      "Epoch [49/50], Train Loss: 0.0539, Val Loss: 0.1690\n",
      "Epoch [50/50], Train Loss: 0.0530, Val Loss: 0.1664\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=32, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1562, Val Loss: 0.3809\n",
      "Epoch [2/50], Train Loss: 0.1487, Val Loss: 0.3695\n",
      "Epoch [3/50], Train Loss: 0.1423, Val Loss: 0.3588\n",
      "Epoch [4/50], Train Loss: 0.1364, Val Loss: 0.3487\n",
      "Epoch [5/50], Train Loss: 0.1329, Val Loss: 0.3392\n",
      "Epoch [6/50], Train Loss: 0.1247, Val Loss: 0.3303\n",
      "Epoch [7/50], Train Loss: 0.1223, Val Loss: 0.3218\n",
      "Epoch [8/50], Train Loss: 0.1161, Val Loss: 0.3137\n",
      "Epoch [9/50], Train Loss: 0.1113, Val Loss: 0.3062\n",
      "Epoch [10/50], Train Loss: 0.1083, Val Loss: 0.2990\n",
      "Epoch [11/50], Train Loss: 0.1050, Val Loss: 0.2922\n",
      "Epoch [12/50], Train Loss: 0.1007, Val Loss: 0.2856\n",
      "Epoch [13/50], Train Loss: 0.0992, Val Loss: 0.2795\n",
      "Epoch [14/50], Train Loss: 0.0952, Val Loss: 0.2734\n",
      "Epoch [15/50], Train Loss: 0.0917, Val Loss: 0.2678\n",
      "Epoch [16/50], Train Loss: 0.0893, Val Loss: 0.2623\n",
      "Epoch [17/50], Train Loss: 0.0867, Val Loss: 0.2571\n",
      "Epoch [18/50], Train Loss: 0.0865, Val Loss: 0.2521\n",
      "Epoch [19/50], Train Loss: 0.0818, Val Loss: 0.2474\n",
      "Epoch [20/50], Train Loss: 0.0821, Val Loss: 0.2428\n",
      "Epoch [21/50], Train Loss: 0.0793, Val Loss: 0.2383\n",
      "Epoch [22/50], Train Loss: 0.0778, Val Loss: 0.2342\n",
      "Epoch [23/50], Train Loss: 0.0770, Val Loss: 0.2300\n",
      "Epoch [24/50], Train Loss: 0.0733, Val Loss: 0.2260\n",
      "Epoch [25/50], Train Loss: 0.0713, Val Loss: 0.2223\n",
      "Epoch [26/50], Train Loss: 0.0713, Val Loss: 0.2187\n",
      "Epoch [27/50], Train Loss: 0.0703, Val Loss: 0.2151\n",
      "Epoch [28/50], Train Loss: 0.0678, Val Loss: 0.2117\n",
      "Epoch [29/50], Train Loss: 0.0678, Val Loss: 0.2084\n",
      "Epoch [30/50], Train Loss: 0.0654, Val Loss: 0.2053\n",
      "Epoch [31/50], Train Loss: 0.0655, Val Loss: 0.2022\n",
      "Epoch [32/50], Train Loss: 0.0652, Val Loss: 0.1992\n",
      "Epoch [33/50], Train Loss: 0.0622, Val Loss: 0.1964\n",
      "Epoch [34/50], Train Loss: 0.0627, Val Loss: 0.1936\n",
      "Epoch [35/50], Train Loss: 0.0618, Val Loss: 0.1910\n",
      "Epoch [36/50], Train Loss: 0.0605, Val Loss: 0.1884\n",
      "Epoch [37/50], Train Loss: 0.0600, Val Loss: 0.1859\n",
      "Epoch [38/50], Train Loss: 0.0579, Val Loss: 0.1835\n",
      "Epoch [39/50], Train Loss: 0.0583, Val Loss: 0.1812\n",
      "Epoch [40/50], Train Loss: 0.0556, Val Loss: 0.1791\n",
      "Epoch [41/50], Train Loss: 0.0568, Val Loss: 0.1769\n",
      "Epoch [42/50], Train Loss: 0.0561, Val Loss: 0.1749\n",
      "Epoch [43/50], Train Loss: 0.0558, Val Loss: 0.1728\n",
      "Epoch [44/50], Train Loss: 0.0547, Val Loss: 0.1708\n",
      "Epoch [45/50], Train Loss: 0.0544, Val Loss: 0.1689\n",
      "Epoch [46/50], Train Loss: 0.0534, Val Loss: 0.1671\n",
      "Epoch [47/50], Train Loss: 0.0541, Val Loss: 0.1653\n",
      "Epoch [48/50], Train Loss: 0.0524, Val Loss: 0.1635\n",
      "Epoch [49/50], Train Loss: 0.0527, Val Loss: 0.1619\n",
      "Epoch [50/50], Train Loss: 0.0526, Val Loss: 0.1603\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=32, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1518, Val Loss: 0.3329\n",
      "Epoch [2/50], Train Loss: 0.1461, Val Loss: 0.3190\n",
      "Epoch [3/50], Train Loss: 0.1407, Val Loss: 0.3061\n",
      "Epoch [4/50], Train Loss: 0.1315, Val Loss: 0.2946\n",
      "Epoch [5/50], Train Loss: 0.1255, Val Loss: 0.2845\n",
      "Epoch [6/50], Train Loss: 0.1220, Val Loss: 0.2748\n",
      "Epoch [7/50], Train Loss: 0.1233, Val Loss: 0.2657\n",
      "Epoch [8/50], Train Loss: 0.1143, Val Loss: 0.2577\n",
      "Epoch [9/50], Train Loss: 0.1123, Val Loss: 0.2499\n",
      "Epoch [10/50], Train Loss: 0.1102, Val Loss: 0.2424\n",
      "Epoch [11/50], Train Loss: 0.1051, Val Loss: 0.2358\n",
      "Epoch [12/50], Train Loss: 0.1032, Val Loss: 0.2294\n",
      "Epoch [13/50], Train Loss: 0.1007, Val Loss: 0.2238\n",
      "Epoch [14/50], Train Loss: 0.0999, Val Loss: 0.2183\n",
      "Epoch [15/50], Train Loss: 0.0959, Val Loss: 0.2133\n",
      "Epoch [16/50], Train Loss: 0.0935, Val Loss: 0.2084\n",
      "Epoch [17/50], Train Loss: 0.0892, Val Loss: 0.2038\n",
      "Epoch [18/50], Train Loss: 0.0921, Val Loss: 0.1997\n",
      "Epoch [19/50], Train Loss: 0.0872, Val Loss: 0.1958\n",
      "Epoch [20/50], Train Loss: 0.0859, Val Loss: 0.1921\n",
      "Epoch [21/50], Train Loss: 0.0883, Val Loss: 0.1886\n",
      "Epoch [22/50], Train Loss: 0.0858, Val Loss: 0.1851\n",
      "Epoch [23/50], Train Loss: 0.0815, Val Loss: 0.1819\n",
      "Epoch [24/50], Train Loss: 0.0833, Val Loss: 0.1789\n",
      "Epoch [25/50], Train Loss: 0.0789, Val Loss: 0.1762\n",
      "Epoch [26/50], Train Loss: 0.0808, Val Loss: 0.1734\n",
      "Epoch [27/50], Train Loss: 0.0792, Val Loss: 0.1709\n",
      "Epoch [28/50], Train Loss: 0.0805, Val Loss: 0.1688\n",
      "Epoch [29/50], Train Loss: 0.0774, Val Loss: 0.1667\n",
      "Epoch [30/50], Train Loss: 0.0785, Val Loss: 0.1646\n",
      "Epoch [31/50], Train Loss: 0.0763, Val Loss: 0.1626\n",
      "Epoch [32/50], Train Loss: 0.0801, Val Loss: 0.1609\n",
      "Epoch [33/50], Train Loss: 0.0777, Val Loss: 0.1589\n",
      "Epoch [34/50], Train Loss: 0.0750, Val Loss: 0.1571\n",
      "Epoch [35/50], Train Loss: 0.0759, Val Loss: 0.1555\n",
      "Epoch [36/50], Train Loss: 0.0761, Val Loss: 0.1538\n",
      "Epoch [37/50], Train Loss: 0.0772, Val Loss: 0.1525\n",
      "Epoch [38/50], Train Loss: 0.0777, Val Loss: 0.1513\n",
      "Epoch [39/50], Train Loss: 0.0738, Val Loss: 0.1500\n",
      "Epoch [40/50], Train Loss: 0.0725, Val Loss: 0.1486\n",
      "Epoch [41/50], Train Loss: 0.0702, Val Loss: 0.1474\n",
      "Epoch [42/50], Train Loss: 0.0749, Val Loss: 0.1461\n",
      "Epoch [43/50], Train Loss: 0.0711, Val Loss: 0.1451\n",
      "Epoch [44/50], Train Loss: 0.0719, Val Loss: 0.1441\n",
      "Epoch [45/50], Train Loss: 0.0722, Val Loss: 0.1433\n",
      "Epoch [46/50], Train Loss: 0.0718, Val Loss: 0.1425\n",
      "Epoch [47/50], Train Loss: 0.0694, Val Loss: 0.1414\n",
      "Epoch [48/50], Train Loss: 0.0716, Val Loss: 0.1404\n",
      "Epoch [49/50], Train Loss: 0.0672, Val Loss: 0.1398\n",
      "Epoch [50/50], Train Loss: 0.0700, Val Loss: 0.1390\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=64, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1318, Val Loss: 0.3222\n",
      "Epoch [2/50], Train Loss: 0.1278, Val Loss: 0.3148\n",
      "Epoch [3/50], Train Loss: 0.1239, Val Loss: 0.3076\n",
      "Epoch [4/50], Train Loss: 0.1202, Val Loss: 0.3007\n",
      "Epoch [5/50], Train Loss: 0.1167, Val Loss: 0.2940\n",
      "Epoch [6/50], Train Loss: 0.1133, Val Loss: 0.2875\n",
      "Epoch [7/50], Train Loss: 0.1101, Val Loss: 0.2813\n",
      "Epoch [8/50], Train Loss: 0.1070, Val Loss: 0.2752\n",
      "Epoch [9/50], Train Loss: 0.1040, Val Loss: 0.2694\n",
      "Epoch [10/50], Train Loss: 0.1011, Val Loss: 0.2637\n",
      "Epoch [11/50], Train Loss: 0.0984, Val Loss: 0.2582\n",
      "Epoch [12/50], Train Loss: 0.0957, Val Loss: 0.2528\n",
      "Epoch [13/50], Train Loss: 0.0931, Val Loss: 0.2476\n",
      "Epoch [14/50], Train Loss: 0.0907, Val Loss: 0.2426\n",
      "Epoch [15/50], Train Loss: 0.0883, Val Loss: 0.2377\n",
      "Epoch [16/50], Train Loss: 0.0860, Val Loss: 0.2330\n",
      "Epoch [17/50], Train Loss: 0.0839, Val Loss: 0.2283\n",
      "Epoch [18/50], Train Loss: 0.0817, Val Loss: 0.2238\n",
      "Epoch [19/50], Train Loss: 0.0797, Val Loss: 0.2195\n",
      "Epoch [20/50], Train Loss: 0.0777, Val Loss: 0.2153\n",
      "Epoch [21/50], Train Loss: 0.0759, Val Loss: 0.2111\n",
      "Epoch [22/50], Train Loss: 0.0740, Val Loss: 0.2071\n",
      "Epoch [23/50], Train Loss: 0.0723, Val Loss: 0.2032\n",
      "Epoch [24/50], Train Loss: 0.0706, Val Loss: 0.1994\n",
      "Epoch [25/50], Train Loss: 0.0690, Val Loss: 0.1957\n",
      "Epoch [26/50], Train Loss: 0.0674, Val Loss: 0.1922\n",
      "Epoch [27/50], Train Loss: 0.0659, Val Loss: 0.1887\n",
      "Epoch [28/50], Train Loss: 0.0645, Val Loss: 0.1853\n",
      "Epoch [29/50], Train Loss: 0.0631, Val Loss: 0.1820\n",
      "Epoch [30/50], Train Loss: 0.0617, Val Loss: 0.1788\n",
      "Epoch [31/50], Train Loss: 0.0604, Val Loss: 0.1756\n",
      "Epoch [32/50], Train Loss: 0.0592, Val Loss: 0.1726\n",
      "Epoch [33/50], Train Loss: 0.0580, Val Loss: 0.1696\n",
      "Epoch [34/50], Train Loss: 0.0568, Val Loss: 0.1667\n",
      "Epoch [35/50], Train Loss: 0.0557, Val Loss: 0.1640\n",
      "Epoch [36/50], Train Loss: 0.0546, Val Loss: 0.1612\n",
      "Epoch [37/50], Train Loss: 0.0536, Val Loss: 0.1586\n",
      "Epoch [38/50], Train Loss: 0.0526, Val Loss: 0.1560\n",
      "Epoch [39/50], Train Loss: 0.0517, Val Loss: 0.1535\n",
      "Epoch [40/50], Train Loss: 0.0507, Val Loss: 0.1511\n",
      "Epoch [41/50], Train Loss: 0.0499, Val Loss: 0.1487\n",
      "Epoch [42/50], Train Loss: 0.0490, Val Loss: 0.1464\n",
      "Epoch [43/50], Train Loss: 0.0482, Val Loss: 0.1442\n",
      "Epoch [44/50], Train Loss: 0.0474, Val Loss: 0.1420\n",
      "Epoch [45/50], Train Loss: 0.0467, Val Loss: 0.1399\n",
      "Epoch [46/50], Train Loss: 0.0460, Val Loss: 0.1378\n",
      "Epoch [47/50], Train Loss: 0.0453, Val Loss: 0.1359\n",
      "Epoch [48/50], Train Loss: 0.0446, Val Loss: 0.1339\n",
      "Epoch [49/50], Train Loss: 0.0440, Val Loss: 0.1320\n",
      "Epoch [50/50], Train Loss: 0.0434, Val Loss: 0.1302\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=64, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1590, Val Loss: 0.4223\n",
      "Epoch [2/50], Train Loss: 0.1537, Val Loss: 0.4119\n",
      "Epoch [3/50], Train Loss: 0.1485, Val Loss: 0.4021\n",
      "Epoch [4/50], Train Loss: 0.1437, Val Loss: 0.3925\n",
      "Epoch [5/50], Train Loss: 0.1403, Val Loss: 0.3833\n",
      "Epoch [6/50], Train Loss: 0.1352, Val Loss: 0.3744\n",
      "Epoch [7/50], Train Loss: 0.1312, Val Loss: 0.3659\n",
      "Epoch [8/50], Train Loss: 0.1268, Val Loss: 0.3576\n",
      "Epoch [9/50], Train Loss: 0.1228, Val Loss: 0.3497\n",
      "Epoch [10/50], Train Loss: 0.1204, Val Loss: 0.3419\n",
      "Epoch [11/50], Train Loss: 0.1161, Val Loss: 0.3345\n",
      "Epoch [12/50], Train Loss: 0.1135, Val Loss: 0.3272\n",
      "Epoch [13/50], Train Loss: 0.1100, Val Loss: 0.3201\n",
      "Epoch [14/50], Train Loss: 0.1069, Val Loss: 0.3133\n",
      "Epoch [15/50], Train Loss: 0.1043, Val Loss: 0.3067\n",
      "Epoch [16/50], Train Loss: 0.1011, Val Loss: 0.3002\n",
      "Epoch [17/50], Train Loss: 0.0984, Val Loss: 0.2939\n",
      "Epoch [18/50], Train Loss: 0.0960, Val Loss: 0.2878\n",
      "Epoch [19/50], Train Loss: 0.0934, Val Loss: 0.2818\n",
      "Epoch [20/50], Train Loss: 0.0905, Val Loss: 0.2761\n",
      "Epoch [21/50], Train Loss: 0.0888, Val Loss: 0.2705\n",
      "Epoch [22/50], Train Loss: 0.0860, Val Loss: 0.2650\n",
      "Epoch [23/50], Train Loss: 0.0837, Val Loss: 0.2597\n",
      "Epoch [24/50], Train Loss: 0.0820, Val Loss: 0.2544\n",
      "Epoch [25/50], Train Loss: 0.0793, Val Loss: 0.2494\n",
      "Epoch [26/50], Train Loss: 0.0774, Val Loss: 0.2444\n",
      "Epoch [27/50], Train Loss: 0.0764, Val Loss: 0.2395\n",
      "Epoch [28/50], Train Loss: 0.0738, Val Loss: 0.2349\n",
      "Epoch [29/50], Train Loss: 0.0726, Val Loss: 0.2303\n",
      "Epoch [30/50], Train Loss: 0.0716, Val Loss: 0.2258\n",
      "Epoch [31/50], Train Loss: 0.0691, Val Loss: 0.2215\n",
      "Epoch [32/50], Train Loss: 0.0681, Val Loss: 0.2172\n",
      "Epoch [33/50], Train Loss: 0.0658, Val Loss: 0.2131\n",
      "Epoch [34/50], Train Loss: 0.0640, Val Loss: 0.2090\n",
      "Epoch [35/50], Train Loss: 0.0628, Val Loss: 0.2051\n",
      "Epoch [36/50], Train Loss: 0.0621, Val Loss: 0.2012\n",
      "Epoch [37/50], Train Loss: 0.0610, Val Loss: 0.1974\n",
      "Epoch [38/50], Train Loss: 0.0595, Val Loss: 0.1937\n",
      "Epoch [39/50], Train Loss: 0.0581, Val Loss: 0.1902\n",
      "Epoch [40/50], Train Loss: 0.0570, Val Loss: 0.1867\n",
      "Epoch [41/50], Train Loss: 0.0562, Val Loss: 0.1833\n",
      "Epoch [42/50], Train Loss: 0.0545, Val Loss: 0.1800\n",
      "Epoch [43/50], Train Loss: 0.0543, Val Loss: 0.1767\n",
      "Epoch [44/50], Train Loss: 0.0526, Val Loss: 0.1736\n",
      "Epoch [45/50], Train Loss: 0.0520, Val Loss: 0.1705\n",
      "Epoch [46/50], Train Loss: 0.0512, Val Loss: 0.1675\n",
      "Epoch [47/50], Train Loss: 0.0496, Val Loss: 0.1646\n",
      "Epoch [48/50], Train Loss: 0.0492, Val Loss: 0.1617\n",
      "Epoch [49/50], Train Loss: 0.0482, Val Loss: 0.1590\n",
      "Epoch [50/50], Train Loss: 0.0467, Val Loss: 0.1563\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=64, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1530, Val Loss: 0.4051\n",
      "Epoch [2/50], Train Loss: 0.1486, Val Loss: 0.3956\n",
      "Epoch [3/50], Train Loss: 0.1437, Val Loss: 0.3867\n",
      "Epoch [4/50], Train Loss: 0.1402, Val Loss: 0.3779\n",
      "Epoch [5/50], Train Loss: 0.1358, Val Loss: 0.3694\n",
      "Epoch [6/50], Train Loss: 0.1328, Val Loss: 0.3611\n",
      "Epoch [7/50], Train Loss: 0.1288, Val Loss: 0.3532\n",
      "Epoch [8/50], Train Loss: 0.1245, Val Loss: 0.3455\n",
      "Epoch [9/50], Train Loss: 0.1206, Val Loss: 0.3380\n",
      "Epoch [10/50], Train Loss: 0.1173, Val Loss: 0.3308\n",
      "Epoch [11/50], Train Loss: 0.1116, Val Loss: 0.3239\n",
      "Epoch [12/50], Train Loss: 0.1103, Val Loss: 0.3171\n",
      "Epoch [13/50], Train Loss: 0.1077, Val Loss: 0.3105\n",
      "Epoch [14/50], Train Loss: 0.1061, Val Loss: 0.3039\n",
      "Epoch [15/50], Train Loss: 0.1003, Val Loss: 0.2977\n",
      "Epoch [16/50], Train Loss: 0.1009, Val Loss: 0.2915\n",
      "Epoch [17/50], Train Loss: 0.0975, Val Loss: 0.2856\n",
      "Epoch [18/50], Train Loss: 0.0942, Val Loss: 0.2798\n",
      "Epoch [19/50], Train Loss: 0.0929, Val Loss: 0.2741\n",
      "Epoch [20/50], Train Loss: 0.0911, Val Loss: 0.2686\n",
      "Epoch [21/50], Train Loss: 0.0889, Val Loss: 0.2633\n",
      "Epoch [22/50], Train Loss: 0.0870, Val Loss: 0.2580\n",
      "Epoch [23/50], Train Loss: 0.0846, Val Loss: 0.2530\n",
      "Epoch [24/50], Train Loss: 0.0829, Val Loss: 0.2480\n",
      "Epoch [25/50], Train Loss: 0.0794, Val Loss: 0.2432\n",
      "Epoch [26/50], Train Loss: 0.0787, Val Loss: 0.2385\n",
      "Epoch [27/50], Train Loss: 0.0780, Val Loss: 0.2340\n",
      "Epoch [28/50], Train Loss: 0.0751, Val Loss: 0.2295\n",
      "Epoch [29/50], Train Loss: 0.0741, Val Loss: 0.2252\n",
      "Epoch [30/50], Train Loss: 0.0710, Val Loss: 0.2210\n",
      "Epoch [31/50], Train Loss: 0.0701, Val Loss: 0.2169\n",
      "Epoch [32/50], Train Loss: 0.0693, Val Loss: 0.2129\n",
      "Epoch [33/50], Train Loss: 0.0669, Val Loss: 0.2090\n",
      "Epoch [34/50], Train Loss: 0.0663, Val Loss: 0.2052\n",
      "Epoch [35/50], Train Loss: 0.0651, Val Loss: 0.2015\n",
      "Epoch [36/50], Train Loss: 0.0643, Val Loss: 0.1979\n",
      "Epoch [37/50], Train Loss: 0.0620, Val Loss: 0.1945\n",
      "Epoch [38/50], Train Loss: 0.0616, Val Loss: 0.1911\n",
      "Epoch [39/50], Train Loss: 0.0600, Val Loss: 0.1878\n",
      "Epoch [40/50], Train Loss: 0.0599, Val Loss: 0.1846\n",
      "Epoch [41/50], Train Loss: 0.0584, Val Loss: 0.1814\n",
      "Epoch [42/50], Train Loss: 0.0579, Val Loss: 0.1783\n",
      "Epoch [43/50], Train Loss: 0.0557, Val Loss: 0.1754\n",
      "Epoch [44/50], Train Loss: 0.0567, Val Loss: 0.1725\n",
      "Epoch [45/50], Train Loss: 0.0554, Val Loss: 0.1697\n",
      "Epoch [46/50], Train Loss: 0.0537, Val Loss: 0.1670\n",
      "Epoch [47/50], Train Loss: 0.0545, Val Loss: 0.1642\n",
      "Epoch [48/50], Train Loss: 0.0530, Val Loss: 0.1616\n",
      "Epoch [49/50], Train Loss: 0.0517, Val Loss: 0.1591\n",
      "Epoch [50/50], Train Loss: 0.0510, Val Loss: 0.1567\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=64, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1431, Val Loss: 0.3892\n",
      "Epoch [2/50], Train Loss: 0.1369, Val Loss: 0.3776\n",
      "Epoch [3/50], Train Loss: 0.1310, Val Loss: 0.3663\n",
      "Epoch [4/50], Train Loss: 0.1254, Val Loss: 0.3554\n",
      "Epoch [5/50], Train Loss: 0.1200, Val Loss: 0.3449\n",
      "Epoch [6/50], Train Loss: 0.1149, Val Loss: 0.3346\n",
      "Epoch [7/50], Train Loss: 0.1100, Val Loss: 0.3247\n",
      "Epoch [8/50], Train Loss: 0.1053, Val Loss: 0.3151\n",
      "Epoch [9/50], Train Loss: 0.1008, Val Loss: 0.3057\n",
      "Epoch [10/50], Train Loss: 0.0966, Val Loss: 0.2967\n",
      "Epoch [11/50], Train Loss: 0.0925, Val Loss: 0.2879\n",
      "Epoch [12/50], Train Loss: 0.0886, Val Loss: 0.2793\n",
      "Epoch [13/50], Train Loss: 0.0849, Val Loss: 0.2710\n",
      "Epoch [14/50], Train Loss: 0.0814, Val Loss: 0.2630\n",
      "Epoch [15/50], Train Loss: 0.0781, Val Loss: 0.2552\n",
      "Epoch [16/50], Train Loss: 0.0749, Val Loss: 0.2477\n",
      "Epoch [17/50], Train Loss: 0.0719, Val Loss: 0.2404\n",
      "Epoch [18/50], Train Loss: 0.0691, Val Loss: 0.2334\n",
      "Epoch [19/50], Train Loss: 0.0664, Val Loss: 0.2266\n",
      "Epoch [20/50], Train Loss: 0.0639, Val Loss: 0.2200\n",
      "Epoch [21/50], Train Loss: 0.0615, Val Loss: 0.2137\n",
      "Epoch [22/50], Train Loss: 0.0593, Val Loss: 0.2076\n",
      "Epoch [23/50], Train Loss: 0.0572, Val Loss: 0.2017\n",
      "Epoch [24/50], Train Loss: 0.0553, Val Loss: 0.1961\n",
      "Epoch [25/50], Train Loss: 0.0534, Val Loss: 0.1907\n",
      "Epoch [26/50], Train Loss: 0.0517, Val Loss: 0.1855\n",
      "Epoch [27/50], Train Loss: 0.0501, Val Loss: 0.1806\n",
      "Epoch [28/50], Train Loss: 0.0487, Val Loss: 0.1758\n",
      "Epoch [29/50], Train Loss: 0.0473, Val Loss: 0.1713\n",
      "Epoch [30/50], Train Loss: 0.0460, Val Loss: 0.1670\n",
      "Epoch [31/50], Train Loss: 0.0449, Val Loss: 0.1628\n",
      "Epoch [32/50], Train Loss: 0.0438, Val Loss: 0.1589\n",
      "Epoch [33/50], Train Loss: 0.0428, Val Loss: 0.1552\n",
      "Epoch [34/50], Train Loss: 0.0419, Val Loss: 0.1516\n",
      "Epoch [35/50], Train Loss: 0.0411, Val Loss: 0.1482\n",
      "Epoch [36/50], Train Loss: 0.0403, Val Loss: 0.1450\n",
      "Epoch [37/50], Train Loss: 0.0396, Val Loss: 0.1419\n",
      "Epoch [38/50], Train Loss: 0.0390, Val Loss: 0.1391\n",
      "Epoch [39/50], Train Loss: 0.0384, Val Loss: 0.1363\n",
      "Epoch [40/50], Train Loss: 0.0379, Val Loss: 0.1337\n",
      "Epoch [41/50], Train Loss: 0.0374, Val Loss: 0.1313\n",
      "Epoch [42/50], Train Loss: 0.0370, Val Loss: 0.1290\n",
      "Epoch [43/50], Train Loss: 0.0366, Val Loss: 0.1268\n",
      "Epoch [44/50], Train Loss: 0.0362, Val Loss: 0.1247\n",
      "Epoch [45/50], Train Loss: 0.0359, Val Loss: 0.1227\n",
      "Epoch [46/50], Train Loss: 0.0356, Val Loss: 0.1209\n",
      "Epoch [47/50], Train Loss: 0.0353, Val Loss: 0.1192\n",
      "Epoch [48/50], Train Loss: 0.0351, Val Loss: 0.1175\n",
      "Epoch [49/50], Train Loss: 0.0349, Val Loss: 0.1160\n",
      "Epoch [50/50], Train Loss: 0.0347, Val Loss: 0.1145\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=64, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.2048, Val Loss: 0.4160\n",
      "Epoch [2/50], Train Loss: 0.1964, Val Loss: 0.4010\n",
      "Epoch [3/50], Train Loss: 0.1849, Val Loss: 0.3871\n",
      "Epoch [4/50], Train Loss: 0.1772, Val Loss: 0.3738\n",
      "Epoch [5/50], Train Loss: 0.1698, Val Loss: 0.3612\n",
      "Epoch [6/50], Train Loss: 0.1616, Val Loss: 0.3492\n",
      "Epoch [7/50], Train Loss: 0.1538, Val Loss: 0.3379\n",
      "Epoch [8/50], Train Loss: 0.1467, Val Loss: 0.3270\n",
      "Epoch [9/50], Train Loss: 0.1409, Val Loss: 0.3167\n",
      "Epoch [10/50], Train Loss: 0.1338, Val Loss: 0.3067\n",
      "Epoch [11/50], Train Loss: 0.1290, Val Loss: 0.2972\n",
      "Epoch [12/50], Train Loss: 0.1237, Val Loss: 0.2881\n",
      "Epoch [13/50], Train Loss: 0.1188, Val Loss: 0.2794\n",
      "Epoch [14/50], Train Loss: 0.1135, Val Loss: 0.2709\n",
      "Epoch [15/50], Train Loss: 0.1089, Val Loss: 0.2627\n",
      "Epoch [16/50], Train Loss: 0.1046, Val Loss: 0.2549\n",
      "Epoch [17/50], Train Loss: 0.0994, Val Loss: 0.2474\n",
      "Epoch [18/50], Train Loss: 0.0970, Val Loss: 0.2401\n",
      "Epoch [19/50], Train Loss: 0.0920, Val Loss: 0.2331\n",
      "Epoch [20/50], Train Loss: 0.0898, Val Loss: 0.2263\n",
      "Epoch [21/50], Train Loss: 0.0857, Val Loss: 0.2198\n",
      "Epoch [22/50], Train Loss: 0.0831, Val Loss: 0.2136\n",
      "Epoch [23/50], Train Loss: 0.0786, Val Loss: 0.2077\n",
      "Epoch [24/50], Train Loss: 0.0772, Val Loss: 0.2019\n",
      "Epoch [25/50], Train Loss: 0.0740, Val Loss: 0.1963\n",
      "Epoch [26/50], Train Loss: 0.0712, Val Loss: 0.1910\n",
      "Epoch [27/50], Train Loss: 0.0693, Val Loss: 0.1859\n",
      "Epoch [28/50], Train Loss: 0.0663, Val Loss: 0.1810\n",
      "Epoch [29/50], Train Loss: 0.0647, Val Loss: 0.1762\n",
      "Epoch [30/50], Train Loss: 0.0623, Val Loss: 0.1716\n",
      "Epoch [31/50], Train Loss: 0.0598, Val Loss: 0.1672\n",
      "Epoch [32/50], Train Loss: 0.0583, Val Loss: 0.1631\n",
      "Epoch [33/50], Train Loss: 0.0572, Val Loss: 0.1591\n",
      "Epoch [34/50], Train Loss: 0.0551, Val Loss: 0.1553\n",
      "Epoch [35/50], Train Loss: 0.0532, Val Loss: 0.1516\n",
      "Epoch [36/50], Train Loss: 0.0531, Val Loss: 0.1482\n",
      "Epoch [37/50], Train Loss: 0.0519, Val Loss: 0.1448\n",
      "Epoch [38/50], Train Loss: 0.0496, Val Loss: 0.1417\n",
      "Epoch [39/50], Train Loss: 0.0486, Val Loss: 0.1386\n",
      "Epoch [40/50], Train Loss: 0.0476, Val Loss: 0.1357\n",
      "Epoch [41/50], Train Loss: 0.0464, Val Loss: 0.1330\n",
      "Epoch [42/50], Train Loss: 0.0465, Val Loss: 0.1303\n",
      "Epoch [43/50], Train Loss: 0.0460, Val Loss: 0.1279\n",
      "Epoch [44/50], Train Loss: 0.0449, Val Loss: 0.1255\n",
      "Epoch [45/50], Train Loss: 0.0432, Val Loss: 0.1233\n",
      "Epoch [46/50], Train Loss: 0.0424, Val Loss: 0.1211\n",
      "Epoch [47/50], Train Loss: 0.0424, Val Loss: 0.1191\n",
      "Epoch [48/50], Train Loss: 0.0418, Val Loss: 0.1172\n",
      "Epoch [49/50], Train Loss: 0.0421, Val Loss: 0.1153\n",
      "Epoch [50/50], Train Loss: 0.0413, Val Loss: 0.1135\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=64, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1700, Val Loss: 0.3851\n",
      "Epoch [2/50], Train Loss: 0.1643, Val Loss: 0.3703\n",
      "Epoch [3/50], Train Loss: 0.1556, Val Loss: 0.3563\n",
      "Epoch [4/50], Train Loss: 0.1475, Val Loss: 0.3434\n",
      "Epoch [5/50], Train Loss: 0.1396, Val Loss: 0.3313\n",
      "Epoch [6/50], Train Loss: 0.1371, Val Loss: 0.3197\n",
      "Epoch [7/50], Train Loss: 0.1317, Val Loss: 0.3091\n",
      "Epoch [8/50], Train Loss: 0.1245, Val Loss: 0.2988\n",
      "Epoch [9/50], Train Loss: 0.1185, Val Loss: 0.2894\n",
      "Epoch [10/50], Train Loss: 0.1135, Val Loss: 0.2805\n",
      "Epoch [11/50], Train Loss: 0.1096, Val Loss: 0.2719\n",
      "Epoch [12/50], Train Loss: 0.1068, Val Loss: 0.2637\n",
      "Epoch [13/50], Train Loss: 0.1050, Val Loss: 0.2557\n",
      "Epoch [14/50], Train Loss: 0.1000, Val Loss: 0.2481\n",
      "Epoch [15/50], Train Loss: 0.0957, Val Loss: 0.2412\n",
      "Epoch [16/50], Train Loss: 0.0917, Val Loss: 0.2344\n",
      "Epoch [17/50], Train Loss: 0.0884, Val Loss: 0.2281\n",
      "Epoch [18/50], Train Loss: 0.0869, Val Loss: 0.2221\n",
      "Epoch [19/50], Train Loss: 0.0854, Val Loss: 0.2162\n",
      "Epoch [20/50], Train Loss: 0.0827, Val Loss: 0.2107\n",
      "Epoch [21/50], Train Loss: 0.0808, Val Loss: 0.2054\n",
      "Epoch [22/50], Train Loss: 0.0775, Val Loss: 0.2003\n",
      "Epoch [23/50], Train Loss: 0.0758, Val Loss: 0.1954\n",
      "Epoch [24/50], Train Loss: 0.0743, Val Loss: 0.1910\n",
      "Epoch [25/50], Train Loss: 0.0727, Val Loss: 0.1867\n",
      "Epoch [26/50], Train Loss: 0.0732, Val Loss: 0.1826\n",
      "Epoch [27/50], Train Loss: 0.0699, Val Loss: 0.1787\n",
      "Epoch [28/50], Train Loss: 0.0681, Val Loss: 0.1750\n",
      "Epoch [29/50], Train Loss: 0.0674, Val Loss: 0.1714\n",
      "Epoch [30/50], Train Loss: 0.0676, Val Loss: 0.1680\n",
      "Epoch [31/50], Train Loss: 0.0658, Val Loss: 0.1648\n",
      "Epoch [32/50], Train Loss: 0.0635, Val Loss: 0.1618\n",
      "Epoch [33/50], Train Loss: 0.0647, Val Loss: 0.1588\n",
      "Epoch [34/50], Train Loss: 0.0632, Val Loss: 0.1559\n",
      "Epoch [35/50], Train Loss: 0.0600, Val Loss: 0.1533\n",
      "Epoch [36/50], Train Loss: 0.0604, Val Loss: 0.1507\n",
      "Epoch [37/50], Train Loss: 0.0595, Val Loss: 0.1482\n",
      "Epoch [38/50], Train Loss: 0.0601, Val Loss: 0.1458\n",
      "Epoch [39/50], Train Loss: 0.0587, Val Loss: 0.1437\n",
      "Epoch [40/50], Train Loss: 0.0575, Val Loss: 0.1416\n",
      "Epoch [41/50], Train Loss: 0.0563, Val Loss: 0.1396\n",
      "Epoch [42/50], Train Loss: 0.0562, Val Loss: 0.1376\n",
      "Epoch [43/50], Train Loss: 0.0584, Val Loss: 0.1358\n",
      "Epoch [44/50], Train Loss: 0.0553, Val Loss: 0.1342\n",
      "Epoch [45/50], Train Loss: 0.0556, Val Loss: 0.1326\n",
      "Epoch [46/50], Train Loss: 0.0568, Val Loss: 0.1310\n",
      "Epoch [47/50], Train Loss: 0.0558, Val Loss: 0.1295\n",
      "Epoch [48/50], Train Loss: 0.0557, Val Loss: 0.1281\n",
      "Epoch [49/50], Train Loss: 0.0541, Val Loss: 0.1266\n",
      "Epoch [50/50], Train Loss: 0.0542, Val Loss: 0.1253\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=64, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1946, Val Loss: 0.4521\n",
      "Epoch [2/50], Train Loss: 0.1842, Val Loss: 0.4370\n",
      "Epoch [3/50], Train Loss: 0.1747, Val Loss: 0.4230\n",
      "Epoch [4/50], Train Loss: 0.1660, Val Loss: 0.4100\n",
      "Epoch [5/50], Train Loss: 0.1580, Val Loss: 0.3978\n",
      "Epoch [6/50], Train Loss: 0.1507, Val Loss: 0.3864\n",
      "Epoch [7/50], Train Loss: 0.1438, Val Loss: 0.3756\n",
      "Epoch [8/50], Train Loss: 0.1375, Val Loss: 0.3654\n",
      "Epoch [9/50], Train Loss: 0.1316, Val Loss: 0.3558\n",
      "Epoch [10/50], Train Loss: 0.1261, Val Loss: 0.3465\n",
      "Epoch [11/50], Train Loss: 0.1209, Val Loss: 0.3377\n",
      "Epoch [12/50], Train Loss: 0.1160, Val Loss: 0.3293\n",
      "Epoch [13/50], Train Loss: 0.1114, Val Loss: 0.3212\n",
      "Epoch [14/50], Train Loss: 0.1071, Val Loss: 0.3134\n",
      "Epoch [15/50], Train Loss: 0.1030, Val Loss: 0.3059\n",
      "Epoch [16/50], Train Loss: 0.0992, Val Loss: 0.2987\n",
      "Epoch [17/50], Train Loss: 0.0955, Val Loss: 0.2918\n",
      "Epoch [18/50], Train Loss: 0.0920, Val Loss: 0.2850\n",
      "Epoch [19/50], Train Loss: 0.0887, Val Loss: 0.2785\n",
      "Epoch [20/50], Train Loss: 0.0856, Val Loss: 0.2722\n",
      "Epoch [21/50], Train Loss: 0.0827, Val Loss: 0.2661\n",
      "Epoch [22/50], Train Loss: 0.0799, Val Loss: 0.2601\n",
      "Epoch [23/50], Train Loss: 0.0772, Val Loss: 0.2544\n",
      "Epoch [24/50], Train Loss: 0.0747, Val Loss: 0.2488\n",
      "Epoch [25/50], Train Loss: 0.0723, Val Loss: 0.2434\n",
      "Epoch [26/50], Train Loss: 0.0701, Val Loss: 0.2382\n",
      "Epoch [27/50], Train Loss: 0.0680, Val Loss: 0.2331\n",
      "Epoch [28/50], Train Loss: 0.0659, Val Loss: 0.2282\n",
      "Epoch [29/50], Train Loss: 0.0640, Val Loss: 0.2234\n",
      "Epoch [30/50], Train Loss: 0.0622, Val Loss: 0.2188\n",
      "Epoch [31/50], Train Loss: 0.0605, Val Loss: 0.2143\n",
      "Epoch [32/50], Train Loss: 0.0589, Val Loss: 0.2100\n",
      "Epoch [33/50], Train Loss: 0.0574, Val Loss: 0.2058\n",
      "Epoch [34/50], Train Loss: 0.0559, Val Loss: 0.2017\n",
      "Epoch [35/50], Train Loss: 0.0546, Val Loss: 0.1978\n",
      "Epoch [36/50], Train Loss: 0.0533, Val Loss: 0.1940\n",
      "Epoch [37/50], Train Loss: 0.0521, Val Loss: 0.1903\n",
      "Epoch [38/50], Train Loss: 0.0510, Val Loss: 0.1868\n",
      "Epoch [39/50], Train Loss: 0.0500, Val Loss: 0.1833\n",
      "Epoch [40/50], Train Loss: 0.0490, Val Loss: 0.1800\n",
      "Epoch [41/50], Train Loss: 0.0481, Val Loss: 0.1768\n",
      "Epoch [42/50], Train Loss: 0.0472, Val Loss: 0.1737\n",
      "Epoch [43/50], Train Loss: 0.0464, Val Loss: 0.1707\n",
      "Epoch [44/50], Train Loss: 0.0456, Val Loss: 0.1678\n",
      "Epoch [45/50], Train Loss: 0.0449, Val Loss: 0.1651\n",
      "Epoch [46/50], Train Loss: 0.0443, Val Loss: 0.1624\n",
      "Epoch [47/50], Train Loss: 0.0437, Val Loss: 0.1598\n",
      "Epoch [48/50], Train Loss: 0.0431, Val Loss: 0.1574\n",
      "Epoch [49/50], Train Loss: 0.0425, Val Loss: 0.1550\n",
      "Epoch [50/50], Train Loss: 0.0420, Val Loss: 0.1527\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=64, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1090, Val Loss: 0.3191\n",
      "Epoch [2/50], Train Loss: 0.1053, Val Loss: 0.3107\n",
      "Epoch [3/50], Train Loss: 0.1010, Val Loss: 0.3028\n",
      "Epoch [4/50], Train Loss: 0.0978, Val Loss: 0.2951\n",
      "Epoch [5/50], Train Loss: 0.0934, Val Loss: 0.2878\n",
      "Epoch [6/50], Train Loss: 0.0908, Val Loss: 0.2808\n",
      "Epoch [7/50], Train Loss: 0.0875, Val Loss: 0.2740\n",
      "Epoch [8/50], Train Loss: 0.0846, Val Loss: 0.2675\n",
      "Epoch [9/50], Train Loss: 0.0815, Val Loss: 0.2613\n",
      "Epoch [10/50], Train Loss: 0.0790, Val Loss: 0.2554\n",
      "Epoch [11/50], Train Loss: 0.0776, Val Loss: 0.2497\n",
      "Epoch [12/50], Train Loss: 0.0746, Val Loss: 0.2442\n",
      "Epoch [13/50], Train Loss: 0.0716, Val Loss: 0.2389\n",
      "Epoch [14/50], Train Loss: 0.0705, Val Loss: 0.2338\n",
      "Epoch [15/50], Train Loss: 0.0682, Val Loss: 0.2290\n",
      "Epoch [16/50], Train Loss: 0.0661, Val Loss: 0.2243\n",
      "Epoch [17/50], Train Loss: 0.0654, Val Loss: 0.2198\n",
      "Epoch [18/50], Train Loss: 0.0631, Val Loss: 0.2155\n",
      "Epoch [19/50], Train Loss: 0.0625, Val Loss: 0.2114\n",
      "Epoch [20/50], Train Loss: 0.0603, Val Loss: 0.2074\n",
      "Epoch [21/50], Train Loss: 0.0588, Val Loss: 0.2035\n",
      "Epoch [22/50], Train Loss: 0.0577, Val Loss: 0.1999\n",
      "Epoch [23/50], Train Loss: 0.0565, Val Loss: 0.1963\n",
      "Epoch [24/50], Train Loss: 0.0553, Val Loss: 0.1929\n",
      "Epoch [25/50], Train Loss: 0.0555, Val Loss: 0.1895\n",
      "Epoch [26/50], Train Loss: 0.0535, Val Loss: 0.1863\n",
      "Epoch [27/50], Train Loss: 0.0525, Val Loss: 0.1833\n",
      "Epoch [28/50], Train Loss: 0.0521, Val Loss: 0.1804\n",
      "Epoch [29/50], Train Loss: 0.0514, Val Loss: 0.1775\n",
      "Epoch [30/50], Train Loss: 0.0503, Val Loss: 0.1749\n",
      "Epoch [31/50], Train Loss: 0.0500, Val Loss: 0.1723\n",
      "Epoch [32/50], Train Loss: 0.0491, Val Loss: 0.1699\n",
      "Epoch [33/50], Train Loss: 0.0495, Val Loss: 0.1675\n",
      "Epoch [34/50], Train Loss: 0.0483, Val Loss: 0.1651\n",
      "Epoch [35/50], Train Loss: 0.0477, Val Loss: 0.1628\n",
      "Epoch [36/50], Train Loss: 0.0470, Val Loss: 0.1607\n",
      "Epoch [37/50], Train Loss: 0.0471, Val Loss: 0.1587\n",
      "Epoch [38/50], Train Loss: 0.0462, Val Loss: 0.1567\n",
      "Epoch [39/50], Train Loss: 0.0463, Val Loss: 0.1548\n",
      "Epoch [40/50], Train Loss: 0.0448, Val Loss: 0.1530\n",
      "Epoch [41/50], Train Loss: 0.0446, Val Loss: 0.1514\n",
      "Epoch [42/50], Train Loss: 0.0450, Val Loss: 0.1496\n",
      "Epoch [43/50], Train Loss: 0.0445, Val Loss: 0.1481\n",
      "Epoch [44/50], Train Loss: 0.0434, Val Loss: 0.1466\n",
      "Epoch [45/50], Train Loss: 0.0436, Val Loss: 0.1451\n",
      "Epoch [46/50], Train Loss: 0.0442, Val Loss: 0.1437\n",
      "Epoch [47/50], Train Loss: 0.0440, Val Loss: 0.1424\n",
      "Epoch [48/50], Train Loss: 0.0427, Val Loss: 0.1411\n",
      "Epoch [49/50], Train Loss: 0.0431, Val Loss: 0.1398\n",
      "Epoch [50/50], Train Loss: 0.0428, Val Loss: 0.1386\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=64, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1599, Val Loss: 0.3215\n",
      "Epoch [2/50], Train Loss: 0.1501, Val Loss: 0.3053\n",
      "Epoch [3/50], Train Loss: 0.1399, Val Loss: 0.2905\n",
      "Epoch [4/50], Train Loss: 0.1330, Val Loss: 0.2771\n",
      "Epoch [5/50], Train Loss: 0.1259, Val Loss: 0.2649\n",
      "Epoch [6/50], Train Loss: 0.1229, Val Loss: 0.2534\n",
      "Epoch [7/50], Train Loss: 0.1182, Val Loss: 0.2429\n",
      "Epoch [8/50], Train Loss: 0.1109, Val Loss: 0.2333\n",
      "Epoch [9/50], Train Loss: 0.1082, Val Loss: 0.2244\n",
      "Epoch [10/50], Train Loss: 0.1049, Val Loss: 0.2160\n",
      "Epoch [11/50], Train Loss: 0.0991, Val Loss: 0.2082\n",
      "Epoch [12/50], Train Loss: 0.0946, Val Loss: 0.2012\n",
      "Epoch [13/50], Train Loss: 0.0923, Val Loss: 0.1946\n",
      "Epoch [14/50], Train Loss: 0.0908, Val Loss: 0.1882\n",
      "Epoch [15/50], Train Loss: 0.0883, Val Loss: 0.1823\n",
      "Epoch [16/50], Train Loss: 0.0875, Val Loss: 0.1768\n",
      "Epoch [17/50], Train Loss: 0.0849, Val Loss: 0.1719\n",
      "Epoch [18/50], Train Loss: 0.0836, Val Loss: 0.1673\n",
      "Epoch [19/50], Train Loss: 0.0806, Val Loss: 0.1629\n",
      "Epoch [20/50], Train Loss: 0.0816, Val Loss: 0.1588\n",
      "Epoch [21/50], Train Loss: 0.0744, Val Loss: 0.1551\n",
      "Epoch [22/50], Train Loss: 0.0745, Val Loss: 0.1516\n",
      "Epoch [23/50], Train Loss: 0.0725, Val Loss: 0.1483\n",
      "Epoch [24/50], Train Loss: 0.0745, Val Loss: 0.1451\n",
      "Epoch [25/50], Train Loss: 0.0728, Val Loss: 0.1423\n",
      "Epoch [26/50], Train Loss: 0.0729, Val Loss: 0.1395\n",
      "Epoch [27/50], Train Loss: 0.0710, Val Loss: 0.1370\n",
      "Epoch [28/50], Train Loss: 0.0713, Val Loss: 0.1344\n",
      "Epoch [29/50], Train Loss: 0.0689, Val Loss: 0.1322\n",
      "Epoch [30/50], Train Loss: 0.0708, Val Loss: 0.1299\n",
      "Epoch [31/50], Train Loss: 0.0693, Val Loss: 0.1279\n",
      "Epoch [32/50], Train Loss: 0.0679, Val Loss: 0.1261\n",
      "Epoch [33/50], Train Loss: 0.0670, Val Loss: 0.1246\n",
      "Epoch [34/50], Train Loss: 0.0645, Val Loss: 0.1231\n",
      "Epoch [35/50], Train Loss: 0.0646, Val Loss: 0.1217\n",
      "Epoch [36/50], Train Loss: 0.0645, Val Loss: 0.1204\n",
      "Epoch [37/50], Train Loss: 0.0658, Val Loss: 0.1189\n",
      "Epoch [38/50], Train Loss: 0.0664, Val Loss: 0.1176\n",
      "Epoch [39/50], Train Loss: 0.0655, Val Loss: 0.1163\n",
      "Epoch [40/50], Train Loss: 0.0626, Val Loss: 0.1151\n",
      "Epoch [41/50], Train Loss: 0.0645, Val Loss: 0.1139\n",
      "Epoch [42/50], Train Loss: 0.0667, Val Loss: 0.1129\n",
      "Epoch [43/50], Train Loss: 0.0632, Val Loss: 0.1122\n",
      "Epoch [44/50], Train Loss: 0.0638, Val Loss: 0.1114\n",
      "Epoch [45/50], Train Loss: 0.0624, Val Loss: 0.1106\n",
      "Epoch [46/50], Train Loss: 0.0617, Val Loss: 0.1099\n",
      "Epoch [47/50], Train Loss: 0.0604, Val Loss: 0.1092\n",
      "Epoch [48/50], Train Loss: 0.0615, Val Loss: 0.1085\n",
      "Epoch [49/50], Train Loss: 0.0607, Val Loss: 0.1078\n",
      "Epoch [50/50], Train Loss: 0.0617, Val Loss: 0.1073\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=128, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1101, Val Loss: 0.3282\n",
      "Epoch [2/50], Train Loss: 0.1070, Val Loss: 0.3216\n",
      "Epoch [3/50], Train Loss: 0.1041, Val Loss: 0.3152\n",
      "Epoch [4/50], Train Loss: 0.1012, Val Loss: 0.3090\n",
      "Epoch [5/50], Train Loss: 0.0985, Val Loss: 0.3030\n",
      "Epoch [6/50], Train Loss: 0.0958, Val Loss: 0.2971\n",
      "Epoch [7/50], Train Loss: 0.0933, Val Loss: 0.2914\n",
      "Epoch [8/50], Train Loss: 0.0909, Val Loss: 0.2859\n",
      "Epoch [9/50], Train Loss: 0.0885, Val Loss: 0.2805\n",
      "Epoch [10/50], Train Loss: 0.0862, Val Loss: 0.2752\n",
      "Epoch [11/50], Train Loss: 0.0840, Val Loss: 0.2701\n",
      "Epoch [12/50], Train Loss: 0.0819, Val Loss: 0.2651\n",
      "Epoch [13/50], Train Loss: 0.0799, Val Loss: 0.2603\n",
      "Epoch [14/50], Train Loss: 0.0779, Val Loss: 0.2555\n",
      "Epoch [15/50], Train Loss: 0.0760, Val Loss: 0.2509\n",
      "Epoch [16/50], Train Loss: 0.0742, Val Loss: 0.2464\n",
      "Epoch [17/50], Train Loss: 0.0724, Val Loss: 0.2421\n",
      "Epoch [18/50], Train Loss: 0.0707, Val Loss: 0.2378\n",
      "Epoch [19/50], Train Loss: 0.0691, Val Loss: 0.2336\n",
      "Epoch [20/50], Train Loss: 0.0675, Val Loss: 0.2295\n",
      "Epoch [21/50], Train Loss: 0.0659, Val Loss: 0.2255\n",
      "Epoch [22/50], Train Loss: 0.0645, Val Loss: 0.2217\n",
      "Epoch [23/50], Train Loss: 0.0630, Val Loss: 0.2179\n",
      "Epoch [24/50], Train Loss: 0.0617, Val Loss: 0.2142\n",
      "Epoch [25/50], Train Loss: 0.0603, Val Loss: 0.2105\n",
      "Epoch [26/50], Train Loss: 0.0591, Val Loss: 0.2070\n",
      "Epoch [27/50], Train Loss: 0.0578, Val Loss: 0.2036\n",
      "Epoch [28/50], Train Loss: 0.0566, Val Loss: 0.2002\n",
      "Epoch [29/50], Train Loss: 0.0555, Val Loss: 0.1970\n",
      "Epoch [30/50], Train Loss: 0.0544, Val Loss: 0.1938\n",
      "Epoch [31/50], Train Loss: 0.0533, Val Loss: 0.1906\n",
      "Epoch [32/50], Train Loss: 0.0523, Val Loss: 0.1876\n",
      "Epoch [33/50], Train Loss: 0.0513, Val Loss: 0.1846\n",
      "Epoch [34/50], Train Loss: 0.0503, Val Loss: 0.1817\n",
      "Epoch [35/50], Train Loss: 0.0494, Val Loss: 0.1789\n",
      "Epoch [36/50], Train Loss: 0.0485, Val Loss: 0.1761\n",
      "Epoch [37/50], Train Loss: 0.0477, Val Loss: 0.1734\n",
      "Epoch [38/50], Train Loss: 0.0469, Val Loss: 0.1708\n",
      "Epoch [39/50], Train Loss: 0.0461, Val Loss: 0.1682\n",
      "Epoch [40/50], Train Loss: 0.0453, Val Loss: 0.1657\n",
      "Epoch [41/50], Train Loss: 0.0446, Val Loss: 0.1633\n",
      "Epoch [42/50], Train Loss: 0.0439, Val Loss: 0.1609\n",
      "Epoch [43/50], Train Loss: 0.0432, Val Loss: 0.1586\n",
      "Epoch [44/50], Train Loss: 0.0426, Val Loss: 0.1563\n",
      "Epoch [45/50], Train Loss: 0.0420, Val Loss: 0.1541\n",
      "Epoch [46/50], Train Loss: 0.0414, Val Loss: 0.1519\n",
      "Epoch [47/50], Train Loss: 0.0408, Val Loss: 0.1498\n",
      "Epoch [48/50], Train Loss: 0.0403, Val Loss: 0.1478\n",
      "Epoch [49/50], Train Loss: 0.0397, Val Loss: 0.1458\n",
      "Epoch [50/50], Train Loss: 0.0392, Val Loss: 0.1438\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=128, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1357, Val Loss: 0.3820\n",
      "Epoch [2/50], Train Loss: 0.1295, Val Loss: 0.3720\n",
      "Epoch [3/50], Train Loss: 0.1256, Val Loss: 0.3623\n",
      "Epoch [4/50], Train Loss: 0.1212, Val Loss: 0.3529\n",
      "Epoch [5/50], Train Loss: 0.1162, Val Loss: 0.3439\n",
      "Epoch [6/50], Train Loss: 0.1131, Val Loss: 0.3352\n",
      "Epoch [7/50], Train Loss: 0.1093, Val Loss: 0.3268\n",
      "Epoch [8/50], Train Loss: 0.1059, Val Loss: 0.3186\n",
      "Epoch [9/50], Train Loss: 0.1018, Val Loss: 0.3108\n",
      "Epoch [10/50], Train Loss: 0.0983, Val Loss: 0.3032\n",
      "Epoch [11/50], Train Loss: 0.0959, Val Loss: 0.2959\n",
      "Epoch [12/50], Train Loss: 0.0928, Val Loss: 0.2888\n",
      "Epoch [13/50], Train Loss: 0.0897, Val Loss: 0.2819\n",
      "Epoch [14/50], Train Loss: 0.0870, Val Loss: 0.2752\n",
      "Epoch [15/50], Train Loss: 0.0841, Val Loss: 0.2687\n",
      "Epoch [16/50], Train Loss: 0.0817, Val Loss: 0.2624\n",
      "Epoch [17/50], Train Loss: 0.0794, Val Loss: 0.2564\n",
      "Epoch [18/50], Train Loss: 0.0762, Val Loss: 0.2505\n",
      "Epoch [19/50], Train Loss: 0.0747, Val Loss: 0.2448\n",
      "Epoch [20/50], Train Loss: 0.0728, Val Loss: 0.2392\n",
      "Epoch [21/50], Train Loss: 0.0709, Val Loss: 0.2338\n",
      "Epoch [22/50], Train Loss: 0.0683, Val Loss: 0.2286\n",
      "Epoch [23/50], Train Loss: 0.0666, Val Loss: 0.2235\n",
      "Epoch [24/50], Train Loss: 0.0647, Val Loss: 0.2186\n",
      "Epoch [25/50], Train Loss: 0.0634, Val Loss: 0.2139\n",
      "Epoch [26/50], Train Loss: 0.0621, Val Loss: 0.2093\n",
      "Epoch [27/50], Train Loss: 0.0603, Val Loss: 0.2048\n",
      "Epoch [28/50], Train Loss: 0.0588, Val Loss: 0.2005\n",
      "Epoch [29/50], Train Loss: 0.0577, Val Loss: 0.1963\n",
      "Epoch [30/50], Train Loss: 0.0559, Val Loss: 0.1922\n",
      "Epoch [31/50], Train Loss: 0.0551, Val Loss: 0.1883\n",
      "Epoch [32/50], Train Loss: 0.0533, Val Loss: 0.1845\n",
      "Epoch [33/50], Train Loss: 0.0524, Val Loss: 0.1808\n",
      "Epoch [34/50], Train Loss: 0.0509, Val Loss: 0.1772\n",
      "Epoch [35/50], Train Loss: 0.0506, Val Loss: 0.1737\n",
      "Epoch [36/50], Train Loss: 0.0497, Val Loss: 0.1704\n",
      "Epoch [37/50], Train Loss: 0.0484, Val Loss: 0.1671\n",
      "Epoch [38/50], Train Loss: 0.0474, Val Loss: 0.1640\n",
      "Epoch [39/50], Train Loss: 0.0466, Val Loss: 0.1610\n",
      "Epoch [40/50], Train Loss: 0.0456, Val Loss: 0.1581\n",
      "Epoch [41/50], Train Loss: 0.0450, Val Loss: 0.1552\n",
      "Epoch [42/50], Train Loss: 0.0441, Val Loss: 0.1525\n",
      "Epoch [43/50], Train Loss: 0.0433, Val Loss: 0.1499\n",
      "Epoch [44/50], Train Loss: 0.0430, Val Loss: 0.1473\n",
      "Epoch [45/50], Train Loss: 0.0418, Val Loss: 0.1448\n",
      "Epoch [46/50], Train Loss: 0.0411, Val Loss: 0.1425\n",
      "Epoch [47/50], Train Loss: 0.0413, Val Loss: 0.1402\n",
      "Epoch [48/50], Train Loss: 0.0401, Val Loss: 0.1380\n",
      "Epoch [49/50], Train Loss: 0.0405, Val Loss: 0.1358\n",
      "Epoch [50/50], Train Loss: 0.0395, Val Loss: 0.1337\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=128, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1153, Val Loss: 0.3236\n",
      "Epoch [2/50], Train Loss: 0.1134, Val Loss: 0.3177\n",
      "Epoch [3/50], Train Loss: 0.1090, Val Loss: 0.3120\n",
      "Epoch [4/50], Train Loss: 0.1075, Val Loss: 0.3063\n",
      "Epoch [5/50], Train Loss: 0.1050, Val Loss: 0.3010\n",
      "Epoch [6/50], Train Loss: 0.1016, Val Loss: 0.2957\n",
      "Epoch [7/50], Train Loss: 0.0990, Val Loss: 0.2905\n",
      "Epoch [8/50], Train Loss: 0.0975, Val Loss: 0.2855\n",
      "Epoch [9/50], Train Loss: 0.0949, Val Loss: 0.2807\n",
      "Epoch [10/50], Train Loss: 0.0926, Val Loss: 0.2760\n",
      "Epoch [11/50], Train Loss: 0.0902, Val Loss: 0.2714\n",
      "Epoch [12/50], Train Loss: 0.0888, Val Loss: 0.2669\n",
      "Epoch [13/50], Train Loss: 0.0867, Val Loss: 0.2626\n",
      "Epoch [14/50], Train Loss: 0.0845, Val Loss: 0.2583\n",
      "Epoch [15/50], Train Loss: 0.0833, Val Loss: 0.2542\n",
      "Epoch [16/50], Train Loss: 0.0809, Val Loss: 0.2501\n",
      "Epoch [17/50], Train Loss: 0.0795, Val Loss: 0.2462\n",
      "Epoch [18/50], Train Loss: 0.0777, Val Loss: 0.2424\n",
      "Epoch [19/50], Train Loss: 0.0750, Val Loss: 0.2386\n",
      "Epoch [20/50], Train Loss: 0.0745, Val Loss: 0.2350\n",
      "Epoch [21/50], Train Loss: 0.0741, Val Loss: 0.2314\n",
      "Epoch [22/50], Train Loss: 0.0718, Val Loss: 0.2279\n",
      "Epoch [23/50], Train Loss: 0.0711, Val Loss: 0.2244\n",
      "Epoch [24/50], Train Loss: 0.0677, Val Loss: 0.2211\n",
      "Epoch [25/50], Train Loss: 0.0679, Val Loss: 0.2178\n",
      "Epoch [26/50], Train Loss: 0.0664, Val Loss: 0.2146\n",
      "Epoch [27/50], Train Loss: 0.0653, Val Loss: 0.2115\n",
      "Epoch [28/50], Train Loss: 0.0641, Val Loss: 0.2084\n",
      "Epoch [29/50], Train Loss: 0.0632, Val Loss: 0.2054\n",
      "Epoch [30/50], Train Loss: 0.0625, Val Loss: 0.2025\n",
      "Epoch [31/50], Train Loss: 0.0608, Val Loss: 0.1997\n",
      "Epoch [32/50], Train Loss: 0.0593, Val Loss: 0.1969\n",
      "Epoch [33/50], Train Loss: 0.0586, Val Loss: 0.1942\n",
      "Epoch [34/50], Train Loss: 0.0579, Val Loss: 0.1915\n",
      "Epoch [35/50], Train Loss: 0.0569, Val Loss: 0.1889\n",
      "Epoch [36/50], Train Loss: 0.0564, Val Loss: 0.1863\n",
      "Epoch [37/50], Train Loss: 0.0561, Val Loss: 0.1838\n",
      "Epoch [38/50], Train Loss: 0.0546, Val Loss: 0.1814\n",
      "Epoch [39/50], Train Loss: 0.0534, Val Loss: 0.1790\n",
      "Epoch [40/50], Train Loss: 0.0528, Val Loss: 0.1767\n",
      "Epoch [41/50], Train Loss: 0.0522, Val Loss: 0.1744\n",
      "Epoch [42/50], Train Loss: 0.0521, Val Loss: 0.1722\n",
      "Epoch [43/50], Train Loss: 0.0516, Val Loss: 0.1701\n",
      "Epoch [44/50], Train Loss: 0.0509, Val Loss: 0.1679\n",
      "Epoch [45/50], Train Loss: 0.0489, Val Loss: 0.1658\n",
      "Epoch [46/50], Train Loss: 0.0488, Val Loss: 0.1638\n",
      "Epoch [47/50], Train Loss: 0.0488, Val Loss: 0.1618\n",
      "Epoch [48/50], Train Loss: 0.0480, Val Loss: 0.1599\n",
      "Epoch [49/50], Train Loss: 0.0470, Val Loss: 0.1580\n",
      "Epoch [50/50], Train Loss: 0.0470, Val Loss: 0.1561\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=128, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1174, Val Loss: 0.3237\n",
      "Epoch [2/50], Train Loss: 0.1121, Val Loss: 0.3129\n",
      "Epoch [3/50], Train Loss: 0.1071, Val Loss: 0.3024\n",
      "Epoch [4/50], Train Loss: 0.1023, Val Loss: 0.2924\n",
      "Epoch [5/50], Train Loss: 0.0979, Val Loss: 0.2829\n",
      "Epoch [6/50], Train Loss: 0.0937, Val Loss: 0.2737\n",
      "Epoch [7/50], Train Loss: 0.0897, Val Loss: 0.2649\n",
      "Epoch [8/50], Train Loss: 0.0859, Val Loss: 0.2565\n",
      "Epoch [9/50], Train Loss: 0.0823, Val Loss: 0.2483\n",
      "Epoch [10/50], Train Loss: 0.0790, Val Loss: 0.2405\n",
      "Epoch [11/50], Train Loss: 0.0758, Val Loss: 0.2330\n",
      "Epoch [12/50], Train Loss: 0.0728, Val Loss: 0.2258\n",
      "Epoch [13/50], Train Loss: 0.0700, Val Loss: 0.2189\n",
      "Epoch [14/50], Train Loss: 0.0673, Val Loss: 0.2123\n",
      "Epoch [15/50], Train Loss: 0.0648, Val Loss: 0.2059\n",
      "Epoch [16/50], Train Loss: 0.0625, Val Loss: 0.1998\n",
      "Epoch [17/50], Train Loss: 0.0602, Val Loss: 0.1939\n",
      "Epoch [18/50], Train Loss: 0.0582, Val Loss: 0.1884\n",
      "Epoch [19/50], Train Loss: 0.0562, Val Loss: 0.1830\n",
      "Epoch [20/50], Train Loss: 0.0544, Val Loss: 0.1778\n",
      "Epoch [21/50], Train Loss: 0.0527, Val Loss: 0.1729\n",
      "Epoch [22/50], Train Loss: 0.0511, Val Loss: 0.1682\n",
      "Epoch [23/50], Train Loss: 0.0496, Val Loss: 0.1637\n",
      "Epoch [24/50], Train Loss: 0.0482, Val Loss: 0.1595\n",
      "Epoch [25/50], Train Loss: 0.0470, Val Loss: 0.1554\n",
      "Epoch [26/50], Train Loss: 0.0458, Val Loss: 0.1515\n",
      "Epoch [27/50], Train Loss: 0.0447, Val Loss: 0.1478\n",
      "Epoch [28/50], Train Loss: 0.0437, Val Loss: 0.1443\n",
      "Epoch [29/50], Train Loss: 0.0427, Val Loss: 0.1410\n",
      "Epoch [30/50], Train Loss: 0.0419, Val Loss: 0.1378\n",
      "Epoch [31/50], Train Loss: 0.0411, Val Loss: 0.1348\n",
      "Epoch [32/50], Train Loss: 0.0403, Val Loss: 0.1320\n",
      "Epoch [33/50], Train Loss: 0.0396, Val Loss: 0.1293\n",
      "Epoch [34/50], Train Loss: 0.0390, Val Loss: 0.1268\n",
      "Epoch [35/50], Train Loss: 0.0385, Val Loss: 0.1243\n",
      "Epoch [36/50], Train Loss: 0.0379, Val Loss: 0.1221\n",
      "Epoch [37/50], Train Loss: 0.0375, Val Loss: 0.1199\n",
      "Epoch [38/50], Train Loss: 0.0370, Val Loss: 0.1179\n",
      "Epoch [39/50], Train Loss: 0.0366, Val Loss: 0.1160\n",
      "Epoch [40/50], Train Loss: 0.0363, Val Loss: 0.1142\n",
      "Epoch [41/50], Train Loss: 0.0360, Val Loss: 0.1125\n",
      "Epoch [42/50], Train Loss: 0.0356, Val Loss: 0.1109\n",
      "Epoch [43/50], Train Loss: 0.0354, Val Loss: 0.1093\n",
      "Epoch [44/50], Train Loss: 0.0351, Val Loss: 0.1079\n",
      "Epoch [45/50], Train Loss: 0.0349, Val Loss: 0.1066\n",
      "Epoch [46/50], Train Loss: 0.0347, Val Loss: 0.1053\n",
      "Epoch [47/50], Train Loss: 0.0345, Val Loss: 0.1041\n",
      "Epoch [48/50], Train Loss: 0.0343, Val Loss: 0.1030\n",
      "Epoch [49/50], Train Loss: 0.0342, Val Loss: 0.1019\n",
      "Epoch [50/50], Train Loss: 0.0340, Val Loss: 0.1009\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=128, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1376, Val Loss: 0.3706\n",
      "Epoch [2/50], Train Loss: 0.1316, Val Loss: 0.3609\n",
      "Epoch [3/50], Train Loss: 0.1279, Val Loss: 0.3515\n",
      "Epoch [4/50], Train Loss: 0.1228, Val Loss: 0.3425\n",
      "Epoch [5/50], Train Loss: 0.1181, Val Loss: 0.3337\n",
      "Epoch [6/50], Train Loss: 0.1142, Val Loss: 0.3253\n",
      "Epoch [7/50], Train Loss: 0.1107, Val Loss: 0.3171\n",
      "Epoch [8/50], Train Loss: 0.1066, Val Loss: 0.3091\n",
      "Epoch [9/50], Train Loss: 0.1029, Val Loss: 0.3013\n",
      "Epoch [10/50], Train Loss: 0.0986, Val Loss: 0.2938\n",
      "Epoch [11/50], Train Loss: 0.0967, Val Loss: 0.2864\n",
      "Epoch [12/50], Train Loss: 0.0933, Val Loss: 0.2793\n",
      "Epoch [13/50], Train Loss: 0.0901, Val Loss: 0.2724\n",
      "Epoch [14/50], Train Loss: 0.0865, Val Loss: 0.2657\n",
      "Epoch [15/50], Train Loss: 0.0844, Val Loss: 0.2592\n",
      "Epoch [16/50], Train Loss: 0.0810, Val Loss: 0.2529\n",
      "Epoch [17/50], Train Loss: 0.0785, Val Loss: 0.2467\n",
      "Epoch [18/50], Train Loss: 0.0768, Val Loss: 0.2408\n",
      "Epoch [19/50], Train Loss: 0.0746, Val Loss: 0.2350\n",
      "Epoch [20/50], Train Loss: 0.0726, Val Loss: 0.2293\n",
      "Epoch [21/50], Train Loss: 0.0704, Val Loss: 0.2239\n",
      "Epoch [22/50], Train Loss: 0.0679, Val Loss: 0.2186\n",
      "Epoch [23/50], Train Loss: 0.0659, Val Loss: 0.2134\n",
      "Epoch [24/50], Train Loss: 0.0637, Val Loss: 0.2085\n",
      "Epoch [25/50], Train Loss: 0.0626, Val Loss: 0.2036\n",
      "Epoch [26/50], Train Loss: 0.0606, Val Loss: 0.1990\n",
      "Epoch [27/50], Train Loss: 0.0595, Val Loss: 0.1945\n",
      "Epoch [28/50], Train Loss: 0.0578, Val Loss: 0.1901\n",
      "Epoch [29/50], Train Loss: 0.0563, Val Loss: 0.1858\n",
      "Epoch [30/50], Train Loss: 0.0545, Val Loss: 0.1817\n",
      "Epoch [31/50], Train Loss: 0.0538, Val Loss: 0.1777\n",
      "Epoch [32/50], Train Loss: 0.0524, Val Loss: 0.1739\n",
      "Epoch [33/50], Train Loss: 0.0515, Val Loss: 0.1702\n",
      "Epoch [34/50], Train Loss: 0.0498, Val Loss: 0.1666\n",
      "Epoch [35/50], Train Loss: 0.0489, Val Loss: 0.1632\n",
      "Epoch [36/50], Train Loss: 0.0479, Val Loss: 0.1598\n",
      "Epoch [37/50], Train Loss: 0.0472, Val Loss: 0.1566\n",
      "Epoch [38/50], Train Loss: 0.0470, Val Loss: 0.1534\n",
      "Epoch [39/50], Train Loss: 0.0452, Val Loss: 0.1505\n",
      "Epoch [40/50], Train Loss: 0.0445, Val Loss: 0.1476\n",
      "Epoch [41/50], Train Loss: 0.0437, Val Loss: 0.1448\n",
      "Epoch [42/50], Train Loss: 0.0427, Val Loss: 0.1421\n",
      "Epoch [43/50], Train Loss: 0.0433, Val Loss: 0.1395\n",
      "Epoch [44/50], Train Loss: 0.0418, Val Loss: 0.1370\n",
      "Epoch [45/50], Train Loss: 0.0417, Val Loss: 0.1347\n",
      "Epoch [46/50], Train Loss: 0.0413, Val Loss: 0.1324\n",
      "Epoch [47/50], Train Loss: 0.0403, Val Loss: 0.1302\n",
      "Epoch [48/50], Train Loss: 0.0404, Val Loss: 0.1280\n",
      "Epoch [49/50], Train Loss: 0.0404, Val Loss: 0.1260\n",
      "Epoch [50/50], Train Loss: 0.0397, Val Loss: 0.1241\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=128, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1308, Val Loss: 0.3306\n",
      "Epoch [2/50], Train Loss: 0.1244, Val Loss: 0.3198\n",
      "Epoch [3/50], Train Loss: 0.1201, Val Loss: 0.3096\n",
      "Epoch [4/50], Train Loss: 0.1127, Val Loss: 0.2999\n",
      "Epoch [5/50], Train Loss: 0.1088, Val Loss: 0.2907\n",
      "Epoch [6/50], Train Loss: 0.1031, Val Loss: 0.2819\n",
      "Epoch [7/50], Train Loss: 0.1015, Val Loss: 0.2734\n",
      "Epoch [8/50], Train Loss: 0.0977, Val Loss: 0.2653\n",
      "Epoch [9/50], Train Loss: 0.0941, Val Loss: 0.2575\n",
      "Epoch [10/50], Train Loss: 0.0915, Val Loss: 0.2501\n",
      "Epoch [11/50], Train Loss: 0.0868, Val Loss: 0.2432\n",
      "Epoch [12/50], Train Loss: 0.0848, Val Loss: 0.2366\n",
      "Epoch [13/50], Train Loss: 0.0830, Val Loss: 0.2302\n",
      "Epoch [14/50], Train Loss: 0.0793, Val Loss: 0.2240\n",
      "Epoch [15/50], Train Loss: 0.0775, Val Loss: 0.2181\n",
      "Epoch [16/50], Train Loss: 0.0733, Val Loss: 0.2126\n",
      "Epoch [17/50], Train Loss: 0.0726, Val Loss: 0.2073\n",
      "Epoch [18/50], Train Loss: 0.0709, Val Loss: 0.2022\n",
      "Epoch [19/50], Train Loss: 0.0674, Val Loss: 0.1973\n",
      "Epoch [20/50], Train Loss: 0.0664, Val Loss: 0.1926\n",
      "Epoch [21/50], Train Loss: 0.0645, Val Loss: 0.1883\n",
      "Epoch [22/50], Train Loss: 0.0660, Val Loss: 0.1842\n",
      "Epoch [23/50], Train Loss: 0.0617, Val Loss: 0.1801\n",
      "Epoch [24/50], Train Loss: 0.0612, Val Loss: 0.1764\n",
      "Epoch [25/50], Train Loss: 0.0609, Val Loss: 0.1727\n",
      "Epoch [26/50], Train Loss: 0.0608, Val Loss: 0.1692\n",
      "Epoch [27/50], Train Loss: 0.0587, Val Loss: 0.1658\n",
      "Epoch [28/50], Train Loss: 0.0580, Val Loss: 0.1628\n",
      "Epoch [29/50], Train Loss: 0.0595, Val Loss: 0.1598\n",
      "Epoch [30/50], Train Loss: 0.0560, Val Loss: 0.1570\n",
      "Epoch [31/50], Train Loss: 0.0553, Val Loss: 0.1542\n",
      "Epoch [32/50], Train Loss: 0.0557, Val Loss: 0.1516\n",
      "Epoch [33/50], Train Loss: 0.0535, Val Loss: 0.1491\n",
      "Epoch [34/50], Train Loss: 0.0534, Val Loss: 0.1469\n",
      "Epoch [35/50], Train Loss: 0.0537, Val Loss: 0.1447\n",
      "Epoch [36/50], Train Loss: 0.0546, Val Loss: 0.1427\n",
      "Epoch [37/50], Train Loss: 0.0531, Val Loss: 0.1407\n",
      "Epoch [38/50], Train Loss: 0.0519, Val Loss: 0.1389\n",
      "Epoch [39/50], Train Loss: 0.0503, Val Loss: 0.1371\n",
      "Epoch [40/50], Train Loss: 0.0511, Val Loss: 0.1356\n",
      "Epoch [41/50], Train Loss: 0.0511, Val Loss: 0.1339\n",
      "Epoch [42/50], Train Loss: 0.0513, Val Loss: 0.1325\n",
      "Epoch [43/50], Train Loss: 0.0504, Val Loss: 0.1312\n",
      "Epoch [44/50], Train Loss: 0.0493, Val Loss: 0.1299\n",
      "Epoch [45/50], Train Loss: 0.0497, Val Loss: 0.1287\n",
      "Epoch [46/50], Train Loss: 0.0496, Val Loss: 0.1276\n",
      "Epoch [47/50], Train Loss: 0.0501, Val Loss: 0.1265\n",
      "Epoch [48/50], Train Loss: 0.0488, Val Loss: 0.1255\n",
      "Epoch [49/50], Train Loss: 0.0495, Val Loss: 0.1246\n",
      "Epoch [50/50], Train Loss: 0.0486, Val Loss: 0.1236\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=128, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1419, Val Loss: 0.3945\n",
      "Epoch [2/50], Train Loss: 0.1337, Val Loss: 0.3789\n",
      "Epoch [3/50], Train Loss: 0.1260, Val Loss: 0.3641\n",
      "Epoch [4/50], Train Loss: 0.1189, Val Loss: 0.3500\n",
      "Epoch [5/50], Train Loss: 0.1123, Val Loss: 0.3367\n",
      "Epoch [6/50], Train Loss: 0.1061, Val Loss: 0.3239\n",
      "Epoch [7/50], Train Loss: 0.1003, Val Loss: 0.3117\n",
      "Epoch [8/50], Train Loss: 0.0949, Val Loss: 0.3001\n",
      "Epoch [9/50], Train Loss: 0.0899, Val Loss: 0.2890\n",
      "Epoch [10/50], Train Loss: 0.0852, Val Loss: 0.2784\n",
      "Epoch [11/50], Train Loss: 0.0808, Val Loss: 0.2682\n",
      "Epoch [12/50], Train Loss: 0.0768, Val Loss: 0.2585\n",
      "Epoch [13/50], Train Loss: 0.0730, Val Loss: 0.2492\n",
      "Epoch [14/50], Train Loss: 0.0695, Val Loss: 0.2404\n",
      "Epoch [15/50], Train Loss: 0.0663, Val Loss: 0.2319\n",
      "Epoch [16/50], Train Loss: 0.0633, Val Loss: 0.2239\n",
      "Epoch [17/50], Train Loss: 0.0605, Val Loss: 0.2162\n",
      "Epoch [18/50], Train Loss: 0.0579, Val Loss: 0.2089\n",
      "Epoch [19/50], Train Loss: 0.0556, Val Loss: 0.2019\n",
      "Epoch [20/50], Train Loss: 0.0534, Val Loss: 0.1953\n",
      "Epoch [21/50], Train Loss: 0.0515, Val Loss: 0.1890\n",
      "Epoch [22/50], Train Loss: 0.0497, Val Loss: 0.1831\n",
      "Epoch [23/50], Train Loss: 0.0481, Val Loss: 0.1775\n",
      "Epoch [24/50], Train Loss: 0.0466, Val Loss: 0.1722\n",
      "Epoch [25/50], Train Loss: 0.0453, Val Loss: 0.1671\n",
      "Epoch [26/50], Train Loss: 0.0441, Val Loss: 0.1624\n",
      "Epoch [27/50], Train Loss: 0.0430, Val Loss: 0.1580\n",
      "Epoch [28/50], Train Loss: 0.0420, Val Loss: 0.1538\n",
      "Epoch [29/50], Train Loss: 0.0412, Val Loss: 0.1499\n",
      "Epoch [30/50], Train Loss: 0.0404, Val Loss: 0.1462\n",
      "Epoch [31/50], Train Loss: 0.0397, Val Loss: 0.1427\n",
      "Epoch [32/50], Train Loss: 0.0391, Val Loss: 0.1395\n",
      "Epoch [33/50], Train Loss: 0.0385, Val Loss: 0.1365\n",
      "Epoch [34/50], Train Loss: 0.0381, Val Loss: 0.1336\n",
      "Epoch [35/50], Train Loss: 0.0376, Val Loss: 0.1310\n",
      "Epoch [36/50], Train Loss: 0.0373, Val Loss: 0.1285\n",
      "Epoch [37/50], Train Loss: 0.0369, Val Loss: 0.1262\n",
      "Epoch [38/50], Train Loss: 0.0366, Val Loss: 0.1240\n",
      "Epoch [39/50], Train Loss: 0.0364, Val Loss: 0.1220\n",
      "Epoch [40/50], Train Loss: 0.0361, Val Loss: 0.1201\n",
      "Epoch [41/50], Train Loss: 0.0359, Val Loss: 0.1184\n",
      "Epoch [42/50], Train Loss: 0.0358, Val Loss: 0.1167\n",
      "Epoch [43/50], Train Loss: 0.0356, Val Loss: 0.1152\n",
      "Epoch [44/50], Train Loss: 0.0355, Val Loss: 0.1138\n",
      "Epoch [45/50], Train Loss: 0.0353, Val Loss: 0.1125\n",
      "Epoch [46/50], Train Loss: 0.0352, Val Loss: 0.1112\n",
      "Epoch [47/50], Train Loss: 0.0351, Val Loss: 0.1101\n",
      "Epoch [48/50], Train Loss: 0.0351, Val Loss: 0.1090\n",
      "Epoch [49/50], Train Loss: 0.0350, Val Loss: 0.1080\n",
      "Epoch [50/50], Train Loss: 0.0349, Val Loss: 0.1071\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=128, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1155, Val Loss: 0.3266\n",
      "Epoch [2/50], Train Loss: 0.1113, Val Loss: 0.3166\n",
      "Epoch [3/50], Train Loss: 0.1050, Val Loss: 0.3070\n",
      "Epoch [4/50], Train Loss: 0.1010, Val Loss: 0.2977\n",
      "Epoch [5/50], Train Loss: 0.0968, Val Loss: 0.2889\n",
      "Epoch [6/50], Train Loss: 0.0918, Val Loss: 0.2804\n",
      "Epoch [7/50], Train Loss: 0.0893, Val Loss: 0.2722\n",
      "Epoch [8/50], Train Loss: 0.0849, Val Loss: 0.2643\n",
      "Epoch [9/50], Train Loss: 0.0816, Val Loss: 0.2566\n",
      "Epoch [10/50], Train Loss: 0.0788, Val Loss: 0.2493\n",
      "Epoch [11/50], Train Loss: 0.0757, Val Loss: 0.2423\n",
      "Epoch [12/50], Train Loss: 0.0727, Val Loss: 0.2355\n",
      "Epoch [13/50], Train Loss: 0.0698, Val Loss: 0.2290\n",
      "Epoch [14/50], Train Loss: 0.0666, Val Loss: 0.2227\n",
      "Epoch [15/50], Train Loss: 0.0654, Val Loss: 0.2166\n",
      "Epoch [16/50], Train Loss: 0.0629, Val Loss: 0.2109\n",
      "Epoch [17/50], Train Loss: 0.0612, Val Loss: 0.2054\n",
      "Epoch [18/50], Train Loss: 0.0593, Val Loss: 0.2000\n",
      "Epoch [19/50], Train Loss: 0.0570, Val Loss: 0.1949\n",
      "Epoch [20/50], Train Loss: 0.0565, Val Loss: 0.1900\n",
      "Epoch [21/50], Train Loss: 0.0547, Val Loss: 0.1853\n",
      "Epoch [22/50], Train Loss: 0.0527, Val Loss: 0.1808\n",
      "Epoch [23/50], Train Loss: 0.0521, Val Loss: 0.1764\n",
      "Epoch [24/50], Train Loss: 0.0510, Val Loss: 0.1723\n",
      "Epoch [25/50], Train Loss: 0.0501, Val Loss: 0.1684\n",
      "Epoch [26/50], Train Loss: 0.0490, Val Loss: 0.1647\n",
      "Epoch [27/50], Train Loss: 0.0478, Val Loss: 0.1611\n",
      "Epoch [28/50], Train Loss: 0.0477, Val Loss: 0.1577\n",
      "Epoch [29/50], Train Loss: 0.0465, Val Loss: 0.1545\n",
      "Epoch [30/50], Train Loss: 0.0457, Val Loss: 0.1514\n",
      "Epoch [31/50], Train Loss: 0.0448, Val Loss: 0.1486\n",
      "Epoch [32/50], Train Loss: 0.0450, Val Loss: 0.1458\n",
      "Epoch [33/50], Train Loss: 0.0442, Val Loss: 0.1431\n",
      "Epoch [34/50], Train Loss: 0.0438, Val Loss: 0.1406\n",
      "Epoch [35/50], Train Loss: 0.0434, Val Loss: 0.1383\n",
      "Epoch [36/50], Train Loss: 0.0433, Val Loss: 0.1360\n",
      "Epoch [37/50], Train Loss: 0.0427, Val Loss: 0.1339\n",
      "Epoch [38/50], Train Loss: 0.0415, Val Loss: 0.1320\n",
      "Epoch [39/50], Train Loss: 0.0418, Val Loss: 0.1301\n",
      "Epoch [40/50], Train Loss: 0.0408, Val Loss: 0.1282\n",
      "Epoch [41/50], Train Loss: 0.0409, Val Loss: 0.1264\n",
      "Epoch [42/50], Train Loss: 0.0401, Val Loss: 0.1248\n",
      "Epoch [43/50], Train Loss: 0.0410, Val Loss: 0.1232\n",
      "Epoch [44/50], Train Loss: 0.0404, Val Loss: 0.1218\n",
      "Epoch [45/50], Train Loss: 0.0401, Val Loss: 0.1203\n",
      "Epoch [46/50], Train Loss: 0.0401, Val Loss: 0.1190\n",
      "Epoch [47/50], Train Loss: 0.0398, Val Loss: 0.1177\n",
      "Epoch [48/50], Train Loss: 0.0394, Val Loss: 0.1165\n",
      "Epoch [49/50], Train Loss: 0.0397, Val Loss: 0.1154\n",
      "Epoch [50/50], Train Loss: 0.0394, Val Loss: 0.1143\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=128, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1356, Val Loss: 0.3448\n",
      "Epoch [2/50], Train Loss: 0.1301, Val Loss: 0.3354\n",
      "Epoch [3/50], Train Loss: 0.1247, Val Loss: 0.3264\n",
      "Epoch [4/50], Train Loss: 0.1190, Val Loss: 0.3180\n",
      "Epoch [5/50], Train Loss: 0.1153, Val Loss: 0.3099\n",
      "Epoch [6/50], Train Loss: 0.1116, Val Loss: 0.3021\n",
      "Epoch [7/50], Train Loss: 0.1089, Val Loss: 0.2947\n",
      "Epoch [8/50], Train Loss: 0.1036, Val Loss: 0.2875\n",
      "Epoch [9/50], Train Loss: 0.1005, Val Loss: 0.2806\n",
      "Epoch [10/50], Train Loss: 0.0975, Val Loss: 0.2739\n",
      "Epoch [11/50], Train Loss: 0.0938, Val Loss: 0.2676\n",
      "Epoch [12/50], Train Loss: 0.0923, Val Loss: 0.2614\n",
      "Epoch [13/50], Train Loss: 0.0894, Val Loss: 0.2555\n",
      "Epoch [14/50], Train Loss: 0.0861, Val Loss: 0.2499\n",
      "Epoch [15/50], Train Loss: 0.0835, Val Loss: 0.2444\n",
      "Epoch [16/50], Train Loss: 0.0816, Val Loss: 0.2392\n",
      "Epoch [17/50], Train Loss: 0.0799, Val Loss: 0.2341\n",
      "Epoch [18/50], Train Loss: 0.0769, Val Loss: 0.2292\n",
      "Epoch [19/50], Train Loss: 0.0779, Val Loss: 0.2244\n",
      "Epoch [20/50], Train Loss: 0.0752, Val Loss: 0.2199\n",
      "Epoch [21/50], Train Loss: 0.0713, Val Loss: 0.2154\n",
      "Epoch [22/50], Train Loss: 0.0707, Val Loss: 0.2113\n",
      "Epoch [23/50], Train Loss: 0.0699, Val Loss: 0.2071\n",
      "Epoch [24/50], Train Loss: 0.0687, Val Loss: 0.2032\n",
      "Epoch [25/50], Train Loss: 0.0674, Val Loss: 0.1993\n",
      "Epoch [26/50], Train Loss: 0.0647, Val Loss: 0.1957\n",
      "Epoch [27/50], Train Loss: 0.0640, Val Loss: 0.1922\n",
      "Epoch [28/50], Train Loss: 0.0634, Val Loss: 0.1888\n",
      "Epoch [29/50], Train Loss: 0.0624, Val Loss: 0.1855\n",
      "Epoch [30/50], Train Loss: 0.0610, Val Loss: 0.1823\n",
      "Epoch [31/50], Train Loss: 0.0609, Val Loss: 0.1792\n",
      "Epoch [32/50], Train Loss: 0.0620, Val Loss: 0.1763\n",
      "Epoch [33/50], Train Loss: 0.0584, Val Loss: 0.1734\n",
      "Epoch [34/50], Train Loss: 0.0579, Val Loss: 0.1708\n",
      "Epoch [35/50], Train Loss: 0.0578, Val Loss: 0.1682\n",
      "Epoch [36/50], Train Loss: 0.0577, Val Loss: 0.1657\n",
      "Epoch [37/50], Train Loss: 0.0573, Val Loss: 0.1633\n",
      "Epoch [38/50], Train Loss: 0.0555, Val Loss: 0.1609\n",
      "Epoch [39/50], Train Loss: 0.0564, Val Loss: 0.1586\n",
      "Epoch [40/50], Train Loss: 0.0556, Val Loss: 0.1564\n",
      "Epoch [41/50], Train Loss: 0.0544, Val Loss: 0.1545\n",
      "Epoch [42/50], Train Loss: 0.0546, Val Loss: 0.1525\n",
      "Epoch [43/50], Train Loss: 0.0538, Val Loss: 0.1506\n",
      "Epoch [44/50], Train Loss: 0.0521, Val Loss: 0.1488\n",
      "Epoch [45/50], Train Loss: 0.0525, Val Loss: 0.1470\n",
      "Epoch [46/50], Train Loss: 0.0526, Val Loss: 0.1454\n",
      "Epoch [47/50], Train Loss: 0.0512, Val Loss: 0.1438\n",
      "Epoch [48/50], Train Loss: 0.0520, Val Loss: 0.1423\n",
      "Epoch [49/50], Train Loss: 0.0507, Val Loss: 0.1409\n",
      "Epoch [50/50], Train Loss: 0.0519, Val Loss: 0.1396\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=16, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1808, Val Loss: 0.3295\n",
      "Epoch [2/50], Train Loss: 0.1004, Val Loss: 0.2129\n",
      "Epoch [3/50], Train Loss: 0.0587, Val Loss: 0.1349\n",
      "Epoch [4/50], Train Loss: 0.0387, Val Loss: 0.0904\n",
      "Epoch [5/50], Train Loss: 0.0324, Val Loss: 0.0713\n",
      "Epoch [6/50], Train Loss: 0.0305, Val Loss: 0.0631\n",
      "Epoch [7/50], Train Loss: 0.0288, Val Loss: 0.0576\n",
      "Epoch [8/50], Train Loss: 0.0270, Val Loss: 0.0527\n",
      "Epoch [9/50], Train Loss: 0.0253, Val Loss: 0.0481\n",
      "Epoch [10/50], Train Loss: 0.0237, Val Loss: 0.0436\n",
      "Epoch [11/50], Train Loss: 0.0220, Val Loss: 0.0394\n",
      "Epoch [12/50], Train Loss: 0.0205, Val Loss: 0.0352\n",
      "Epoch [13/50], Train Loss: 0.0190, Val Loss: 0.0314\n",
      "Epoch [14/50], Train Loss: 0.0176, Val Loss: 0.0278\n",
      "Epoch [15/50], Train Loss: 0.0164, Val Loss: 0.0246\n",
      "Epoch [16/50], Train Loss: 0.0153, Val Loss: 0.0219\n",
      "Epoch [17/50], Train Loss: 0.0143, Val Loss: 0.0195\n",
      "Epoch [18/50], Train Loss: 0.0135, Val Loss: 0.0175\n",
      "Epoch [19/50], Train Loss: 0.0127, Val Loss: 0.0158\n",
      "Epoch [20/50], Train Loss: 0.0120, Val Loss: 0.0144\n",
      "Epoch [21/50], Train Loss: 0.0113, Val Loss: 0.0132\n",
      "Epoch [22/50], Train Loss: 0.0105, Val Loss: 0.0121\n",
      "Epoch [23/50], Train Loss: 0.0098, Val Loss: 0.0112\n",
      "Epoch [24/50], Train Loss: 0.0091, Val Loss: 0.0103\n",
      "Epoch [25/50], Train Loss: 0.0084, Val Loss: 0.0096\n",
      "Epoch [26/50], Train Loss: 0.0076, Val Loss: 0.0089\n",
      "Epoch [27/50], Train Loss: 0.0069, Val Loss: 0.0083\n",
      "Epoch [28/50], Train Loss: 0.0061, Val Loss: 0.0077\n",
      "Epoch [29/50], Train Loss: 0.0053, Val Loss: 0.0072\n",
      "Epoch [30/50], Train Loss: 0.0045, Val Loss: 0.0067\n",
      "Epoch [31/50], Train Loss: 0.0038, Val Loss: 0.0062\n",
      "Epoch [32/50], Train Loss: 0.0031, Val Loss: 0.0057\n",
      "Epoch [33/50], Train Loss: 0.0026, Val Loss: 0.0052\n",
      "Epoch [34/50], Train Loss: 0.0023, Val Loss: 0.0047\n",
      "Epoch [35/50], Train Loss: 0.0022, Val Loss: 0.0044\n",
      "Epoch [36/50], Train Loss: 0.0021, Val Loss: 0.0042\n",
      "Epoch [37/50], Train Loss: 0.0020, Val Loss: 0.0040\n",
      "Epoch [38/50], Train Loss: 0.0020, Val Loss: 0.0039\n",
      "Epoch [39/50], Train Loss: 0.0019, Val Loss: 0.0037\n",
      "Epoch [40/50], Train Loss: 0.0019, Val Loss: 0.0036\n",
      "Epoch [41/50], Train Loss: 0.0019, Val Loss: 0.0035\n",
      "Epoch [42/50], Train Loss: 0.0019, Val Loss: 0.0034\n",
      "Epoch [43/50], Train Loss: 0.0018, Val Loss: 0.0033\n",
      "Epoch [44/50], Train Loss: 0.0018, Val Loss: 0.0033\n",
      "Epoch [45/50], Train Loss: 0.0018, Val Loss: 0.0032\n",
      "Epoch [46/50], Train Loss: 0.0018, Val Loss: 0.0031\n",
      "Epoch [47/50], Train Loss: 0.0017, Val Loss: 0.0031\n",
      "Epoch [48/50], Train Loss: 0.0017, Val Loss: 0.0030\n",
      "Epoch [49/50], Train Loss: 0.0017, Val Loss: 0.0030\n",
      "Epoch [50/50], Train Loss: 0.0017, Val Loss: 0.0029\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=16, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.2156, Val Loss: 0.4027\n",
      "Epoch [2/50], Train Loss: 0.1127, Val Loss: 0.2573\n",
      "Epoch [3/50], Train Loss: 0.0620, Val Loss: 0.1670\n",
      "Epoch [4/50], Train Loss: 0.0457, Val Loss: 0.1247\n",
      "Epoch [5/50], Train Loss: 0.0450, Val Loss: 0.1056\n",
      "Epoch [6/50], Train Loss: 0.0426, Val Loss: 0.0964\n",
      "Epoch [7/50], Train Loss: 0.0410, Val Loss: 0.0900\n",
      "Epoch [8/50], Train Loss: 0.0403, Val Loss: 0.0851\n",
      "Epoch [9/50], Train Loss: 0.0396, Val Loss: 0.0806\n",
      "Epoch [10/50], Train Loss: 0.0399, Val Loss: 0.0775\n",
      "Epoch [11/50], Train Loss: 0.0350, Val Loss: 0.0742\n",
      "Epoch [12/50], Train Loss: 0.0341, Val Loss: 0.0695\n",
      "Epoch [13/50], Train Loss: 0.0326, Val Loss: 0.0631\n",
      "Epoch [14/50], Train Loss: 0.0306, Val Loss: 0.0589\n",
      "Epoch [15/50], Train Loss: 0.0305, Val Loss: 0.0528\n",
      "Epoch [16/50], Train Loss: 0.0280, Val Loss: 0.0480\n",
      "Epoch [17/50], Train Loss: 0.0252, Val Loss: 0.0406\n",
      "Epoch [18/50], Train Loss: 0.0238, Val Loss: 0.0349\n",
      "Epoch [19/50], Train Loss: 0.0217, Val Loss: 0.0285\n",
      "Epoch [20/50], Train Loss: 0.0196, Val Loss: 0.0229\n",
      "Epoch [21/50], Train Loss: 0.0175, Val Loss: 0.0183\n",
      "Epoch [22/50], Train Loss: 0.0166, Val Loss: 0.0143\n",
      "Epoch [23/50], Train Loss: 0.0158, Val Loss: 0.0126\n",
      "Epoch [24/50], Train Loss: 0.0140, Val Loss: 0.0114\n",
      "Epoch [25/50], Train Loss: 0.0138, Val Loss: 0.0101\n",
      "Epoch [26/50], Train Loss: 0.0133, Val Loss: 0.0102\n",
      "Epoch [27/50], Train Loss: 0.0126, Val Loss: 0.0096\n",
      "Epoch [28/50], Train Loss: 0.0122, Val Loss: 0.0088\n",
      "Epoch [29/50], Train Loss: 0.0118, Val Loss: 0.0088\n",
      "Epoch [30/50], Train Loss: 0.0116, Val Loss: 0.0085\n",
      "Epoch [31/50], Train Loss: 0.0114, Val Loss: 0.0084\n",
      "Epoch [32/50], Train Loss: 0.0111, Val Loss: 0.0080\n",
      "Epoch [33/50], Train Loss: 0.0107, Val Loss: 0.0075\n",
      "Epoch [34/50], Train Loss: 0.0104, Val Loss: 0.0070\n",
      "Epoch [35/50], Train Loss: 0.0104, Val Loss: 0.0071\n",
      "Epoch [36/50], Train Loss: 0.0097, Val Loss: 0.0067\n",
      "Epoch [37/50], Train Loss: 0.0097, Val Loss: 0.0059\n",
      "Epoch [38/50], Train Loss: 0.0091, Val Loss: 0.0060\n",
      "Epoch [39/50], Train Loss: 0.0098, Val Loss: 0.0055\n",
      "Epoch [40/50], Train Loss: 0.0089, Val Loss: 0.0059\n",
      "Epoch [41/50], Train Loss: 0.0084, Val Loss: 0.0062\n",
      "Epoch [42/50], Train Loss: 0.0080, Val Loss: 0.0050\n",
      "Epoch [43/50], Train Loss: 0.0081, Val Loss: 0.0049\n",
      "Epoch [44/50], Train Loss: 0.0080, Val Loss: 0.0052\n",
      "Epoch [45/50], Train Loss: 0.0079, Val Loss: 0.0045\n",
      "Epoch [46/50], Train Loss: 0.0077, Val Loss: 0.0046\n",
      "Epoch [47/50], Train Loss: 0.0075, Val Loss: 0.0040\n",
      "Epoch [48/50], Train Loss: 0.0078, Val Loss: 0.0043\n",
      "Epoch [49/50], Train Loss: 0.0076, Val Loss: 0.0047\n",
      "Epoch [50/50], Train Loss: 0.0072, Val Loss: 0.0047\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=16, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.2188, Val Loss: 0.4017\n",
      "Epoch [2/50], Train Loss: 0.1139, Val Loss: 0.2508\n",
      "Epoch [3/50], Train Loss: 0.0783, Val Loss: 0.1714\n",
      "Epoch [4/50], Train Loss: 0.0679, Val Loss: 0.1322\n",
      "Epoch [5/50], Train Loss: 0.0612, Val Loss: 0.1144\n",
      "Epoch [6/50], Train Loss: 0.0583, Val Loss: 0.1037\n",
      "Epoch [7/50], Train Loss: 0.0543, Val Loss: 0.0968\n",
      "Epoch [8/50], Train Loss: 0.0505, Val Loss: 0.0899\n",
      "Epoch [9/50], Train Loss: 0.0493, Val Loss: 0.0843\n",
      "Epoch [10/50], Train Loss: 0.0462, Val Loss: 0.0783\n",
      "Epoch [11/50], Train Loss: 0.0424, Val Loss: 0.0721\n",
      "Epoch [12/50], Train Loss: 0.0418, Val Loss: 0.0664\n",
      "Epoch [13/50], Train Loss: 0.0402, Val Loss: 0.0619\n",
      "Epoch [14/50], Train Loss: 0.0368, Val Loss: 0.0563\n",
      "Epoch [15/50], Train Loss: 0.0355, Val Loss: 0.0483\n",
      "Epoch [16/50], Train Loss: 0.0332, Val Loss: 0.0433\n",
      "Epoch [17/50], Train Loss: 0.0327, Val Loss: 0.0394\n",
      "Epoch [18/50], Train Loss: 0.0306, Val Loss: 0.0352\n",
      "Epoch [19/50], Train Loss: 0.0272, Val Loss: 0.0315\n",
      "Epoch [20/50], Train Loss: 0.0261, Val Loss: 0.0288\n",
      "Epoch [21/50], Train Loss: 0.0248, Val Loss: 0.0244\n",
      "Epoch [22/50], Train Loss: 0.0223, Val Loss: 0.0231\n",
      "Epoch [23/50], Train Loss: 0.0221, Val Loss: 0.0214\n",
      "Epoch [24/50], Train Loss: 0.0205, Val Loss: 0.0181\n",
      "Epoch [25/50], Train Loss: 0.0209, Val Loss: 0.0190\n",
      "Epoch [26/50], Train Loss: 0.0197, Val Loss: 0.0162\n",
      "Epoch [27/50], Train Loss: 0.0199, Val Loss: 0.0166\n",
      "Epoch [28/50], Train Loss: 0.0189, Val Loss: 0.0162\n",
      "Epoch [29/50], Train Loss: 0.0187, Val Loss: 0.0159\n",
      "Epoch [30/50], Train Loss: 0.0167, Val Loss: 0.0136\n",
      "Epoch [31/50], Train Loss: 0.0177, Val Loss: 0.0144\n",
      "Epoch [32/50], Train Loss: 0.0170, Val Loss: 0.0135\n",
      "Epoch [33/50], Train Loss: 0.0168, Val Loss: 0.0115\n",
      "Epoch [34/50], Train Loss: 0.0170, Val Loss: 0.0142\n",
      "Epoch [35/50], Train Loss: 0.0159, Val Loss: 0.0131\n",
      "Epoch [36/50], Train Loss: 0.0160, Val Loss: 0.0119\n",
      "Epoch [37/50], Train Loss: 0.0158, Val Loss: 0.0123\n",
      "Epoch [38/50], Train Loss: 0.0155, Val Loss: 0.0114\n",
      "Epoch [39/50], Train Loss: 0.0139, Val Loss: 0.0117\n",
      "Epoch [40/50], Train Loss: 0.0143, Val Loss: 0.0120\n",
      "Epoch [41/50], Train Loss: 0.0145, Val Loss: 0.0107\n",
      "Epoch [42/50], Train Loss: 0.0135, Val Loss: 0.0090\n",
      "Epoch [43/50], Train Loss: 0.0130, Val Loss: 0.0099\n",
      "Epoch [44/50], Train Loss: 0.0131, Val Loss: 0.0111\n",
      "Epoch [45/50], Train Loss: 0.0141, Val Loss: 0.0111\n",
      "Epoch [46/50], Train Loss: 0.0129, Val Loss: 0.0108\n",
      "Epoch [47/50], Train Loss: 0.0132, Val Loss: 0.0087\n",
      "Epoch [48/50], Train Loss: 0.0128, Val Loss: 0.0083\n",
      "Epoch [49/50], Train Loss: 0.0130, Val Loss: 0.0110\n",
      "Epoch [50/50], Train Loss: 0.0130, Val Loss: 0.0090\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=16, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0663, Val Loss: 0.1565\n",
      "Epoch [2/50], Train Loss: 0.0349, Val Loss: 0.0863\n",
      "Epoch [3/50], Train Loss: 0.0348, Val Loss: 0.0737\n",
      "Epoch [4/50], Train Loss: 0.0339, Val Loss: 0.0699\n",
      "Epoch [5/50], Train Loss: 0.0318, Val Loss: 0.0641\n",
      "Epoch [6/50], Train Loss: 0.0295, Val Loss: 0.0561\n",
      "Epoch [7/50], Train Loss: 0.0267, Val Loss: 0.0457\n",
      "Epoch [8/50], Train Loss: 0.0229, Val Loss: 0.0323\n",
      "Epoch [9/50], Train Loss: 0.0181, Val Loss: 0.0189\n",
      "Epoch [10/50], Train Loss: 0.0132, Val Loss: 0.0147\n",
      "Epoch [11/50], Train Loss: 0.0088, Val Loss: 0.0160\n",
      "Epoch [12/50], Train Loss: 0.0064, Val Loss: 0.0085\n",
      "Epoch [13/50], Train Loss: 0.0055, Val Loss: 0.0078\n",
      "Epoch [14/50], Train Loss: 0.0035, Val Loss: 0.0069\n",
      "Epoch [15/50], Train Loss: 0.0040, Val Loss: 0.0092\n",
      "Epoch [16/50], Train Loss: 0.0035, Val Loss: 0.0056\n",
      "Epoch [17/50], Train Loss: 0.0047, Val Loss: 0.0080\n",
      "Epoch [18/50], Train Loss: 0.0035, Val Loss: 0.0074\n",
      "Epoch [19/50], Train Loss: 0.0046, Val Loss: 0.0067\n",
      "Epoch [20/50], Train Loss: 0.0042, Val Loss: 0.0064\n",
      "Epoch [21/50], Train Loss: 0.0032, Val Loss: 0.0044\n",
      "Epoch [22/50], Train Loss: 0.0039, Val Loss: 0.0072\n",
      "Epoch [23/50], Train Loss: 0.0041, Val Loss: 0.0074\n",
      "Epoch [24/50], Train Loss: 0.0042, Val Loss: 0.0045\n",
      "Epoch [25/50], Train Loss: 0.0033, Val Loss: 0.0067\n",
      "Epoch [26/50], Train Loss: 0.0045, Val Loss: 0.0091\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=16, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.2491, Val Loss: 0.3462\n",
      "Epoch [2/50], Train Loss: 0.1313, Val Loss: 0.2174\n",
      "Epoch [3/50], Train Loss: 0.0771, Val Loss: 0.1264\n",
      "Epoch [4/50], Train Loss: 0.0599, Val Loss: 0.0863\n",
      "Epoch [5/50], Train Loss: 0.0520, Val Loss: 0.0699\n",
      "Epoch [6/50], Train Loss: 0.0476, Val Loss: 0.0635\n",
      "Epoch [7/50], Train Loss: 0.0454, Val Loss: 0.0544\n",
      "Epoch [8/50], Train Loss: 0.0427, Val Loss: 0.0450\n",
      "Epoch [9/50], Train Loss: 0.0397, Val Loss: 0.0397\n",
      "Epoch [10/50], Train Loss: 0.0373, Val Loss: 0.0316\n",
      "Epoch [11/50], Train Loss: 0.0361, Val Loss: 0.0275\n",
      "Epoch [12/50], Train Loss: 0.0317, Val Loss: 0.0238\n",
      "Epoch [13/50], Train Loss: 0.0313, Val Loss: 0.0193\n",
      "Epoch [14/50], Train Loss: 0.0299, Val Loss: 0.0198\n",
      "Epoch [15/50], Train Loss: 0.0287, Val Loss: 0.0174\n",
      "Epoch [16/50], Train Loss: 0.0280, Val Loss: 0.0180\n",
      "Epoch [17/50], Train Loss: 0.0273, Val Loss: 0.0142\n",
      "Epoch [18/50], Train Loss: 0.0265, Val Loss: 0.0156\n",
      "Epoch [19/50], Train Loss: 0.0236, Val Loss: 0.0136\n",
      "Epoch [20/50], Train Loss: 0.0234, Val Loss: 0.0121\n",
      "Epoch [21/50], Train Loss: 0.0215, Val Loss: 0.0136\n",
      "Epoch [22/50], Train Loss: 0.0198, Val Loss: 0.0134\n",
      "Epoch [23/50], Train Loss: 0.0183, Val Loss: 0.0098\n",
      "Epoch [24/50], Train Loss: 0.0175, Val Loss: 0.0099\n",
      "Epoch [25/50], Train Loss: 0.0166, Val Loss: 0.0110\n",
      "Epoch [26/50], Train Loss: 0.0158, Val Loss: 0.0093\n",
      "Epoch [27/50], Train Loss: 0.0151, Val Loss: 0.0099\n",
      "Epoch [28/50], Train Loss: 0.0148, Val Loss: 0.0095\n",
      "Epoch [29/50], Train Loss: 0.0133, Val Loss: 0.0095\n",
      "Epoch [30/50], Train Loss: 0.0136, Val Loss: 0.0094\n",
      "Epoch [31/50], Train Loss: 0.0130, Val Loss: 0.0079\n",
      "Epoch [32/50], Train Loss: 0.0135, Val Loss: 0.0106\n",
      "Epoch [33/50], Train Loss: 0.0137, Val Loss: 0.0101\n",
      "Epoch [34/50], Train Loss: 0.0126, Val Loss: 0.0066\n",
      "Epoch [35/50], Train Loss: 0.0130, Val Loss: 0.0091\n",
      "Epoch [36/50], Train Loss: 0.0126, Val Loss: 0.0108\n",
      "Epoch [37/50], Train Loss: 0.0125, Val Loss: 0.0055\n",
      "Epoch [38/50], Train Loss: 0.0122, Val Loss: 0.0072\n",
      "Epoch [39/50], Train Loss: 0.0123, Val Loss: 0.0124\n",
      "Epoch [40/50], Train Loss: 0.0114, Val Loss: 0.0074\n",
      "Epoch [41/50], Train Loss: 0.0121, Val Loss: 0.0064\n",
      "Epoch [42/50], Train Loss: 0.0110, Val Loss: 0.0108\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=16, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1784, Val Loss: 0.2649\n",
      "Epoch [2/50], Train Loss: 0.1160, Val Loss: 0.1784\n",
      "Epoch [3/50], Train Loss: 0.0948, Val Loss: 0.1437\n",
      "Epoch [4/50], Train Loss: 0.0824, Val Loss: 0.1175\n",
      "Epoch [5/50], Train Loss: 0.0735, Val Loss: 0.0950\n",
      "Epoch [6/50], Train Loss: 0.0648, Val Loss: 0.0740\n",
      "Epoch [7/50], Train Loss: 0.0562, Val Loss: 0.0568\n",
      "Epoch [8/50], Train Loss: 0.0530, Val Loss: 0.0471\n",
      "Epoch [9/50], Train Loss: 0.0488, Val Loss: 0.0418\n",
      "Epoch [10/50], Train Loss: 0.0451, Val Loss: 0.0352\n",
      "Epoch [11/50], Train Loss: 0.0432, Val Loss: 0.0326\n",
      "Epoch [12/50], Train Loss: 0.0441, Val Loss: 0.0334\n",
      "Epoch [13/50], Train Loss: 0.0406, Val Loss: 0.0349\n",
      "Epoch [14/50], Train Loss: 0.0374, Val Loss: 0.0299\n",
      "Epoch [15/50], Train Loss: 0.0351, Val Loss: 0.0270\n",
      "Epoch [16/50], Train Loss: 0.0329, Val Loss: 0.0279\n",
      "Epoch [17/50], Train Loss: 0.0314, Val Loss: 0.0283\n",
      "Epoch [18/50], Train Loss: 0.0301, Val Loss: 0.0212\n",
      "Epoch [19/50], Train Loss: 0.0284, Val Loss: 0.0250\n",
      "Epoch [20/50], Train Loss: 0.0274, Val Loss: 0.0281\n",
      "Epoch [21/50], Train Loss: 0.0256, Val Loss: 0.0182\n",
      "Epoch [22/50], Train Loss: 0.0234, Val Loss: 0.0121\n",
      "Epoch [23/50], Train Loss: 0.0241, Val Loss: 0.0269\n",
      "Epoch [24/50], Train Loss: 0.0225, Val Loss: 0.0178\n",
      "Epoch [25/50], Train Loss: 0.0224, Val Loss: 0.0089\n",
      "Epoch [26/50], Train Loss: 0.0206, Val Loss: 0.0142\n",
      "Epoch [27/50], Train Loss: 0.0245, Val Loss: 0.0293\n",
      "Epoch [28/50], Train Loss: 0.0217, Val Loss: 0.0124\n",
      "Epoch [29/50], Train Loss: 0.0229, Val Loss: 0.0075\n",
      "Epoch [30/50], Train Loss: 0.0224, Val Loss: 0.0256\n",
      "Epoch [31/50], Train Loss: 0.0201, Val Loss: 0.0134\n",
      "Epoch [32/50], Train Loss: 0.0204, Val Loss: 0.0087\n",
      "Epoch [33/50], Train Loss: 0.0179, Val Loss: 0.0161\n",
      "Epoch [34/50], Train Loss: 0.0167, Val Loss: 0.0183\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=16, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0874, Val Loss: 0.1583\n",
      "Epoch [2/50], Train Loss: 0.0409, Val Loss: 0.1079\n",
      "Epoch [3/50], Train Loss: 0.0394, Val Loss: 0.0993\n",
      "Epoch [4/50], Train Loss: 0.0396, Val Loss: 0.0988\n",
      "Epoch [5/50], Train Loss: 0.0386, Val Loss: 0.0975\n",
      "Epoch [6/50], Train Loss: 0.0374, Val Loss: 0.0954\n",
      "Epoch [7/50], Train Loss: 0.0360, Val Loss: 0.0922\n",
      "Epoch [8/50], Train Loss: 0.0340, Val Loss: 0.0872\n",
      "Epoch [9/50], Train Loss: 0.0311, Val Loss: 0.0787\n",
      "Epoch [10/50], Train Loss: 0.0264, Val Loss: 0.0655\n",
      "Epoch [11/50], Train Loss: 0.0211, Val Loss: 0.0521\n",
      "Epoch [12/50], Train Loss: 0.0185, Val Loss: 0.0434\n",
      "Epoch [13/50], Train Loss: 0.0126, Val Loss: 0.0360\n",
      "Epoch [14/50], Train Loss: 0.0073, Val Loss: 0.0327\n",
      "Epoch [15/50], Train Loss: 0.0061, Val Loss: 0.0251\n",
      "Epoch [16/50], Train Loss: 0.0041, Val Loss: 0.0227\n",
      "Epoch [17/50], Train Loss: 0.0035, Val Loss: 0.0245\n",
      "Epoch [18/50], Train Loss: 0.0026, Val Loss: 0.0211\n",
      "Epoch [19/50], Train Loss: 0.0025, Val Loss: 0.0194\n",
      "Epoch [20/50], Train Loss: 0.0030, Val Loss: 0.0206\n",
      "Epoch [21/50], Train Loss: 0.0027, Val Loss: 0.0203\n",
      "Epoch [22/50], Train Loss: 0.0024, Val Loss: 0.0170\n",
      "Epoch [23/50], Train Loss: 0.0025, Val Loss: 0.0180\n",
      "Epoch [24/50], Train Loss: 0.0024, Val Loss: 0.0191\n",
      "Epoch [25/50], Train Loss: 0.0026, Val Loss: 0.0165\n",
      "Epoch [26/50], Train Loss: 0.0023, Val Loss: 0.0155\n",
      "Epoch [27/50], Train Loss: 0.0025, Val Loss: 0.0182\n",
      "Epoch [28/50], Train Loss: 0.0026, Val Loss: 0.0169\n",
      "Epoch [29/50], Train Loss: 0.0023, Val Loss: 0.0141\n",
      "Epoch [30/50], Train Loss: 0.0025, Val Loss: 0.0160\n",
      "Epoch [31/50], Train Loss: 0.0023, Val Loss: 0.0179\n",
      "Epoch [32/50], Train Loss: 0.0026, Val Loss: 0.0142\n",
      "Epoch [33/50], Train Loss: 0.0022, Val Loss: 0.0134\n",
      "Epoch [34/50], Train Loss: 0.0024, Val Loss: 0.0170\n",
      "Epoch [35/50], Train Loss: 0.0025, Val Loss: 0.0158\n",
      "Epoch [36/50], Train Loss: 0.0023, Val Loss: 0.0124\n",
      "Epoch [37/50], Train Loss: 0.0024, Val Loss: 0.0141\n",
      "Epoch [38/50], Train Loss: 0.0023, Val Loss: 0.0171\n",
      "Epoch [39/50], Train Loss: 0.0025, Val Loss: 0.0132\n",
      "Epoch [40/50], Train Loss: 0.0021, Val Loss: 0.0117\n",
      "Epoch [41/50], Train Loss: 0.0024, Val Loss: 0.0152\n",
      "Epoch [42/50], Train Loss: 0.0024, Val Loss: 0.0154\n",
      "Epoch [43/50], Train Loss: 0.0023, Val Loss: 0.0113\n",
      "Epoch [44/50], Train Loss: 0.0022, Val Loss: 0.0121\n",
      "Epoch [45/50], Train Loss: 0.0022, Val Loss: 0.0157\n",
      "Epoch [46/50], Train Loss: 0.0024, Val Loss: 0.0130\n",
      "Epoch [47/50], Train Loss: 0.0021, Val Loss: 0.0104\n",
      "Epoch [48/50], Train Loss: 0.0023, Val Loss: 0.0130\n",
      "Epoch [49/50], Train Loss: 0.0022, Val Loss: 0.0147\n",
      "Epoch [50/50], Train Loss: 0.0022, Val Loss: 0.0108\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=16, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0892, Val Loss: 0.1932\n",
      "Epoch [2/50], Train Loss: 0.0581, Val Loss: 0.1325\n",
      "Epoch [3/50], Train Loss: 0.0485, Val Loss: 0.1055\n",
      "Epoch [4/50], Train Loss: 0.0491, Val Loss: 0.0975\n",
      "Epoch [5/50], Train Loss: 0.0465, Val Loss: 0.0883\n",
      "Epoch [6/50], Train Loss: 0.0439, Val Loss: 0.0815\n",
      "Epoch [7/50], Train Loss: 0.0367, Val Loss: 0.0736\n",
      "Epoch [8/50], Train Loss: 0.0351, Val Loss: 0.0622\n",
      "Epoch [9/50], Train Loss: 0.0309, Val Loss: 0.0516\n",
      "Epoch [10/50], Train Loss: 0.0274, Val Loss: 0.0384\n",
      "Epoch [11/50], Train Loss: 0.0230, Val Loss: 0.0317\n",
      "Epoch [12/50], Train Loss: 0.0204, Val Loss: 0.0245\n",
      "Epoch [13/50], Train Loss: 0.0191, Val Loss: 0.0176\n",
      "Epoch [14/50], Train Loss: 0.0182, Val Loss: 0.0154\n",
      "Epoch [15/50], Train Loss: 0.0167, Val Loss: 0.0165\n",
      "Epoch [16/50], Train Loss: 0.0162, Val Loss: 0.0167\n",
      "Epoch [17/50], Train Loss: 0.0155, Val Loss: 0.0094\n",
      "Epoch [18/50], Train Loss: 0.0144, Val Loss: 0.0101\n",
      "Epoch [19/50], Train Loss: 0.0149, Val Loss: 0.0171\n",
      "Epoch [20/50], Train Loss: 0.0137, Val Loss: 0.0104\n",
      "Epoch [21/50], Train Loss: 0.0136, Val Loss: 0.0079\n",
      "Epoch [22/50], Train Loss: 0.0143, Val Loss: 0.0141\n",
      "Epoch [23/50], Train Loss: 0.0138, Val Loss: 0.0178\n",
      "Epoch [24/50], Train Loss: 0.0143, Val Loss: 0.0059\n",
      "Epoch [25/50], Train Loss: 0.0145, Val Loss: 0.0075\n",
      "Epoch [26/50], Train Loss: 0.0147, Val Loss: 0.0191\n",
      "Epoch [27/50], Train Loss: 0.0130, Val Loss: 0.0069\n",
      "Epoch [28/50], Train Loss: 0.0127, Val Loss: 0.0068\n",
      "Epoch [29/50], Train Loss: 0.0131, Val Loss: 0.0196\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=16, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1108, Val Loss: 0.2002\n",
      "Epoch [2/50], Train Loss: 0.0775, Val Loss: 0.1609\n",
      "Epoch [3/50], Train Loss: 0.0717, Val Loss: 0.1434\n",
      "Epoch [4/50], Train Loss: 0.0641, Val Loss: 0.1343\n",
      "Epoch [5/50], Train Loss: 0.0616, Val Loss: 0.1185\n",
      "Epoch [6/50], Train Loss: 0.0535, Val Loss: 0.1002\n",
      "Epoch [7/50], Train Loss: 0.0489, Val Loss: 0.0765\n",
      "Epoch [8/50], Train Loss: 0.0432, Val Loss: 0.0519\n",
      "Epoch [9/50], Train Loss: 0.0405, Val Loss: 0.0419\n",
      "Epoch [10/50], Train Loss: 0.0374, Val Loss: 0.0438\n",
      "Epoch [11/50], Train Loss: 0.0346, Val Loss: 0.0393\n",
      "Epoch [12/50], Train Loss: 0.0324, Val Loss: 0.0221\n",
      "Epoch [13/50], Train Loss: 0.0332, Val Loss: 0.0165\n",
      "Epoch [14/50], Train Loss: 0.0329, Val Loss: 0.0387\n",
      "Epoch [15/50], Train Loss: 0.0307, Val Loss: 0.0312\n",
      "Epoch [16/50], Train Loss: 0.0297, Val Loss: 0.0159\n",
      "Epoch [17/50], Train Loss: 0.0268, Val Loss: 0.0166\n",
      "Epoch [18/50], Train Loss: 0.0262, Val Loss: 0.0354\n",
      "Epoch [19/50], Train Loss: 0.0276, Val Loss: 0.0215\n",
      "Epoch [20/50], Train Loss: 0.0256, Val Loss: 0.0153\n",
      "Epoch [21/50], Train Loss: 0.0228, Val Loss: 0.0275\n",
      "Epoch [22/50], Train Loss: 0.0229, Val Loss: 0.0196\n",
      "Epoch [23/50], Train Loss: 0.0226, Val Loss: 0.0157\n",
      "Epoch [24/50], Train Loss: 0.0204, Val Loss: 0.0260\n",
      "Epoch [25/50], Train Loss: 0.0201, Val Loss: 0.0148\n",
      "Epoch [26/50], Train Loss: 0.0193, Val Loss: 0.0161\n",
      "Epoch [27/50], Train Loss: 0.0171, Val Loss: 0.0228\n",
      "Epoch [28/50], Train Loss: 0.0177, Val Loss: 0.0126\n",
      "Epoch [29/50], Train Loss: 0.0173, Val Loss: 0.0133\n",
      "Epoch [30/50], Train Loss: 0.0177, Val Loss: 0.0210\n",
      "Epoch [31/50], Train Loss: 0.0181, Val Loss: 0.0138\n",
      "Epoch [32/50], Train Loss: 0.0174, Val Loss: 0.0177\n",
      "Epoch [33/50], Train Loss: 0.0165, Val Loss: 0.0205\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=32, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1457, Val Loss: 0.2627\n",
      "Epoch [2/50], Train Loss: 0.0502, Val Loss: 0.0711\n",
      "Epoch [3/50], Train Loss: 0.0306, Val Loss: 0.0462\n",
      "Epoch [4/50], Train Loss: 0.0274, Val Loss: 0.0385\n",
      "Epoch [5/50], Train Loss: 0.0224, Val Loss: 0.0274\n",
      "Epoch [6/50], Train Loss: 0.0180, Val Loss: 0.0178\n",
      "Epoch [7/50], Train Loss: 0.0136, Val Loss: 0.0121\n",
      "Epoch [8/50], Train Loss: 0.0113, Val Loss: 0.0128\n",
      "Epoch [9/50], Train Loss: 0.0097, Val Loss: 0.0130\n",
      "Epoch [10/50], Train Loss: 0.0080, Val Loss: 0.0120\n",
      "Epoch [11/50], Train Loss: 0.0067, Val Loss: 0.0114\n",
      "Epoch [12/50], Train Loss: 0.0051, Val Loss: 0.0095\n",
      "Epoch [13/50], Train Loss: 0.0037, Val Loss: 0.0074\n",
      "Epoch [14/50], Train Loss: 0.0029, Val Loss: 0.0068\n",
      "Epoch [15/50], Train Loss: 0.0027, Val Loss: 0.0080\n",
      "Epoch [16/50], Train Loss: 0.0024, Val Loss: 0.0080\n",
      "Epoch [17/50], Train Loss: 0.0023, Val Loss: 0.0063\n",
      "Epoch [18/50], Train Loss: 0.0022, Val Loss: 0.0065\n",
      "Epoch [19/50], Train Loss: 0.0023, Val Loss: 0.0076\n",
      "Epoch [20/50], Train Loss: 0.0021, Val Loss: 0.0066\n",
      "Epoch [21/50], Train Loss: 0.0021, Val Loss: 0.0055\n",
      "Epoch [22/50], Train Loss: 0.0022, Val Loss: 0.0070\n",
      "Epoch [23/50], Train Loss: 0.0021, Val Loss: 0.0069\n",
      "Epoch [24/50], Train Loss: 0.0021, Val Loss: 0.0048\n",
      "Epoch [25/50], Train Loss: 0.0021, Val Loss: 0.0050\n",
      "Epoch [26/50], Train Loss: 0.0025, Val Loss: 0.0084\n",
      "Epoch [27/50], Train Loss: 0.0020, Val Loss: 0.0050\n",
      "Epoch [28/50], Train Loss: 0.0025, Val Loss: 0.0029\n",
      "Epoch [29/50], Train Loss: 0.0024, Val Loss: 0.0059\n",
      "Epoch [30/50], Train Loss: 0.0027, Val Loss: 0.0091\n",
      "Epoch [31/50], Train Loss: 0.0026, Val Loss: 0.0028\n",
      "Epoch [32/50], Train Loss: 0.0031, Val Loss: 0.0020\n",
      "Epoch [33/50], Train Loss: 0.0032, Val Loss: 0.0094\n",
      "Epoch [34/50], Train Loss: 0.0025, Val Loss: 0.0061\n",
      "Epoch [35/50], Train Loss: 0.0036, Val Loss: 0.0021\n",
      "Epoch [36/50], Train Loss: 0.0027, Val Loss: 0.0029\n",
      "Epoch [37/50], Train Loss: 0.0033, Val Loss: 0.0091\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=32, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1082, Val Loss: 0.2038\n",
      "Epoch [2/50], Train Loss: 0.0472, Val Loss: 0.0860\n",
      "Epoch [3/50], Train Loss: 0.0423, Val Loss: 0.0700\n",
      "Epoch [4/50], Train Loss: 0.0379, Val Loss: 0.0614\n",
      "Epoch [5/50], Train Loss: 0.0336, Val Loss: 0.0521\n",
      "Epoch [6/50], Train Loss: 0.0293, Val Loss: 0.0428\n",
      "Epoch [7/50], Train Loss: 0.0263, Val Loss: 0.0332\n",
      "Epoch [8/50], Train Loss: 0.0222, Val Loss: 0.0232\n",
      "Epoch [9/50], Train Loss: 0.0179, Val Loss: 0.0162\n",
      "Epoch [10/50], Train Loss: 0.0151, Val Loss: 0.0102\n",
      "Epoch [11/50], Train Loss: 0.0125, Val Loss: 0.0089\n",
      "Epoch [12/50], Train Loss: 0.0116, Val Loss: 0.0070\n",
      "Epoch [13/50], Train Loss: 0.0095, Val Loss: 0.0064\n",
      "Epoch [14/50], Train Loss: 0.0079, Val Loss: 0.0050\n",
      "Epoch [15/50], Train Loss: 0.0074, Val Loss: 0.0046\n",
      "Epoch [16/50], Train Loss: 0.0073, Val Loss: 0.0060\n",
      "Epoch [17/50], Train Loss: 0.0070, Val Loss: 0.0039\n",
      "Epoch [18/50], Train Loss: 0.0063, Val Loss: 0.0045\n",
      "Epoch [19/50], Train Loss: 0.0063, Val Loss: 0.0063\n",
      "Epoch [20/50], Train Loss: 0.0063, Val Loss: 0.0047\n",
      "Epoch [21/50], Train Loss: 0.0062, Val Loss: 0.0046\n",
      "Epoch [22/50], Train Loss: 0.0058, Val Loss: 0.0052\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=32, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0927, Val Loss: 0.1411\n",
      "Epoch [2/50], Train Loss: 0.0460, Val Loss: 0.0649\n",
      "Epoch [3/50], Train Loss: 0.0382, Val Loss: 0.0497\n",
      "Epoch [4/50], Train Loss: 0.0319, Val Loss: 0.0321\n",
      "Epoch [5/50], Train Loss: 0.0282, Val Loss: 0.0241\n",
      "Epoch [6/50], Train Loss: 0.0235, Val Loss: 0.0180\n",
      "Epoch [7/50], Train Loss: 0.0218, Val Loss: 0.0131\n",
      "Epoch [8/50], Train Loss: 0.0192, Val Loss: 0.0134\n",
      "Epoch [9/50], Train Loss: 0.0162, Val Loss: 0.0145\n",
      "Epoch [10/50], Train Loss: 0.0141, Val Loss: 0.0110\n",
      "Epoch [11/50], Train Loss: 0.0135, Val Loss: 0.0071\n",
      "Epoch [12/50], Train Loss: 0.0119, Val Loss: 0.0094\n",
      "Epoch [13/50], Train Loss: 0.0123, Val Loss: 0.0113\n",
      "Epoch [14/50], Train Loss: 0.0119, Val Loss: 0.0048\n",
      "Epoch [15/50], Train Loss: 0.0120, Val Loss: 0.0102\n",
      "Epoch [16/50], Train Loss: 0.0121, Val Loss: 0.0141\n",
      "Epoch [17/50], Train Loss: 0.0107, Val Loss: 0.0057\n",
      "Epoch [18/50], Train Loss: 0.0113, Val Loss: 0.0096\n",
      "Epoch [19/50], Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=32, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0451, Val Loss: 0.0429\n",
      "Epoch [2/50], Train Loss: 0.0525, Val Loss: 0.0762\n",
      "Epoch [3/50], Train Loss: 0.0322, Val Loss: 0.0502\n",
      "Epoch [4/50], Train Loss: 0.0290, Val Loss: 0.0387\n",
      "Epoch [5/50], Train Loss: 0.0220, Val Loss: 0.0226\n",
      "Epoch [6/50], Train Loss: 0.0135, Val Loss: 0.0104\n",
      "Epoch [7/50], Train Loss: 0.0068, Val Loss: 0.0106\n",
      "Epoch [8/50], Train Loss: 0.0063, Val Loss: 0.0123\n",
      "Epoch [9/50], Train Loss: 0.0037, Val Loss: 0.0084\n",
      "Epoch [10/50], Train Loss: 0.0028, Val Loss: 0.0084\n",
      "Epoch [11/50], Train Loss: 0.0027, Val Loss: 0.0093\n",
      "Epoch [12/50], Train Loss: 0.0022, Val Loss: 0.0077\n",
      "Epoch [13/50], Train Loss: 0.0023, Val Loss: 0.0060\n",
      "Epoch [14/50], Train Loss: 0.0025, Val Loss: 0.0076\n",
      "Epoch [15/50], Train Loss: 0.0025, Val Loss: 0.0092\n",
      "Epoch [16/50], Train Loss: 0.0021, Val Loss: 0.0065\n",
      "Epoch [17/50], Train Loss: 0.0023, Val Loss: 0.0050\n",
      "Epoch [18/50], Train Loss: 0.0026, Val Loss: 0.0078\n",
      "Epoch [19/50], Train Loss: 0.0024, Val Loss: 0.0094\n",
      "Epoch [20/50], Train Loss: 0.0021, Val Loss: 0.0053\n",
      "Epoch [21/50], Train Loss: 0.0024, Val Loss: 0.0042\n",
      "Epoch [22/50], Train Loss: 0.0030, Val Loss: 0.0081\n",
      "Epoch [23/50], Train Loss: 0.0024, Val Loss: 0.0098\n",
      "Epoch [24/50], Train Loss: 0.0022, Val Loss: 0.0043\n",
      "Epoch [25/50], Train Loss: 0.0025, Val Loss: 0.0037\n",
      "Epoch [26/50], Train Loss: 0.0033, Val Loss: 0.0084\n",
      "Epoch [27/50], Train Loss: 0.0023, Val Loss: 0.0098\n",
      "Epoch [28/50], Train Loss: 0.0022, Val Loss: 0.0034\n",
      "Epoch [29/50], Train Loss: 0.0024, Val Loss: 0.0033\n",
      "Epoch [30/50], Train Loss: 0.0034, Val Loss: 0.0087\n",
      "Epoch [31/50], Train Loss: 0.0022, Val Loss: 0.0087\n",
      "Epoch [32/50], Train Loss: 0.0022, Val Loss: 0.0026\n",
      "Epoch [33/50], Train Loss: 0.0023, Val Loss: 0.0036\n",
      "Epoch [34/50], Train Loss: 0.0035, Val Loss: 0.0092\n",
      "Epoch [35/50], Train Loss: 0.0023, Val Loss: 0.0066\n",
      "Epoch [36/50], Train Loss: 0.0024, Val Loss: 0.0022\n",
      "Epoch [37/50], Train Loss: 0.0023, Val Loss: 0.0050\n",
      "Epoch [38/50], Train Loss: 0.0031, Val Loss: 0.0094\n",
      "Epoch [39/50], Train Loss: 0.0024, Val Loss: 0.0043\n",
      "Epoch [40/50], Train Loss: 0.0026, Val Loss: 0.0023\n",
      "Epoch [41/50], Train Loss: 0.0026, Val Loss: 0.0070\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=32, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0603, Val Loss: 0.0989\n",
      "Epoch [2/50], Train Loss: 0.0461, Val Loss: 0.0751\n",
      "Epoch [3/50], Train Loss: 0.0402, Val Loss: 0.0670\n",
      "Epoch [4/50], Train Loss: 0.0335, Val Loss: 0.0518\n",
      "Epoch [5/50], Train Loss: 0.0282, Val Loss: 0.0393\n",
      "Epoch [6/50], Train Loss: 0.0238, Val Loss: 0.0314\n",
      "Epoch [7/50], Train Loss: 0.0204, Val Loss: 0.0245\n",
      "Epoch [8/50], Train Loss: 0.0160, Val Loss: 0.0149\n",
      "Epoch [9/50], Train Loss: 0.0116, Val Loss: 0.0086\n",
      "Epoch [10/50], Train Loss: 0.0104, Val Loss: 0.0110\n",
      "Epoch [11/50], Train Loss: 0.0095, Val Loss: 0.0095\n",
      "Epoch [12/50], Train Loss: 0.0085, Val Loss: 0.0057\n",
      "Epoch [13/50], Train Loss: 0.0084, Val Loss: 0.0087\n",
      "Epoch [14/50], Train Loss: 0.0085, Val Loss: 0.0075\n",
      "Epoch [15/50], Train Loss: 0.0079, Val Loss: 0.0073\n",
      "Epoch [16/50], Train Loss: 0.0080, Val Loss: 0.0093\n",
      "Epoch [17/50], Train Loss: 0.0071, Val Loss: 0.0085\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=32, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1014, Val Loss: 0.1463\n",
      "Epoch [2/50], Train Loss: 0.0670, Val Loss: 0.0918\n",
      "Epoch [3/50], Train Loss: 0.0556, Val Loss: 0.0630\n",
      "Epoch [4/50], Train Loss: 0.0438, Val Loss: 0.0410\n",
      "Epoch [5/50], Train Loss: 0.0361, Val Loss: 0.0311\n",
      "Epoch [6/50], Train Loss: 0.0336, Val Loss: 0.0289\n",
      "Epoch [7/50], Train Loss: 0.0303, Val Loss: 0.0171\n",
      "Epoch [8/50], Train Loss: 0.0292, Val Loss: 0.0166\n",
      "Epoch [9/50], Train Loss: 0.0238, Val Loss: 0.0155\n",
      "Epoch [10/50], Train Loss: 0.0210, Val Loss: 0.0133\n",
      "Epoch [11/50], Train Loss: 0.0192, Val Loss: 0.0106\n",
      "Epoch [12/50], Train Loss: 0.0184, Val Loss: 0.0135\n",
      "Epoch [13/50], Train Loss: 0.0181, Val Loss: 0.0103\n",
      "Epoch [14/50], Train Loss: 0.0167, Val Loss: 0.0101\n",
      "Epoch [15/50], Train Loss: 0.0163, Val Loss: 0.0094\n",
      "Epoch [16/50], Train Loss: 0.0164, Val Loss: 0.0070\n",
      "Epoch [17/50], Train Loss: 0.0165, Val Loss: 0.0206\n",
      "Epoch [18/50], Train Loss: 0.0156, Val Loss: 0.0071\n",
      "Epoch [19/50], Train Loss: 0.0160, Val Loss: 0.0047\n",
      "Epoch [20/50], Train Loss: 0.0148, Val Loss: 0.0167\n",
      "Epoch [21/50], Train Loss: 0.0147, Val Loss: 0.0144\n",
      "Epoch [22/50], Train Loss: 0.0160, Val Loss: 0.0044\n",
      "Epoch [23/50], Train Loss: 0.0147, Val Loss: 0.0090\n",
      "Epoch [24/50], Train Loss: 0.0146, Val Loss: 0.0185\n",
      "Epoch [25/50], Train Loss: 0.0139, Val Loss: 0.0049\n",
      "Epoch [26/50], Train Loss: 0.0143, Val Loss: 0.0048\n",
      "Epoch [27/50], Train Loss: 0.0146, Val Loss: 0.0223\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=32, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0628, Val Loss: 0.0783\n",
      "Epoch [2/50], Train Loss: 0.0571, Val Loss: 0.0731\n",
      "Epoch [3/50], Train Loss: 0.0493, Val Loss: 0.0924\n",
      "Epoch [4/50], Train Loss: 0.0411, Val Loss: 0.0806\n",
      "Epoch [5/50], Train Loss: 0.0397, Val Loss: 0.0732\n",
      "Epoch [6/50], Train Loss: 0.0359, Val Loss: 0.0607\n",
      "Epoch [7/50], Train Loss: 0.0295, Val Loss: 0.0410\n",
      "Epoch [8/50], Train Loss: 0.0183, Val Loss: 0.0237\n",
      "Epoch [9/50], Train Loss: 0.0185, Val Loss: 0.0426\n",
      "Epoch [10/50], Train Loss: 0.0138, Val Loss: 0.0185\n",
      "Epoch [11/50], Train Loss: 0.0052, Val Loss: 0.0183\n",
      "Epoch [12/50], Train Loss: 0.0064, Val Loss: 0.0186\n",
      "Epoch [13/50], Train Loss: 0.0049, Val Loss: 0.0219\n",
      "Epoch [14/50], Train Loss: 0.0036, Val Loss: 0.0151\n",
      "Epoch [15/50], Train Loss: 0.0035, Val Loss: 0.0137\n",
      "Epoch [16/50], Train Loss: 0.0047, Val Loss: 0.0173\n",
      "Epoch [17/50], Train Loss: 0.0029, Val Loss: 0.0152\n",
      "Epoch [18/50], Train Loss: 0.0029, Val Loss: 0.0122\n",
      "Epoch [19/50], Train Loss: 0.0041, Val Loss: 0.0157\n",
      "Epoch [20/50], Train Loss: 0.0026, Val Loss: 0.0159\n",
      "Epoch [21/50], Train Loss: 0.0026, Val Loss: 0.0110\n",
      "Epoch [22/50], Train Loss: 0.0039, Val Loss: 0.0140\n",
      "Epoch [23/50], Train Loss: 0.0026, Val Loss: 0.0167\n",
      "Epoch [24/50], Train Loss: 0.0025, Val Loss: 0.0109\n",
      "Epoch [25/50], Train Loss: 0.0030, Val Loss: 0.0120\n",
      "Epoch [26/50], Train Loss: 0.0030, Val Loss: 0.0168\n",
      "Epoch [27/50], Train Loss: 0.0026, Val Loss: 0.0130\n",
      "Epoch [28/50], Train Loss: 0.0025, Val Loss: 0.0099\n",
      "Epoch [29/50], Train Loss: 0.0036, Val Loss: 0.0190\n",
      "Epoch [30/50], Train Loss: 0.0030, Val Loss: 0.0188\n",
      "Epoch [31/50], Train Loss: 0.0035, Val Loss: 0.0109\n",
      "Epoch [32/50], Train Loss: 0.0039, Val Loss: 0.0194\n",
      "Epoch [33/50], Train Loss: 0.0029, Val Loss: 0.0223\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=32, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0481, Val Loss: 0.0445\n",
      "Epoch [2/50], Train Loss: 0.0843, Val Loss: 0.1016\n",
      "Epoch [3/50], Train Loss: 0.0515, Val Loss: 0.0770\n",
      "Epoch [4/50], Train Loss: 0.0490, Val Loss: 0.0669\n",
      "Epoch [5/50], Train Loss: 0.0434, Val Loss: 0.0466\n",
      "Epoch [6/50], Train Loss: 0.0326, Val Loss: 0.0241\n",
      "Epoch [7/50], Train Loss: 0.0224, Val Loss: 0.0109\n",
      "Epoch [8/50], Train Loss: 0.0201, Val Loss: 0.0184\n",
      "Epoch [9/50], Train Loss: 0.0158, Val Loss: 0.0182\n",
      "Epoch [10/50], Train Loss: 0.0165, Val Loss: 0.0154\n",
      "Epoch [11/50], Train Loss: 0.0135, Val Loss: 0.0078\n",
      "Epoch [12/50], Train Loss: 0.0164, Val Loss: 0.0144\n",
      "Epoch [13/50], Train Loss: 0.0135, Val Loss: 0.0169\n",
      "Epoch [14/50], Train Loss: 0.0124, Val Loss: 0.0126\n",
      "Epoch [15/50], Train Loss: 0.0117, Val Loss: 0.0090\n",
      "Epoch [16/50], Train Loss: 0.0114, Val Loss: 0.0090\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=32, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1014, Val Loss: 0.1003\n",
      "Epoch [2/50], Train Loss: 0.0739, Val Loss: 0.1010\n",
      "Epoch [3/50], Train Loss: 0.0580, Val Loss: 0.0699\n",
      "Epoch [4/50], Train Loss: 0.0491, Val Loss: 0.0404\n",
      "Epoch [5/50], Train Loss: 0.0408, Val Loss: 0.0220\n",
      "Epoch [6/50], Train Loss: 0.0371, Val Loss: 0.0338\n",
      "Epoch [7/50], Train Loss: 0.0328, Val Loss: 0.0248\n",
      "Epoch [8/50], Train Loss: 0.0287, Val Loss: 0.0100\n",
      "Epoch [9/50], Train Loss: 0.0283, Val Loss: 0.0064\n",
      "Epoch [10/50], Train Loss: 0.0266, Val Loss: 0.0276\n",
      "Epoch [11/50], Train Loss: 0.0251, Val Loss: 0.0190\n",
      "Epoch [12/50], Train Loss: 0.0250, Val Loss: 0.0037\n",
      "Epoch [13/50], Train Loss: 0.0258, Val Loss: 0.0252\n",
      "Epoch [14/50], Train Loss: 0.0228, Val Loss: 0.0124\n",
      "Epoch [15/50], Train Loss: 0.0244, Val Loss: 0.0060\n",
      "Epoch [16/50], Train Loss: 0.0183, Val Loss: 0.0207\n",
      "Epoch [17/50], Train Loss: 0.0188, Val Loss: 0.0249\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=64, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0540, Val Loss: 0.0626\n",
      "Epoch [2/50], Train Loss: 0.0437, Val Loss: 0.0589\n",
      "Epoch [3/50], Train Loss: 0.0291, Val Loss: 0.0416\n",
      "Epoch [4/50], Train Loss: 0.0241, Val Loss: 0.0257\n",
      "Epoch [5/50], Train Loss: 0.0176, Val Loss: 0.0115\n",
      "Epoch [6/50], Train Loss: 0.0098, Val Loss: 0.0043\n",
      "Epoch [7/50], Train Loss: 0.0061, Val Loss: 0.0068\n",
      "Epoch [8/50], Train Loss: 0.0043, Val Loss: 0.0102\n",
      "Epoch [9/50], Train Loss: 0.0035, Val Loss: 0.0062\n",
      "Epoch [10/50], Train Loss: 0.0028, Val Loss: 0.0026\n",
      "Epoch [11/50], Train Loss: 0.0025, Val Loss: 0.0069\n",
      "Epoch [12/50], Train Loss: 0.0023, Val Loss: 0.0056\n",
      "Epoch [13/50], Train Loss: 0.0030, Val Loss: 0.0026\n",
      "Epoch [14/50], Train Loss: 0.0026, Val Loss: 0.0026\n",
      "Epoch [15/50], Train Loss: 0.0035, Val Loss: 0.0091\n",
      "Epoch [16/50], Train Loss: 0.0035, Val Loss: 0.0032\n",
      "Epoch [17/50], Train Loss: 0.0036, Val Loss: 0.0022\n",
      "Epoch [18/50], Train Loss: 0.0035, Val Loss: 0.0109\n",
      "Epoch [19/50], Train Loss: 0.0021, Val Loss: 0.0025\n",
      "Epoch [20/50], Train Loss: 0.0036, Val Loss: 0.0026\n",
      "Epoch [21/50], Train Loss: 0.0025, Val Loss: 0.0062\n",
      "Epoch [22/50], Train Loss: 0.0022, Val Loss: 0.0036\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=64, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0565, Val Loss: 0.0647\n",
      "Epoch [2/50], Train Loss: 0.0511, Val Loss: 0.0759\n",
      "Epoch [3/50], Train Loss: 0.0329, Val Loss: 0.0485\n",
      "Epoch [4/50], Train Loss: 0.0288, Val Loss: 0.0344\n",
      "Epoch [5/50], Train Loss: 0.0216, Val Loss: 0.0161\n",
      "Epoch [6/50], Train Loss: 0.0128, Val Loss: 0.0057\n",
      "Epoch [7/50], Train Loss: 0.0105, Val Loss: 0.0049\n",
      "Epoch [8/50], Train Loss: 0.0070, Val Loss: 0.0064\n",
      "Epoch [9/50], Train Loss: 0.0079, Val Loss: 0.0026\n",
      "Epoch [10/50], Train Loss: 0.0065, Val Loss: 0.0063\n",
      "Epoch [11/50], Train Loss: 0.0059, Val Loss: 0.0069\n",
      "Epoch [12/50], Train Loss: 0.0052, Val Loss: 0.0024\n",
      "Epoch [13/50], Train Loss: 0.0054, Val Loss: 0.0025\n",
      "Epoch [14/50], Train Loss: 0.0051, Val Loss: 0.0063\n",
      "Epoch [15/50], Train Loss: 0.0047, Val Loss: 0.0021\n",
      "Epoch [16/50], Train Loss: 0.0053, Val Loss: 0.0020\n",
      "Epoch [17/50], Train Loss: 0.0045, Val Loss: 0.0024\n",
      "Epoch [18/50], Train Loss: 0.0042, Val Loss: 0.0034\n",
      "Epoch [19/50], Train Loss: 0.0042, Val Loss: 0.0018\n",
      "Epoch [20/50], Train Loss: 0.0041, Val Loss: 0.0026\n",
      "Epoch [21/50], Train Loss: 0.0042, Val Loss: 0.0033\n",
      "Epoch [22/50], Train Loss: 0.0041, Val Loss: 0.0023\n",
      "Epoch [23/50], Train Loss: 0.0041, Val Loss: 0.0019\n",
      "Epoch [24/50], Train Loss: 0.0037, Val Loss: 0.0030\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=64, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0843, Val Loss: 0.1403\n",
      "Epoch [2/50], Train Loss: 0.0524, Val Loss: 0.0814\n",
      "Epoch [3/50], Train Loss: 0.0433, Val Loss: 0.0581\n",
      "Epoch [4/50], Train Loss: 0.0329, Val Loss: 0.0274\n",
      "Epoch [5/50], Train Loss: 0.0244, Val Loss: 0.0099\n",
      "Epoch [6/50], Train Loss: 0.0203, Val Loss: 0.0121\n",
      "Epoch [7/50], Train Loss: 0.0150, Val Loss: 0.0060\n",
      "Epoch [8/50], Train Loss: 0.0141, Val Loss: 0.0106\n",
      "Epoch [9/50], Train Loss: 0.0126, Val Loss: 0.0037\n",
      "Epoch [10/50], Train Loss: 0.0112, Val Loss: 0.0020\n",
      "Epoch [11/50], Train Loss: 0.0114, Val Loss: 0.0063\n",
      "Epoch [12/50], Train Loss: 0.0102, Val Loss: 0.0052\n",
      "Epoch [13/50], Train Loss: 0.0097, Val Loss: 0.0055\n",
      "Epoch [14/50], Train Loss: 0.0100, Val Loss: 0.0075\n",
      "Epoch [15/50], Train Loss: 0.0093, Val Loss: 0.0036\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=64, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0425, Val Loss: 0.0509\n",
      "Epoch [2/50], Train Loss: 0.0688, Val Loss: 0.0528\n",
      "Epoch [3/50], Train Loss: 0.0321, Val Loss: 0.0145\n",
      "Epoch [4/50], Train Loss: 0.0193, Val Loss: 0.0104\n",
      "Epoch [5/50], Train Loss: 0.0196, Val Loss: 0.0185\n",
      "Epoch [6/50], Train Loss: 0.0224, Val Loss: 0.0043\n",
      "Epoch [7/50], Train Loss: 0.0078, Val Loss: 0.0097\n",
      "Epoch [8/50], Train Loss: 0.0084, Val Loss: 0.0082\n",
      "Epoch [9/50], Train Loss: 0.0083, Val Loss: 0.0094\n",
      "Epoch [10/50], Train Loss: 0.0035, Val Loss: 0.0076\n",
      "Epoch [11/50], Train Loss: 0.0038, Val Loss: 0.0033\n",
      "Epoch [12/50], Train Loss: 0.0037, Val Loss: 0.0097\n",
      "Epoch [13/50], Train Loss: 0.0031, Val Loss: 0.0066\n",
      "Epoch [14/50], Train Loss: 0.0028, Val Loss: 0.0035\n",
      "Epoch [15/50], Train Loss: 0.0030, Val Loss: 0.0057\n",
      "Epoch [16/50], Train Loss: 0.0029, Val Loss: 0.0054\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=64, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0660, Val Loss: 0.0646\n",
      "Epoch [2/50], Train Loss: 0.0647, Val Loss: 0.0780\n",
      "Epoch [3/50], Train Loss: 0.0376, Val Loss: 0.0426\n",
      "Epoch [4/50], Train Loss: 0.0285, Val Loss: 0.0109\n",
      "Epoch [5/50], Train Loss: 0.0156, Val Loss: 0.0052\n",
      "Epoch [6/50], Train Loss: 0.0176, Val Loss: 0.0228\n",
      "Epoch [7/50], Train Loss: 0.0181, Val Loss: 0.0169\n",
      "Epoch [8/50], Train Loss: 0.0082, Val Loss: 0.0048\n",
      "Epoch [9/50], Train Loss: 0.0067, Val Loss: 0.0030\n",
      "Epoch [10/50], Train Loss: 0.0066, Val Loss: 0.0046\n",
      "Epoch [11/50], Train Loss: 0.0062, Val Loss: 0.0065\n",
      "Epoch [12/50], Train Loss: 0.0061, Val Loss: 0.0022\n",
      "Epoch [13/50], Train Loss: 0.0063, Val Loss: 0.0045\n",
      "Epoch [14/50], Train Loss: 0.0055, Val Loss: 0.0062\n",
      "Epoch [15/50], Train Loss: 0.0060, Val Loss: 0.0028\n",
      "Epoch [16/50], Train Loss: 0.0060, Val Loss: 0.0019\n",
      "Epoch [17/50], Train Loss: 0.0060, Val Loss: 0.0119\n",
      "Epoch [18/50], Train Loss: 0.0056, Val Loss: 0.0019\n",
      "Epoch [19/50], Train Loss: 0.0071, Val Loss: 0.0040\n",
      "Epoch [20/50], Train Loss: 0.0062, Val Loss: 0.0090\n",
      "Epoch [21/50], Train Loss: 0.0053, Val Loss: 0.0058\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=64, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0924, Val Loss: 0.1011\n",
      "Epoch [2/50], Train Loss: 0.0623, Val Loss: 0.0584\n",
      "Epoch [3/50], Train Loss: 0.0477, Val Loss: 0.0527\n",
      "Epoch [4/50], Train Loss: 0.0309, Val Loss: 0.0174\n",
      "Epoch [5/50], Train Loss: 0.0219, Val Loss: 0.0060\n",
      "Epoch [6/50], Train Loss: 0.0262, Val Loss: 0.0291\n",
      "Epoch [7/50], Train Loss: 0.0239, Val Loss: 0.0168\n",
      "Epoch [8/50], Train Loss: 0.0181, Val Loss: 0.0094\n",
      "Epoch [9/50], Train Loss: 0.0141, Val Loss: 0.0121\n",
      "Epoch [10/50], Train Loss: 0.0129, Val Loss: 0.0084\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=64, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0315, Val Loss: 0.0752\n",
      "Epoch [2/50], Train Loss: 0.0761, Val Loss: 0.1004\n",
      "Epoch [3/50], Train Loss: 0.0439, Val Loss: 0.0529\n",
      "Epoch [4/50], Train Loss: 0.0356, Val Loss: 0.0196\n",
      "Epoch [5/50], Train Loss: 0.0192, Val Loss: 0.0282\n",
      "Epoch [6/50], Train Loss: 0.0073, Val Loss: 0.0133\n",
      "Epoch [7/50], Train Loss: 0.0085, Val Loss: 0.0060\n",
      "Epoch [8/50], Train Loss: 0.0072, Val Loss: 0.0143\n",
      "Epoch [9/50], Train Loss: 0.0047, Val Loss: 0.0032\n",
      "Epoch [10/50], Train Loss: 0.0050, Val Loss: 0.0138\n",
      "Epoch [11/50], Train Loss: 0.0077, Val Loss: 0.0256\n",
      "Epoch [12/50], Train Loss: 0.0168, Val Loss: 0.0038\n",
      "Epoch [13/50], Train Loss: 0.0048, Val Loss: 0.0058\n",
      "Epoch [14/50], Train Loss: 0.0031, Val Loss: 0.0136\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=64, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0387, Val Loss: 0.0722\n",
      "Epoch [2/50], Train Loss: 0.0721, Val Loss: 0.0737\n",
      "Epoch [3/50], Train Loss: 0.0579, Val Loss: 0.0727\n",
      "Epoch [4/50], Train Loss: 0.0439, Val Loss: 0.0410\n",
      "Epoch [5/50], Train Loss: 0.0321, Val Loss: 0.0101\n",
      "Epoch [6/50], Train Loss: 0.0201, Val Loss: 0.0127\n",
      "Epoch [7/50], Train Loss: 0.0152, Val Loss: 0.0254\n",
      "Epoch [8/50], Train Loss: 0.0145, Val Loss: 0.0224\n",
      "Epoch [9/50], Train Loss: 0.0166, Val Loss: 0.0032\n",
      "Epoch [10/50], Train Loss: 0.0127, Val Loss: 0.0219\n",
      "Epoch [11/50], Train Loss: 0.0150, Val Loss: 0.0276\n",
      "Epoch [12/50], Train Loss: 0.0196, Val Loss: 0.0124\n",
      "Epoch [13/50], Train Loss: 0.0146, Val Loss: 0.0159\n",
      "Epoch [14/50], Train Loss: 0.0148, Val Loss: 0.0140\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=64, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0552, Val Loss: 0.0729\n",
      "Epoch [2/50], Train Loss: 0.0563, Val Loss: 0.0235\n",
      "Epoch [3/50], Train Loss: 0.0591, Val Loss: 0.0136\n",
      "Epoch [4/50], Train Loss: 0.0368, Val Loss: 0.0044\n",
      "Epoch [5/50], Train Loss: 0.0330, Val Loss: 0.0306\n",
      "Epoch [6/50], Train Loss: 0.0374, Val Loss: 0.0157\n",
      "Epoch [7/50], Train Loss: 0.0233, Val Loss: 0.0057\n",
      "Epoch [8/50], Train Loss: 0.0224, Val Loss: 0.0402\n",
      "Epoch [9/50], Train Loss: 0.0261, Val Loss: 0.0160\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=128, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0345, Val Loss: 0.0757\n",
      "Epoch [2/50], Train Loss: 0.0522, Val Loss: 0.0361\n",
      "Epoch [3/50], Train Loss: 0.0296, Val Loss: 0.0110\n",
      "Epoch [4/50], Train Loss: 0.0153, Val Loss: 0.0075\n",
      "Epoch [5/50], Train Loss: 0.0131, Val Loss: 0.0063\n",
      "Epoch [6/50], Train Loss: 0.0145, Val Loss: 0.0031\n",
      "Epoch [7/50], Train Loss: 0.0056, Val Loss: 0.0048\n",
      "Epoch [8/50], Train Loss: 0.0047, Val Loss: 0.0106\n",
      "Epoch [9/50], Train Loss: 0.0044, Val Loss: 0.0042\n",
      "Epoch [10/50], Train Loss: 0.0044, Val Loss: 0.0050\n",
      "Epoch [11/50], Train Loss: 0.0032, Val Loss: 0.0087\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=128, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0340, Val Loss: 0.0873\n",
      "Epoch [2/50], Train Loss: 0.0502, Val Loss: 0.0192\n",
      "Epoch [3/50], Train Loss: 0.0354, Val Loss: 0.0147\n",
      "Epoch [4/50], Train Loss: 0.0227, Val Loss: 0.0057\n",
      "Epoch [5/50], Train Loss: 0.0157, Val Loss: 0.0038\n",
      "Epoch [6/50], Train Loss: 0.0123, Val Loss: 0.0074\n",
      "Epoch [7/50], Train Loss: 0.0116, Val Loss: 0.0056\n",
      "Epoch [8/50], Train Loss: 0.0110, Val Loss: 0.0025\n",
      "Epoch [9/50], Train Loss: 0.0063, Val Loss: 0.0126\n",
      "Epoch [10/50], Train Loss: 0.0060, Val Loss: 0.0063\n",
      "Epoch [11/50], Train Loss: 0.0058, Val Loss: 0.0029\n",
      "Epoch [12/50], Train Loss: 0.0043, Val Loss: 0.0058\n",
      "Epoch [13/50], Train Loss: 0.0038, Val Loss: 0.0068\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=128, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0345, Val Loss: 0.0695\n",
      "Epoch [2/50], Train Loss: 0.0484, Val Loss: 0.0267\n",
      "Epoch [3/50], Train Loss: 0.0301, Val Loss: 0.0056\n",
      "Epoch [4/50], Train Loss: 0.0178, Val Loss: 0.0036\n",
      "Epoch [5/50], Train Loss: 0.0180, Val Loss: 0.0087\n",
      "Epoch [6/50], Train Loss: 0.0185, Val Loss: 0.0091\n",
      "Epoch [7/50], Train Loss: 0.0110, Val Loss: 0.0080\n",
      "Epoch [8/50], Train Loss: 0.0092, Val Loss: 0.0111\n",
      "Epoch [9/50], Train Loss: 0.0079, Val Loss: 0.0018\n",
      "Epoch [10/50], Train Loss: 0.0085, Val Loss: 0.0032\n",
      "Epoch [11/50], Train Loss: 0.0056, Val Loss: 0.0042\n",
      "Epoch [12/50], Train Loss: 0.0057, Val Loss: 0.0044\n",
      "Epoch [13/50], Train Loss: 0.0053, Val Loss: 0.0027\n",
      "Epoch [14/50], Train Loss: 0.0054, Val Loss: 0.0020\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=128, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0215, Val Loss: 0.0783\n",
      "Epoch [2/50], Train Loss: 0.0618, Val Loss: 0.0162\n",
      "Epoch [3/50], Train Loss: 0.0513, Val Loss: 0.0112\n",
      "Epoch [4/50], Train Loss: 0.0315, Val Loss: 0.0108\n",
      "Epoch [5/50], Train Loss: 0.0146, Val Loss: 0.0106\n",
      "Epoch [6/50], Train Loss: 0.0222, Val Loss: 0.0362\n",
      "Epoch [7/50], Train Loss: 0.0187, Val Loss: 0.0376\n",
      "Epoch [8/50], Train Loss: 0.0077, Val Loss: 0.0032\n",
      "Epoch [9/50], Train Loss: 0.0037, Val Loss: 0.0046\n",
      "Epoch [10/50], Train Loss: 0.0028, Val Loss: 0.0019\n",
      "Epoch [11/50], Train Loss: 0.0027, Val Loss: 0.0051\n",
      "Epoch [12/50], Train Loss: 0.0022, Val Loss: 0.0012\n",
      "Epoch [13/50], Train Loss: 0.0025, Val Loss: 0.0036\n",
      "Epoch [14/50], Train Loss: 0.0020, Val Loss: 0.0020\n",
      "Epoch [15/50], Train Loss: 0.0020, Val Loss: 0.0017\n",
      "Epoch [16/50], Train Loss: 0.0024, Val Loss: 0.0026\n",
      "Epoch [17/50], Train Loss: 0.0021, Val Loss: 0.0020\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=128, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0200, Val Loss: 0.0787\n",
      "Epoch [2/50], Train Loss: 0.0567, Val Loss: 0.0261\n",
      "Epoch [3/50], Train Loss: 0.0484, Val Loss: 0.0118\n",
      "Epoch [4/50], Train Loss: 0.0318, Val Loss: 0.0112\n",
      "Epoch [5/50], Train Loss: 0.0204, Val Loss: 0.0071\n",
      "Epoch [6/50], Train Loss: 0.0131, Val Loss: 0.0184\n",
      "Epoch [7/50], Train Loss: 0.0097, Val Loss: 0.0036\n",
      "Epoch [8/50], Train Loss: 0.0057, Val Loss: 0.0053\n",
      "Epoch [9/50], Train Loss: 0.0045, Val Loss: 0.0018\n",
      "Epoch [10/50], Train Loss: 0.0041, Val Loss: 0.0021\n",
      "Epoch [11/50], Train Loss: 0.0039, Val Loss: 0.0026\n",
      "Epoch [12/50], Train Loss: 0.0040, Val Loss: 0.0026\n",
      "Epoch [13/50], Train Loss: 0.0036, Val Loss: 0.0022\n",
      "Epoch [14/50], Train Loss: 0.0039, Val Loss: 0.0068\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=128, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0368, Val Loss: 0.0992\n",
      "Epoch [2/50], Train Loss: 0.0658, Val Loss: 0.0282\n",
      "Epoch [3/50], Train Loss: 0.0523, Val Loss: 0.0098\n",
      "Epoch [4/50], Train Loss: 0.0286, Val Loss: 0.0051\n",
      "Epoch [5/50], Train Loss: 0.0213, Val Loss: 0.0252\n",
      "Epoch [6/50], Train Loss: 0.0185, Val Loss: 0.0057\n",
      "Epoch [7/50], Train Loss: 0.0121, Val Loss: 0.0054\n",
      "Epoch [8/50], Train Loss: 0.0106, Val Loss: 0.0123\n",
      "Epoch [9/50], Train Loss: 0.0118, Val Loss: 0.0023\n",
      "Epoch [10/50], Train Loss: 0.0100, Val Loss: 0.0028\n",
      "Epoch [11/50], Train Loss: 0.0099, Val Loss: 0.0130\n",
      "Epoch [12/50], Train Loss: 0.0118, Val Loss: 0.0160\n",
      "Epoch [13/50], Train Loss: 0.0125, Val Loss: 0.0017\n",
      "Epoch [14/50], Train Loss: 0.0107, Val Loss: 0.0113\n",
      "Epoch [15/50], Train Loss: 0.0081, Val Loss: 0.0020\n",
      "Epoch [16/50], Train Loss: 0.0104, Val Loss: 0.0086\n",
      "Epoch [17/50], Train Loss: 0.0093, Val Loss: 0.0109\n",
      "Epoch [18/50], Train Loss: 0.0146, Val Loss: 0.0102\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=128, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0194, Val Loss: 0.0934\n",
      "Epoch [2/50], Train Loss: 0.0475, Val Loss: 0.0871\n",
      "Epoch [3/50], Train Loss: 0.0389, Val Loss: 0.0355\n",
      "Epoch [4/50], Train Loss: 0.0368, Val Loss: 0.0086\n",
      "Epoch [5/50], Train Loss: 0.0183, Val Loss: 0.0061\n",
      "Epoch [6/50], Train Loss: 0.0301, Val Loss: 0.0396\n",
      "Epoch [7/50], Train Loss: 0.0178, Val Loss: 0.0065\n",
      "Epoch [8/50], Train Loss: 0.0134, Val Loss: 0.0143\n",
      "Epoch [9/50], Train Loss: 0.0170, Val Loss: 0.0272\n",
      "Epoch [10/50], Train Loss: 0.0085, Val Loss: 0.0115\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=128, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0272, Val Loss: 0.0918\n",
      "Epoch [2/50], Train Loss: 0.0643, Val Loss: 0.0957\n",
      "Epoch [3/50], Train Loss: 0.0543, Val Loss: 0.0606\n",
      "Epoch [4/50], Train Loss: 0.0423, Val Loss: 0.0175\n",
      "Epoch [5/50], Train Loss: 0.0330, Val Loss: 0.0065\n",
      "Epoch [6/50], Train Loss: 0.0202, Val Loss: 0.0062\n",
      "Epoch [7/50], Train Loss: 0.0183, Val Loss: 0.0235\n",
      "Epoch [8/50], Train Loss: 0.0126, Val Loss: 0.0022\n",
      "Epoch [9/50], Train Loss: 0.0093, Val Loss: 0.0039\n",
      "Epoch [10/50], Train Loss: 0.0099, Val Loss: 0.0048\n",
      "Epoch [11/50], Train Loss: 0.0100, Val Loss: 0.0110\n",
      "Epoch [12/50], Train Loss: 0.0150, Val Loss: 0.0114\n",
      "Epoch [13/50], Train Loss: 0.0274, Val Loss: 0.0208\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=128, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0368, Val Loss: 0.0970\n",
      "Epoch [2/50], Train Loss: 0.0608, Val Loss: 0.0644\n",
      "Epoch [3/50], Train Loss: 0.0535, Val Loss: 0.0208\n",
      "Epoch [4/50], Train Loss: 0.0448, Val Loss: 0.0043\n",
      "Epoch [5/50], Train Loss: 0.0304, Val Loss: 0.0049\n",
      "Epoch [6/50], Train Loss: 0.0205, Val Loss: 0.0369\n",
      "Epoch [7/50], Train Loss: 0.0160, Val Loss: 0.0033\n",
      "Epoch [8/50], Train Loss: 0.0261, Val Loss: 0.0062\n",
      "Epoch [9/50], Train Loss: 0.0228, Val Loss: 0.0565\n",
      "Epoch [10/50], Train Loss: 0.0213, Val Loss: 0.0119\n",
      "Epoch [11/50], Train Loss: 0.0253, Val Loss: 0.0247\n",
      "Epoch [12/50], Train Loss: 0.0136, Val Loss: 0.0294\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=16, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1330, Val Loss: 0.3488\n",
      "Epoch [2/50], Train Loss: 0.1184, Val Loss: 0.3224\n",
      "Epoch [3/50], Train Loss: 0.1060, Val Loss: 0.2984\n",
      "Epoch [4/50], Train Loss: 0.0950, Val Loss: 0.2760\n",
      "Epoch [5/50], Train Loss: 0.0850, Val Loss: 0.2548\n",
      "Epoch [6/50], Train Loss: 0.0760, Val Loss: 0.2346\n",
      "Epoch [7/50], Train Loss: 0.0678, Val Loss: 0.2153\n",
      "Epoch [8/50], Train Loss: 0.0603, Val Loss: 0.1968\n",
      "Epoch [9/50], Train Loss: 0.0536, Val Loss: 0.1793\n",
      "Epoch [10/50], Train Loss: 0.0478, Val Loss: 0.1630\n",
      "Epoch [11/50], Train Loss: 0.0428, Val Loss: 0.1482\n",
      "Epoch [12/50], Train Loss: 0.0388, Val Loss: 0.1351\n",
      "Epoch [13/50], Train Loss: 0.0356, Val Loss: 0.1238\n",
      "Epoch [14/50], Train Loss: 0.0333, Val Loss: 0.1143\n",
      "Epoch [15/50], Train Loss: 0.0317, Val Loss: 0.1065\n",
      "Epoch [16/50], Train Loss: 0.0305, Val Loss: 0.1002\n",
      "Epoch [17/50], Train Loss: 0.0297, Val Loss: 0.0952\n",
      "Epoch [18/50], Train Loss: 0.0291, Val Loss: 0.0912\n",
      "Epoch [19/50], Train Loss: 0.0287, Val Loss: 0.0879\n",
      "Epoch [20/50], Train Loss: 0.0283, Val Loss: 0.0852\n",
      "Epoch [21/50], Train Loss: 0.0279, Val Loss: 0.0829\n",
      "Epoch [22/50], Train Loss: 0.0275, Val Loss: 0.0810\n",
      "Epoch [23/50], Train Loss: 0.0272, Val Loss: 0.0792\n",
      "Epoch [24/50], Train Loss: 0.0268, Val Loss: 0.0776\n",
      "Epoch [25/50], Train Loss: 0.0264, Val Loss: 0.0760\n",
      "Epoch [26/50], Train Loss: 0.0260, Val Loss: 0.0746\n",
      "Epoch [27/50], Train Loss: 0.0256, Val Loss: 0.0731\n",
      "Epoch [28/50], Train Loss: 0.0252, Val Loss: 0.0717\n",
      "Epoch [29/50], Train Loss: 0.0247, Val Loss: 0.0703\n",
      "Epoch [30/50], Train Loss: 0.0243, Val Loss: 0.0688\n",
      "Epoch [31/50], Train Loss: 0.0238, Val Loss: 0.0674\n",
      "Epoch [32/50], Train Loss: 0.0234, Val Loss: 0.0659\n",
      "Epoch [33/50], Train Loss: 0.0229, Val Loss: 0.0645\n",
      "Epoch [34/50], Train Loss: 0.0224, Val Loss: 0.0630\n",
      "Epoch [35/50], Train Loss: 0.0219, Val Loss: 0.0614\n",
      "Epoch [36/50], Train Loss: 0.0214, Val Loss: 0.0599\n",
      "Epoch [37/50], Train Loss: 0.0209, Val Loss: 0.0583\n",
      "Epoch [38/50], Train Loss: 0.0203, Val Loss: 0.0567\n",
      "Epoch [39/50], Train Loss: 0.0198, Val Loss: 0.0551\n",
      "Epoch [40/50], Train Loss: 0.0192, Val Loss: 0.0534\n",
      "Epoch [41/50], Train Loss: 0.0187, Val Loss: 0.0518\n",
      "Epoch [42/50], Train Loss: 0.0181, Val Loss: 0.0501\n",
      "Epoch [43/50], Train Loss: 0.0175, Val Loss: 0.0484\n",
      "Epoch [44/50], Train Loss: 0.0170, Val Loss: 0.0467\n",
      "Epoch [45/50], Train Loss: 0.0164, Val Loss: 0.0450\n",
      "Epoch [46/50], Train Loss: 0.0158, Val Loss: 0.0433\n",
      "Epoch [47/50], Train Loss: 0.0152, Val Loss: 0.0416\n",
      "Epoch [48/50], Train Loss: 0.0147, Val Loss: 0.0400\n",
      "Epoch [49/50], Train Loss: 0.0141, Val Loss: 0.0383\n",
      "Epoch [50/50], Train Loss: 0.0136, Val Loss: 0.0368\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=16, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1311, Val Loss: 0.2790\n",
      "Epoch [2/50], Train Loss: 0.1183, Val Loss: 0.2553\n",
      "Epoch [3/50], Train Loss: 0.1080, Val Loss: 0.2324\n",
      "Epoch [4/50], Train Loss: 0.0968, Val Loss: 0.2100\n",
      "Epoch [5/50], Train Loss: 0.0864, Val Loss: 0.1887\n",
      "Epoch [6/50], Train Loss: 0.0783, Val Loss: 0.1688\n",
      "Epoch [7/50], Train Loss: 0.0706, Val Loss: 0.1510\n",
      "Epoch [8/50], Train Loss: 0.0641, Val Loss: 0.1356\n",
      "Epoch [9/50], Train Loss: 0.0608, Val Loss: 0.1225\n",
      "Epoch [10/50], Train Loss: 0.0578, Val Loss: 0.1113\n",
      "Epoch [11/50], Train Loss: 0.0529, Val Loss: 0.1023\n",
      "Epoch [12/50], Train Loss: 0.0504, Val Loss: 0.0951\n",
      "Epoch [13/50], Train Loss: 0.0495, Val Loss: 0.0890\n",
      "Epoch [14/50], Train Loss: 0.0467, Val Loss: 0.0844\n",
      "Epoch [15/50], Train Loss: 0.0458, Val Loss: 0.0809\n",
      "Epoch [16/50], Train Loss: 0.0433, Val Loss: 0.0777\n",
      "Epoch [17/50], Train Loss: 0.0436, Val Loss: 0.0747\n",
      "Epoch [18/50], Train Loss: 0.0441, Val Loss: 0.0720\n",
      "Epoch [19/50], Train Loss: 0.0426, Val Loss: 0.0695\n",
      "Epoch [20/50], Train Loss: 0.0425, Val Loss: 0.0671\n",
      "Epoch [21/50], Train Loss: 0.0400, Val Loss: 0.0650\n",
      "Epoch [22/50], Train Loss: 0.0392, Val Loss: 0.0630\n",
      "Epoch [23/50], Train Loss: 0.0404, Val Loss: 0.0612\n",
      "Epoch [24/50], Train Loss: 0.0397, Val Loss: 0.0595\n",
      "Epoch [25/50], Train Loss: 0.0388, Val Loss: 0.0581\n",
      "Epoch [26/50], Train Loss: 0.0391, Val Loss: 0.0563\n",
      "Epoch [27/50], Train Loss: 0.0377, Val Loss: 0.0544\n",
      "Epoch [28/50], Train Loss: 0.0379, Val Loss: 0.0529\n",
      "Epoch [29/50], Train Loss: 0.0371, Val Loss: 0.0508\n",
      "Epoch [30/50], Train Loss: 0.0350, Val Loss: 0.0491\n",
      "Epoch [31/50], Train Loss: 0.0345, Val Loss: 0.0475\n",
      "Epoch [32/50], Train Loss: 0.0348, Val Loss: 0.0455\n",
      "Epoch [33/50], Train Loss: 0.0342, Val Loss: 0.0437\n",
      "Epoch [34/50], Train Loss: 0.0332, Val Loss: 0.0427\n",
      "Epoch [35/50], Train Loss: 0.0322, Val Loss: 0.0416\n",
      "Epoch [36/50], Train Loss: 0.0331, Val Loss: 0.0402\n",
      "Epoch [37/50], Train Loss: 0.0325, Val Loss: 0.0385\n",
      "Epoch [38/50], Train Loss: 0.0323, Val Loss: 0.0369\n",
      "Epoch [39/50], Train Loss: 0.0299, Val Loss: 0.0356\n",
      "Epoch [40/50], Train Loss: 0.0312, Val Loss: 0.0342\n",
      "Epoch [41/50], Train Loss: 0.0293, Val Loss: 0.0330\n",
      "Epoch [42/50], Train Loss: 0.0298, Val Loss: 0.0324\n",
      "Epoch [43/50], Train Loss: 0.0292, Val Loss: 0.0314\n",
      "Epoch [44/50], Train Loss: 0.0277, Val Loss: 0.0299\n",
      "Epoch [45/50], Train Loss: 0.0288, Val Loss: 0.0288\n",
      "Epoch [46/50], Train Loss: 0.0268, Val Loss: 0.0277\n",
      "Epoch [47/50], Train Loss: 0.0280, Val Loss: 0.0266\n",
      "Epoch [48/50], Train Loss: 0.0279, Val Loss: 0.0259\n",
      "Epoch [49/50], Train Loss: 0.0267, Val Loss: 0.0256\n",
      "Epoch [50/50], Train Loss: 0.0264, Val Loss: 0.0251\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=16, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1008, Val Loss: 0.2593\n",
      "Epoch [2/50], Train Loss: 0.0880, Val Loss: 0.2335\n",
      "Epoch [3/50], Train Loss: 0.0845, Val Loss: 0.2126\n",
      "Epoch [4/50], Train Loss: 0.0804, Val Loss: 0.1959\n",
      "Epoch [5/50], Train Loss: 0.0742, Val Loss: 0.1819\n",
      "Epoch [6/50], Train Loss: 0.0702, Val Loss: 0.1705\n",
      "Epoch [7/50], Train Loss: 0.0693, Val Loss: 0.1607\n",
      "Epoch [8/50], Train Loss: 0.0698, Val Loss: 0.1519\n",
      "Epoch [9/50], Train Loss: 0.0645, Val Loss: 0.1436\n",
      "Epoch [10/50], Train Loss: 0.0612, Val Loss: 0.1368\n",
      "Epoch [11/50], Train Loss: 0.0621, Val Loss: 0.1315\n",
      "Epoch [12/50], Train Loss: 0.0599, Val Loss: 0.1269\n",
      "Epoch [13/50], Train Loss: 0.0580, Val Loss: 0.1222\n",
      "Epoch [14/50], Train Loss: 0.0545, Val Loss: 0.1187\n",
      "Epoch [15/50], Train Loss: 0.0553, Val Loss: 0.1138\n",
      "Epoch [16/50], Train Loss: 0.0540, Val Loss: 0.1090\n",
      "Epoch [17/50], Train Loss: 0.0517, Val Loss: 0.1045\n",
      "Epoch [18/50], Train Loss: 0.0517, Val Loss: 0.1011\n",
      "Epoch [19/50], Train Loss: 0.0510, Val Loss: 0.0978\n",
      "Epoch [20/50], Train Loss: 0.0486, Val Loss: 0.0940\n",
      "Epoch [21/50], Train Loss: 0.0499, Val Loss: 0.0908\n",
      "Epoch [22/50], Train Loss: 0.0471, Val Loss: 0.0883\n",
      "Epoch [23/50], Train Loss: 0.0495, Val Loss: 0.0859\n",
      "Epoch [24/50], Train Loss: 0.0463, Val Loss: 0.0811\n",
      "Epoch [25/50], Train Loss: 0.0444, Val Loss: 0.0774\n",
      "Epoch [26/50], Train Loss: 0.0436, Val Loss: 0.0745\n",
      "Epoch [27/50], Train Loss: 0.0434, Val Loss: 0.0717\n",
      "Epoch [28/50], Train Loss: 0.0425, Val Loss: 0.0696\n",
      "Epoch [29/50], Train Loss: 0.0422, Val Loss: 0.0671\n",
      "Epoch [30/50], Train Loss: 0.0419, Val Loss: 0.0647\n",
      "Epoch [31/50], Train Loss: 0.0397, Val Loss: 0.0625\n",
      "Epoch [32/50], Train Loss: 0.0393, Val Loss: 0.0610\n",
      "Epoch [33/50], Train Loss: 0.0393, Val Loss: 0.0595\n",
      "Epoch [34/50], Train Loss: 0.0399, Val Loss: 0.0569\n",
      "Epoch [35/50], Train Loss: 0.0401, Val Loss: 0.0557\n",
      "Epoch [36/50], Train Loss: 0.0366, Val Loss: 0.0542\n",
      "Epoch [37/50], Train Loss: 0.0364, Val Loss: 0.0529\n",
      "Epoch [38/50], Train Loss: 0.0357, Val Loss: 0.0504\n",
      "Epoch [39/50], Train Loss: 0.0385, Val Loss: 0.0475\n",
      "Epoch [40/50], Train Loss: 0.0356, Val Loss: 0.0457\n",
      "Epoch [41/50], Train Loss: 0.0350, Val Loss: 0.0455\n",
      "Epoch [42/50], Train Loss: 0.0347, Val Loss: 0.0448\n",
      "Epoch [43/50], Train Loss: 0.0332, Val Loss: 0.0444\n",
      "Epoch [44/50], Train Loss: 0.0339, Val Loss: 0.0425\n",
      "Epoch [45/50], Train Loss: 0.0337, Val Loss: 0.0422\n",
      "Epoch [46/50], Train Loss: 0.0321, Val Loss: 0.0419\n",
      "Epoch [47/50], Train Loss: 0.0341, Val Loss: 0.0401\n",
      "Epoch [48/50], Train Loss: 0.0332, Val Loss: 0.0391\n",
      "Epoch [49/50], Train Loss: 0.0334, Val Loss: 0.0393\n",
      "Epoch [50/50], Train Loss: 0.0335, Val Loss: 0.0388\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=16, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.2581, Val Loss: 0.4569\n",
      "Epoch [2/50], Train Loss: 0.1943, Val Loss: 0.3898\n",
      "Epoch [3/50], Train Loss: 0.1484, Val Loss: 0.3357\n",
      "Epoch [4/50], Train Loss: 0.1145, Val Loss: 0.2904\n",
      "Epoch [5/50], Train Loss: 0.0895, Val Loss: 0.2523\n",
      "Epoch [6/50], Train Loss: 0.0715, Val Loss: 0.2202\n",
      "Epoch [7/50], Train Loss: 0.0587, Val Loss: 0.1934\n",
      "Epoch [8/50], Train Loss: 0.0497, Val Loss: 0.1712\n",
      "Epoch [9/50], Train Loss: 0.0434, Val Loss: 0.1531\n",
      "Epoch [10/50], Train Loss: 0.0389, Val Loss: 0.1384\n",
      "Epoch [11/50], Train Loss: 0.0358, Val Loss: 0.1264\n",
      "Epoch [12/50], Train Loss: 0.0336, Val Loss: 0.1167\n",
      "Epoch [13/50], Train Loss: 0.0320, Val Loss: 0.1088\n",
      "Epoch [14/50], Train Loss: 0.0308, Val Loss: 0.1022\n",
      "Epoch [15/50], Train Loss: 0.0299, Val Loss: 0.0968\n",
      "Epoch [16/50], Train Loss: 0.0292, Val Loss: 0.0923\n",
      "Epoch [17/50], Train Loss: 0.0286, Val Loss: 0.0885\n",
      "Epoch [18/50], Train Loss: 0.0282, Val Loss: 0.0852\n",
      "Epoch [19/50], Train Loss: 0.0278, Val Loss: 0.0824\n",
      "Epoch [20/50], Train Loss: 0.0274, Val Loss: 0.0799\n",
      "Epoch [21/50], Train Loss: 0.0271, Val Loss: 0.0777\n",
      "Epoch [22/50], Train Loss: 0.0267, Val Loss: 0.0756\n",
      "Epoch [23/50], Train Loss: 0.0264, Val Loss: 0.0738\n",
      "Epoch [24/50], Train Loss: 0.0261, Val Loss: 0.0720\n",
      "Epoch [25/50], Train Loss: 0.0258, Val Loss: 0.0704\n",
      "Epoch [26/50], Train Loss: 0.0255, Val Loss: 0.0689\n",
      "Epoch [27/50], Train Loss: 0.0251, Val Loss: 0.0674\n",
      "Epoch [28/50], Train Loss: 0.0248, Val Loss: 0.0660\n",
      "Epoch [29/50], Train Loss: 0.0245, Val Loss: 0.0645\n",
      "Epoch [30/50], Train Loss: 0.0242, Val Loss: 0.0632\n",
      "Epoch [31/50], Train Loss: 0.0239, Val Loss: 0.0618\n",
      "Epoch [32/50], Train Loss: 0.0236, Val Loss: 0.0605\n",
      "Epoch [33/50], Train Loss: 0.0232, Val Loss: 0.0592\n",
      "Epoch [34/50], Train Loss: 0.0229, Val Loss: 0.0580\n",
      "Epoch [35/50], Train Loss: 0.0226, Val Loss: 0.0567\n",
      "Epoch [36/50], Train Loss: 0.0223, Val Loss: 0.0555\n",
      "Epoch [37/50], Train Loss: 0.0220, Val Loss: 0.0543\n",
      "Epoch [38/50], Train Loss: 0.0217, Val Loss: 0.0531\n",
      "Epoch [39/50], Train Loss: 0.0214, Val Loss: 0.0519\n",
      "Epoch [40/50], Train Loss: 0.0211, Val Loss: 0.0507\n",
      "Epoch [41/50], Train Loss: 0.0208, Val Loss: 0.0496\n",
      "Epoch [42/50], Train Loss: 0.0205, Val Loss: 0.0485\n",
      "Epoch [43/50], Train Loss: 0.0203, Val Loss: 0.0474\n",
      "Epoch [44/50], Train Loss: 0.0200, Val Loss: 0.0463\n",
      "Epoch [45/50], Train Loss: 0.0197, Val Loss: 0.0453\n",
      "Epoch [46/50], Train Loss: 0.0194, Val Loss: 0.0443\n",
      "Epoch [47/50], Train Loss: 0.0191, Val Loss: 0.0433\n",
      "Epoch [48/50], Train Loss: 0.0188, Val Loss: 0.0423\n",
      "Epoch [49/50], Train Loss: 0.0185, Val Loss: 0.0413\n",
      "Epoch [50/50], Train Loss: 0.0182, Val Loss: 0.0403\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=16, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.2367, Val Loss: 0.3741\n",
      "Epoch [2/50], Train Loss: 0.2071, Val Loss: 0.3496\n",
      "Epoch [3/50], Train Loss: 0.1845, Val Loss: 0.3286\n",
      "Epoch [4/50], Train Loss: 0.1666, Val Loss: 0.3094\n",
      "Epoch [5/50], Train Loss: 0.1486, Val Loss: 0.2914\n",
      "Epoch [6/50], Train Loss: 0.1356, Val Loss: 0.2741\n",
      "Epoch [7/50], Train Loss: 0.1208, Val Loss: 0.2569\n",
      "Epoch [8/50], Train Loss: 0.1106, Val Loss: 0.2400\n",
      "Epoch [9/50], Train Loss: 0.1020, Val Loss: 0.2232\n",
      "Epoch [10/50], Train Loss: 0.0916, Val Loss: 0.2070\n",
      "Epoch [11/50], Train Loss: 0.0840, Val Loss: 0.1912\n",
      "Epoch [12/50], Train Loss: 0.0763, Val Loss: 0.1767\n",
      "Epoch [13/50], Train Loss: 0.0715, Val Loss: 0.1631\n",
      "Epoch [14/50], Train Loss: 0.0673, Val Loss: 0.1505\n",
      "Epoch [15/50], Train Loss: 0.0627, Val Loss: 0.1399\n",
      "Epoch [16/50], Train Loss: 0.0602, Val Loss: 0.1305\n",
      "Epoch [17/50], Train Loss: 0.0575, Val Loss: 0.1231\n",
      "Epoch [18/50], Train Loss: 0.0564, Val Loss: 0.1166\n",
      "Epoch [19/50], Train Loss: 0.0533, Val Loss: 0.1109\n",
      "Epoch [20/50], Train Loss: 0.0523, Val Loss: 0.1055\n",
      "Epoch [21/50], Train Loss: 0.0478, Val Loss: 0.1003\n",
      "Epoch [22/50], Train Loss: 0.0487, Val Loss: 0.0967\n",
      "Epoch [23/50], Train Loss: 0.0463, Val Loss: 0.0926\n",
      "Epoch [24/50], Train Loss: 0.0451, Val Loss: 0.0893\n",
      "Epoch [25/50], Train Loss: 0.0439, Val Loss: 0.0850\n",
      "Epoch [26/50], Train Loss: 0.0437, Val Loss: 0.0810\n",
      "Epoch [27/50], Train Loss: 0.0405, Val Loss: 0.0769\n",
      "Epoch [28/50], Train Loss: 0.0404, Val Loss: 0.0733\n",
      "Epoch [29/50], Train Loss: 0.0395, Val Loss: 0.0702\n",
      "Epoch [30/50], Train Loss: 0.0376, Val Loss: 0.0668\n",
      "Epoch [31/50], Train Loss: 0.0370, Val Loss: 0.0632\n",
      "Epoch [32/50], Train Loss: 0.0357, Val Loss: 0.0597\n",
      "Epoch [33/50], Train Loss: 0.0354, Val Loss: 0.0561\n",
      "Epoch [34/50], Train Loss: 0.0337, Val Loss: 0.0527\n",
      "Epoch [35/50], Train Loss: 0.0327, Val Loss: 0.0496\n",
      "Epoch [36/50], Train Loss: 0.0315, Val Loss: 0.0463\n",
      "Epoch [37/50], Train Loss: 0.0307, Val Loss: 0.0428\n",
      "Epoch [38/50], Train Loss: 0.0294, Val Loss: 0.0401\n",
      "Epoch [39/50], Train Loss: 0.0296, Val Loss: 0.0378\n",
      "Epoch [40/50], Train Loss: 0.0289, Val Loss: 0.0358\n",
      "Epoch [41/50], Train Loss: 0.0283, Val Loss: 0.0340\n",
      "Epoch [42/50], Train Loss: 0.0283, Val Loss: 0.0321\n",
      "Epoch [43/50], Train Loss: 0.0277, Val Loss: 0.0302\n",
      "Epoch [44/50], Train Loss: 0.0260, Val Loss: 0.0293\n",
      "Epoch [45/50], Train Loss: 0.0253, Val Loss: 0.0282\n",
      "Epoch [46/50], Train Loss: 0.0255, Val Loss: 0.0268\n",
      "Epoch [47/50], Train Loss: 0.0247, Val Loss: 0.0261\n",
      "Epoch [48/50], Train Loss: 0.0246, Val Loss: 0.0258\n",
      "Epoch [49/50], Train Loss: 0.0249, Val Loss: 0.0249\n",
      "Epoch [50/50], Train Loss: 0.0238, Val Loss: 0.0239\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=16, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1531, Val Loss: 0.2624\n",
      "Epoch [2/50], Train Loss: 0.1328, Val Loss: 0.2390\n",
      "Epoch [3/50], Train Loss: 0.1177, Val Loss: 0.2194\n",
      "Epoch [4/50], Train Loss: 0.1061, Val Loss: 0.2039\n",
      "Epoch [5/50], Train Loss: 0.0979, Val Loss: 0.1907\n",
      "Epoch [6/50], Train Loss: 0.0881, Val Loss: 0.1792\n",
      "Epoch [7/50], Train Loss: 0.0809, Val Loss: 0.1692\n",
      "Epoch [8/50], Train Loss: 0.0783, Val Loss: 0.1608\n",
      "Epoch [9/50], Train Loss: 0.0749, Val Loss: 0.1534\n",
      "Epoch [10/50], Train Loss: 0.0715, Val Loss: 0.1472\n",
      "Epoch [11/50], Train Loss: 0.0686, Val Loss: 0.1415\n",
      "Epoch [12/50], Train Loss: 0.0672, Val Loss: 0.1362\n",
      "Epoch [13/50], Train Loss: 0.0615, Val Loss: 0.1317\n",
      "Epoch [14/50], Train Loss: 0.0632, Val Loss: 0.1280\n",
      "Epoch [15/50], Train Loss: 0.0600, Val Loss: 0.1239\n",
      "Epoch [16/50], Train Loss: 0.0573, Val Loss: 0.1197\n",
      "Epoch [17/50], Train Loss: 0.0554, Val Loss: 0.1160\n",
      "Epoch [18/50], Train Loss: 0.0545, Val Loss: 0.1131\n",
      "Epoch [19/50], Train Loss: 0.0542, Val Loss: 0.1109\n",
      "Epoch [20/50], Train Loss: 0.0534, Val Loss: 0.1082\n",
      "Epoch [21/50], Train Loss: 0.0515, Val Loss: 0.1052\n",
      "Epoch [22/50], Train Loss: 0.0493, Val Loss: 0.1035\n",
      "Epoch [23/50], Train Loss: 0.0507, Val Loss: 0.1013\n",
      "Epoch [24/50], Train Loss: 0.0483, Val Loss: 0.0992\n",
      "Epoch [25/50], Train Loss: 0.0485, Val Loss: 0.0971\n",
      "Epoch [26/50], Train Loss: 0.0482, Val Loss: 0.0952\n",
      "Epoch [27/50], Train Loss: 0.0466, Val Loss: 0.0928\n",
      "Epoch [28/50], Train Loss: 0.0463, Val Loss: 0.0907\n",
      "Epoch [29/50], Train Loss: 0.0476, Val Loss: 0.0897\n",
      "Epoch [30/50], Train Loss: 0.0460, Val Loss: 0.0878\n",
      "Epoch [31/50], Train Loss: 0.0453, Val Loss: 0.0863\n",
      "Epoch [32/50], Train Loss: 0.0434, Val Loss: 0.0840\n",
      "Epoch [33/50], Train Loss: 0.0424, Val Loss: 0.0834\n",
      "Epoch [34/50], Train Loss: 0.0396, Val Loss: 0.0817\n",
      "Epoch [35/50], Train Loss: 0.0433, Val Loss: 0.0803\n",
      "Epoch [36/50], Train Loss: 0.0396, Val Loss: 0.0782\n",
      "Epoch [37/50], Train Loss: 0.0420, Val Loss: 0.0765\n",
      "Epoch [38/50], Train Loss: 0.0418, Val Loss: 0.0751\n",
      "Epoch [39/50], Train Loss: 0.0392, Val Loss: 0.0734\n",
      "Epoch [40/50], Train Loss: 0.0377, Val Loss: 0.0715\n",
      "Epoch [41/50], Train Loss: 0.0383, Val Loss: 0.0694\n",
      "Epoch [42/50], Train Loss: 0.0375, Val Loss: 0.0681\n",
      "Epoch [43/50], Train Loss: 0.0381, Val Loss: 0.0671\n",
      "Epoch [44/50], Train Loss: 0.0367, Val Loss: 0.0655\n",
      "Epoch [45/50], Train Loss: 0.0364, Val Loss: 0.0645\n",
      "Epoch [46/50], Train Loss: 0.0366, Val Loss: 0.0625\n",
      "Epoch [47/50], Train Loss: 0.0351, Val Loss: 0.0610\n",
      "Epoch [48/50], Train Loss: 0.0358, Val Loss: 0.0599\n",
      "Epoch [49/50], Train Loss: 0.0360, Val Loss: 0.0589\n",
      "Epoch [50/50], Train Loss: 0.0341, Val Loss: 0.0581\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=16, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.3546, Val Loss: 0.6606\n",
      "Epoch [2/50], Train Loss: 0.2998, Val Loss: 0.5825\n",
      "Epoch [3/50], Train Loss: 0.2532, Val Loss: 0.5079\n",
      "Epoch [4/50], Train Loss: 0.2079, Val Loss: 0.4321\n",
      "Epoch [5/50], Train Loss: 0.1633, Val Loss: 0.3567\n",
      "Epoch [6/50], Train Loss: 0.1229, Val Loss: 0.2878\n",
      "Epoch [7/50], Train Loss: 0.0913, Val Loss: 0.2324\n",
      "Epoch [8/50], Train Loss: 0.0706, Val Loss: 0.1930\n",
      "Epoch [9/50], Train Loss: 0.0582, Val Loss: 0.1666\n",
      "Epoch [10/50], Train Loss: 0.0506, Val Loss: 0.1487\n",
      "Epoch [11/50], Train Loss: 0.0456, Val Loss: 0.1361\n",
      "Epoch [12/50], Train Loss: 0.0421, Val Loss: 0.1267\n",
      "Epoch [13/50], Train Loss: 0.0396, Val Loss: 0.1195\n",
      "Epoch [14/50], Train Loss: 0.0379, Val Loss: 0.1138\n",
      "Epoch [15/50], Train Loss: 0.0366, Val Loss: 0.1092\n",
      "Epoch [16/50], Train Loss: 0.0357, Val Loss: 0.1053\n",
      "Epoch [17/50], Train Loss: 0.0350, Val Loss: 0.1020\n",
      "Epoch [18/50], Train Loss: 0.0345, Val Loss: 0.0993\n",
      "Epoch [19/50], Train Loss: 0.0341, Val Loss: 0.0969\n",
      "Epoch [20/50], Train Loss: 0.0337, Val Loss: 0.0948\n",
      "Epoch [21/50], Train Loss: 0.0333, Val Loss: 0.0929\n",
      "Epoch [22/50], Train Loss: 0.0330, Val Loss: 0.0911\n",
      "Epoch [23/50], Train Loss: 0.0326, Val Loss: 0.0894\n",
      "Epoch [24/50], Train Loss: 0.0323, Val Loss: 0.0878\n",
      "Epoch [25/50], Train Loss: 0.0319, Val Loss: 0.0863\n",
      "Epoch [26/50], Train Loss: 0.0315, Val Loss: 0.0847\n",
      "Epoch [27/50], Train Loss: 0.0311, Val Loss: 0.0831\n",
      "Epoch [28/50], Train Loss: 0.0307, Val Loss: 0.0815\n",
      "Epoch [29/50], Train Loss: 0.0302, Val Loss: 0.0798\n",
      "Epoch [30/50], Train Loss: 0.0297, Val Loss: 0.0781\n",
      "Epoch [31/50], Train Loss: 0.0292, Val Loss: 0.0763\n",
      "Epoch [32/50], Train Loss: 0.0286, Val Loss: 0.0744\n",
      "Epoch [33/50], Train Loss: 0.0280, Val Loss: 0.0724\n",
      "Epoch [34/50], Train Loss: 0.0274, Val Loss: 0.0703\n",
      "Epoch [35/50], Train Loss: 0.0267, Val Loss: 0.0681\n",
      "Epoch [36/50], Train Loss: 0.0260, Val Loss: 0.0658\n",
      "Epoch [37/50], Train Loss: 0.0253, Val Loss: 0.0634\n",
      "Epoch [38/50], Train Loss: 0.0245, Val Loss: 0.0608\n",
      "Epoch [39/50], Train Loss: 0.0237, Val Loss: 0.0582\n",
      "Epoch [40/50], Train Loss: 0.0229, Val Loss: 0.0555\n",
      "Epoch [41/50], Train Loss: 0.0222, Val Loss: 0.0527\n",
      "Epoch [42/50], Train Loss: 0.0214, Val Loss: 0.0500\n",
      "Epoch [43/50], Train Loss: 0.0207, Val Loss: 0.0475\n",
      "Epoch [44/50], Train Loss: 0.0201, Val Loss: 0.0451\n",
      "Epoch [45/50], Train Loss: 0.0196, Val Loss: 0.0429\n",
      "Epoch [46/50], Train Loss: 0.0192, Val Loss: 0.0410\n",
      "Epoch [47/50], Train Loss: 0.0188, Val Loss: 0.0394\n",
      "Epoch [48/50], Train Loss: 0.0184, Val Loss: 0.0381\n",
      "Epoch [49/50], Train Loss: 0.0181, Val Loss: 0.0369\n",
      "Epoch [50/50], Train Loss: 0.0177, Val Loss: 0.0359\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=16, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.2207, Val Loss: 0.3536\n",
      "Epoch [2/50], Train Loss: 0.1861, Val Loss: 0.3108\n",
      "Epoch [3/50], Train Loss: 0.1571, Val Loss: 0.2772\n",
      "Epoch [4/50], Train Loss: 0.1343, Val Loss: 0.2490\n",
      "Epoch [5/50], Train Loss: 0.1192, Val Loss: 0.2238\n",
      "Epoch [6/50], Train Loss: 0.1028, Val Loss: 0.2015\n",
      "Epoch [7/50], Train Loss: 0.0889, Val Loss: 0.1810\n",
      "Epoch [8/50], Train Loss: 0.0808, Val Loss: 0.1633\n",
      "Epoch [9/50], Train Loss: 0.0717, Val Loss: 0.1486\n",
      "Epoch [10/50], Train Loss: 0.0641, Val Loss: 0.1365\n",
      "Epoch [11/50], Train Loss: 0.0633, Val Loss: 0.1263\n",
      "Epoch [12/50], Train Loss: 0.0582, Val Loss: 0.1170\n",
      "Epoch [13/50], Train Loss: 0.0569, Val Loss: 0.1102\n",
      "Epoch [14/50], Train Loss: 0.0563, Val Loss: 0.1060\n",
      "Epoch [15/50], Train Loss: 0.0556, Val Loss: 0.1027\n",
      "Epoch [16/50], Train Loss: 0.0542, Val Loss: 0.1000\n",
      "Epoch [17/50], Train Loss: 0.0535, Val Loss: 0.0970\n",
      "Epoch [18/50], Train Loss: 0.0521, Val Loss: 0.0939\n",
      "Epoch [19/50], Train Loss: 0.0514, Val Loss: 0.0913\n",
      "Epoch [20/50], Train Loss: 0.0491, Val Loss: 0.0895\n",
      "Epoch [21/50], Train Loss: 0.0472, Val Loss: 0.0880\n",
      "Epoch [22/50], Train Loss: 0.0478, Val Loss: 0.0858\n",
      "Epoch [23/50], Train Loss: 0.0462, Val Loss: 0.0840\n",
      "Epoch [24/50], Train Loss: 0.0464, Val Loss: 0.0818\n",
      "Epoch [25/50], Train Loss: 0.0439, Val Loss: 0.0787\n",
      "Epoch [26/50], Train Loss: 0.0431, Val Loss: 0.0778\n",
      "Epoch [27/50], Train Loss: 0.0444, Val Loss: 0.0735\n",
      "Epoch [28/50], Train Loss: 0.0435, Val Loss: 0.0701\n",
      "Epoch [29/50], Train Loss: 0.0414, Val Loss: 0.0671\n",
      "Epoch [30/50], Train Loss: 0.0408, Val Loss: 0.0641\n",
      "Epoch [31/50], Train Loss: 0.0387, Val Loss: 0.0610\n",
      "Epoch [32/50], Train Loss: 0.0384, Val Loss: 0.0575\n",
      "Epoch [33/50], Train Loss: 0.0389, Val Loss: 0.0557\n",
      "Epoch [34/50], Train Loss: 0.0371, Val Loss: 0.0522\n",
      "Epoch [35/50], Train Loss: 0.0351, Val Loss: 0.0492\n",
      "Epoch [36/50], Train Loss: 0.0340, Val Loss: 0.0467\n",
      "Epoch [37/50], Train Loss: 0.0328, Val Loss: 0.0441\n",
      "Epoch [38/50], Train Loss: 0.0326, Val Loss: 0.0419\n",
      "Epoch [39/50], Train Loss: 0.0324, Val Loss: 0.0393\n",
      "Epoch [40/50], Train Loss: 0.0310, Val Loss: 0.0382\n",
      "Epoch [41/50], Train Loss: 0.0306, Val Loss: 0.0372\n",
      "Epoch [42/50], Train Loss: 0.0303, Val Loss: 0.0355\n",
      "Epoch [43/50], Train Loss: 0.0302, Val Loss: 0.0335\n",
      "Epoch [44/50], Train Loss: 0.0300, Val Loss: 0.0327\n",
      "Epoch [45/50], Train Loss: 0.0294, Val Loss: 0.0323\n",
      "Epoch [46/50], Train Loss: 0.0284, Val Loss: 0.0310\n",
      "Epoch [47/50], Train Loss: 0.0282, Val Loss: 0.0306\n",
      "Epoch [48/50], Train Loss: 0.0283, Val Loss: 0.0300\n",
      "Epoch [49/50], Train Loss: 0.0268, Val Loss: 0.0289\n",
      "Epoch [50/50], Train Loss: 0.0266, Val Loss: 0.0286\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=16, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1414, Val Loss: 0.1761\n",
      "Epoch [2/50], Train Loss: 0.1187, Val Loss: 0.1666\n",
      "Epoch [3/50], Train Loss: 0.1103, Val Loss: 0.1618\n",
      "Epoch [4/50], Train Loss: 0.1054, Val Loss: 0.1575\n",
      "Epoch [5/50], Train Loss: 0.0990, Val Loss: 0.1544\n",
      "Epoch [6/50], Train Loss: 0.0940, Val Loss: 0.1529\n",
      "Epoch [7/50], Train Loss: 0.0917, Val Loss: 0.1506\n",
      "Epoch [8/50], Train Loss: 0.0873, Val Loss: 0.1485\n",
      "Epoch [9/50], Train Loss: 0.0848, Val Loss: 0.1465\n",
      "Epoch [10/50], Train Loss: 0.0822, Val Loss: 0.1450\n",
      "Epoch [11/50], Train Loss: 0.0790, Val Loss: 0.1435\n",
      "Epoch [12/50], Train Loss: 0.0746, Val Loss: 0.1417\n",
      "Epoch [13/50], Train Loss: 0.0756, Val Loss: 0.1405\n",
      "Epoch [14/50], Train Loss: 0.0719, Val Loss: 0.1389\n",
      "Epoch [15/50], Train Loss: 0.0703, Val Loss: 0.1372\n",
      "Epoch [16/50], Train Loss: 0.0691, Val Loss: 0.1361\n",
      "Epoch [17/50], Train Loss: 0.0663, Val Loss: 0.1355\n",
      "Epoch [18/50], Train Loss: 0.0661, Val Loss: 0.1344\n",
      "Epoch [19/50], Train Loss: 0.0654, Val Loss: 0.1324\n",
      "Epoch [20/50], Train Loss: 0.0640, Val Loss: 0.1316\n",
      "Epoch [21/50], Train Loss: 0.0609, Val Loss: 0.1305\n",
      "Epoch [22/50], Train Loss: 0.0599, Val Loss: 0.1287\n",
      "Epoch [23/50], Train Loss: 0.0587, Val Loss: 0.1270\n",
      "Epoch [24/50], Train Loss: 0.0588, Val Loss: 0.1254\n",
      "Epoch [25/50], Train Loss: 0.0583, Val Loss: 0.1244\n",
      "Epoch [26/50], Train Loss: 0.0553, Val Loss: 0.1228\n",
      "Epoch [27/50], Train Loss: 0.0555, Val Loss: 0.1219\n",
      "Epoch [28/50], Train Loss: 0.0548, Val Loss: 0.1204\n",
      "Epoch [29/50], Train Loss: 0.0535, Val Loss: 0.1193\n",
      "Epoch [30/50], Train Loss: 0.0513, Val Loss: 0.1173\n",
      "Epoch [31/50], Train Loss: 0.0517, Val Loss: 0.1153\n",
      "Epoch [32/50], Train Loss: 0.0496, Val Loss: 0.1133\n",
      "Epoch [33/50], Train Loss: 0.0482, Val Loss: 0.1121\n",
      "Epoch [34/50], Train Loss: 0.0478, Val Loss: 0.1111\n",
      "Epoch [35/50], Train Loss: 0.0494, Val Loss: 0.1090\n",
      "Epoch [36/50], Train Loss: 0.0476, Val Loss: 0.1081\n",
      "Epoch [37/50], Train Loss: 0.0467, Val Loss: 0.1069\n",
      "Epoch [38/50], Train Loss: 0.0466, Val Loss: 0.1063\n",
      "Epoch [39/50], Train Loss: 0.0453, Val Loss: 0.1041\n",
      "Epoch [40/50], Train Loss: 0.0455, Val Loss: 0.1027\n",
      "Epoch [41/50], Train Loss: 0.0449, Val Loss: 0.1014\n",
      "Epoch [42/50], Train Loss: 0.0446, Val Loss: 0.1002\n",
      "Epoch [43/50], Train Loss: 0.0431, Val Loss: 0.0982\n",
      "Epoch [44/50], Train Loss: 0.0422, Val Loss: 0.0968\n",
      "Epoch [45/50], Train Loss: 0.0414, Val Loss: 0.0952\n",
      "Epoch [46/50], Train Loss: 0.0417, Val Loss: 0.0939\n",
      "Epoch [47/50], Train Loss: 0.0410, Val Loss: 0.0921\n",
      "Epoch [48/50], Train Loss: 0.0410, Val Loss: 0.0919\n",
      "Epoch [49/50], Train Loss: 0.0406, Val Loss: 0.0903\n",
      "Epoch [50/50], Train Loss: 0.0402, Val Loss: 0.0880\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=32, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1043, Val Loss: 0.2617\n",
      "Epoch [2/50], Train Loss: 0.0847, Val Loss: 0.2258\n",
      "Epoch [3/50], Train Loss: 0.0694, Val Loss: 0.1943\n",
      "Epoch [4/50], Train Loss: 0.0569, Val Loss: 0.1662\n",
      "Epoch [5/50], Train Loss: 0.0468, Val Loss: 0.1414\n",
      "Epoch [6/50], Train Loss: 0.0391, Val Loss: 0.1200\n",
      "Epoch [7/50], Train Loss: 0.0334, Val Loss: 0.1022\n",
      "Epoch [8/50], Train Loss: 0.0293, Val Loss: 0.0879\n",
      "Epoch [9/50], Train Loss: 0.0265, Val Loss: 0.0766\n",
      "Epoch [10/50], Train Loss: 0.0245, Val Loss: 0.0678\n",
      "Epoch [11/50], Train Loss: 0.0230, Val Loss: 0.0608\n",
      "Epoch [12/50], Train Loss: 0.0218, Val Loss: 0.0550\n",
      "Epoch [13/50], Train Loss: 0.0206, Val Loss: 0.0499\n",
      "Epoch [14/50], Train Loss: 0.0194, Val Loss: 0.0452\n",
      "Epoch [15/50], Train Loss: 0.0182, Val Loss: 0.0407\n",
      "Epoch [16/50], Train Loss: 0.0169, Val Loss: 0.0362\n",
      "Epoch [17/50], Train Loss: 0.0156, Val Loss: 0.0318\n",
      "Epoch [18/50], Train Loss: 0.0143, Val Loss: 0.0275\n",
      "Epoch [19/50], Train Loss: 0.0130, Val Loss: 0.0235\n",
      "Epoch [20/50], Train Loss: 0.0118, Val Loss: 0.0200\n",
      "Epoch [21/50], Train Loss: 0.0109, Val Loss: 0.0173\n",
      "Epoch [22/50], Train Loss: 0.0102, Val Loss: 0.0154\n",
      "Epoch [23/50], Train Loss: 0.0097, Val Loss: 0.0142\n",
      "Epoch [24/50], Train Loss: 0.0092, Val Loss: 0.0134\n",
      "Epoch [25/50], Train Loss: 0.0088, Val Loss: 0.0128\n",
      "Epoch [26/50], Train Loss: 0.0083, Val Loss: 0.0125\n",
      "Epoch [27/50], Train Loss: 0.0080, Val Loss: 0.0122\n",
      "Epoch [28/50], Train Loss: 0.0076, Val Loss: 0.0121\n",
      "Epoch [29/50], Train Loss: 0.0073, Val Loss: 0.0121\n",
      "Epoch [30/50], Train Loss: 0.0070, Val Loss: 0.0120\n",
      "Epoch [31/50], Train Loss: 0.0067, Val Loss: 0.0121\n",
      "Epoch [32/50], Train Loss: 0.0064, Val Loss: 0.0121\n",
      "Epoch [33/50], Train Loss: 0.0061, Val Loss: 0.0122\n",
      "Epoch [34/50], Train Loss: 0.0059, Val Loss: 0.0123\n",
      "Epoch [35/50], Train Loss: 0.0056, Val Loss: 0.0123\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=32, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1935, Val Loss: 0.4114\n",
      "Epoch [2/50], Train Loss: 0.1730, Val Loss: 0.3753\n",
      "Epoch [3/50], Train Loss: 0.1550, Val Loss: 0.3409\n",
      "Epoch [4/50], Train Loss: 0.1377, Val Loss: 0.3059\n",
      "Epoch [5/50], Train Loss: 0.1216, Val Loss: 0.2689\n",
      "Epoch [6/50], Train Loss: 0.1041, Val Loss: 0.2297\n",
      "Epoch [7/50], Train Loss: 0.0875, Val Loss: 0.1891\n",
      "Epoch [8/50], Train Loss: 0.0715, Val Loss: 0.1514\n",
      "Epoch [9/50], Train Loss: 0.0570, Val Loss: 0.1204\n",
      "Epoch [10/50], Train Loss: 0.0487, Val Loss: 0.0990\n",
      "Epoch [11/50], Train Loss: 0.0430, Val Loss: 0.0858\n",
      "Epoch [12/50], Train Loss: 0.0401, Val Loss: 0.0779\n",
      "Epoch [13/50], Train Loss: 0.0382, Val Loss: 0.0738\n",
      "Epoch [14/50], Train Loss: 0.0367, Val Loss: 0.0702\n",
      "Epoch [15/50], Train Loss: 0.0352, Val Loss: 0.0677\n",
      "Epoch [16/50], Train Loss: 0.0343, Val Loss: 0.0640\n",
      "Epoch [17/50], Train Loss: 0.0334, Val Loss: 0.0609\n",
      "Epoch [18/50], Train Loss: 0.0317, Val Loss: 0.0583\n",
      "Epoch [19/50], Train Loss: 0.0304, Val Loss: 0.0559\n",
      "Epoch [20/50], Train Loss: 0.0294, Val Loss: 0.0530\n",
      "Epoch [21/50], Train Loss: 0.0287, Val Loss: 0.0509\n",
      "Epoch [22/50], Train Loss: 0.0284, Val Loss: 0.0486\n",
      "Epoch [23/50], Train Loss: 0.0274, Val Loss: 0.0458\n",
      "Epoch [24/50], Train Loss: 0.0267, Val Loss: 0.0442\n",
      "Epoch [25/50], Train Loss: 0.0259, Val Loss: 0.0417\n",
      "Epoch [26/50], Train Loss: 0.0253, Val Loss: 0.0391\n",
      "Epoch [27/50], Train Loss: 0.0242, Val Loss: 0.0370\n",
      "Epoch [28/50], Train Loss: 0.0232, Val Loss: 0.0361\n",
      "Epoch [29/50], Train Loss: 0.0228, Val Loss: 0.0341\n",
      "Epoch [30/50], Train Loss: 0.0216, Val Loss: 0.0326\n",
      "Epoch [31/50], Train Loss: 0.0205, Val Loss: 0.0302\n",
      "Epoch [32/50], Train Loss: 0.0196, Val Loss: 0.0289\n",
      "Epoch [33/50], Train Loss: 0.0198, Val Loss: 0.0272\n",
      "Epoch [34/50], Train Loss: 0.0192, Val Loss: 0.0255\n",
      "Epoch [35/50], Train Loss: 0.0181, Val Loss: 0.0250\n",
      "Epoch [36/50], Train Loss: 0.0169, Val Loss: 0.0231\n",
      "Epoch [37/50], Train Loss: 0.0169, Val Loss: 0.0222\n",
      "Epoch [38/50], Train Loss: 0.0158, Val Loss: 0.0207\n",
      "Epoch [39/50], Train Loss: 0.0149, Val Loss: 0.0200\n",
      "Epoch [40/50], Train Loss: 0.0147, Val Loss: 0.0191\n",
      "Epoch [41/50], Train Loss: 0.0136, Val Loss: 0.0184\n",
      "Epoch [42/50], Train Loss: 0.0139, Val Loss: 0.0175\n",
      "Epoch [43/50], Train Loss: 0.0132, Val Loss: 0.0167\n",
      "Epoch [44/50], Train Loss: 0.0125, Val Loss: 0.0164\n",
      "Epoch [45/50], Train Loss: 0.0126, Val Loss: 0.0158\n",
      "Epoch [46/50], Train Loss: 0.0119, Val Loss: 0.0149\n",
      "Epoch [47/50], Train Loss: 0.0119, Val Loss: 0.0145\n",
      "Epoch [48/50], Train Loss: 0.0107, Val Loss: 0.0148\n",
      "Epoch [49/50], Train Loss: 0.0114, Val Loss: 0.0142\n",
      "Epoch [50/50], Train Loss: 0.0105, Val Loss: 0.0140\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=32, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1372, Val Loss: 0.3128\n",
      "Epoch [2/50], Train Loss: 0.1178, Val Loss: 0.2824\n",
      "Epoch [3/50], Train Loss: 0.1028, Val Loss: 0.2525\n",
      "Epoch [4/50], Train Loss: 0.0891, Val Loss: 0.2227\n",
      "Epoch [5/50], Train Loss: 0.0776, Val Loss: 0.1940\n",
      "Epoch [6/50], Train Loss: 0.0668, Val Loss: 0.1674\n",
      "Epoch [7/50], Train Loss: 0.0593, Val Loss: 0.1454\n",
      "Epoch [8/50], Train Loss: 0.0544, Val Loss: 0.1272\n",
      "Epoch [9/50], Train Loss: 0.0518, Val Loss: 0.1143\n",
      "Epoch [10/50], Train Loss: 0.0491, Val Loss: 0.1045\n",
      "Epoch [11/50], Train Loss: 0.0481, Val Loss: 0.0964\n",
      "Epoch [12/50], Train Loss: 0.0465, Val Loss: 0.0890\n",
      "Epoch [13/50], Train Loss: 0.0431, Val Loss: 0.0840\n",
      "Epoch [14/50], Train Loss: 0.0409, Val Loss: 0.0781\n",
      "Epoch [15/50], Train Loss: 0.0398, Val Loss: 0.0730\n",
      "Epoch [16/50], Train Loss: 0.0372, Val Loss: 0.0669\n",
      "Epoch [17/50], Train Loss: 0.0363, Val Loss: 0.0621\n",
      "Epoch [18/50], Train Loss: 0.0349, Val Loss: 0.0579\n",
      "Epoch [19/50], Train Loss: 0.0346, Val Loss: 0.0540\n",
      "Epoch [20/50], Train Loss: 0.0342, Val Loss: 0.0494\n",
      "Epoch [21/50], Train Loss: 0.0317, Val Loss: 0.0448\n",
      "Epoch [22/50], Train Loss: 0.0295, Val Loss: 0.0415\n",
      "Epoch [23/50], Train Loss: 0.0293, Val Loss: 0.0386\n",
      "Epoch [24/50], Train Loss: 0.0288, Val Loss: 0.0355\n",
      "Epoch [25/50], Train Loss: 0.0267, Val Loss: 0.0319\n",
      "Epoch [26/50], Train Loss: 0.0260, Val Loss: 0.0301\n",
      "Epoch [27/50], Train Loss: 0.0245, Val Loss: 0.0263\n",
      "Epoch [28/50], Train Loss: 0.0244, Val Loss: 0.0240\n",
      "Epoch [29/50], Train Loss: 0.0226, Val Loss: 0.0221\n",
      "Epoch [30/50], Train Loss: 0.0222, Val Loss: 0.0206\n",
      "Epoch [31/50], Train Loss: 0.0213, Val Loss: 0.0185\n",
      "Epoch [32/50], Train Loss: 0.0207, Val Loss: 0.0172\n",
      "Epoch [33/50], Train Loss: 0.0211, Val Loss: 0.0155\n",
      "Epoch [34/50], Train Loss: 0.0206, Val Loss: 0.0157\n",
      "Epoch [35/50], Train Loss: 0.0198, Val Loss: 0.0142\n",
      "Epoch [36/50], Train Loss: 0.0185, Val Loss: 0.0131\n",
      "Epoch [37/50], Train Loss: 0.0186, Val Loss: 0.0129\n",
      "Epoch [38/50], Train Loss: 0.0181, Val Loss: 0.0137\n",
      "Epoch [39/50], Train Loss: 0.0181, Val Loss: 0.0130\n",
      "Epoch [40/50], Train Loss: 0.0166, Val Loss: 0.0122\n",
      "Epoch [41/50], Train Loss: 0.0179, Val Loss: 0.0118\n",
      "Epoch [42/50], Train Loss: 0.0163, Val Loss: 0.0112\n",
      "Epoch [43/50], Train Loss: 0.0165, Val Loss: 0.0112\n",
      "Epoch [44/50], Train Loss: 0.0164, Val Loss: 0.0117\n",
      "Epoch [45/50], Train Loss: 0.0157, Val Loss: 0.0110\n",
      "Epoch [46/50], Train Loss: 0.0165, Val Loss: 0.0109\n",
      "Epoch [47/50], Train Loss: 0.0153, Val Loss: 0.0109\n",
      "Epoch [48/50], Train Loss: 0.0156, Val Loss: 0.0097\n",
      "Epoch [49/50], Train Loss: 0.0161, Val Loss: 0.0083\n",
      "Epoch [50/50], Train Loss: 0.0144, Val Loss: 0.0091\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=32, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1731, Val Loss: 0.3411\n",
      "Epoch [2/50], Train Loss: 0.1179, Val Loss: 0.2479\n",
      "Epoch [3/50], Train Loss: 0.0818, Val Loss: 0.1779\n",
      "Epoch [4/50], Train Loss: 0.0564, Val Loss: 0.1277\n",
      "Epoch [5/50], Train Loss: 0.0424, Val Loss: 0.0996\n",
      "Epoch [6/50], Train Loss: 0.0374, Val Loss: 0.0879\n",
      "Epoch [7/50], Train Loss: 0.0359, Val Loss: 0.0832\n",
      "Epoch [8/50], Train Loss: 0.0350, Val Loss: 0.0804\n",
      "Epoch [9/50], Train Loss: 0.0340, Val Loss: 0.0776\n",
      "Epoch [10/50], Train Loss: 0.0330, Val Loss: 0.0746\n",
      "Epoch [11/50], Train Loss: 0.0320, Val Loss: 0.0713\n",
      "Epoch [12/50], Train Loss: 0.0309, Val Loss: 0.0677\n",
      "Epoch [13/50], Train Loss: 0.0298, Val Loss: 0.0638\n",
      "Epoch [14/50], Train Loss: 0.0286, Val Loss: 0.0596\n",
      "Epoch [15/50], Train Loss: 0.0274, Val Loss: 0.0551\n",
      "Epoch [16/50], Train Loss: 0.0260, Val Loss: 0.0503\n",
      "Epoch [17/50], Train Loss: 0.0245, Val Loss: 0.0451\n",
      "Epoch [18/50], Train Loss: 0.0229, Val Loss: 0.0397\n",
      "Epoch [19/50], Train Loss: 0.0212, Val Loss: 0.0341\n",
      "Epoch [20/50], Train Loss: 0.0194, Val Loss: 0.0285\n",
      "Epoch [21/50], Train Loss: 0.0176, Val Loss: 0.0233\n",
      "Epoch [22/50], Train Loss: 0.0159, Val Loss: 0.0189\n",
      "Epoch [23/50], Train Loss: 0.0145, Val Loss: 0.0156\n",
      "Epoch [24/50], Train Loss: 0.0134, Val Loss: 0.0134\n",
      "Epoch [25/50], Train Loss: 0.0126, Val Loss: 0.0121\n",
      "Epoch [26/50], Train Loss: 0.0120, Val Loss: 0.0115\n",
      "Epoch [27/50], Train Loss: 0.0115, Val Loss: 0.0112\n",
      "Epoch [28/50], Train Loss: 0.0110, Val Loss: 0.0112\n",
      "Epoch [29/50], Train Loss: 0.0106, Val Loss: 0.0112\n",
      "Epoch [30/50], Train Loss: 0.0102, Val Loss: 0.0114\n",
      "Epoch [31/50], Train Loss: 0.0098, Val Loss: 0.0116\n",
      "Epoch [32/50], Train Loss: 0.0095, Val Loss: 0.0118\n",
      "Epoch [33/50], Train Loss: 0.0091, Val Loss: 0.0120\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=32, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1128, Val Loss: 0.2524\n",
      "Epoch [2/50], Train Loss: 0.0803, Val Loss: 0.2043\n",
      "Epoch [3/50], Train Loss: 0.0623, Val Loss: 0.1663\n",
      "Epoch [4/50], Train Loss: 0.0511, Val Loss: 0.1378\n",
      "Epoch [5/50], Train Loss: 0.0446, Val Loss: 0.1180\n",
      "Epoch [6/50], Train Loss: 0.0430, Val Loss: 0.1057\n",
      "Epoch [7/50], Train Loss: 0.0410, Val Loss: 0.0970\n",
      "Epoch [8/50], Train Loss: 0.0400, Val Loss: 0.0924\n",
      "Epoch [9/50], Train Loss: 0.0396, Val Loss: 0.0886\n",
      "Epoch [10/50], Train Loss: 0.0392, Val Loss: 0.0854\n",
      "Epoch [11/50], Train Loss: 0.0369, Val Loss: 0.0825\n",
      "Epoch [12/50], Train Loss: 0.0370, Val Loss: 0.0796\n",
      "Epoch [13/50], Train Loss: 0.0356, Val Loss: 0.0766\n",
      "Epoch [14/50], Train Loss: 0.0342, Val Loss: 0.0728\n",
      "Epoch [15/50], Train Loss: 0.0327, Val Loss: 0.0687\n",
      "Epoch [16/50], Train Loss: 0.0325, Val Loss: 0.0651\n",
      "Epoch [17/50], Train Loss: 0.0303, Val Loss: 0.0611\n",
      "Epoch [18/50], Train Loss: 0.0284, Val Loss: 0.0571\n",
      "Epoch [19/50], Train Loss: 0.0273, Val Loss: 0.0522\n",
      "Epoch [20/50], Train Loss: 0.0252, Val Loss: 0.0473\n",
      "Epoch [21/50], Train Loss: 0.0226, Val Loss: 0.0419\n",
      "Epoch [22/50], Train Loss: 0.0218, Val Loss: 0.0369\n",
      "Epoch [23/50], Train Loss: 0.0198, Val Loss: 0.0333\n",
      "Epoch [24/50], Train Loss: 0.0191, Val Loss: 0.0310\n",
      "Epoch [25/50], Train Loss: 0.0186, Val Loss: 0.0269\n",
      "Epoch [26/50], Train Loss: 0.0174, Val Loss: 0.0253\n",
      "Epoch [27/50], Train Loss: 0.0160, Val Loss: 0.0229\n",
      "Epoch [28/50], Train Loss: 0.0156, Val Loss: 0.0203\n",
      "Epoch [29/50], Train Loss: 0.0151, Val Loss: 0.0179\n",
      "Epoch [30/50], Train Loss: 0.0137, Val Loss: 0.0166\n",
      "Epoch [31/50], Train Loss: 0.0125, Val Loss: 0.0149\n",
      "Epoch [32/50], Train Loss: 0.0115, Val Loss: 0.0132\n",
      "Epoch [33/50], Train Loss: 0.0113, Val Loss: 0.0127\n",
      "Epoch [34/50], Train Loss: 0.0109, Val Loss: 0.0112\n",
      "Epoch [35/50], Train Loss: 0.0103, Val Loss: 0.0107\n",
      "Epoch [36/50], Train Loss: 0.0098, Val Loss: 0.0103\n",
      "Epoch [37/50], Train Loss: 0.0094, Val Loss: 0.0098\n",
      "Epoch [38/50], Train Loss: 0.0094, Val Loss: 0.0096\n",
      "Epoch [39/50], Train Loss: 0.0093, Val Loss: 0.0082\n",
      "Epoch [40/50], Train Loss: 0.0092, Val Loss: 0.0081\n",
      "Epoch [41/50], Train Loss: 0.0091, Val Loss: 0.0077\n",
      "Epoch [42/50], Train Loss: 0.0094, Val Loss: 0.0082\n",
      "Epoch [43/50], Train Loss: 0.0089, Val Loss: 0.0080\n",
      "Epoch [44/50], Train Loss: 0.0086, Val Loss: 0.0083\n",
      "Epoch [45/50], Train Loss: 0.0088, Val Loss: 0.0075\n",
      "Epoch [46/50], Train Loss: 0.0089, Val Loss: 0.0074\n",
      "Epoch [47/50], Train Loss: 0.0082, Val Loss: 0.0078\n",
      "Epoch [48/50], Train Loss: 0.0083, Val Loss: 0.0073\n",
      "Epoch [49/50], Train Loss: 0.0081, Val Loss: 0.0072\n",
      "Epoch [50/50], Train Loss: 0.0078, Val Loss: 0.0075\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=32, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1353, Val Loss: 0.3028\n",
      "Epoch [2/50], Train Loss: 0.1006, Val Loss: 0.2508\n",
      "Epoch [3/50], Train Loss: 0.0905, Val Loss: 0.2171\n",
      "Epoch [4/50], Train Loss: 0.0799, Val Loss: 0.1947\n",
      "Epoch [5/50], Train Loss: 0.0743, Val Loss: 0.1780\n",
      "Epoch [6/50], Train Loss: 0.0702, Val Loss: 0.1653\n",
      "Epoch [7/50], Train Loss: 0.0674, Val Loss: 0.1548\n",
      "Epoch [8/50], Train Loss: 0.0623, Val Loss: 0.1465\n",
      "Epoch [9/50], Train Loss: 0.0613, Val Loss: 0.1405\n",
      "Epoch [10/50], Train Loss: 0.0580, Val Loss: 0.1343\n",
      "Epoch [11/50], Train Loss: 0.0546, Val Loss: 0.1282\n",
      "Epoch [12/50], Train Loss: 0.0551, Val Loss: 0.1230\n",
      "Epoch [13/50], Train Loss: 0.0520, Val Loss: 0.1165\n",
      "Epoch [14/50], Train Loss: 0.0499, Val Loss: 0.1117\n",
      "Epoch [15/50], Train Loss: 0.0501, Val Loss: 0.1063\n",
      "Epoch [16/50], Train Loss: 0.0487, Val Loss: 0.1010\n",
      "Epoch [17/50], Train Loss: 0.0455, Val Loss: 0.0951\n",
      "Epoch [18/50], Train Loss: 0.0455, Val Loss: 0.0928\n",
      "Epoch [19/50], Train Loss: 0.0448, Val Loss: 0.0845\n",
      "Epoch [20/50], Train Loss: 0.0412, Val Loss: 0.0786\n",
      "Epoch [21/50], Train Loss: 0.0430, Val Loss: 0.0739\n",
      "Epoch [22/50], Train Loss: 0.0411, Val Loss: 0.0716\n",
      "Epoch [23/50], Train Loss: 0.0380, Val Loss: 0.0659\n",
      "Epoch [24/50], Train Loss: 0.0383, Val Loss: 0.0647\n",
      "Epoch [25/50], Train Loss: 0.0370, Val Loss: 0.0583\n",
      "Epoch [26/50], Train Loss: 0.0360, Val Loss: 0.0577\n",
      "Epoch [27/50], Train Loss: 0.0354, Val Loss: 0.0529\n",
      "Epoch [28/50], Train Loss: 0.0353, Val Loss: 0.0528\n",
      "Epoch [29/50], Train Loss: 0.0336, Val Loss: 0.0493\n",
      "Epoch [30/50], Train Loss: 0.0336, Val Loss: 0.0484\n",
      "Epoch [31/50], Train Loss: 0.0333, Val Loss: 0.0469\n",
      "Epoch [32/50], Train Loss: 0.0325, Val Loss: 0.0447\n",
      "Epoch [33/50], Train Loss: 0.0315, Val Loss: 0.0412\n",
      "Epoch [34/50], Train Loss: 0.0303, Val Loss: 0.0400\n",
      "Epoch [35/50], Train Loss: 0.0284, Val Loss: 0.0379\n",
      "Epoch [36/50], Train Loss: 0.0284, Val Loss: 0.0383\n",
      "Epoch [37/50], Train Loss: 0.0283, Val Loss: 0.0348\n",
      "Epoch [38/50], Train Loss: 0.0282, Val Loss: 0.0325\n",
      "Epoch [39/50], Train Loss: 0.0288, Val Loss: 0.0316\n",
      "Epoch [40/50], Train Loss: 0.0272, Val Loss: 0.0277\n",
      "Epoch [41/50], Train Loss: 0.0262, Val Loss: 0.0286\n",
      "Epoch [42/50], Train Loss: 0.0236, Val Loss: 0.0261\n",
      "Epoch [43/50], Train Loss: 0.0240, Val Loss: 0.0240\n",
      "Epoch [44/50], Train Loss: 0.0244, Val Loss: 0.0220\n",
      "Epoch [45/50], Train Loss: 0.0237, Val Loss: 0.0203\n",
      "Epoch [46/50], Train Loss: 0.0233, Val Loss: 0.0190\n",
      "Epoch [47/50], Train Loss: 0.0215, Val Loss: 0.0203\n",
      "Epoch [48/50], Train Loss: 0.0218, Val Loss: 0.0183\n",
      "Epoch [49/50], Train Loss: 0.0198, Val Loss: 0.0174\n",
      "Epoch [50/50], Train Loss: 0.0205, Val Loss: 0.0186\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=32, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1396, Val Loss: 0.3125\n",
      "Epoch [2/50], Train Loss: 0.0915, Val Loss: 0.2280\n",
      "Epoch [3/50], Train Loss: 0.0587, Val Loss: 0.1576\n",
      "Epoch [4/50], Train Loss: 0.0394, Val Loss: 0.1094\n",
      "Epoch [5/50], Train Loss: 0.0327, Val Loss: 0.0856\n",
      "Epoch [6/50], Train Loss: 0.0312, Val Loss: 0.0758\n",
      "Epoch [7/50], Train Loss: 0.0299, Val Loss: 0.0702\n",
      "Epoch [8/50], Train Loss: 0.0282, Val Loss: 0.0648\n",
      "Epoch [9/50], Train Loss: 0.0261, Val Loss: 0.0588\n",
      "Epoch [10/50], Train Loss: 0.0239, Val Loss: 0.0521\n",
      "Epoch [11/50], Train Loss: 0.0215, Val Loss: 0.0451\n",
      "Epoch [12/50], Train Loss: 0.0193, Val Loss: 0.0385\n",
      "Epoch [13/50], Train Loss: 0.0176, Val Loss: 0.0332\n",
      "Epoch [14/50], Train Loss: 0.0167, Val Loss: 0.0298\n",
      "Epoch [15/50], Train Loss: 0.0162, Val Loss: 0.0278\n",
      "Epoch [16/50], Train Loss: 0.0156, Val Loss: 0.0263\n",
      "Epoch [17/50], Train Loss: 0.0150, Val Loss: 0.0250\n",
      "Epoch [18/50], Train Loss: 0.0144, Val Loss: 0.0239\n",
      "Epoch [19/50], Train Loss: 0.0137, Val Loss: 0.0230\n",
      "Epoch [20/50], Train Loss: 0.0130, Val Loss: 0.0222\n",
      "Epoch [21/50], Train Loss: 0.0121, Val Loss: 0.0217\n",
      "Epoch [22/50], Train Loss: 0.0111, Val Loss: 0.0215\n",
      "Epoch [23/50], Train Loss: 0.0101, Val Loss: 0.0215\n",
      "Epoch [24/50], Train Loss: 0.0089, Val Loss: 0.0216\n",
      "Epoch [25/50], Train Loss: 0.0076, Val Loss: 0.0216\n",
      "Epoch [26/50], Train Loss: 0.0064, Val Loss: 0.0210\n",
      "Epoch [27/50], Train Loss: 0.0053, Val Loss: 0.0198\n",
      "Epoch [28/50], Train Loss: 0.0043, Val Loss: 0.0182\n",
      "Epoch [29/50], Train Loss: 0.0036, Val Loss: 0.0168\n",
      "Epoch [30/50], Train Loss: 0.0031, Val Loss: 0.0158\n",
      "Epoch [31/50], Train Loss: 0.0029, Val Loss: 0.0152\n",
      "Epoch [32/50], Train Loss: 0.0029, Val Loss: 0.0150\n",
      "Epoch [33/50], Train Loss: 0.0028, Val Loss: 0.0148\n",
      "Epoch [34/50], Train Loss: 0.0028, Val Loss: 0.0147\n",
      "Epoch [35/50], Train Loss: 0.0028, Val Loss: 0.0146\n",
      "Epoch [36/50], Train Loss: 0.0027, Val Loss: 0.0144\n",
      "Epoch [37/50], Train Loss: 0.0027, Val Loss: 0.0143\n",
      "Epoch [38/50], Train Loss: 0.0027, Val Loss: 0.0142\n",
      "Epoch [39/50], Train Loss: 0.0027, Val Loss: 0.0141\n",
      "Epoch [40/50], Train Loss: 0.0026, Val Loss: 0.0139\n",
      "Epoch [41/50], Train Loss: 0.0026, Val Loss: 0.0138\n",
      "Epoch [42/50], Train Loss: 0.0026, Val Loss: 0.0137\n",
      "Epoch [43/50], Train Loss: 0.0026, Val Loss: 0.0136\n",
      "Epoch [44/50], Train Loss: 0.0026, Val Loss: 0.0135\n",
      "Epoch [45/50], Train Loss: 0.0025, Val Loss: 0.0133\n",
      "Epoch [46/50], Train Loss: 0.0025, Val Loss: 0.0132\n",
      "Epoch [47/50], Train Loss: 0.0025, Val Loss: 0.0131\n",
      "Epoch [48/50], Train Loss: 0.0025, Val Loss: 0.0130\n",
      "Epoch [49/50], Train Loss: 0.0025, Val Loss: 0.0129\n",
      "Epoch [50/50], Train Loss: 0.0024, Val Loss: 0.0128\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=32, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1140, Val Loss: 0.2947\n",
      "Epoch [2/50], Train Loss: 0.0860, Val Loss: 0.2379\n",
      "Epoch [3/50], Train Loss: 0.0684, Val Loss: 0.1932\n",
      "Epoch [4/50], Train Loss: 0.0575, Val Loss: 0.1567\n",
      "Epoch [5/50], Train Loss: 0.0526, Val Loss: 0.1313\n",
      "Epoch [6/50], Train Loss: 0.0487, Val Loss: 0.1146\n",
      "Epoch [7/50], Train Loss: 0.0472, Val Loss: 0.1048\n",
      "Epoch [8/50], Train Loss: 0.0435, Val Loss: 0.0969\n",
      "Epoch [9/50], Train Loss: 0.0448, Val Loss: 0.0925\n",
      "Epoch [10/50], Train Loss: 0.0420, Val Loss: 0.0877\n",
      "Epoch [11/50], Train Loss: 0.0419, Val Loss: 0.0843\n",
      "Epoch [12/50], Train Loss: 0.0398, Val Loss: 0.0781\n",
      "Epoch [13/50], Train Loss: 0.0379, Val Loss: 0.0714\n",
      "Epoch [14/50], Train Loss: 0.0372, Val Loss: 0.0639\n",
      "Epoch [15/50], Train Loss: 0.0345, Val Loss: 0.0551\n",
      "Epoch [16/50], Train Loss: 0.0310, Val Loss: 0.0459\n",
      "Epoch [17/50], Train Loss: 0.0278, Val Loss: 0.0351\n",
      "Epoch [18/50], Train Loss: 0.0257, Val Loss: 0.0258\n",
      "Epoch [19/50], Train Loss: 0.0215, Val Loss: 0.0196\n",
      "Epoch [20/50], Train Loss: 0.0211, Val Loss: 0.0181\n",
      "Epoch [21/50], Train Loss: 0.0195, Val Loss: 0.0165\n",
      "Epoch [22/50], Train Loss: 0.0171, Val Loss: 0.0173\n",
      "Epoch [23/50], Train Loss: 0.0162, Val Loss: 0.0158\n",
      "Epoch [24/50], Train Loss: 0.0147, Val Loss: 0.0150\n",
      "Epoch [25/50], Train Loss: 0.0138, Val Loss: 0.0136\n",
      "Epoch [26/50], Train Loss: 0.0125, Val Loss: 0.0124\n",
      "Epoch [27/50], Train Loss: 0.0125, Val Loss: 0.0115\n",
      "Epoch [28/50], Train Loss: 0.0118, Val Loss: 0.0115\n",
      "Epoch [29/50], Train Loss: 0.0119, Val Loss: 0.0118\n",
      "Epoch [30/50], Train Loss: 0.0115, Val Loss: 0.0111\n",
      "Epoch [31/50], Train Loss: 0.0116, Val Loss: 0.0105\n",
      "Epoch [32/50], Train Loss: 0.0114, Val Loss: 0.0103\n",
      "Epoch [33/50], Train Loss: 0.0113, Val Loss: 0.0109\n",
      "Epoch [34/50], Train Loss: 0.0110, Val Loss: 0.0101\n",
      "Epoch [35/50], Train Loss: 0.0112, Val Loss: 0.0108\n",
      "Epoch [36/50], Train Loss: 0.0103, Val Loss: 0.0104\n",
      "Epoch [37/50], Train Loss: 0.0102, Val Loss: 0.0096\n",
      "Epoch [38/50], Train Loss: 0.0106, Val Loss: 0.0096\n",
      "Epoch [39/50], Train Loss: 0.0106, Val Loss: 0.0085\n",
      "Epoch [40/50], Train Loss: 0.0105, Val Loss: 0.0078\n",
      "Epoch [41/50], Train Loss: 0.0099, Val Loss: 0.0107\n",
      "Epoch [42/50], Train Loss: 0.0097, Val Loss: 0.0083\n",
      "Epoch [43/50], Train Loss: 0.0102, Val Loss: 0.0085\n",
      "Epoch [44/50], Train Loss: 0.0099, Val Loss: 0.0090\n",
      "Epoch [45/50], Train Loss: 0.0099, Val Loss: 0.0083\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=32, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1502, Val Loss: 0.3325\n",
      "Epoch [2/50], Train Loss: 0.1227, Val Loss: 0.2734\n",
      "Epoch [3/50], Train Loss: 0.1065, Val Loss: 0.2200\n",
      "Epoch [4/50], Train Loss: 0.0934, Val Loss: 0.1760\n",
      "Epoch [5/50], Train Loss: 0.0837, Val Loss: 0.1461\n",
      "Epoch [6/50], Train Loss: 0.0788, Val Loss: 0.1256\n",
      "Epoch [7/50], Train Loss: 0.0743, Val Loss: 0.1152\n",
      "Epoch [8/50], Train Loss: 0.0724, Val Loss: 0.1073\n",
      "Epoch [9/50], Train Loss: 0.0675, Val Loss: 0.0986\n",
      "Epoch [10/50], Train Loss: 0.0640, Val Loss: 0.0900\n",
      "Epoch [11/50], Train Loss: 0.0614, Val Loss: 0.0773\n",
      "Epoch [12/50], Train Loss: 0.0560, Val Loss: 0.0659\n",
      "Epoch [13/50], Train Loss: 0.0553, Val Loss: 0.0603\n",
      "Epoch [14/50], Train Loss: 0.0548, Val Loss: 0.0456\n",
      "Epoch [15/50], Train Loss: 0.0493, Val Loss: 0.0397\n",
      "Epoch [16/50], Train Loss: 0.0485, Val Loss: 0.0339\n",
      "Epoch [17/50], Train Loss: 0.0465, Val Loss: 0.0322\n",
      "Epoch [18/50], Train Loss: 0.0468, Val Loss: 0.0296\n",
      "Epoch [19/50], Train Loss: 0.0443, Val Loss: 0.0279\n",
      "Epoch [20/50], Train Loss: 0.0431, Val Loss: 0.0266\n",
      "Epoch [21/50], Train Loss: 0.0411, Val Loss: 0.0252\n",
      "Epoch [22/50], Train Loss: 0.0391, Val Loss: 0.0249\n",
      "Epoch [23/50], Train Loss: 0.0411, Val Loss: 0.0243\n",
      "Epoch [24/50], Train Loss: 0.0398, Val Loss: 0.0225\n",
      "Epoch [25/50], Train Loss: 0.0375, Val Loss: 0.0196\n",
      "Epoch [26/50], Train Loss: 0.0368, Val Loss: 0.0213\n",
      "Epoch [27/50], Train Loss: 0.0362, Val Loss: 0.0218\n",
      "Epoch [28/50], Train Loss: 0.0338, Val Loss: 0.0192\n",
      "Epoch [29/50], Train Loss: 0.0337, Val Loss: 0.0177\n",
      "Epoch [30/50], Train Loss: 0.0337, Val Loss: 0.0152\n",
      "Epoch [31/50], Train Loss: 0.0314, Val Loss: 0.0164\n",
      "Epoch [32/50], Train Loss: 0.0300, Val Loss: 0.0145\n",
      "Epoch [33/50], Train Loss: 0.0297, Val Loss: 0.0119\n",
      "Epoch [34/50], Train Loss: 0.0281, Val Loss: 0.0103\n",
      "Epoch [35/50], Train Loss: 0.0278, Val Loss: 0.0128\n",
      "Epoch [36/50], Train Loss: 0.0269, Val Loss: 0.0114\n",
      "Epoch [37/50], Train Loss: 0.0266, Val Loss: 0.0131\n",
      "Epoch [38/50], Train Loss: 0.0259, Val Loss: 0.0113\n",
      "Epoch [39/50], Train Loss: 0.0261, Val Loss: 0.0107\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=64, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1366, Val Loss: 0.3205\n",
      "Epoch [2/50], Train Loss: 0.0976, Val Loss: 0.2373\n",
      "Epoch [3/50], Train Loss: 0.0662, Val Loss: 0.1560\n",
      "Epoch [4/50], Train Loss: 0.0402, Val Loss: 0.0826\n",
      "Epoch [5/50], Train Loss: 0.0300, Val Loss: 0.0496\n",
      "Epoch [6/50], Train Loss: 0.0305, Val Loss: 0.0465\n",
      "Epoch [7/50], Train Loss: 0.0294, Val Loss: 0.0452\n",
      "Epoch [8/50], Train Loss: 0.0282, Val Loss: 0.0427\n",
      "Epoch [9/50], Train Loss: 0.0272, Val Loss: 0.0402\n",
      "Epoch [10/50], Train Loss: 0.0262, Val Loss: 0.0376\n",
      "Epoch [11/50], Train Loss: 0.0252, Val Loss: 0.0351\n",
      "Epoch [12/50], Train Loss: 0.0243, Val Loss: 0.0325\n",
      "Epoch [13/50], Train Loss: 0.0233, Val Loss: 0.0299\n",
      "Epoch [14/50], Train Loss: 0.0223, Val Loss: 0.0273\n",
      "Epoch [15/50], Train Loss: 0.0214, Val Loss: 0.0246\n",
      "Epoch [16/50], Train Loss: 0.0204, Val Loss: 0.0219\n",
      "Epoch [17/50], Train Loss: 0.0193, Val Loss: 0.0193\n",
      "Epoch [18/50], Train Loss: 0.0183, Val Loss: 0.0166\n",
      "Epoch [19/50], Train Loss: 0.0172, Val Loss: 0.0141\n",
      "Epoch [20/50], Train Loss: 0.0162, Val Loss: 0.0117\n",
      "Epoch [21/50], Train Loss: 0.0152, Val Loss: 0.0098\n",
      "Epoch [22/50], Train Loss: 0.0143, Val Loss: 0.0082\n",
      "Epoch [23/50], Train Loss: 0.0135, Val Loss: 0.0072\n",
      "Epoch [24/50], Train Loss: 0.0129, Val Loss: 0.0066\n",
      "Epoch [25/50], Train Loss: 0.0124, Val Loss: 0.0062\n",
      "Epoch [26/50], Train Loss: 0.0119, Val Loss: 0.0059\n",
      "Epoch [27/50], Train Loss: 0.0114, Val Loss: 0.0057\n",
      "Epoch [28/50], Train Loss: 0.0108, Val Loss: 0.0056\n",
      "Epoch [29/50], Train Loss: 0.0103, Val Loss: 0.0055\n",
      "Epoch [30/50], Train Loss: 0.0097, Val Loss: 0.0055\n",
      "Epoch [31/50], Train Loss: 0.0090, Val Loss: 0.0056\n",
      "Epoch [32/50], Train Loss: 0.0083, Val Loss: 0.0058\n",
      "Epoch [33/50], Train Loss: 0.0076, Val Loss: 0.0060\n",
      "Epoch [34/50], Train Loss: 0.0068, Val Loss: 0.0063\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=64, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1704, Val Loss: 0.3602\n",
      "Epoch [2/50], Train Loss: 0.1165, Val Loss: 0.2605\n",
      "Epoch [3/50], Train Loss: 0.0752, Val Loss: 0.1608\n",
      "Epoch [4/50], Train Loss: 0.0450, Val Loss: 0.0860\n",
      "Epoch [5/50], Train Loss: 0.0396, Val Loss: 0.0658\n",
      "Epoch [6/50], Train Loss: 0.0403, Val Loss: 0.0648\n",
      "Epoch [7/50], Train Loss: 0.0380, Val Loss: 0.0603\n",
      "Epoch [8/50], Train Loss: 0.0358, Val Loss: 0.0553\n",
      "Epoch [9/50], Train Loss: 0.0344, Val Loss: 0.0513\n",
      "Epoch [10/50], Train Loss: 0.0329, Val Loss: 0.0486\n",
      "Epoch [11/50], Train Loss: 0.0314, Val Loss: 0.0453\n",
      "Epoch [12/50], Train Loss: 0.0303, Val Loss: 0.0402\n",
      "Epoch [13/50], Train Loss: 0.0288, Val Loss: 0.0378\n",
      "Epoch [14/50], Train Loss: 0.0280, Val Loss: 0.0352\n",
      "Epoch [15/50], Train Loss: 0.0264, Val Loss: 0.0303\n",
      "Epoch [16/50], Train Loss: 0.0247, Val Loss: 0.0272\n",
      "Epoch [17/50], Train Loss: 0.0246, Val Loss: 0.0242\n",
      "Epoch [18/50], Train Loss: 0.0231, Val Loss: 0.0218\n",
      "Epoch [19/50], Train Loss: 0.0222, Val Loss: 0.0196\n",
      "Epoch [20/50], Train Loss: 0.0215, Val Loss: 0.0181\n",
      "Epoch [21/50], Train Loss: 0.0211, Val Loss: 0.0167\n",
      "Epoch [22/50], Train Loss: 0.0202, Val Loss: 0.0152\n",
      "Epoch [23/50], Train Loss: 0.0192, Val Loss: 0.0141\n",
      "Epoch [24/50], Train Loss: 0.0189, Val Loss: 0.0128\n",
      "Epoch [25/50], Train Loss: 0.0183, Val Loss: 0.0119\n",
      "Epoch [26/50], Train Loss: 0.0176, Val Loss: 0.0104\n",
      "Epoch [27/50], Train Loss: 0.0167, Val Loss: 0.0098\n",
      "Epoch [28/50], Train Loss: 0.0163, Val Loss: 0.0091\n",
      "Epoch [29/50], Train Loss: 0.0153, Val Loss: 0.0080\n",
      "Epoch [30/50], Train Loss: 0.0146, Val Loss: 0.0072\n",
      "Epoch [31/50], Train Loss: 0.0140, Val Loss: 0.0062\n",
      "Epoch [32/50], Train Loss: 0.0133, Val Loss: 0.0055\n",
      "Epoch [33/50], Train Loss: 0.0124, Val Loss: 0.0053\n",
      "Epoch [34/50], Train Loss: 0.0114, Val Loss: 0.0049\n",
      "Epoch [35/50], Train Loss: 0.0102, Val Loss: 0.0056\n",
      "Epoch [36/50], Train Loss: 0.0099, Val Loss: 0.0051\n",
      "Epoch [37/50], Train Loss: 0.0092, Val Loss: 0.0051\n",
      "Epoch [38/50], Train Loss: 0.0082, Val Loss: 0.0055\n",
      "Epoch [39/50], Train Loss: 0.0080, Val Loss: 0.0044\n",
      "Epoch [40/50], Train Loss: 0.0075, Val Loss: 0.0045\n",
      "Epoch [41/50], Train Loss: 0.0071, Val Loss: 0.0041\n",
      "Epoch [42/50], Train Loss: 0.0067, Val Loss: 0.0045\n",
      "Epoch [43/50], Train Loss: 0.0064, Val Loss: 0.0040\n",
      "Epoch [44/50], Train Loss: 0.0061, Val Loss: 0.0031\n",
      "Epoch [45/50], Train Loss: 0.0062, Val Loss: 0.0035\n",
      "Epoch [46/50], Train Loss: 0.0058, Val Loss: 0.0033\n",
      "Epoch [47/50], Train Loss: 0.0057, Val Loss: 0.0034\n",
      "Epoch [48/50], Train Loss: 0.0056, Val Loss: 0.0030\n",
      "Epoch [49/50], Train Loss: 0.0055, Val Loss: 0.0029\n",
      "Epoch [50/50], Train Loss: 0.0055, Val Loss: 0.0028\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=64, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1090, Val Loss: 0.3140\n",
      "Epoch [2/50], Train Loss: 0.0795, Val Loss: 0.2455\n",
      "Epoch [3/50], Train Loss: 0.0619, Val Loss: 0.1910\n",
      "Epoch [4/50], Train Loss: 0.0517, Val Loss: 0.1491\n",
      "Epoch [5/50], Train Loss: 0.0473, Val Loss: 0.1230\n",
      "Epoch [6/50], Train Loss: 0.0443, Val Loss: 0.1063\n",
      "Epoch [7/50], Train Loss: 0.0418, Val Loss: 0.0946\n",
      "Epoch [8/50], Train Loss: 0.0405, Val Loss: 0.0873\n",
      "Epoch [9/50], Train Loss: 0.0382, Val Loss: 0.0808\n",
      "Epoch [10/50], Train Loss: 0.0367, Val Loss: 0.0730\n",
      "Epoch [11/50], Train Loss: 0.0351, Val Loss: 0.0667\n",
      "Epoch [12/50], Train Loss: 0.0324, Val Loss: 0.0593\n",
      "Epoch [13/50], Train Loss: 0.0312, Val Loss: 0.0492\n",
      "Epoch [14/50], Train Loss: 0.0293, Val Loss: 0.0408\n",
      "Epoch [15/50], Train Loss: 0.0266, Val Loss: 0.0307\n",
      "Epoch [16/50], Train Loss: 0.0238, Val Loss: 0.0239\n",
      "Epoch [17/50], Train Loss: 0.0235, Val Loss: 0.0225\n",
      "Epoch [18/50], Train Loss: 0.0224, Val Loss: 0.0196\n",
      "Epoch [19/50], Train Loss: 0.0207, Val Loss: 0.0174\n",
      "Epoch [20/50], Train Loss: 0.0199, Val Loss: 0.0174\n",
      "Epoch [21/50], Train Loss: 0.0200, Val Loss: 0.0162\n",
      "Epoch [22/50], Train Loss: 0.0190, Val Loss: 0.0139\n",
      "Epoch [23/50], Train Loss: 0.0190, Val Loss: 0.0148\n",
      "Epoch [24/50], Train Loss: 0.0182, Val Loss: 0.0119\n",
      "Epoch [25/50], Train Loss: 0.0174, Val Loss: 0.0118\n",
      "Epoch [26/50], Train Loss: 0.0170, Val Loss: 0.0125\n",
      "Epoch [27/50], Train Loss: 0.0163, Val Loss: 0.0113\n",
      "Epoch [28/50], Train Loss: 0.0147, Val Loss: 0.0098\n",
      "Epoch [29/50], Train Loss: 0.0146, Val Loss: 0.0099\n",
      "Epoch [30/50], Train Loss: 0.0137, Val Loss: 0.0096\n",
      "Epoch [31/50], Train Loss: 0.0137, Val Loss: 0.0097\n",
      "Epoch [32/50], Train Loss: 0.0126, Val Loss: 0.0089\n",
      "Epoch [33/50], Train Loss: 0.0116, Val Loss: 0.0078\n",
      "Epoch [34/50], Train Loss: 0.0112, Val Loss: 0.0079\n",
      "Epoch [35/50], Train Loss: 0.0109, Val Loss: 0.0064\n",
      "Epoch [36/50], Train Loss: 0.0107, Val Loss: 0.0072\n",
      "Epoch [37/50], Train Loss: 0.0101, Val Loss: 0.0061\n",
      "Epoch [38/50], Train Loss: 0.0102, Val Loss: 0.0058\n",
      "Epoch [39/50], Train Loss: 0.0098, Val Loss: 0.0061\n",
      "Epoch [40/50], Train Loss: 0.0094, Val Loss: 0.0050\n",
      "Epoch [41/50], Train Loss: 0.0098, Val Loss: 0.0059\n",
      "Epoch [42/50], Train Loss: 0.0092, Val Loss: 0.0060\n",
      "Epoch [43/50], Train Loss: 0.0094, Val Loss: 0.0043\n",
      "Epoch [44/50], Train Loss: 0.0092, Val Loss: 0.0075\n",
      "Epoch [45/50], Train Loss: 0.0096, Val Loss: 0.0046\n",
      "Epoch [46/50], Train Loss: 0.0092, Val Loss: 0.0053\n",
      "Epoch [47/50], Train Loss: 0.0085, Val Loss: 0.0069\n",
      "Epoch [48/50], Train Loss: 0.0087, Val Loss: 0.0040\n",
      "Epoch [49/50], Train Loss: 0.0088, Val Loss: 0.0052\n",
      "Epoch [50/50], Train Loss: 0.0088, Val Loss: 0.0040\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=64, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1052, Val Loss: 0.2310\n",
      "Epoch [2/50], Train Loss: 0.0502, Val Loss: 0.1256\n",
      "Epoch [3/50], Train Loss: 0.0327, Val Loss: 0.0811\n",
      "Epoch [4/50], Train Loss: 0.0330, Val Loss: 0.0739\n",
      "Epoch [5/50], Train Loss: 0.0309, Val Loss: 0.0684\n",
      "Epoch [6/50], Train Loss: 0.0286, Val Loss: 0.0620\n",
      "Epoch [7/50], Train Loss: 0.0262, Val Loss: 0.0556\n",
      "Epoch [8/50], Train Loss: 0.0236, Val Loss: 0.0487\n",
      "Epoch [9/50], Train Loss: 0.0207, Val Loss: 0.0410\n",
      "Epoch [10/50], Train Loss: 0.0175, Val Loss: 0.0328\n",
      "Epoch [11/50], Train Loss: 0.0140, Val Loss: 0.0245\n",
      "Epoch [12/50], Train Loss: 0.0106, Val Loss: 0.0169\n",
      "Epoch [13/50], Train Loss: 0.0075, Val Loss: 0.0116\n",
      "Epoch [14/50], Train Loss: 0.0049, Val Loss: 0.0088\n",
      "Epoch [15/50], Train Loss: 0.0034, Val Loss: 0.0078\n",
      "Epoch [16/50], Train Loss: 0.0027, Val Loss: 0.0069\n",
      "Epoch [17/50], Train Loss: 0.0025, Val Loss: 0.0071\n",
      "Epoch [18/50], Train Loss: 0.0024, Val Loss: 0.0073\n",
      "Epoch [19/50], Train Loss: 0.0023, Val Loss: 0.0070\n",
      "Epoch [20/50], Train Loss: 0.0023, Val Loss: 0.0069\n",
      "Epoch [21/50], Train Loss: 0.0023, Val Loss: 0.0069\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=64, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1362, Val Loss: 0.2664\n",
      "Epoch [2/50], Train Loss: 0.0636, Val Loss: 0.1241\n",
      "Epoch [3/50], Train Loss: 0.0400, Val Loss: 0.0673\n",
      "Epoch [4/50], Train Loss: 0.0363, Val Loss: 0.0595\n",
      "Epoch [5/50], Train Loss: 0.0345, Val Loss: 0.0533\n",
      "Epoch [6/50], Train Loss: 0.0309, Val Loss: 0.0444\n",
      "Epoch [7/50], Train Loss: 0.0274, Val Loss: 0.0372\n",
      "Epoch [8/50], Train Loss: 0.0247, Val Loss: 0.0297\n",
      "Epoch [9/50], Train Loss: 0.0215, Val Loss: 0.0224\n",
      "Epoch [10/50], Train Loss: 0.0183, Val Loss: 0.0153\n",
      "Epoch [11/50], Train Loss: 0.0156, Val Loss: 0.0122\n",
      "Epoch [12/50], Train Loss: 0.0132, Val Loss: 0.0086\n",
      "Epoch [13/50], Train Loss: 0.0118, Val Loss: 0.0070\n",
      "Epoch [14/50], Train Loss: 0.0101, Val Loss: 0.0069\n",
      "Epoch [15/50], Train Loss: 0.0091, Val Loss: 0.0072\n",
      "Epoch [16/50], Train Loss: 0.0089, Val Loss: 0.0059\n",
      "Epoch [17/50], Train Loss: 0.0083, Val Loss: 0.0051\n",
      "Epoch [18/50], Train Loss: 0.0086, Val Loss: 0.0057\n",
      "Epoch [19/50], Train Loss: 0.0076, Val Loss: 0.0060\n",
      "Epoch [20/50], Train Loss: 0.0074, Val Loss: 0.0049\n",
      "Epoch [21/50], Train Loss: 0.0073, Val Loss: 0.0048\n",
      "Epoch [22/50], Train Loss: 0.0076, Val Loss: 0.0055\n",
      "Epoch [23/50], Train Loss: 0.0072, Val Loss: 0.0054\n",
      "Epoch [24/50], Train Loss: 0.0069, Val Loss: 0.0048\n",
      "Epoch [25/50], Train Loss: 0.0076, Val Loss: 0.0053\n",
      "Epoch [26/50], Train Loss: 0.0072, Val Loss: 0.0051\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=64, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1141, Val Loss: 0.2808\n",
      "Epoch [2/50], Train Loss: 0.0795, Val Loss: 0.1972\n",
      "Epoch [3/50], Train Loss: 0.0617, Val Loss: 0.1384\n",
      "Epoch [4/50], Train Loss: 0.0559, Val Loss: 0.1103\n",
      "Epoch [5/50], Train Loss: 0.0539, Val Loss: 0.1000\n",
      "Epoch [6/50], Train Loss: 0.0480, Val Loss: 0.0861\n",
      "Epoch [7/50], Train Loss: 0.0435, Val Loss: 0.0724\n",
      "Epoch [8/50], Train Loss: 0.0382, Val Loss: 0.0569\n",
      "Epoch [9/50], Train Loss: 0.0354, Val Loss: 0.0441\n",
      "Epoch [10/50], Train Loss: 0.0295, Val Loss: 0.0299\n",
      "Epoch [11/50], Train Loss: 0.0250, Val Loss: 0.0198\n",
      "Epoch [12/50], Train Loss: 0.0239, Val Loss: 0.0132\n",
      "Epoch [13/50], Train Loss: 0.0211, Val Loss: 0.0124\n",
      "Epoch [14/50], Train Loss: 0.0200, Val Loss: 0.0131\n",
      "Epoch [15/50], Train Loss: 0.0190, Val Loss: 0.0102\n",
      "Epoch [16/50], Train Loss: 0.0187, Val Loss: 0.0098\n",
      "Epoch [17/50], Train Loss: 0.0196, Val Loss: 0.0093\n",
      "Epoch [18/50], Train Loss: 0.0177, Val Loss: 0.0085\n",
      "Epoch [19/50], Train Loss: 0.0166, Val Loss: 0.0102\n",
      "Epoch [20/50], Train Loss: 0.0165, Val Loss: 0.0070\n",
      "Epoch [21/50], Train Loss: 0.0168, Val Loss: 0.0069\n",
      "Epoch [22/50], Train Loss: 0.0159, Val Loss: 0.0084\n",
      "Epoch [23/50], Train Loss: 0.0154, Val Loss: 0.0064\n",
      "Epoch [24/50], Train Loss: 0.0149, Val Loss: 0.0069\n",
      "Epoch [25/50], Train Loss: 0.0146, Val Loss: 0.0068\n",
      "Epoch [26/50], Train Loss: 0.0147, Val Loss: 0.0051\n",
      "Epoch [27/50], Train Loss: 0.0142, Val Loss: 0.0056\n",
      "Epoch [28/50], Train Loss: 0.0146, Val Loss: 0.0068\n",
      "Epoch [29/50], Train Loss: 0.0143, Val Loss: 0.0059\n",
      "Epoch [30/50], Train Loss: 0.0135, Val Loss: 0.0055\n",
      "Epoch [31/50], Train Loss: 0.0135, Val Loss: 0.0062\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=64, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1019, Val Loss: 0.1929\n",
      "Epoch [2/50], Train Loss: 0.0398, Val Loss: 0.0881\n",
      "Epoch [3/50], Train Loss: 0.0379, Val Loss: 0.0750\n",
      "Epoch [4/50], Train Loss: 0.0357, Val Loss: 0.0670\n",
      "Epoch [5/50], Train Loss: 0.0323, Val Loss: 0.0544\n",
      "Epoch [6/50], Train Loss: 0.0276, Val Loss: 0.0369\n",
      "Epoch [7/50], Train Loss: 0.0204, Val Loss: 0.0204\n",
      "Epoch [8/50], Train Loss: 0.0161, Val Loss: 0.0195\n",
      "Epoch [9/50], Train Loss: 0.0141, Val Loss: 0.0206\n",
      "Epoch [10/50], Train Loss: 0.0132, Val Loss: 0.0191\n",
      "Epoch [11/50], Train Loss: 0.0122, Val Loss: 0.0196\n",
      "Epoch [12/50], Train Loss: 0.0107, Val Loss: 0.0203\n",
      "Epoch [13/50], Train Loss: 0.0091, Val Loss: 0.0187\n",
      "Epoch [14/50], Train Loss: 0.0070, Val Loss: 0.0166\n",
      "Epoch [15/50], Train Loss: 0.0044, Val Loss: 0.0129\n",
      "Epoch [16/50], Train Loss: 0.0033, Val Loss: 0.0110\n",
      "Epoch [17/50], Train Loss: 0.0037, Val Loss: 0.0144\n",
      "Epoch [18/50], Train Loss: 0.0030, Val Loss: 0.0138\n",
      "Epoch [19/50], Train Loss: 0.0027, Val Loss: 0.0104\n",
      "Epoch [20/50], Train Loss: 0.0029, Val Loss: 0.0109\n",
      "Epoch [21/50], Train Loss: 0.0030, Val Loss: 0.0139\n",
      "Epoch [22/50], Train Loss: 0.0026, Val Loss: 0.0113\n",
      "Epoch [23/50], Train Loss: 0.0026, Val Loss: 0.0095\n",
      "Epoch [24/50], Train Loss: 0.0030, Val Loss: 0.0117\n",
      "Epoch [25/50], Train Loss: 0.0026, Val Loss: 0.0127\n",
      "Epoch [26/50], Train Loss: 0.0025, Val Loss: 0.0094\n",
      "Epoch [27/50], Train Loss: 0.0026, Val Loss: 0.0094\n",
      "Epoch [28/50], Train Loss: 0.0029, Val Loss: 0.0124\n",
      "Epoch [29/50], Train Loss: 0.0025, Val Loss: 0.0111\n",
      "Epoch [30/50], Train Loss: 0.0024, Val Loss: 0.0077\n",
      "Epoch [31/50], Train Loss: 0.0029, Val Loss: 0.0104\n",
      "Epoch [32/50], Train Loss: 0.0027, Val Loss: 0.0127\n",
      "Epoch [33/50], Train Loss: 0.0026, Val Loss: 0.0092\n",
      "Epoch [34/50], Train Loss: 0.0025, Val Loss: 0.0065\n",
      "Epoch [35/50], Train Loss: 0.0038, Val Loss: 0.0134\n",
      "Epoch [36/50], Train Loss: 0.0028, Val Loss: 0.0114\n",
      "Epoch [37/50], Train Loss: 0.0031, Val Loss: 0.0070\n",
      "Epoch [38/50], Train Loss: 0.0033, Val Loss: 0.0079\n",
      "Epoch [39/50], Train Loss: 0.0038, Val Loss: 0.0162\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=64, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1009, Val Loss: 0.1947\n",
      "Epoch [2/50], Train Loss: 0.0533, Val Loss: 0.1172\n",
      "Epoch [3/50], Train Loss: 0.0445, Val Loss: 0.0915\n",
      "Epoch [4/50], Train Loss: 0.0449, Val Loss: 0.0841\n",
      "Epoch [5/50], Train Loss: 0.0425, Val Loss: 0.0782\n",
      "Epoch [6/50], Train Loss: 0.0397, Val Loss: 0.0708\n",
      "Epoch [7/50], Train Loss: 0.0353, Val Loss: 0.0591\n",
      "Epoch [8/50], Train Loss: 0.0327, Val Loss: 0.0464\n",
      "Epoch [9/50], Train Loss: 0.0274, Val Loss: 0.0307\n",
      "Epoch [10/50], Train Loss: 0.0218, Val Loss: 0.0195\n",
      "Epoch [11/50], Train Loss: 0.0187, Val Loss: 0.0175\n",
      "Epoch [12/50], Train Loss: 0.0143, Val Loss: 0.0124\n",
      "Epoch [13/50], Train Loss: 0.0126, Val Loss: 0.0138\n",
      "Epoch [14/50], Train Loss: 0.0105, Val Loss: 0.0098\n",
      "Epoch [15/50], Train Loss: 0.0111, Val Loss: 0.0108\n",
      "Epoch [16/50], Train Loss: 0.0098, Val Loss: 0.0101\n",
      "Epoch [17/50], Train Loss: 0.0094, Val Loss: 0.0092\n",
      "Epoch [18/50], Train Loss: 0.0095, Val Loss: 0.0082\n",
      "Epoch [19/50], Train Loss: 0.0097, Val Loss: 0.0088\n",
      "Epoch [20/50], Train Loss: 0.0089, Val Loss: 0.0073\n",
      "Epoch [21/50], Train Loss: 0.0091, Val Loss: 0.0085\n",
      "Epoch [22/50], Train Loss: 0.0085, Val Loss: 0.0076\n",
      "Epoch [23/50], Train Loss: 0.0084, Val Loss: 0.0052\n",
      "Epoch [24/50], Train Loss: 0.0084, Val Loss: 0.0064\n",
      "Epoch [25/50], Train Loss: 0.0085, Val Loss: 0.0090\n",
      "Epoch [26/50], Train Loss: 0.0081, Val Loss: 0.0041\n",
      "Epoch [27/50], Train Loss: 0.0077, Val Loss: 0.0061\n",
      "Epoch [28/50], Train Loss: 0.0077, Val Loss: 0.0064\n",
      "Epoch [29/50], Train Loss: 0.0078, Val Loss: 0.0041\n",
      "Epoch [30/50], Train Loss: 0.0078, Val Loss: 0.0055\n",
      "Epoch [31/50], Train Loss: 0.0073, Val Loss: 0.0079\n",
      "Epoch [32/50], Train Loss: 0.0074, Val Loss: 0.0042\n",
      "Epoch [33/50], Train Loss: 0.0080, Val Loss: 0.0028\n",
      "Epoch [34/50], Train Loss: 0.0075, Val Loss: 0.0088\n",
      "Epoch [35/50], Train Loss: 0.0073, Val Loss: 0.0072\n",
      "Epoch [36/50], Train Loss: 0.0075, Val Loss: 0.0034\n",
      "Epoch [37/50], Train Loss: 0.0067, Val Loss: 0.0040\n",
      "Epoch [38/50], Train Loss: 0.0073, Val Loss: 0.0067\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=64, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1330, Val Loss: 0.2388\n",
      "Epoch [2/50], Train Loss: 0.0827, Val Loss: 0.1519\n",
      "Epoch [3/50], Train Loss: 0.0722, Val Loss: 0.1220\n",
      "Epoch [4/50], Train Loss: 0.0690, Val Loss: 0.1059\n",
      "Epoch [5/50], Train Loss: 0.0627, Val Loss: 0.0967\n",
      "Epoch [6/50], Train Loss: 0.0556, Val Loss: 0.0835\n",
      "Epoch [7/50], Train Loss: 0.0535, Val Loss: 0.0725\n",
      "Epoch [8/50], Train Loss: 0.0494, Val Loss: 0.0607\n",
      "Epoch [9/50], Train Loss: 0.0430, Val Loss: 0.0487\n",
      "Epoch [10/50], Train Loss: 0.0409, Val Loss: 0.0405\n",
      "Epoch [11/50], Train Loss: 0.0374, Val Loss: 0.0359\n",
      "Epoch [12/50], Train Loss: 0.0350, Val Loss: 0.0315\n",
      "Epoch [13/50], Train Loss: 0.0334, Val Loss: 0.0308\n",
      "Epoch [14/50], Train Loss: 0.0330, Val Loss: 0.0261\n",
      "Epoch [15/50], Train Loss: 0.0318, Val Loss: 0.0193\n",
      "Epoch [16/50], Train Loss: 0.0298, Val Loss: 0.0174\n",
      "Epoch [17/50], Train Loss: 0.0266, Val Loss: 0.0184\n",
      "Epoch [18/50], Train Loss: 0.0244, Val Loss: 0.0124\n",
      "Epoch [19/50], Train Loss: 0.0226, Val Loss: 0.0117\n",
      "Epoch [20/50], Train Loss: 0.0211, Val Loss: 0.0127\n",
      "Epoch [21/50], Train Loss: 0.0211, Val Loss: 0.0127\n",
      "Epoch [22/50], Train Loss: 0.0211, Val Loss: 0.0094\n",
      "Epoch [23/50], Train Loss: 0.0204, Val Loss: 0.0140\n",
      "Epoch [24/50], Train Loss: 0.0208, Val Loss: 0.0146\n",
      "Epoch [25/50], Train Loss: 0.0221, Val Loss: 0.0062\n",
      "Epoch [26/50], Train Loss: 0.0192, Val Loss: 0.0058\n",
      "Epoch [27/50], Train Loss: 0.0220, Val Loss: 0.0179\n",
      "Epoch [28/50], Train Loss: 0.0206, Val Loss: 0.0089\n",
      "Epoch [29/50], Train Loss: 0.0194, Val Loss: 0.0069\n",
      "Epoch [30/50], Train Loss: 0.0187, Val Loss: 0.0124\n",
      "Epoch [31/50], Train Loss: 0.0175, Val Loss: 0.0080\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=128, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1003, Val Loss: 0.2758\n",
      "Epoch [2/50], Train Loss: 0.0564, Val Loss: 0.1531\n",
      "Epoch [3/50], Train Loss: 0.0279, Val Loss: 0.0502\n",
      "Epoch [4/50], Train Loss: 0.0340, Val Loss: 0.0585\n",
      "Epoch [5/50], Train Loss: 0.0266, Val Loss: 0.0430\n",
      "Epoch [6/50], Train Loss: 0.0245, Val Loss: 0.0337\n",
      "Epoch [7/50], Train Loss: 0.0208, Val Loss: 0.0214\n",
      "Epoch [8/50], Train Loss: 0.0166, Val Loss: 0.0096\n",
      "Epoch [9/50], Train Loss: 0.0126, Val Loss: 0.0050\n",
      "Epoch [10/50], Train Loss: 0.0128, Val Loss: 0.0127\n",
      "Epoch [11/50], Train Loss: 0.0097, Val Loss: 0.0068\n",
      "Epoch [12/50], Train Loss: 0.0084, Val Loss: 0.0077\n",
      "Epoch [13/50], Train Loss: 0.0071, Val Loss: 0.0095\n",
      "Epoch [14/50], Train Loss: 0.0049, Val Loss: 0.0077\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=128, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0841, Val Loss: 0.1916\n",
      "Epoch [2/50], Train Loss: 0.0367, Val Loss: 0.0812\n",
      "Epoch [3/50], Train Loss: 0.0321, Val Loss: 0.0634\n",
      "Epoch [4/50], Train Loss: 0.0299, Val Loss: 0.0541\n",
      "Epoch [5/50], Train Loss: 0.0274, Val Loss: 0.0436\n",
      "Epoch [6/50], Train Loss: 0.0248, Val Loss: 0.0358\n",
      "Epoch [7/50], Train Loss: 0.0223, Val Loss: 0.0279\n",
      "Epoch [8/50], Train Loss: 0.0189, Val Loss: 0.0166\n",
      "Epoch [9/50], Train Loss: 0.0147, Val Loss: 0.0077\n",
      "Epoch [10/50], Train Loss: 0.0105, Val Loss: 0.0036\n",
      "Epoch [11/50], Train Loss: 0.0094, Val Loss: 0.0082\n",
      "Epoch [12/50], Train Loss: 0.0069, Val Loss: 0.0057\n",
      "Epoch [13/50], Train Loss: 0.0050, Val Loss: 0.0030\n",
      "Epoch [14/50], Train Loss: 0.0052, Val Loss: 0.0032\n",
      "Epoch [15/50], Train Loss: 0.0049, Val Loss: 0.0045\n",
      "Epoch [16/50], Train Loss: 0.0045, Val Loss: 0.0049\n",
      "Epoch [17/50], Train Loss: 0.0045, Val Loss: 0.0028\n",
      "Epoch [18/50], Train Loss: 0.0044, Val Loss: 0.0038\n",
      "Epoch [19/50], Train Loss: 0.0045, Val Loss: 0.0041\n",
      "Epoch [20/50], Train Loss: 0.0047, Val Loss: 0.0034\n",
      "Epoch [21/50], Train Loss: 0.0044, Val Loss: 0.0030\n",
      "Epoch [22/50], Train Loss: 0.0046, Val Loss: 0.0058\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=128, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0935, Val Loss: 0.2083\n",
      "Epoch [2/50], Train Loss: 0.0478, Val Loss: 0.0946\n",
      "Epoch [3/50], Train Loss: 0.0434, Val Loss: 0.0790\n",
      "Epoch [4/50], Train Loss: 0.0372, Val Loss: 0.0641\n",
      "Epoch [5/50], Train Loss: 0.0341, Val Loss: 0.0546\n",
      "Epoch [6/50], Train Loss: 0.0316, Val Loss: 0.0423\n",
      "Epoch [7/50], Train Loss: 0.0274, Val Loss: 0.0312\n",
      "Epoch [8/50], Train Loss: 0.0228, Val Loss: 0.0198\n",
      "Epoch [9/50], Train Loss: 0.0175, Val Loss: 0.0187\n",
      "Epoch [10/50], Train Loss: 0.0171, Val Loss: 0.0129\n",
      "Epoch [11/50], Train Loss: 0.0142, Val Loss: 0.0101\n",
      "Epoch [12/50], Train Loss: 0.0135, Val Loss: 0.0084\n",
      "Epoch [13/50], Train Loss: 0.0116, Val Loss: 0.0043\n",
      "Epoch [14/50], Train Loss: 0.0112, Val Loss: 0.0042\n",
      "Epoch [15/50], Train Loss: 0.0105, Val Loss: 0.0060\n",
      "Epoch [16/50], Train Loss: 0.0096, Val Loss: 0.0040\n",
      "Epoch [17/50], Train Loss: 0.0089, Val Loss: 0.0058\n",
      "Epoch [18/50], Train Loss: 0.0094, Val Loss: 0.0048\n",
      "Epoch [19/50], Train Loss: 0.0089, Val Loss: 0.0040\n",
      "Epoch [20/50], Train Loss: 0.0080, Val Loss: 0.0039\n",
      "Epoch [21/50], Train Loss: 0.0087, Val Loss: 0.0052\n",
      "Epoch [22/50], Train Loss: 0.0085, Val Loss: 0.0041\n",
      "Epoch [23/50], Train Loss: 0.0082, Val Loss: 0.0071\n",
      "Epoch [24/50], Train Loss: 0.0080, Val Loss: 0.0047\n",
      "Epoch [25/50], Train Loss: 0.0080, Val Loss: 0.0024\n",
      "Epoch [26/50], Train Loss: 0.0079, Val Loss: 0.0082\n",
      "Epoch [27/50], Train Loss: 0.0075, Val Loss: 0.0033\n",
      "Epoch [28/50], Train Loss: 0.0080, Val Loss: 0.0023\n",
      "Epoch [29/50], Train Loss: 0.0078, Val Loss: 0.0041\n",
      "Epoch [30/50], Train Loss: 0.0077, Val Loss: 0.0064\n",
      "Epoch [31/50], Train Loss: 0.0083, Val Loss: 0.0028\n",
      "Epoch [32/50], Train Loss: 0.0076, Val Loss: 0.0027\n",
      "Epoch [33/50], Train Loss: 0.0075, Val Loss: 0.0064\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=128, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0630, Val Loss: 0.1365\n",
      "Epoch [2/50], Train Loss: 0.0366, Val Loss: 0.0822\n",
      "Epoch [3/50], Train Loss: 0.0375, Val Loss: 0.0755\n",
      "Epoch [4/50], Train Loss: 0.0314, Val Loss: 0.0588\n",
      "Epoch [5/50], Train Loss: 0.0259, Val Loss: 0.0381\n",
      "Epoch [6/50], Train Loss: 0.0176, Val Loss: 0.0116\n",
      "Epoch [7/50], Train Loss: 0.0061, Val Loss: 0.0040\n",
      "Epoch [8/50], Train Loss: 0.0101, Val Loss: 0.0174\n",
      "Epoch [9/50], Train Loss: 0.0105, Val Loss: 0.0155\n",
      "Epoch [10/50], Train Loss: 0.0049, Val Loss: 0.0066\n",
      "Epoch [11/50], Train Loss: 0.0024, Val Loss: 0.0055\n",
      "Epoch [12/50], Train Loss: 0.0024, Val Loss: 0.0049\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=128, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0687, Val Loss: 0.1354\n",
      "Epoch [2/50], Train Loss: 0.0396, Val Loss: 0.0738\n",
      "Epoch [3/50], Train Loss: 0.0405, Val Loss: 0.0667\n",
      "Epoch [4/50], Train Loss: 0.0318, Val Loss: 0.0428\n",
      "Epoch [5/50], Train Loss: 0.0270, Val Loss: 0.0223\n",
      "Epoch [6/50], Train Loss: 0.0198, Val Loss: 0.0086\n",
      "Epoch [7/50], Train Loss: 0.0153, Val Loss: 0.0062\n",
      "Epoch [8/50], Train Loss: 0.0125, Val Loss: 0.0070\n",
      "Epoch [9/50], Train Loss: 0.0095, Val Loss: 0.0077\n",
      "Epoch [10/50], Train Loss: 0.0074, Val Loss: 0.0054\n",
      "Epoch [11/50], Train Loss: 0.0065, Val Loss: 0.0066\n",
      "Epoch [12/50], Train Loss: 0.0057, Val Loss: 0.0026\n",
      "Epoch [13/50], Train Loss: 0.0060, Val Loss: 0.0096\n",
      "Epoch [14/50], Train Loss: 0.0060, Val Loss: 0.0056\n",
      "Epoch [15/50], Train Loss: 0.0062, Val Loss: 0.0044\n",
      "Epoch [16/50], Train Loss: 0.0051, Val Loss: 0.0027\n",
      "Epoch [17/50], Train Loss: 0.0053, Val Loss: 0.0073\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=128, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0707, Val Loss: 0.1322\n",
      "Epoch [2/50], Train Loss: 0.0449, Val Loss: 0.0842\n",
      "Epoch [3/50], Train Loss: 0.0436, Val Loss: 0.0656\n",
      "Epoch [4/50], Train Loss: 0.0379, Val Loss: 0.0422\n",
      "Epoch [5/50], Train Loss: 0.0304, Val Loss: 0.0186\n",
      "Epoch [6/50], Train Loss: 0.0241, Val Loss: 0.0076\n",
      "Epoch [7/50], Train Loss: 0.0207, Val Loss: 0.0156\n",
      "Epoch [8/50], Train Loss: 0.0168, Val Loss: 0.0076\n",
      "Epoch [9/50], Train Loss: 0.0145, Val Loss: 0.0051\n",
      "Epoch [10/50], Train Loss: 0.0121, Val Loss: 0.0061\n",
      "Epoch [11/50], Train Loss: 0.0119, Val Loss: 0.0060\n",
      "Epoch [12/50], Train Loss: 0.0122, Val Loss: 0.0092\n",
      "Epoch [13/50], Train Loss: 0.0116, Val Loss: 0.0043\n",
      "Epoch [14/50], Train Loss: 0.0110, Val Loss: 0.0036\n",
      "Epoch [15/50], Train Loss: 0.0106, Val Loss: 0.0087\n",
      "Epoch [16/50], Train Loss: 0.0103, Val Loss: 0.0048\n",
      "Epoch [17/50], Train Loss: 0.0105, Val Loss: 0.0043\n",
      "Epoch [18/50], Train Loss: 0.0096, Val Loss: 0.0082\n",
      "Epoch [19/50], Train Loss: 0.0100, Val Loss: 0.0031\n",
      "Epoch [20/50], Train Loss: 0.0095, Val Loss: 0.0048\n",
      "Epoch [21/50], Train Loss: 0.0092, Val Loss: 0.0043\n",
      "Epoch [22/50], Train Loss: 0.0096, Val Loss: 0.0037\n",
      "Epoch [23/50], Train Loss: 0.0090, Val Loss: 0.0039\n",
      "Epoch [24/50], Train Loss: 0.0084, Val Loss: 0.0060\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=128, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0556, Val Loss: 0.0558\n",
      "Epoch [2/50], Train Loss: 0.0489, Val Loss: 0.0760\n",
      "Epoch [3/50], Train Loss: 0.0366, Val Loss: 0.0609\n",
      "Epoch [4/50], Train Loss: 0.0328, Val Loss: 0.0428\n",
      "Epoch [5/50], Train Loss: 0.0263, Val Loss: 0.0195\n",
      "Epoch [6/50], Train Loss: 0.0171, Val Loss: 0.0141\n",
      "Epoch [7/50], Train Loss: 0.0129, Val Loss: 0.0175\n",
      "Epoch [8/50], Train Loss: 0.0082, Val Loss: 0.0059\n",
      "Epoch [9/50], Train Loss: 0.0048, Val Loss: 0.0152\n",
      "Epoch [10/50], Train Loss: 0.0055, Val Loss: 0.0093\n",
      "Epoch [11/50], Train Loss: 0.0046, Val Loss: 0.0056\n",
      "Epoch [12/50], Train Loss: 0.0050, Val Loss: 0.0125\n",
      "Epoch [13/50], Train Loss: 0.0067, Val Loss: 0.0056\n",
      "Epoch [14/50], Train Loss: 0.0042, Val Loss: 0.0053\n",
      "Epoch [15/50], Train Loss: 0.0045, Val Loss: 0.0087\n",
      "Epoch [16/50], Train Loss: 0.0061, Val Loss: 0.0040\n",
      "Epoch [17/50], Train Loss: 0.0035, Val Loss: 0.0057\n",
      "Epoch [18/50], Train Loss: 0.0039, Val Loss: 0.0069\n",
      "Epoch [19/50], Train Loss: 0.0051, Val Loss: 0.0030\n",
      "Epoch [20/50], Train Loss: 0.0032, Val Loss: 0.0066\n",
      "Epoch [21/50], Train Loss: 0.0032, Val Loss: 0.0062\n",
      "Epoch [22/50], Train Loss: 0.0042, Val Loss: 0.0026\n",
      "Epoch [23/50], Train Loss: 0.0031, Val Loss: 0.0071\n",
      "Epoch [24/50], Train Loss: 0.0025, Val Loss: 0.0043\n",
      "Epoch [25/50], Train Loss: 0.0032, Val Loss: 0.0025\n",
      "Epoch [26/50], Train Loss: 0.0030, Val Loss: 0.0076\n",
      "Epoch [27/50], Train Loss: 0.0024, Val Loss: 0.0026\n",
      "Epoch [28/50], Train Loss: 0.0029, Val Loss: 0.0028\n",
      "Epoch [29/50], Train Loss: 0.0028, Val Loss: 0.0077\n",
      "Epoch [30/50], Train Loss: 0.0029, Val Loss: 0.0024\n",
      "Epoch [31/50], Train Loss: 0.0030, Val Loss: 0.0035\n",
      "Epoch [32/50], Train Loss: 0.0026, Val Loss: 0.0063\n",
      "Epoch [33/50], Train Loss: 0.0027, Val Loss: 0.0021\n",
      "Epoch [34/50], Train Loss: 0.0025, Val Loss: 0.0045\n",
      "Epoch [35/50], Train Loss: 0.0025, Val Loss: 0.0057\n",
      "Epoch [36/50], Train Loss: 0.0026, Val Loss: 0.0024\n",
      "Epoch [37/50], Train Loss: 0.0023, Val Loss: 0.0049\n",
      "Epoch [38/50], Train Loss: 0.0025, Val Loss: 0.0058\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=128, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0549, Val Loss: 0.0667\n",
      "Epoch [2/50], Train Loss: 0.0560, Val Loss: 0.0840\n",
      "Epoch [3/50], Train Loss: 0.0395, Val Loss: 0.0580\n",
      "Epoch [4/50], Train Loss: 0.0345, Val Loss: 0.0355\n",
      "Epoch [5/50], Train Loss: 0.0263, Val Loss: 0.0117\n",
      "Epoch [6/50], Train Loss: 0.0163, Val Loss: 0.0086\n",
      "Epoch [7/50], Train Loss: 0.0136, Val Loss: 0.0160\n",
      "Epoch [8/50], Train Loss: 0.0110, Val Loss: 0.0049\n",
      "Epoch [9/50], Train Loss: 0.0076, Val Loss: 0.0027\n",
      "Epoch [10/50], Train Loss: 0.0073, Val Loss: 0.0094\n",
      "Epoch [11/50], Train Loss: 0.0067, Val Loss: 0.0059\n",
      "Epoch [12/50], Train Loss: 0.0066, Val Loss: 0.0024\n",
      "Epoch [13/50], Train Loss: 0.0069, Val Loss: 0.0080\n",
      "Epoch [14/50], Train Loss: 0.0068, Val Loss: 0.0125\n",
      "Epoch [15/50], Train Loss: 0.0071, Val Loss: 0.0042\n",
      "Epoch [16/50], Train Loss: 0.0066, Val Loss: 0.0087\n",
      "Epoch [17/50], Train Loss: 0.0064, Val Loss: 0.0106\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=128, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0668, Val Loss: 0.1015\n",
      "Epoch [2/50], Train Loss: 0.0543, Val Loss: 0.0803\n",
      "Epoch [3/50], Train Loss: 0.0440, Val Loss: 0.0571\n",
      "Epoch [4/50], Train Loss: 0.0384, Val Loss: 0.0336\n",
      "Epoch [5/50], Train Loss: 0.0297, Val Loss: 0.0214\n",
      "Epoch [6/50], Train Loss: 0.0273, Val Loss: 0.0241\n",
      "Epoch [7/50], Train Loss: 0.0206, Val Loss: 0.0114\n",
      "Epoch [8/50], Train Loss: 0.0175, Val Loss: 0.0068\n",
      "Epoch [9/50], Train Loss: 0.0171, Val Loss: 0.0110\n",
      "Epoch [10/50], Train Loss: 0.0172, Val Loss: 0.0070\n",
      "Epoch [11/50], Train Loss: 0.0147, Val Loss: 0.0050\n",
      "Epoch [12/50], Train Loss: 0.0147, Val Loss: 0.0111\n",
      "Epoch [13/50], Train Loss: 0.0136, Val Loss: 0.0043\n",
      "Epoch [14/50], Train Loss: 0.0144, Val Loss: 0.0089\n",
      "Epoch [15/50], Train Loss: 0.0127, Val Loss: 0.0069\n",
      "Epoch [16/50], Train Loss: 0.0139, Val Loss: 0.0071\n",
      "Epoch [17/50], Train Loss: 0.0123, Val Loss: 0.0091\n",
      "Epoch [18/50], Train Loss: 0.0114, Val Loss: 0.0086\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=16, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.2467, Val Loss: 0.6512\n",
      "Epoch [2/50], Train Loss: 0.2445, Val Loss: 0.6469\n",
      "Epoch [3/50], Train Loss: 0.2423, Val Loss: 0.6427\n",
      "Epoch [4/50], Train Loss: 0.2401, Val Loss: 0.6385\n",
      "Epoch [5/50], Train Loss: 0.2380, Val Loss: 0.6343\n",
      "Epoch [6/50], Train Loss: 0.2359, Val Loss: 0.6302\n",
      "Epoch [7/50], Train Loss: 0.2339, Val Loss: 0.6262\n",
      "Epoch [8/50], Train Loss: 0.2318, Val Loss: 0.6222\n",
      "Epoch [9/50], Train Loss: 0.2298, Val Loss: 0.6182\n",
      "Epoch [10/50], Train Loss: 0.2278, Val Loss: 0.6143\n",
      "Epoch [11/50], Train Loss: 0.2259, Val Loss: 0.6104\n",
      "Epoch [12/50], Train Loss: 0.2239, Val Loss: 0.6066\n",
      "Epoch [13/50], Train Loss: 0.2220, Val Loss: 0.6028\n",
      "Epoch [14/50], Train Loss: 0.2201, Val Loss: 0.5991\n",
      "Epoch [15/50], Train Loss: 0.2183, Val Loss: 0.5953\n",
      "Epoch [16/50], Train Loss: 0.2165, Val Loss: 0.5917\n",
      "Epoch [17/50], Train Loss: 0.2146, Val Loss: 0.5880\n",
      "Epoch [18/50], Train Loss: 0.2129, Val Loss: 0.5844\n",
      "Epoch [19/50], Train Loss: 0.2111, Val Loss: 0.5809\n",
      "Epoch [20/50], Train Loss: 0.2094, Val Loss: 0.5773\n",
      "Epoch [21/50], Train Loss: 0.2076, Val Loss: 0.5738\n",
      "Epoch [22/50], Train Loss: 0.2059, Val Loss: 0.5704\n",
      "Epoch [23/50], Train Loss: 0.2043, Val Loss: 0.5670\n",
      "Epoch [24/50], Train Loss: 0.2026, Val Loss: 0.5636\n",
      "Epoch [25/50], Train Loss: 0.2010, Val Loss: 0.5602\n",
      "Epoch [26/50], Train Loss: 0.1993, Val Loss: 0.5569\n",
      "Epoch [27/50], Train Loss: 0.1977, Val Loss: 0.5536\n",
      "Epoch [28/50], Train Loss: 0.1962, Val Loss: 0.5503\n",
      "Epoch [29/50], Train Loss: 0.1946, Val Loss: 0.5471\n",
      "Epoch [30/50], Train Loss: 0.1930, Val Loss: 0.5439\n",
      "Epoch [31/50], Train Loss: 0.1915, Val Loss: 0.5408\n",
      "Epoch [32/50], Train Loss: 0.1900, Val Loss: 0.5376\n",
      "Epoch [33/50], Train Loss: 0.1885, Val Loss: 0.5345\n",
      "Epoch [34/50], Train Loss: 0.1870, Val Loss: 0.5315\n",
      "Epoch [35/50], Train Loss: 0.1856, Val Loss: 0.5284\n",
      "Epoch [36/50], Train Loss: 0.1841, Val Loss: 0.5254\n",
      "Epoch [37/50], Train Loss: 0.1827, Val Loss: 0.5224\n",
      "Epoch [38/50], Train Loss: 0.1813, Val Loss: 0.5194\n",
      "Epoch [39/50], Train Loss: 0.1799, Val Loss: 0.5165\n",
      "Epoch [40/50], Train Loss: 0.1785, Val Loss: 0.5136\n",
      "Epoch [41/50], Train Loss: 0.1772, Val Loss: 0.5107\n",
      "Epoch [42/50], Train Loss: 0.1758, Val Loss: 0.5079\n",
      "Epoch [43/50], Train Loss: 0.1745, Val Loss: 0.5050\n",
      "Epoch [44/50], Train Loss: 0.1732, Val Loss: 0.5022\n",
      "Epoch [45/50], Train Loss: 0.1719, Val Loss: 0.4994\n",
      "Epoch [46/50], Train Loss: 0.1706, Val Loss: 0.4967\n",
      "Epoch [47/50], Train Loss: 0.1693, Val Loss: 0.4939\n",
      "Epoch [48/50], Train Loss: 0.1681, Val Loss: 0.4912\n",
      "Epoch [49/50], Train Loss: 0.1668, Val Loss: 0.4885\n",
      "Epoch [50/50], Train Loss: 0.1656, Val Loss: 0.4859\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=16, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.2894, Val Loss: 0.6043\n",
      "Epoch [2/50], Train Loss: 0.2858, Val Loss: 0.6009\n",
      "Epoch [3/50], Train Loss: 0.2829, Val Loss: 0.5975\n",
      "Epoch [4/50], Train Loss: 0.2828, Val Loss: 0.5941\n",
      "Epoch [5/50], Train Loss: 0.2800, Val Loss: 0.5907\n",
      "Epoch [6/50], Train Loss: 0.2805, Val Loss: 0.5874\n",
      "Epoch [7/50], Train Loss: 0.2776, Val Loss: 0.5841\n",
      "Epoch [8/50], Train Loss: 0.2739, Val Loss: 0.5808\n",
      "Epoch [9/50], Train Loss: 0.2761, Val Loss: 0.5775\n",
      "Epoch [10/50], Train Loss: 0.2720, Val Loss: 0.5743\n",
      "Epoch [11/50], Train Loss: 0.2707, Val Loss: 0.5712\n",
      "Epoch [12/50], Train Loss: 0.2693, Val Loss: 0.5680\n",
      "Epoch [13/50], Train Loss: 0.2665, Val Loss: 0.5649\n",
      "Epoch [14/50], Train Loss: 0.2632, Val Loss: 0.5619\n",
      "Epoch [15/50], Train Loss: 0.2600, Val Loss: 0.5588\n",
      "Epoch [16/50], Train Loss: 0.2614, Val Loss: 0.5559\n",
      "Epoch [17/50], Train Loss: 0.2551, Val Loss: 0.5529\n",
      "Epoch [18/50], Train Loss: 0.2537, Val Loss: 0.5500\n",
      "Epoch [19/50], Train Loss: 0.2522, Val Loss: 0.5471\n",
      "Epoch [20/50], Train Loss: 0.2514, Val Loss: 0.5443\n",
      "Epoch [21/50], Train Loss: 0.2507, Val Loss: 0.5414\n",
      "Epoch [22/50], Train Loss: 0.2501, Val Loss: 0.5386\n",
      "Epoch [23/50], Train Loss: 0.2459, Val Loss: 0.5358\n",
      "Epoch [24/50], Train Loss: 0.2444, Val Loss: 0.5331\n",
      "Epoch [25/50], Train Loss: 0.2421, Val Loss: 0.5304\n",
      "Epoch [26/50], Train Loss: 0.2423, Val Loss: 0.5277\n",
      "Epoch [27/50], Train Loss: 0.2369, Val Loss: 0.5250\n",
      "Epoch [28/50], Train Loss: 0.2353, Val Loss: 0.5224\n",
      "Epoch [29/50], Train Loss: 0.2356, Val Loss: 0.5198\n",
      "Epoch [30/50], Train Loss: 0.2354, Val Loss: 0.5172\n",
      "Epoch [31/50], Train Loss: 0.2317, Val Loss: 0.5147\n",
      "Epoch [32/50], Train Loss: 0.2298, Val Loss: 0.5121\n",
      "Epoch [33/50], Train Loss: 0.2299, Val Loss: 0.5096\n",
      "Epoch [34/50], Train Loss: 0.2278, Val Loss: 0.5071\n",
      "Epoch [35/50], Train Loss: 0.2248, Val Loss: 0.5047\n",
      "Epoch [36/50], Train Loss: 0.2229, Val Loss: 0.5023\n",
      "Epoch [37/50], Train Loss: 0.2208, Val Loss: 0.4999\n",
      "Epoch [38/50], Train Loss: 0.2201, Val Loss: 0.4975\n",
      "Epoch [39/50], Train Loss: 0.2186, Val Loss: 0.4952\n",
      "Epoch [40/50], Train Loss: 0.2185, Val Loss: 0.4928\n",
      "Epoch [41/50], Train Loss: 0.2178, Val Loss: 0.4905\n",
      "Epoch [42/50], Train Loss: 0.2157, Val Loss: 0.4882\n",
      "Epoch [43/50], Train Loss: 0.2134, Val Loss: 0.4859\n",
      "Epoch [44/50], Train Loss: 0.2139, Val Loss: 0.4836\n",
      "Epoch [45/50], Train Loss: 0.2112, Val Loss: 0.4814\n",
      "Epoch [46/50], Train Loss: 0.2112, Val Loss: 0.4791\n",
      "Epoch [47/50], Train Loss: 0.2090, Val Loss: 0.4769\n",
      "Epoch [48/50], Train Loss: 0.2073, Val Loss: 0.4748\n",
      "Epoch [49/50], Train Loss: 0.2055, Val Loss: 0.4726\n",
      "Epoch [50/50], Train Loss: 0.2041, Val Loss: 0.4705\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=16, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.2380, Val Loss: 0.5004\n",
      "Epoch [2/50], Train Loss: 0.2394, Val Loss: 0.4979\n",
      "Epoch [3/50], Train Loss: 0.2417, Val Loss: 0.4953\n",
      "Epoch [4/50], Train Loss: 0.2310, Val Loss: 0.4929\n",
      "Epoch [5/50], Train Loss: 0.2340, Val Loss: 0.4904\n",
      "Epoch [6/50], Train Loss: 0.2269, Val Loss: 0.4881\n",
      "Epoch [7/50], Train Loss: 0.2269, Val Loss: 0.4857\n",
      "Epoch [8/50], Train Loss: 0.2272, Val Loss: 0.4833\n",
      "Epoch [9/50], Train Loss: 0.2252, Val Loss: 0.4809\n",
      "Epoch [10/50], Train Loss: 0.2267, Val Loss: 0.4786\n",
      "Epoch [11/50], Train Loss: 0.2215, Val Loss: 0.4763\n",
      "Epoch [12/50], Train Loss: 0.2230, Val Loss: 0.4740\n",
      "Epoch [13/50], Train Loss: 0.2287, Val Loss: 0.4717\n",
      "Epoch [14/50], Train Loss: 0.2182, Val Loss: 0.4696\n",
      "Epoch [15/50], Train Loss: 0.2131, Val Loss: 0.4674\n",
      "Epoch [16/50], Train Loss: 0.2176, Val Loss: 0.4651\n",
      "Epoch [17/50], Train Loss: 0.2166, Val Loss: 0.4630\n",
      "Epoch [18/50], Train Loss: 0.2093, Val Loss: 0.4609\n",
      "Epoch [19/50], Train Loss: 0.2117, Val Loss: 0.4588\n",
      "Epoch [20/50], Train Loss: 0.2110, Val Loss: 0.4567\n",
      "Epoch [21/50], Train Loss: 0.2068, Val Loss: 0.4546\n",
      "Epoch [22/50], Train Loss: 0.2084, Val Loss: 0.4525\n",
      "Epoch [23/50], Train Loss: 0.2042, Val Loss: 0.4505\n",
      "Epoch [24/50], Train Loss: 0.2040, Val Loss: 0.4485\n",
      "Epoch [25/50], Train Loss: 0.2062, Val Loss: 0.4464\n",
      "Epoch [26/50], Train Loss: 0.2056, Val Loss: 0.4444\n",
      "Epoch [27/50], Train Loss: 0.2047, Val Loss: 0.4424\n",
      "Epoch [28/50], Train Loss: 0.1970, Val Loss: 0.4405\n",
      "Epoch [29/50], Train Loss: 0.1947, Val Loss: 0.4386\n",
      "Epoch [30/50], Train Loss: 0.2008, Val Loss: 0.4367\n",
      "Epoch [31/50], Train Loss: 0.1994, Val Loss: 0.4348\n",
      "Epoch [32/50], Train Loss: 0.1942, Val Loss: 0.4329\n",
      "Epoch [33/50], Train Loss: 0.1954, Val Loss: 0.4310\n",
      "Epoch [34/50], Train Loss: 0.1947, Val Loss: 0.4291\n",
      "Epoch [35/50], Train Loss: 0.1915, Val Loss: 0.4273\n",
      "Epoch [36/50], Train Loss: 0.1894, Val Loss: 0.4255\n",
      "Epoch [37/50], Train Loss: 0.1956, Val Loss: 0.4237\n",
      "Epoch [38/50], Train Loss: 0.1924, Val Loss: 0.4219\n",
      "Epoch [39/50], Train Loss: 0.1908, Val Loss: 0.4201\n",
      "Epoch [40/50], Train Loss: 0.1875, Val Loss: 0.4183\n",
      "Epoch [41/50], Train Loss: 0.1867, Val Loss: 0.4165\n",
      "Epoch [42/50], Train Loss: 0.1839, Val Loss: 0.4148\n",
      "Epoch [43/50], Train Loss: 0.1844, Val Loss: 0.4130\n",
      "Epoch [44/50], Train Loss: 0.1877, Val Loss: 0.4113\n",
      "Epoch [45/50], Train Loss: 0.1817, Val Loss: 0.4096\n",
      "Epoch [46/50], Train Loss: 0.1794, Val Loss: 0.4079\n",
      "Epoch [47/50], Train Loss: 0.1816, Val Loss: 0.4062\n",
      "Epoch [48/50], Train Loss: 0.1796, Val Loss: 0.4046\n",
      "Epoch [49/50], Train Loss: 0.1792, Val Loss: 0.4030\n",
      "Epoch [50/50], Train Loss: 0.1771, Val Loss: 0.4014\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=16, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1658, Val Loss: 0.3521\n",
      "Epoch [2/50], Train Loss: 0.1645, Val Loss: 0.3502\n",
      "Epoch [3/50], Train Loss: 0.1632, Val Loss: 0.3484\n",
      "Epoch [4/50], Train Loss: 0.1619, Val Loss: 0.3465\n",
      "Epoch [5/50], Train Loss: 0.1607, Val Loss: 0.3447\n",
      "Epoch [6/50], Train Loss: 0.1594, Val Loss: 0.3429\n",
      "Epoch [7/50], Train Loss: 0.1582, Val Loss: 0.3411\n",
      "Epoch [8/50], Train Loss: 0.1570, Val Loss: 0.3394\n",
      "Epoch [9/50], Train Loss: 0.1558, Val Loss: 0.3377\n",
      "Epoch [10/50], Train Loss: 0.1546, Val Loss: 0.3360\n",
      "Epoch [11/50], Train Loss: 0.1534, Val Loss: 0.3343\n",
      "Epoch [12/50], Train Loss: 0.1523, Val Loss: 0.3327\n",
      "Epoch [13/50], Train Loss: 0.1512, Val Loss: 0.3310\n",
      "Epoch [14/50], Train Loss: 0.1500, Val Loss: 0.3294\n",
      "Epoch [15/50], Train Loss: 0.1489, Val Loss: 0.3278\n",
      "Epoch [16/50], Train Loss: 0.1479, Val Loss: 0.3262\n",
      "Epoch [17/50], Train Loss: 0.1468, Val Loss: 0.3247\n",
      "Epoch [18/50], Train Loss: 0.1457, Val Loss: 0.3231\n",
      "Epoch [19/50], Train Loss: 0.1447, Val Loss: 0.3216\n",
      "Epoch [20/50], Train Loss: 0.1437, Val Loss: 0.3201\n",
      "Epoch [21/50], Train Loss: 0.1427, Val Loss: 0.3186\n",
      "Epoch [22/50], Train Loss: 0.1417, Val Loss: 0.3172\n",
      "Epoch [23/50], Train Loss: 0.1407, Val Loss: 0.3157\n",
      "Epoch [24/50], Train Loss: 0.1397, Val Loss: 0.3143\n",
      "Epoch [25/50], Train Loss: 0.1387, Val Loss: 0.3129\n",
      "Epoch [26/50], Train Loss: 0.1378, Val Loss: 0.3115\n",
      "Epoch [27/50], Train Loss: 0.1368, Val Loss: 0.3101\n",
      "Epoch [28/50], Train Loss: 0.1359, Val Loss: 0.3087\n",
      "Epoch [29/50], Train Loss: 0.1350, Val Loss: 0.3074\n",
      "Epoch [30/50], Train Loss: 0.1341, Val Loss: 0.3060\n",
      "Epoch [31/50], Train Loss: 0.1332, Val Loss: 0.3047\n",
      "Epoch [32/50], Train Loss: 0.1323, Val Loss: 0.3034\n",
      "Epoch [33/50], Train Loss: 0.1315, Val Loss: 0.3021\n",
      "Epoch [34/50], Train Loss: 0.1306, Val Loss: 0.3008\n",
      "Epoch [35/50], Train Loss: 0.1298, Val Loss: 0.2995\n",
      "Epoch [36/50], Train Loss: 0.1289, Val Loss: 0.2983\n",
      "Epoch [37/50], Train Loss: 0.1281, Val Loss: 0.2971\n",
      "Epoch [38/50], Train Loss: 0.1273, Val Loss: 0.2959\n",
      "Epoch [39/50], Train Loss: 0.1265, Val Loss: 0.2946\n",
      "Epoch [40/50], Train Loss: 0.1257, Val Loss: 0.2934\n",
      "Epoch [41/50], Train Loss: 0.1249, Val Loss: 0.2923\n",
      "Epoch [42/50], Train Loss: 0.1241, Val Loss: 0.2911\n",
      "Epoch [43/50], Train Loss: 0.1234, Val Loss: 0.2899\n",
      "Epoch [44/50], Train Loss: 0.1226, Val Loss: 0.2888\n",
      "Epoch [45/50], Train Loss: 0.1219, Val Loss: 0.2876\n",
      "Epoch [46/50], Train Loss: 0.1211, Val Loss: 0.2865\n",
      "Epoch [47/50], Train Loss: 0.1204, Val Loss: 0.2854\n",
      "Epoch [48/50], Train Loss: 0.1197, Val Loss: 0.2843\n",
      "Epoch [49/50], Train Loss: 0.1190, Val Loss: 0.2832\n",
      "Epoch [50/50], Train Loss: 0.1183, Val Loss: 0.2821\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=16, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0890, Val Loss: 0.2499\n",
      "Epoch [2/50], Train Loss: 0.0873, Val Loss: 0.2487\n",
      "Epoch [3/50], Train Loss: 0.0886, Val Loss: 0.2475\n",
      "Epoch [4/50], Train Loss: 0.0879, Val Loss: 0.2463\n",
      "Epoch [5/50], Train Loss: 0.0872, Val Loss: 0.2450\n",
      "Epoch [6/50], Train Loss: 0.0871, Val Loss: 0.2439\n",
      "Epoch [7/50], Train Loss: 0.0863, Val Loss: 0.2427\n",
      "Epoch [8/50], Train Loss: 0.0864, Val Loss: 0.2416\n",
      "Epoch [9/50], Train Loss: 0.0831, Val Loss: 0.2404\n",
      "Epoch [10/50], Train Loss: 0.0844, Val Loss: 0.2393\n",
      "Epoch [11/50], Train Loss: 0.0826, Val Loss: 0.2381\n",
      "Epoch [12/50], Train Loss: 0.0820, Val Loss: 0.2371\n",
      "Epoch [13/50], Train Loss: 0.0827, Val Loss: 0.2360\n",
      "Epoch [14/50], Train Loss: 0.0825, Val Loss: 0.2349\n",
      "Epoch [15/50], Train Loss: 0.0811, Val Loss: 0.2338\n",
      "Epoch [16/50], Train Loss: 0.0814, Val Loss: 0.2327\n",
      "Epoch [17/50], Train Loss: 0.0797, Val Loss: 0.2317\n",
      "Epoch [18/50], Train Loss: 0.0807, Val Loss: 0.2306\n",
      "Epoch [19/50], Train Loss: 0.0791, Val Loss: 0.2296\n",
      "Epoch [20/50], Train Loss: 0.0803, Val Loss: 0.2286\n",
      "Epoch [21/50], Train Loss: 0.0786, Val Loss: 0.2275\n",
      "Epoch [22/50], Train Loss: 0.0782, Val Loss: 0.2265\n",
      "Epoch [23/50], Train Loss: 0.0783, Val Loss: 0.2255\n",
      "Epoch [24/50], Train Loss: 0.0782, Val Loss: 0.2245\n",
      "Epoch [25/50], Train Loss: 0.0763, Val Loss: 0.2235\n",
      "Epoch [26/50], Train Loss: 0.0767, Val Loss: 0.2226\n",
      "Epoch [27/50], Train Loss: 0.0759, Val Loss: 0.2216\n",
      "Epoch [28/50], Train Loss: 0.0761, Val Loss: 0.2206\n",
      "Epoch [29/50], Train Loss: 0.0760, Val Loss: 0.2197\n",
      "Epoch [30/50], Train Loss: 0.0749, Val Loss: 0.2187\n",
      "Epoch [31/50], Train Loss: 0.0728, Val Loss: 0.2178\n",
      "Epoch [32/50], Train Loss: 0.0741, Val Loss: 0.2168\n",
      "Epoch [33/50], Train Loss: 0.0731, Val Loss: 0.2159\n",
      "Epoch [34/50], Train Loss: 0.0737, Val Loss: 0.2150\n",
      "Epoch [35/50], Train Loss: 0.0747, Val Loss: 0.2141\n",
      "Epoch [36/50], Train Loss: 0.0737, Val Loss: 0.2132\n",
      "Epoch [37/50], Train Loss: 0.0721, Val Loss: 0.2123\n",
      "Epoch [38/50], Train Loss: 0.0729, Val Loss: 0.2114\n",
      "Epoch [39/50], Train Loss: 0.0716, Val Loss: 0.2105\n",
      "Epoch [40/50], Train Loss: 0.0713, Val Loss: 0.2096\n",
      "Epoch [41/50], Train Loss: 0.0712, Val Loss: 0.2087\n",
      "Epoch [42/50], Train Loss: 0.0703, Val Loss: 0.2079\n",
      "Epoch [43/50], Train Loss: 0.0701, Val Loss: 0.2070\n",
      "Epoch [44/50], Train Loss: 0.0692, Val Loss: 0.2062\n",
      "Epoch [45/50], Train Loss: 0.0690, Val Loss: 0.2053\n",
      "Epoch [46/50], Train Loss: 0.0679, Val Loss: 0.2045\n",
      "Epoch [47/50], Train Loss: 0.0694, Val Loss: 0.2037\n",
      "Epoch [48/50], Train Loss: 0.0698, Val Loss: 0.2029\n",
      "Epoch [49/50], Train Loss: 0.0690, Val Loss: 0.2021\n",
      "Epoch [50/50], Train Loss: 0.0675, Val Loss: 0.2013\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=16, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.2968, Val Loss: 0.4781\n",
      "Epoch [2/50], Train Loss: 0.2893, Val Loss: 0.4745\n",
      "Epoch [3/50], Train Loss: 0.2869, Val Loss: 0.4711\n",
      "Epoch [4/50], Train Loss: 0.2827, Val Loss: 0.4676\n",
      "Epoch [5/50], Train Loss: 0.2780, Val Loss: 0.4642\n",
      "Epoch [6/50], Train Loss: 0.2790, Val Loss: 0.4608\n",
      "Epoch [7/50], Train Loss: 0.2741, Val Loss: 0.4575\n",
      "Epoch [8/50], Train Loss: 0.2662, Val Loss: 0.4543\n",
      "Epoch [9/50], Train Loss: 0.2679, Val Loss: 0.4512\n",
      "Epoch [10/50], Train Loss: 0.2655, Val Loss: 0.4481\n",
      "Epoch [11/50], Train Loss: 0.2671, Val Loss: 0.4450\n",
      "Epoch [12/50], Train Loss: 0.2607, Val Loss: 0.4420\n",
      "Epoch [13/50], Train Loss: 0.2582, Val Loss: 0.4390\n",
      "Epoch [14/50], Train Loss: 0.2560, Val Loss: 0.4360\n",
      "Epoch [15/50], Train Loss: 0.2600, Val Loss: 0.4331\n",
      "Epoch [16/50], Train Loss: 0.2485, Val Loss: 0.4303\n",
      "Epoch [17/50], Train Loss: 0.2447, Val Loss: 0.4275\n",
      "Epoch [18/50], Train Loss: 0.2477, Val Loss: 0.4247\n",
      "Epoch [19/50], Train Loss: 0.2451, Val Loss: 0.4220\n",
      "Epoch [20/50], Train Loss: 0.2423, Val Loss: 0.4193\n",
      "Epoch [21/50], Train Loss: 0.2420, Val Loss: 0.4166\n",
      "Epoch [22/50], Train Loss: 0.2452, Val Loss: 0.4139\n",
      "Epoch [23/50], Train Loss: 0.2412, Val Loss: 0.4113\n",
      "Epoch [24/50], Train Loss: 0.2319, Val Loss: 0.4088\n",
      "Epoch [25/50], Train Loss: 0.2319, Val Loss: 0.4062\n",
      "Epoch [26/50], Train Loss: 0.2345, Val Loss: 0.4038\n",
      "Epoch [27/50], Train Loss: 0.2323, Val Loss: 0.4013\n",
      "Epoch [28/50], Train Loss: 0.2303, Val Loss: 0.3988\n",
      "Epoch [29/50], Train Loss: 0.2248, Val Loss: 0.3964\n",
      "Epoch [30/50], Train Loss: 0.2245, Val Loss: 0.3941\n",
      "Epoch [31/50], Train Loss: 0.2220, Val Loss: 0.3917\n",
      "Epoch [32/50], Train Loss: 0.2183, Val Loss: 0.3894\n",
      "Epoch [33/50], Train Loss: 0.2211, Val Loss: 0.3872\n",
      "Epoch [34/50], Train Loss: 0.2177, Val Loss: 0.3850\n",
      "Epoch [35/50], Train Loss: 0.2137, Val Loss: 0.3827\n",
      "Epoch [36/50], Train Loss: 0.2091, Val Loss: 0.3806\n",
      "Epoch [37/50], Train Loss: 0.2112, Val Loss: 0.3785\n",
      "Epoch [38/50], Train Loss: 0.2157, Val Loss: 0.3763\n",
      "Epoch [39/50], Train Loss: 0.2093, Val Loss: 0.3742\n",
      "Epoch [40/50], Train Loss: 0.2082, Val Loss: 0.3722\n",
      "Epoch [41/50], Train Loss: 0.2073, Val Loss: 0.3701\n",
      "Epoch [42/50], Train Loss: 0.2091, Val Loss: 0.3681\n",
      "Epoch [43/50], Train Loss: 0.2017, Val Loss: 0.3661\n",
      "Epoch [44/50], Train Loss: 0.2044, Val Loss: 0.3641\n",
      "Epoch [45/50], Train Loss: 0.1975, Val Loss: 0.3622\n",
      "Epoch [46/50], Train Loss: 0.1985, Val Loss: 0.3603\n",
      "Epoch [47/50], Train Loss: 0.1986, Val Loss: 0.3584\n",
      "Epoch [48/50], Train Loss: 0.2013, Val Loss: 0.3565\n",
      "Epoch [49/50], Train Loss: 0.1963, Val Loss: 0.3547\n",
      "Epoch [50/50], Train Loss: 0.1929, Val Loss: 0.3529\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=16, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1857, Val Loss: 0.4101\n",
      "Epoch [2/50], Train Loss: 0.1836, Val Loss: 0.4067\n",
      "Epoch [3/50], Train Loss: 0.1816, Val Loss: 0.4033\n",
      "Epoch [4/50], Train Loss: 0.1796, Val Loss: 0.3999\n",
      "Epoch [5/50], Train Loss: 0.1776, Val Loss: 0.3967\n",
      "Epoch [6/50], Train Loss: 0.1756, Val Loss: 0.3935\n",
      "Epoch [7/50], Train Loss: 0.1737, Val Loss: 0.3903\n",
      "Epoch [8/50], Train Loss: 0.1718, Val Loss: 0.3872\n",
      "Epoch [9/50], Train Loss: 0.1699, Val Loss: 0.3841\n",
      "Epoch [10/50], Train Loss: 0.1681, Val Loss: 0.3811\n",
      "Epoch [11/50], Train Loss: 0.1663, Val Loss: 0.3781\n",
      "Epoch [12/50], Train Loss: 0.1645, Val Loss: 0.3751\n",
      "Epoch [13/50], Train Loss: 0.1628, Val Loss: 0.3722\n",
      "Epoch [14/50], Train Loss: 0.1610, Val Loss: 0.3693\n",
      "Epoch [15/50], Train Loss: 0.1593, Val Loss: 0.3665\n",
      "Epoch [16/50], Train Loss: 0.1577, Val Loss: 0.3637\n",
      "Epoch [17/50], Train Loss: 0.1560, Val Loss: 0.3610\n",
      "Epoch [18/50], Train Loss: 0.1544, Val Loss: 0.3582\n",
      "Epoch [19/50], Train Loss: 0.1528, Val Loss: 0.3556\n",
      "Epoch [20/50], Train Loss: 0.1513, Val Loss: 0.3529\n",
      "Epoch [21/50], Train Loss: 0.1497, Val Loss: 0.3503\n",
      "Epoch [22/50], Train Loss: 0.1482, Val Loss: 0.3477\n",
      "Epoch [23/50], Train Loss: 0.1467, Val Loss: 0.3451\n",
      "Epoch [24/50], Train Loss: 0.1452, Val Loss: 0.3427\n",
      "Epoch [25/50], Train Loss: 0.1438, Val Loss: 0.3402\n",
      "Epoch [26/50], Train Loss: 0.1423, Val Loss: 0.3377\n",
      "Epoch [27/50], Train Loss: 0.1409, Val Loss: 0.3353\n",
      "Epoch [28/50], Train Loss: 0.1395, Val Loss: 0.3329\n",
      "Epoch [29/50], Train Loss: 0.1382, Val Loss: 0.3306\n",
      "Epoch [30/50], Train Loss: 0.1368, Val Loss: 0.3283\n",
      "Epoch [31/50], Train Loss: 0.1355, Val Loss: 0.3260\n",
      "Epoch [32/50], Train Loss: 0.1342, Val Loss: 0.3237\n",
      "Epoch [33/50], Train Loss: 0.1329, Val Loss: 0.3215\n",
      "Epoch [34/50], Train Loss: 0.1316, Val Loss: 0.3192\n",
      "Epoch [35/50], Train Loss: 0.1304, Val Loss: 0.3171\n",
      "Epoch [36/50], Train Loss: 0.1291, Val Loss: 0.3149\n",
      "Epoch [37/50], Train Loss: 0.1279, Val Loss: 0.3128\n",
      "Epoch [38/50], Train Loss: 0.1267, Val Loss: 0.3107\n",
      "Epoch [39/50], Train Loss: 0.1255, Val Loss: 0.3086\n",
      "Epoch [40/50], Train Loss: 0.1243, Val Loss: 0.3066\n",
      "Epoch [41/50], Train Loss: 0.1232, Val Loss: 0.3045\n",
      "Epoch [42/50], Train Loss: 0.1221, Val Loss: 0.3025\n",
      "Epoch [43/50], Train Loss: 0.1209, Val Loss: 0.3005\n",
      "Epoch [44/50], Train Loss: 0.1198, Val Loss: 0.2986\n",
      "Epoch [45/50], Train Loss: 0.1187, Val Loss: 0.2967\n",
      "Epoch [46/50], Train Loss: 0.1177, Val Loss: 0.2947\n",
      "Epoch [47/50], Train Loss: 0.1166, Val Loss: 0.2928\n",
      "Epoch [48/50], Train Loss: 0.1156, Val Loss: 0.2910\n",
      "Epoch [49/50], Train Loss: 0.1145, Val Loss: 0.2891\n",
      "Epoch [50/50], Train Loss: 0.1135, Val Loss: 0.2873\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=16, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1891, Val Loss: 0.4354\n",
      "Epoch [2/50], Train Loss: 0.1874, Val Loss: 0.4329\n",
      "Epoch [3/50], Train Loss: 0.1864, Val Loss: 0.4304\n",
      "Epoch [4/50], Train Loss: 0.1826, Val Loss: 0.4279\n",
      "Epoch [5/50], Train Loss: 0.1861, Val Loss: 0.4254\n",
      "Epoch [6/50], Train Loss: 0.1818, Val Loss: 0.4230\n",
      "Epoch [7/50], Train Loss: 0.1799, Val Loss: 0.4205\n",
      "Epoch [8/50], Train Loss: 0.1792, Val Loss: 0.4181\n",
      "Epoch [9/50], Train Loss: 0.1788, Val Loss: 0.4157\n",
      "Epoch [10/50], Train Loss: 0.1767, Val Loss: 0.4133\n",
      "Epoch [11/50], Train Loss: 0.1761, Val Loss: 0.4110\n",
      "Epoch [12/50], Train Loss: 0.1752, Val Loss: 0.4087\n",
      "Epoch [13/50], Train Loss: 0.1719, Val Loss: 0.4064\n",
      "Epoch [14/50], Train Loss: 0.1712, Val Loss: 0.4042\n",
      "Epoch [15/50], Train Loss: 0.1707, Val Loss: 0.4019\n",
      "Epoch [16/50], Train Loss: 0.1693, Val Loss: 0.3997\n",
      "Epoch [17/50], Train Loss: 0.1673, Val Loss: 0.3976\n",
      "Epoch [18/50], Train Loss: 0.1679, Val Loss: 0.3954\n",
      "Epoch [19/50], Train Loss: 0.1662, Val Loss: 0.3932\n",
      "Epoch [20/50], Train Loss: 0.1654, Val Loss: 0.3911\n",
      "Epoch [21/50], Train Loss: 0.1661, Val Loss: 0.3890\n",
      "Epoch [22/50], Train Loss: 0.1632, Val Loss: 0.3868\n",
      "Epoch [23/50], Train Loss: 0.1641, Val Loss: 0.3848\n",
      "Epoch [24/50], Train Loss: 0.1595, Val Loss: 0.3827\n",
      "Epoch [25/50], Train Loss: 0.1598, Val Loss: 0.3807\n",
      "Epoch [26/50], Train Loss: 0.1598, Val Loss: 0.3786\n",
      "Epoch [27/50], Train Loss: 0.1604, Val Loss: 0.3765\n",
      "Epoch [28/50], Train Loss: 0.1552, Val Loss: 0.3746\n",
      "Epoch [29/50], Train Loss: 0.1543, Val Loss: 0.3726\n",
      "Epoch [30/50], Train Loss: 0.1563, Val Loss: 0.3706\n",
      "Epoch [31/50], Train Loss: 0.1531, Val Loss: 0.3687\n",
      "Epoch [32/50], Train Loss: 0.1529, Val Loss: 0.3668\n",
      "Epoch [33/50], Train Loss: 0.1504, Val Loss: 0.3649\n",
      "Epoch [34/50], Train Loss: 0.1521, Val Loss: 0.3630\n",
      "Epoch [35/50], Train Loss: 0.1508, Val Loss: 0.3611\n",
      "Epoch [36/50], Train Loss: 0.1491, Val Loss: 0.3592\n",
      "Epoch [37/50], Train Loss: 0.1503, Val Loss: 0.3574\n",
      "Epoch [38/50], Train Loss: 0.1488, Val Loss: 0.3555\n",
      "Epoch [39/50], Train Loss: 0.1467, Val Loss: 0.3537\n",
      "Epoch [40/50], Train Loss: 0.1452, Val Loss: 0.3519\n",
      "Epoch [41/50], Train Loss: 0.1434, Val Loss: 0.3501\n",
      "Epoch [42/50], Train Loss: 0.1443, Val Loss: 0.3483\n",
      "Epoch [43/50], Train Loss: 0.1403, Val Loss: 0.3466\n",
      "Epoch [44/50], Train Loss: 0.1427, Val Loss: 0.3448\n",
      "Epoch [45/50], Train Loss: 0.1407, Val Loss: 0.3430\n",
      "Epoch [46/50], Train Loss: 0.1367, Val Loss: 0.3414\n",
      "Epoch [47/50], Train Loss: 0.1370, Val Loss: 0.3397\n",
      "Epoch [48/50], Train Loss: 0.1405, Val Loss: 0.3380\n",
      "Epoch [49/50], Train Loss: 0.1368, Val Loss: 0.3363\n",
      "Epoch [50/50], Train Loss: 0.1372, Val Loss: 0.3346\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=16, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.2200, Val Loss: 0.3228\n",
      "Epoch [2/50], Train Loss: 0.2246, Val Loss: 0.3205\n",
      "Epoch [3/50], Train Loss: 0.2085, Val Loss: 0.3183\n",
      "Epoch [4/50], Train Loss: 0.2105, Val Loss: 0.3162\n",
      "Epoch [5/50], Train Loss: 0.2166, Val Loss: 0.3142\n",
      "Epoch [6/50], Train Loss: 0.2111, Val Loss: 0.3121\n",
      "Epoch [7/50], Train Loss: 0.2090, Val Loss: 0.3101\n",
      "Epoch [8/50], Train Loss: 0.2048, Val Loss: 0.3082\n",
      "Epoch [9/50], Train Loss: 0.2062, Val Loss: 0.3062\n",
      "Epoch [10/50], Train Loss: 0.2064, Val Loss: 0.3042\n",
      "Epoch [11/50], Train Loss: 0.2022, Val Loss: 0.3023\n",
      "Epoch [12/50], Train Loss: 0.2033, Val Loss: 0.3004\n",
      "Epoch [13/50], Train Loss: 0.2010, Val Loss: 0.2986\n",
      "Epoch [14/50], Train Loss: 0.1962, Val Loss: 0.2968\n",
      "Epoch [15/50], Train Loss: 0.1990, Val Loss: 0.2950\n",
      "Epoch [16/50], Train Loss: 0.1941, Val Loss: 0.2933\n",
      "Epoch [17/50], Train Loss: 0.1903, Val Loss: 0.2916\n",
      "Epoch [18/50], Train Loss: 0.1920, Val Loss: 0.2898\n",
      "Epoch [19/50], Train Loss: 0.1759, Val Loss: 0.2882\n",
      "Epoch [20/50], Train Loss: 0.1802, Val Loss: 0.2867\n",
      "Epoch [21/50], Train Loss: 0.1844, Val Loss: 0.2851\n",
      "Epoch [22/50], Train Loss: 0.1806, Val Loss: 0.2835\n",
      "Epoch [23/50], Train Loss: 0.1870, Val Loss: 0.2819\n",
      "Epoch [24/50], Train Loss: 0.1770, Val Loss: 0.2805\n",
      "Epoch [25/50], Train Loss: 0.1824, Val Loss: 0.2790\n",
      "Epoch [26/50], Train Loss: 0.1852, Val Loss: 0.2774\n",
      "Epoch [27/50], Train Loss: 0.1713, Val Loss: 0.2760\n",
      "Epoch [28/50], Train Loss: 0.1761, Val Loss: 0.2747\n",
      "Epoch [29/50], Train Loss: 0.1731, Val Loss: 0.2733\n",
      "Epoch [30/50], Train Loss: 0.1699, Val Loss: 0.2720\n",
      "Epoch [31/50], Train Loss: 0.1675, Val Loss: 0.2707\n",
      "Epoch [32/50], Train Loss: 0.1740, Val Loss: 0.2693\n",
      "Epoch [33/50], Train Loss: 0.1684, Val Loss: 0.2680\n",
      "Epoch [34/50], Train Loss: 0.1753, Val Loss: 0.2667\n",
      "Epoch [35/50], Train Loss: 0.1691, Val Loss: 0.2655\n",
      "Epoch [36/50], Train Loss: 0.1676, Val Loss: 0.2642\n",
      "Epoch [37/50], Train Loss: 0.1654, Val Loss: 0.2630\n",
      "Epoch [38/50], Train Loss: 0.1684, Val Loss: 0.2618\n",
      "Epoch [39/50], Train Loss: 0.1614, Val Loss: 0.2605\n",
      "Epoch [40/50], Train Loss: 0.1635, Val Loss: 0.2594\n",
      "Epoch [41/50], Train Loss: 0.1587, Val Loss: 0.2583\n",
      "Epoch [42/50], Train Loss: 0.1598, Val Loss: 0.2571\n",
      "Epoch [43/50], Train Loss: 0.1595, Val Loss: 0.2561\n",
      "Epoch [44/50], Train Loss: 0.1547, Val Loss: 0.2550\n",
      "Epoch [45/50], Train Loss: 0.1577, Val Loss: 0.2538\n",
      "Epoch [46/50], Train Loss: 0.1568, Val Loss: 0.2528\n",
      "Epoch [47/50], Train Loss: 0.1542, Val Loss: 0.2517\n",
      "Epoch [48/50], Train Loss: 0.1485, Val Loss: 0.2508\n",
      "Epoch [49/50], Train Loss: 0.1552, Val Loss: 0.2498\n",
      "Epoch [50/50], Train Loss: 0.1537, Val Loss: 0.2488\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=32, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1027, Val Loss: 0.3141\n",
      "Epoch [2/50], Train Loss: 0.1022, Val Loss: 0.3127\n",
      "Epoch [3/50], Train Loss: 0.1016, Val Loss: 0.3114\n",
      "Epoch [4/50], Train Loss: 0.1011, Val Loss: 0.3101\n",
      "Epoch [5/50], Train Loss: 0.1006, Val Loss: 0.3088\n",
      "Epoch [6/50], Train Loss: 0.1001, Val Loss: 0.3075\n",
      "Epoch [7/50], Train Loss: 0.0995, Val Loss: 0.3062\n",
      "Epoch [8/50], Train Loss: 0.0990, Val Loss: 0.3050\n",
      "Epoch [9/50], Train Loss: 0.0985, Val Loss: 0.3037\n",
      "Epoch [10/50], Train Loss: 0.0980, Val Loss: 0.3024\n",
      "Epoch [11/50], Train Loss: 0.0975, Val Loss: 0.3011\n",
      "Epoch [12/50], Train Loss: 0.0970, Val Loss: 0.2999\n",
      "Epoch [13/50], Train Loss: 0.0965, Val Loss: 0.2987\n",
      "Epoch [14/50], Train Loss: 0.0960, Val Loss: 0.2974\n",
      "Epoch [15/50], Train Loss: 0.0955, Val Loss: 0.2962\n",
      "Epoch [16/50], Train Loss: 0.0950, Val Loss: 0.2950\n",
      "Epoch [17/50], Train Loss: 0.0946, Val Loss: 0.2938\n",
      "Epoch [18/50], Train Loss: 0.0941, Val Loss: 0.2926\n",
      "Epoch [19/50], Train Loss: 0.0936, Val Loss: 0.2914\n",
      "Epoch [20/50], Train Loss: 0.0931, Val Loss: 0.2902\n",
      "Epoch [21/50], Train Loss: 0.0927, Val Loss: 0.2890\n",
      "Epoch [22/50], Train Loss: 0.0922, Val Loss: 0.2879\n",
      "Epoch [23/50], Train Loss: 0.0917, Val Loss: 0.2867\n",
      "Epoch [24/50], Train Loss: 0.0913, Val Loss: 0.2855\n",
      "Epoch [25/50], Train Loss: 0.0908, Val Loss: 0.2843\n",
      "Epoch [26/50], Train Loss: 0.0904, Val Loss: 0.2832\n",
      "Epoch [27/50], Train Loss: 0.0899, Val Loss: 0.2821\n",
      "Epoch [28/50], Train Loss: 0.0895, Val Loss: 0.2809\n",
      "Epoch [29/50], Train Loss: 0.0890, Val Loss: 0.2798\n",
      "Epoch [30/50], Train Loss: 0.0886, Val Loss: 0.2787\n",
      "Epoch [31/50], Train Loss: 0.0882, Val Loss: 0.2775\n",
      "Epoch [32/50], Train Loss: 0.0877, Val Loss: 0.2764\n",
      "Epoch [33/50], Train Loss: 0.0873, Val Loss: 0.2753\n",
      "Epoch [34/50], Train Loss: 0.0869, Val Loss: 0.2742\n",
      "Epoch [35/50], Train Loss: 0.0865, Val Loss: 0.2731\n",
      "Epoch [36/50], Train Loss: 0.0860, Val Loss: 0.2720\n",
      "Epoch [37/50], Train Loss: 0.0856, Val Loss: 0.2709\n",
      "Epoch [38/50], Train Loss: 0.0852, Val Loss: 0.2699\n",
      "Epoch [39/50], Train Loss: 0.0848, Val Loss: 0.2688\n",
      "Epoch [40/50], Train Loss: 0.0844, Val Loss: 0.2677\n",
      "Epoch [41/50], Train Loss: 0.0840, Val Loss: 0.2667\n",
      "Epoch [42/50], Train Loss: 0.0836, Val Loss: 0.2656\n",
      "Epoch [43/50], Train Loss: 0.0832, Val Loss: 0.2646\n",
      "Epoch [44/50], Train Loss: 0.0828, Val Loss: 0.2635\n",
      "Epoch [45/50], Train Loss: 0.0824, Val Loss: 0.2625\n",
      "Epoch [46/50], Train Loss: 0.0820, Val Loss: 0.2614\n",
      "Epoch [47/50], Train Loss: 0.0816, Val Loss: 0.2604\n",
      "Epoch [48/50], Train Loss: 0.0812, Val Loss: 0.2594\n",
      "Epoch [49/50], Train Loss: 0.0808, Val Loss: 0.2584\n",
      "Epoch [50/50], Train Loss: 0.0805, Val Loss: 0.2574\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=32, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1428, Val Loss: 0.3908\n",
      "Epoch [2/50], Train Loss: 0.1412, Val Loss: 0.3892\n",
      "Epoch [3/50], Train Loss: 0.1408, Val Loss: 0.3875\n",
      "Epoch [4/50], Train Loss: 0.1392, Val Loss: 0.3859\n",
      "Epoch [5/50], Train Loss: 0.1385, Val Loss: 0.3844\n",
      "Epoch [6/50], Train Loss: 0.1382, Val Loss: 0.3828\n",
      "Epoch [7/50], Train Loss: 0.1382, Val Loss: 0.3812\n",
      "Epoch [8/50], Train Loss: 0.1375, Val Loss: 0.3796\n",
      "Epoch [9/50], Train Loss: 0.1358, Val Loss: 0.3781\n",
      "Epoch [10/50], Train Loss: 0.1357, Val Loss: 0.3765\n",
      "Epoch [11/50], Train Loss: 0.1343, Val Loss: 0.3750\n",
      "Epoch [12/50], Train Loss: 0.1344, Val Loss: 0.3735\n",
      "Epoch [13/50], Train Loss: 0.1340, Val Loss: 0.3720\n",
      "Epoch [14/50], Train Loss: 0.1323, Val Loss: 0.3705\n",
      "Epoch [15/50], Train Loss: 0.1318, Val Loss: 0.3690\n",
      "Epoch [16/50], Train Loss: 0.1323, Val Loss: 0.3675\n",
      "Epoch [17/50], Train Loss: 0.1318, Val Loss: 0.3660\n",
      "Epoch [18/50], Train Loss: 0.1301, Val Loss: 0.3645\n",
      "Epoch [19/50], Train Loss: 0.1293, Val Loss: 0.3631\n",
      "Epoch [20/50], Train Loss: 0.1281, Val Loss: 0.3616\n",
      "Epoch [21/50], Train Loss: 0.1284, Val Loss: 0.3602\n",
      "Epoch [22/50], Train Loss: 0.1273, Val Loss: 0.3587\n",
      "Epoch [23/50], Train Loss: 0.1268, Val Loss: 0.3573\n",
      "Epoch [24/50], Train Loss: 0.1258, Val Loss: 0.3559\n",
      "Epoch [25/50], Train Loss: 0.1247, Val Loss: 0.3545\n",
      "Epoch [26/50], Train Loss: 0.1250, Val Loss: 0.3531\n",
      "Epoch [27/50], Train Loss: 0.1234, Val Loss: 0.3517\n",
      "Epoch [28/50], Train Loss: 0.1234, Val Loss: 0.3503\n",
      "Epoch [29/50], Train Loss: 0.1228, Val Loss: 0.3489\n",
      "Epoch [30/50], Train Loss: 0.1212, Val Loss: 0.3476\n",
      "Epoch [31/50], Train Loss: 0.1213, Val Loss: 0.3462\n",
      "Epoch [32/50], Train Loss: 0.1214, Val Loss: 0.3448\n",
      "Epoch [33/50], Train Loss: 0.1205, Val Loss: 0.3435\n",
      "Epoch [34/50], Train Loss: 0.1204, Val Loss: 0.3422\n",
      "Epoch [35/50], Train Loss: 0.1192, Val Loss: 0.3408\n",
      "Epoch [36/50], Train Loss: 0.1183, Val Loss: 0.3395\n",
      "Epoch [37/50], Train Loss: 0.1182, Val Loss: 0.3382\n",
      "Epoch [38/50], Train Loss: 0.1174, Val Loss: 0.3369\n",
      "Epoch [39/50], Train Loss: 0.1178, Val Loss: 0.3356\n",
      "Epoch [40/50], Train Loss: 0.1157, Val Loss: 0.3343\n",
      "Epoch [41/50], Train Loss: 0.1162, Val Loss: 0.3330\n",
      "Epoch [42/50], Train Loss: 0.1152, Val Loss: 0.3318\n",
      "Epoch [43/50], Train Loss: 0.1143, Val Loss: 0.3305\n",
      "Epoch [44/50], Train Loss: 0.1142, Val Loss: 0.3292\n",
      "Epoch [45/50], Train Loss: 0.1137, Val Loss: 0.3280\n",
      "Epoch [46/50], Train Loss: 0.1134, Val Loss: 0.3267\n",
      "Epoch [47/50], Train Loss: 0.1132, Val Loss: 0.3255\n",
      "Epoch [48/50], Train Loss: 0.1123, Val Loss: 0.3243\n",
      "Epoch [49/50], Train Loss: 0.1125, Val Loss: 0.3231\n",
      "Epoch [50/50], Train Loss: 0.1102, Val Loss: 0.3218\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=32, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1154, Val Loss: 0.3267\n",
      "Epoch [2/50], Train Loss: 0.1170, Val Loss: 0.3251\n",
      "Epoch [3/50], Train Loss: 0.1141, Val Loss: 0.3236\n",
      "Epoch [4/50], Train Loss: 0.1159, Val Loss: 0.3219\n",
      "Epoch [5/50], Train Loss: 0.1128, Val Loss: 0.3203\n",
      "Epoch [6/50], Train Loss: 0.1127, Val Loss: 0.3187\n",
      "Epoch [7/50], Train Loss: 0.1119, Val Loss: 0.3172\n",
      "Epoch [8/50], Train Loss: 0.1117, Val Loss: 0.3156\n",
      "Epoch [9/50], Train Loss: 0.1126, Val Loss: 0.3141\n",
      "Epoch [10/50], Train Loss: 0.1114, Val Loss: 0.3125\n",
      "Epoch [11/50], Train Loss: 0.1102, Val Loss: 0.3110\n",
      "Epoch [12/50], Train Loss: 0.1084, Val Loss: 0.3095\n",
      "Epoch [13/50], Train Loss: 0.1054, Val Loss: 0.3080\n",
      "Epoch [14/50], Train Loss: 0.1094, Val Loss: 0.3066\n",
      "Epoch [15/50], Train Loss: 0.1095, Val Loss: 0.3051\n",
      "Epoch [16/50], Train Loss: 0.1073, Val Loss: 0.3036\n",
      "Epoch [17/50], Train Loss: 0.1082, Val Loss: 0.3022\n",
      "Epoch [18/50], Train Loss: 0.1054, Val Loss: 0.3008\n",
      "Epoch [19/50], Train Loss: 0.1054, Val Loss: 0.2993\n",
      "Epoch [20/50], Train Loss: 0.1034, Val Loss: 0.2979\n",
      "Epoch [21/50], Train Loss: 0.1048, Val Loss: 0.2965\n",
      "Epoch [22/50], Train Loss: 0.1010, Val Loss: 0.2952\n",
      "Epoch [23/50], Train Loss: 0.1034, Val Loss: 0.2938\n",
      "Epoch [24/50], Train Loss: 0.1050, Val Loss: 0.2924\n",
      "Epoch [25/50], Train Loss: 0.1028, Val Loss: 0.2910\n",
      "Epoch [26/50], Train Loss: 0.1011, Val Loss: 0.2897\n",
      "Epoch [27/50], Train Loss: 0.1038, Val Loss: 0.2884\n",
      "Epoch [28/50], Train Loss: 0.1000, Val Loss: 0.2871\n",
      "Epoch [29/50], Train Loss: 0.1002, Val Loss: 0.2858\n",
      "Epoch [30/50], Train Loss: 0.1000, Val Loss: 0.2845\n",
      "Epoch [31/50], Train Loss: 0.1001, Val Loss: 0.2832\n",
      "Epoch [32/50], Train Loss: 0.0977, Val Loss: 0.2819\n",
      "Epoch [33/50], Train Loss: 0.1010, Val Loss: 0.2806\n",
      "Epoch [34/50], Train Loss: 0.0973, Val Loss: 0.2794\n",
      "Epoch [35/50], Train Loss: 0.0977, Val Loss: 0.2781\n",
      "Epoch [36/50], Train Loss: 0.0965, Val Loss: 0.2769\n",
      "Epoch [37/50], Train Loss: 0.0955, Val Loss: 0.2756\n",
      "Epoch [38/50], Train Loss: 0.0973, Val Loss: 0.2744\n",
      "Epoch [39/50], Train Loss: 0.0962, Val Loss: 0.2732\n",
      "Epoch [40/50], Train Loss: 0.0948, Val Loss: 0.2720\n",
      "Epoch [41/50], Train Loss: 0.0942, Val Loss: 0.2707\n",
      "Epoch [42/50], Train Loss: 0.0929, Val Loss: 0.2695\n",
      "Epoch [43/50], Train Loss: 0.0947, Val Loss: 0.2684\n",
      "Epoch [44/50], Train Loss: 0.0934, Val Loss: 0.2672\n",
      "Epoch [45/50], Train Loss: 0.0935, Val Loss: 0.2660\n",
      "Epoch [46/50], Train Loss: 0.0939, Val Loss: 0.2649\n",
      "Epoch [47/50], Train Loss: 0.0944, Val Loss: 0.2638\n",
      "Epoch [48/50], Train Loss: 0.0916, Val Loss: 0.2626\n",
      "Epoch [49/50], Train Loss: 0.0918, Val Loss: 0.2615\n",
      "Epoch [50/50], Train Loss: 0.0939, Val Loss: 0.2604\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=32, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1995, Val Loss: 0.4627\n",
      "Epoch [2/50], Train Loss: 0.1978, Val Loss: 0.4599\n",
      "Epoch [3/50], Train Loss: 0.1961, Val Loss: 0.4571\n",
      "Epoch [4/50], Train Loss: 0.1944, Val Loss: 0.4543\n",
      "Epoch [5/50], Train Loss: 0.1928, Val Loss: 0.4516\n",
      "Epoch [6/50], Train Loss: 0.1911, Val Loss: 0.4488\n",
      "Epoch [7/50], Train Loss: 0.1895, Val Loss: 0.4461\n",
      "Epoch [8/50], Train Loss: 0.1879, Val Loss: 0.4434\n",
      "Epoch [9/50], Train Loss: 0.1863, Val Loss: 0.4408\n",
      "Epoch [10/50], Train Loss: 0.1847, Val Loss: 0.4381\n",
      "Epoch [11/50], Train Loss: 0.1832, Val Loss: 0.4355\n",
      "Epoch [12/50], Train Loss: 0.1817, Val Loss: 0.4329\n",
      "Epoch [13/50], Train Loss: 0.1801, Val Loss: 0.4304\n",
      "Epoch [14/50], Train Loss: 0.1786, Val Loss: 0.4278\n",
      "Epoch [15/50], Train Loss: 0.1772, Val Loss: 0.4253\n",
      "Epoch [16/50], Train Loss: 0.1757, Val Loss: 0.4228\n",
      "Epoch [17/50], Train Loss: 0.1742, Val Loss: 0.4204\n",
      "Epoch [18/50], Train Loss: 0.1728, Val Loss: 0.4179\n",
      "Epoch [19/50], Train Loss: 0.1714, Val Loss: 0.4155\n",
      "Epoch [20/50], Train Loss: 0.1700, Val Loss: 0.4131\n",
      "Epoch [21/50], Train Loss: 0.1686, Val Loss: 0.4107\n",
      "Epoch [22/50], Train Loss: 0.1672, Val Loss: 0.4084\n",
      "Epoch [23/50], Train Loss: 0.1659, Val Loss: 0.4060\n",
      "Epoch [24/50], Train Loss: 0.1645, Val Loss: 0.4037\n",
      "Epoch [25/50], Train Loss: 0.1632, Val Loss: 0.4014\n",
      "Epoch [26/50], Train Loss: 0.1619, Val Loss: 0.3991\n",
      "Epoch [27/50], Train Loss: 0.1606, Val Loss: 0.3969\n",
      "Epoch [28/50], Train Loss: 0.1593, Val Loss: 0.3946\n",
      "Epoch [29/50], Train Loss: 0.1580, Val Loss: 0.3923\n",
      "Epoch [30/50], Train Loss: 0.1567, Val Loss: 0.3902\n",
      "Epoch [31/50], Train Loss: 0.1555, Val Loss: 0.3880\n",
      "Epoch [32/50], Train Loss: 0.1542, Val Loss: 0.3858\n",
      "Epoch [33/50], Train Loss: 0.1530, Val Loss: 0.3836\n",
      "Epoch [34/50], Train Loss: 0.1518, Val Loss: 0.3815\n",
      "Epoch [35/50], Train Loss: 0.1506, Val Loss: 0.3794\n",
      "Epoch [36/50], Train Loss: 0.1494, Val Loss: 0.3773\n",
      "Epoch [37/50], Train Loss: 0.1482, Val Loss: 0.3752\n",
      "Epoch [38/50], Train Loss: 0.1471, Val Loss: 0.3731\n",
      "Epoch [39/50], Train Loss: 0.1459, Val Loss: 0.3711\n",
      "Epoch [40/50], Train Loss: 0.1448, Val Loss: 0.3691\n",
      "Epoch [41/50], Train Loss: 0.1436, Val Loss: 0.3670\n",
      "Epoch [42/50], Train Loss: 0.1425, Val Loss: 0.3650\n",
      "Epoch [43/50], Train Loss: 0.1414, Val Loss: 0.3631\n",
      "Epoch [44/50], Train Loss: 0.1403, Val Loss: 0.3611\n",
      "Epoch [45/50], Train Loss: 0.1392, Val Loss: 0.3591\n",
      "Epoch [46/50], Train Loss: 0.1381, Val Loss: 0.3572\n",
      "Epoch [47/50], Train Loss: 0.1371, Val Loss: 0.3552\n",
      "Epoch [48/50], Train Loss: 0.1360, Val Loss: 0.3533\n",
      "Epoch [49/50], Train Loss: 0.1350, Val Loss: 0.3514\n",
      "Epoch [50/50], Train Loss: 0.1339, Val Loss: 0.3496\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=32, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1998, Val Loss: 0.4708\n",
      "Epoch [2/50], Train Loss: 0.1974, Val Loss: 0.4684\n",
      "Epoch [3/50], Train Loss: 0.1943, Val Loss: 0.4661\n",
      "Epoch [4/50], Train Loss: 0.1962, Val Loss: 0.4638\n",
      "Epoch [5/50], Train Loss: 0.1941, Val Loss: 0.4614\n",
      "Epoch [6/50], Train Loss: 0.1928, Val Loss: 0.4592\n",
      "Epoch [7/50], Train Loss: 0.1915, Val Loss: 0.4569\n",
      "Epoch [8/50], Train Loss: 0.1897, Val Loss: 0.4546\n",
      "Epoch [9/50], Train Loss: 0.1884, Val Loss: 0.4524\n",
      "Epoch [10/50], Train Loss: 0.1888, Val Loss: 0.4502\n",
      "Epoch [11/50], Train Loss: 0.1846, Val Loss: 0.4480\n",
      "Epoch [12/50], Train Loss: 0.1839, Val Loss: 0.4458\n",
      "Epoch [13/50], Train Loss: 0.1849, Val Loss: 0.4436\n",
      "Epoch [14/50], Train Loss: 0.1835, Val Loss: 0.4415\n",
      "Epoch [15/50], Train Loss: 0.1841, Val Loss: 0.4394\n",
      "Epoch [16/50], Train Loss: 0.1804, Val Loss: 0.4372\n",
      "Epoch [17/50], Train Loss: 0.1806, Val Loss: 0.4351\n",
      "Epoch [18/50], Train Loss: 0.1785, Val Loss: 0.4331\n",
      "Epoch [19/50], Train Loss: 0.1758, Val Loss: 0.4310\n",
      "Epoch [20/50], Train Loss: 0.1755, Val Loss: 0.4290\n",
      "Epoch [21/50], Train Loss: 0.1750, Val Loss: 0.4269\n",
      "Epoch [22/50], Train Loss: 0.1739, Val Loss: 0.4249\n",
      "Epoch [23/50], Train Loss: 0.1712, Val Loss: 0.4229\n",
      "Epoch [24/50], Train Loss: 0.1723, Val Loss: 0.4209\n",
      "Epoch [25/50], Train Loss: 0.1699, Val Loss: 0.4189\n",
      "Epoch [26/50], Train Loss: 0.1681, Val Loss: 0.4170\n",
      "Epoch [27/50], Train Loss: 0.1698, Val Loss: 0.4150\n",
      "Epoch [28/50], Train Loss: 0.1696, Val Loss: 0.4131\n",
      "Epoch [29/50], Train Loss: 0.1672, Val Loss: 0.4112\n",
      "Epoch [30/50], Train Loss: 0.1641, Val Loss: 0.4093\n",
      "Epoch [31/50], Train Loss: 0.1641, Val Loss: 0.4074\n",
      "Epoch [32/50], Train Loss: 0.1620, Val Loss: 0.4056\n",
      "Epoch [33/50], Train Loss: 0.1639, Val Loss: 0.4037\n",
      "Epoch [34/50], Train Loss: 0.1610, Val Loss: 0.4018\n",
      "Epoch [35/50], Train Loss: 0.1606, Val Loss: 0.4000\n",
      "Epoch [36/50], Train Loss: 0.1593, Val Loss: 0.3982\n",
      "Epoch [37/50], Train Loss: 0.1595, Val Loss: 0.3964\n",
      "Epoch [38/50], Train Loss: 0.1581, Val Loss: 0.3946\n",
      "Epoch [39/50], Train Loss: 0.1587, Val Loss: 0.3928\n",
      "Epoch [40/50], Train Loss: 0.1542, Val Loss: 0.3910\n",
      "Epoch [41/50], Train Loss: 0.1572, Val Loss: 0.3893\n",
      "Epoch [42/50], Train Loss: 0.1556, Val Loss: 0.3876\n",
      "Epoch [43/50], Train Loss: 0.1524, Val Loss: 0.3858\n",
      "Epoch [44/50], Train Loss: 0.1502, Val Loss: 0.3841\n",
      "Epoch [45/50], Train Loss: 0.1502, Val Loss: 0.3824\n",
      "Epoch [46/50], Train Loss: 0.1498, Val Loss: 0.3807\n",
      "Epoch [47/50], Train Loss: 0.1494, Val Loss: 0.3790\n",
      "Epoch [48/50], Train Loss: 0.1467, Val Loss: 0.3773\n",
      "Epoch [49/50], Train Loss: 0.1479, Val Loss: 0.3757\n",
      "Epoch [50/50], Train Loss: 0.1477, Val Loss: 0.3740\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=32, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1291, Val Loss: 0.3010\n",
      "Epoch [2/50], Train Loss: 0.1260, Val Loss: 0.2993\n",
      "Epoch [3/50], Train Loss: 0.1240, Val Loss: 0.2976\n",
      "Epoch [4/50], Train Loss: 0.1250, Val Loss: 0.2959\n",
      "Epoch [5/50], Train Loss: 0.1255, Val Loss: 0.2942\n",
      "Epoch [6/50], Train Loss: 0.1194, Val Loss: 0.2925\n",
      "Epoch [7/50], Train Loss: 0.1197, Val Loss: 0.2908\n",
      "Epoch [8/50], Train Loss: 0.1236, Val Loss: 0.2891\n",
      "Epoch [9/50], Train Loss: 0.1178, Val Loss: 0.2875\n",
      "Epoch [10/50], Train Loss: 0.1164, Val Loss: 0.2859\n",
      "Epoch [11/50], Train Loss: 0.1176, Val Loss: 0.2843\n",
      "Epoch [12/50], Train Loss: 0.1199, Val Loss: 0.2827\n",
      "Epoch [13/50], Train Loss: 0.1151, Val Loss: 0.2811\n",
      "Epoch [14/50], Train Loss: 0.1156, Val Loss: 0.2796\n",
      "Epoch [15/50], Train Loss: 0.1160, Val Loss: 0.2780\n",
      "Epoch [16/50], Train Loss: 0.1123, Val Loss: 0.2765\n",
      "Epoch [17/50], Train Loss: 0.1122, Val Loss: 0.2749\n",
      "Epoch [18/50], Train Loss: 0.1106, Val Loss: 0.2734\n",
      "Epoch [19/50], Train Loss: 0.1106, Val Loss: 0.2719\n",
      "Epoch [20/50], Train Loss: 0.1134, Val Loss: 0.2704\n",
      "Epoch [21/50], Train Loss: 0.1119, Val Loss: 0.2690\n",
      "Epoch [22/50], Train Loss: 0.1098, Val Loss: 0.2675\n",
      "Epoch [23/50], Train Loss: 0.1115, Val Loss: 0.2660\n",
      "Epoch [24/50], Train Loss: 0.1131, Val Loss: 0.2645\n",
      "Epoch [25/50], Train Loss: 0.1088, Val Loss: 0.2631\n",
      "Epoch [26/50], Train Loss: 0.1066, Val Loss: 0.2617\n",
      "Epoch [27/50], Train Loss: 0.1083, Val Loss: 0.2603\n",
      "Epoch [28/50], Train Loss: 0.1062, Val Loss: 0.2589\n",
      "Epoch [29/50], Train Loss: 0.1057, Val Loss: 0.2575\n",
      "Epoch [30/50], Train Loss: 0.1062, Val Loss: 0.2562\n",
      "Epoch [31/50], Train Loss: 0.1084, Val Loss: 0.2549\n",
      "Epoch [32/50], Train Loss: 0.1041, Val Loss: 0.2535\n",
      "Epoch [33/50], Train Loss: 0.1043, Val Loss: 0.2522\n",
      "Epoch [34/50], Train Loss: 0.1029, Val Loss: 0.2509\n",
      "Epoch [35/50], Train Loss: 0.1048, Val Loss: 0.2495\n",
      "Epoch [36/50], Train Loss: 0.1036, Val Loss: 0.2482\n",
      "Epoch [37/50], Train Loss: 0.1065, Val Loss: 0.2469\n",
      "Epoch [38/50], Train Loss: 0.0997, Val Loss: 0.2457\n",
      "Epoch [39/50], Train Loss: 0.1026, Val Loss: 0.2444\n",
      "Epoch [40/50], Train Loss: 0.0992, Val Loss: 0.2432\n",
      "Epoch [41/50], Train Loss: 0.0988, Val Loss: 0.2420\n",
      "Epoch [42/50], Train Loss: 0.0998, Val Loss: 0.2407\n",
      "Epoch [43/50], Train Loss: 0.1014, Val Loss: 0.2395\n",
      "Epoch [44/50], Train Loss: 0.1028, Val Loss: 0.2383\n",
      "Epoch [45/50], Train Loss: 0.0992, Val Loss: 0.2371\n",
      "Epoch [46/50], Train Loss: 0.0990, Val Loss: 0.2359\n",
      "Epoch [47/50], Train Loss: 0.0982, Val Loss: 0.2347\n",
      "Epoch [48/50], Train Loss: 0.0977, Val Loss: 0.2336\n",
      "Epoch [49/50], Train Loss: 0.0959, Val Loss: 0.2324\n",
      "Epoch [50/50], Train Loss: 0.0954, Val Loss: 0.2313\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=32, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1448, Val Loss: 0.3420\n",
      "Epoch [2/50], Train Loss: 0.1429, Val Loss: 0.3388\n",
      "Epoch [3/50], Train Loss: 0.1411, Val Loss: 0.3357\n",
      "Epoch [4/50], Train Loss: 0.1393, Val Loss: 0.3327\n",
      "Epoch [5/50], Train Loss: 0.1376, Val Loss: 0.3297\n",
      "Epoch [6/50], Train Loss: 0.1358, Val Loss: 0.3267\n",
      "Epoch [7/50], Train Loss: 0.1341, Val Loss: 0.3238\n",
      "Epoch [8/50], Train Loss: 0.1325, Val Loss: 0.3209\n",
      "Epoch [9/50], Train Loss: 0.1309, Val Loss: 0.3180\n",
      "Epoch [10/50], Train Loss: 0.1293, Val Loss: 0.3153\n",
      "Epoch [11/50], Train Loss: 0.1277, Val Loss: 0.3125\n",
      "Epoch [12/50], Train Loss: 0.1262, Val Loss: 0.3098\n",
      "Epoch [13/50], Train Loss: 0.1247, Val Loss: 0.3072\n",
      "Epoch [14/50], Train Loss: 0.1232, Val Loss: 0.3045\n",
      "Epoch [15/50], Train Loss: 0.1217, Val Loss: 0.3019\n",
      "Epoch [16/50], Train Loss: 0.1203, Val Loss: 0.2994\n",
      "Epoch [17/50], Train Loss: 0.1189, Val Loss: 0.2969\n",
      "Epoch [18/50], Train Loss: 0.1175, Val Loss: 0.2944\n",
      "Epoch [19/50], Train Loss: 0.1162, Val Loss: 0.2919\n",
      "Epoch [20/50], Train Loss: 0.1148, Val Loss: 0.2895\n",
      "Epoch [21/50], Train Loss: 0.1135, Val Loss: 0.2871\n",
      "Epoch [22/50], Train Loss: 0.1123, Val Loss: 0.2848\n",
      "Epoch [23/50], Train Loss: 0.1110, Val Loss: 0.2825\n",
      "Epoch [24/50], Train Loss: 0.1098, Val Loss: 0.2802\n",
      "Epoch [25/50], Train Loss: 0.1085, Val Loss: 0.2779\n",
      "Epoch [26/50], Train Loss: 0.1073, Val Loss: 0.2757\n",
      "Epoch [27/50], Train Loss: 0.1062, Val Loss: 0.2735\n",
      "Epoch [28/50], Train Loss: 0.1050, Val Loss: 0.2714\n",
      "Epoch [29/50], Train Loss: 0.1039, Val Loss: 0.2693\n",
      "Epoch [30/50], Train Loss: 0.1027, Val Loss: 0.2671\n",
      "Epoch [31/50], Train Loss: 0.1016, Val Loss: 0.2651\n",
      "Epoch [32/50], Train Loss: 0.1006, Val Loss: 0.2630\n",
      "Epoch [33/50], Train Loss: 0.0995, Val Loss: 0.2610\n",
      "Epoch [34/50], Train Loss: 0.0985, Val Loss: 0.2590\n",
      "Epoch [35/50], Train Loss: 0.0974, Val Loss: 0.2570\n",
      "Epoch [36/50], Train Loss: 0.0964, Val Loss: 0.2551\n",
      "Epoch [37/50], Train Loss: 0.0954, Val Loss: 0.2532\n",
      "Epoch [38/50], Train Loss: 0.0944, Val Loss: 0.2513\n",
      "Epoch [39/50], Train Loss: 0.0935, Val Loss: 0.2495\n",
      "Epoch [40/50], Train Loss: 0.0925, Val Loss: 0.2476\n",
      "Epoch [41/50], Train Loss: 0.0916, Val Loss: 0.2458\n",
      "Epoch [42/50], Train Loss: 0.0907, Val Loss: 0.2440\n",
      "Epoch [43/50], Train Loss: 0.0898, Val Loss: 0.2423\n",
      "Epoch [44/50], Train Loss: 0.0889, Val Loss: 0.2406\n",
      "Epoch [45/50], Train Loss: 0.0880, Val Loss: 0.2388\n",
      "Epoch [46/50], Train Loss: 0.0872, Val Loss: 0.2371\n",
      "Epoch [47/50], Train Loss: 0.0863, Val Loss: 0.2354\n",
      "Epoch [48/50], Train Loss: 0.0855, Val Loss: 0.2338\n",
      "Epoch [49/50], Train Loss: 0.0847, Val Loss: 0.2321\n",
      "Epoch [50/50], Train Loss: 0.0839, Val Loss: 0.2305\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=32, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1347, Val Loss: 0.3136\n",
      "Epoch [2/50], Train Loss: 0.1340, Val Loss: 0.3121\n",
      "Epoch [3/50], Train Loss: 0.1334, Val Loss: 0.3107\n",
      "Epoch [4/50], Train Loss: 0.1329, Val Loss: 0.3092\n",
      "Epoch [5/50], Train Loss: 0.1325, Val Loss: 0.3078\n",
      "Epoch [6/50], Train Loss: 0.1295, Val Loss: 0.3064\n",
      "Epoch [7/50], Train Loss: 0.1304, Val Loss: 0.3050\n",
      "Epoch [8/50], Train Loss: 0.1281, Val Loss: 0.3036\n",
      "Epoch [9/50], Train Loss: 0.1287, Val Loss: 0.3023\n",
      "Epoch [10/50], Train Loss: 0.1274, Val Loss: 0.3009\n",
      "Epoch [11/50], Train Loss: 0.1270, Val Loss: 0.2996\n",
      "Epoch [12/50], Train Loss: 0.1254, Val Loss: 0.2983\n",
      "Epoch [13/50], Train Loss: 0.1225, Val Loss: 0.2970\n",
      "Epoch [14/50], Train Loss: 0.1225, Val Loss: 0.2957\n",
      "Epoch [15/50], Train Loss: 0.1229, Val Loss: 0.2944\n",
      "Epoch [16/50], Train Loss: 0.1215, Val Loss: 0.2931\n",
      "Epoch [17/50], Train Loss: 0.1227, Val Loss: 0.2919\n",
      "Epoch [18/50], Train Loss: 0.1211, Val Loss: 0.2906\n",
      "Epoch [19/50], Train Loss: 0.1196, Val Loss: 0.2893\n",
      "Epoch [20/50], Train Loss: 0.1203, Val Loss: 0.2881\n",
      "Epoch [21/50], Train Loss: 0.1177, Val Loss: 0.2869\n",
      "Epoch [22/50], Train Loss: 0.1175, Val Loss: 0.2857\n",
      "Epoch [23/50], Train Loss: 0.1166, Val Loss: 0.2844\n",
      "Epoch [24/50], Train Loss: 0.1175, Val Loss: 0.2832\n",
      "Epoch [25/50], Train Loss: 0.1145, Val Loss: 0.2820\n",
      "Epoch [26/50], Train Loss: 0.1148, Val Loss: 0.2809\n",
      "Epoch [27/50], Train Loss: 0.1141, Val Loss: 0.2797\n",
      "Epoch [28/50], Train Loss: 0.1142, Val Loss: 0.2785\n",
      "Epoch [29/50], Train Loss: 0.1129, Val Loss: 0.2774\n",
      "Epoch [30/50], Train Loss: 0.1136, Val Loss: 0.2762\n",
      "Epoch [31/50], Train Loss: 0.1121, Val Loss: 0.2751\n",
      "Epoch [32/50], Train Loss: 0.1100, Val Loss: 0.2740\n",
      "Epoch [33/50], Train Loss: 0.1100, Val Loss: 0.2728\n",
      "Epoch [34/50], Train Loss: 0.1086, Val Loss: 0.2717\n",
      "Epoch [35/50], Train Loss: 0.1085, Val Loss: 0.2706\n",
      "Epoch [36/50], Train Loss: 0.1073, Val Loss: 0.2696\n",
      "Epoch [37/50], Train Loss: 0.1077, Val Loss: 0.2685\n",
      "Epoch [38/50], Train Loss: 0.1057, Val Loss: 0.2674\n",
      "Epoch [39/50], Train Loss: 0.1065, Val Loss: 0.2663\n",
      "Epoch [40/50], Train Loss: 0.1049, Val Loss: 0.2653\n",
      "Epoch [41/50], Train Loss: 0.1053, Val Loss: 0.2642\n",
      "Epoch [42/50], Train Loss: 0.1045, Val Loss: 0.2632\n",
      "Epoch [43/50], Train Loss: 0.1048, Val Loss: 0.2622\n",
      "Epoch [44/50], Train Loss: 0.1045, Val Loss: 0.2611\n",
      "Epoch [45/50], Train Loss: 0.1027, Val Loss: 0.2601\n",
      "Epoch [46/50], Train Loss: 0.1028, Val Loss: 0.2591\n",
      "Epoch [47/50], Train Loss: 0.1021, Val Loss: 0.2581\n",
      "Epoch [48/50], Train Loss: 0.1011, Val Loss: 0.2571\n",
      "Epoch [49/50], Train Loss: 0.1005, Val Loss: 0.2561\n",
      "Epoch [50/50], Train Loss: 0.1002, Val Loss: 0.2551\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=32, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.2093, Val Loss: 0.4824\n",
      "Epoch [2/50], Train Loss: 0.2081, Val Loss: 0.4790\n",
      "Epoch [3/50], Train Loss: 0.2155, Val Loss: 0.4756\n",
      "Epoch [4/50], Train Loss: 0.2070, Val Loss: 0.4724\n",
      "Epoch [5/50], Train Loss: 0.2095, Val Loss: 0.4691\n",
      "Epoch [6/50], Train Loss: 0.2037, Val Loss: 0.4659\n",
      "Epoch [7/50], Train Loss: 0.2051, Val Loss: 0.4628\n",
      "Epoch [8/50], Train Loss: 0.2004, Val Loss: 0.4598\n",
      "Epoch [9/50], Train Loss: 0.2004, Val Loss: 0.4568\n",
      "Epoch [10/50], Train Loss: 0.1990, Val Loss: 0.4538\n",
      "Epoch [11/50], Train Loss: 0.1954, Val Loss: 0.4509\n",
      "Epoch [12/50], Train Loss: 0.1987, Val Loss: 0.4480\n",
      "Epoch [13/50], Train Loss: 0.1996, Val Loss: 0.4450\n",
      "Epoch [14/50], Train Loss: 0.1927, Val Loss: 0.4423\n",
      "Epoch [15/50], Train Loss: 0.1908, Val Loss: 0.4395\n",
      "Epoch [16/50], Train Loss: 0.1950, Val Loss: 0.4367\n",
      "Epoch [17/50], Train Loss: 0.1864, Val Loss: 0.4340\n",
      "Epoch [18/50], Train Loss: 0.1901, Val Loss: 0.4313\n",
      "Epoch [19/50], Train Loss: 0.1816, Val Loss: 0.4287\n",
      "Epoch [20/50], Train Loss: 0.1848, Val Loss: 0.4261\n",
      "Epoch [21/50], Train Loss: 0.1824, Val Loss: 0.4236\n",
      "Epoch [22/50], Train Loss: 0.1856, Val Loss: 0.4210\n",
      "Epoch [23/50], Train Loss: 0.1826, Val Loss: 0.4185\n",
      "Epoch [24/50], Train Loss: 0.1835, Val Loss: 0.4160\n",
      "Epoch [25/50], Train Loss: 0.1775, Val Loss: 0.4137\n",
      "Epoch [26/50], Train Loss: 0.1809, Val Loss: 0.4112\n",
      "Epoch [27/50], Train Loss: 0.1765, Val Loss: 0.4088\n",
      "Epoch [28/50], Train Loss: 0.1748, Val Loss: 0.4065\n",
      "Epoch [29/50], Train Loss: 0.1767, Val Loss: 0.4042\n",
      "Epoch [30/50], Train Loss: 0.1793, Val Loss: 0.4019\n",
      "Epoch [31/50], Train Loss: 0.1717, Val Loss: 0.3997\n",
      "Epoch [32/50], Train Loss: 0.1720, Val Loss: 0.3974\n",
      "Epoch [33/50], Train Loss: 0.1712, Val Loss: 0.3952\n",
      "Epoch [34/50], Train Loss: 0.1693, Val Loss: 0.3931\n",
      "Epoch [35/50], Train Loss: 0.1659, Val Loss: 0.3909\n",
      "Epoch [36/50], Train Loss: 0.1651, Val Loss: 0.3888\n",
      "Epoch [37/50], Train Loss: 0.1651, Val Loss: 0.3867\n",
      "Epoch [38/50], Train Loss: 0.1627, Val Loss: 0.3847\n",
      "Epoch [39/50], Train Loss: 0.1684, Val Loss: 0.3826\n",
      "Epoch [40/50], Train Loss: 0.1634, Val Loss: 0.3807\n",
      "Epoch [41/50], Train Loss: 0.1627, Val Loss: 0.3787\n",
      "Epoch [42/50], Train Loss: 0.1602, Val Loss: 0.3767\n",
      "Epoch [43/50], Train Loss: 0.1620, Val Loss: 0.3747\n",
      "Epoch [44/50], Train Loss: 0.1589, Val Loss: 0.3728\n",
      "Epoch [45/50], Train Loss: 0.1556, Val Loss: 0.3710\n",
      "Epoch [46/50], Train Loss: 0.1583, Val Loss: 0.3691\n",
      "Epoch [47/50], Train Loss: 0.1565, Val Loss: 0.3672\n",
      "Epoch [48/50], Train Loss: 0.1566, Val Loss: 0.3653\n",
      "Epoch [49/50], Train Loss: 0.1546, Val Loss: 0.3634\n",
      "Epoch [50/50], Train Loss: 0.1578, Val Loss: 0.3616\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=64, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1690, Val Loss: 0.4430\n",
      "Epoch [2/50], Train Loss: 0.1678, Val Loss: 0.4409\n",
      "Epoch [3/50], Train Loss: 0.1666, Val Loss: 0.4387\n",
      "Epoch [4/50], Train Loss: 0.1654, Val Loss: 0.4366\n",
      "Epoch [5/50], Train Loss: 0.1643, Val Loss: 0.4345\n",
      "Epoch [6/50], Train Loss: 0.1631, Val Loss: 0.4324\n",
      "Epoch [7/50], Train Loss: 0.1620, Val Loss: 0.4303\n",
      "Epoch [8/50], Train Loss: 0.1609, Val Loss: 0.4283\n",
      "Epoch [9/50], Train Loss: 0.1598, Val Loss: 0.4262\n",
      "Epoch [10/50], Train Loss: 0.1586, Val Loss: 0.4242\n",
      "Epoch [11/50], Train Loss: 0.1576, Val Loss: 0.4222\n",
      "Epoch [12/50], Train Loss: 0.1565, Val Loss: 0.4202\n",
      "Epoch [13/50], Train Loss: 0.1554, Val Loss: 0.4182\n",
      "Epoch [14/50], Train Loss: 0.1543, Val Loss: 0.4162\n",
      "Epoch [15/50], Train Loss: 0.1533, Val Loss: 0.4143\n",
      "Epoch [16/50], Train Loss: 0.1522, Val Loss: 0.4123\n",
      "Epoch [17/50], Train Loss: 0.1512, Val Loss: 0.4104\n",
      "Epoch [18/50], Train Loss: 0.1502, Val Loss: 0.4085\n",
      "Epoch [19/50], Train Loss: 0.1491, Val Loss: 0.4066\n",
      "Epoch [20/50], Train Loss: 0.1481, Val Loss: 0.4047\n",
      "Epoch [21/50], Train Loss: 0.1471, Val Loss: 0.4028\n",
      "Epoch [22/50], Train Loss: 0.1461, Val Loss: 0.4010\n",
      "Epoch [23/50], Train Loss: 0.1452, Val Loss: 0.3991\n",
      "Epoch [24/50], Train Loss: 0.1442, Val Loss: 0.3973\n",
      "Epoch [25/50], Train Loss: 0.1432, Val Loss: 0.3955\n",
      "Epoch [26/50], Train Loss: 0.1423, Val Loss: 0.3937\n",
      "Epoch [27/50], Train Loss: 0.1413, Val Loss: 0.3919\n",
      "Epoch [28/50], Train Loss: 0.1404, Val Loss: 0.3901\n",
      "Epoch [29/50], Train Loss: 0.1394, Val Loss: 0.3883\n",
      "Epoch [30/50], Train Loss: 0.1385, Val Loss: 0.3866\n",
      "Epoch [31/50], Train Loss: 0.1376, Val Loss: 0.3848\n",
      "Epoch [32/50], Train Loss: 0.1367, Val Loss: 0.3831\n",
      "Epoch [33/50], Train Loss: 0.1358, Val Loss: 0.3813\n",
      "Epoch [34/50], Train Loss: 0.1349, Val Loss: 0.3796\n",
      "Epoch [35/50], Train Loss: 0.1340, Val Loss: 0.3779\n",
      "Epoch [36/50], Train Loss: 0.1331, Val Loss: 0.3762\n",
      "Epoch [37/50], Train Loss: 0.1322, Val Loss: 0.3745\n",
      "Epoch [38/50], Train Loss: 0.1314, Val Loss: 0.3729\n",
      "Epoch [39/50], Train Loss: 0.1305, Val Loss: 0.3712\n",
      "Epoch [40/50], Train Loss: 0.1297, Val Loss: 0.3695\n",
      "Epoch [41/50], Train Loss: 0.1288, Val Loss: 0.3679\n",
      "Epoch [42/50], Train Loss: 0.1280, Val Loss: 0.3663\n",
      "Epoch [43/50], Train Loss: 0.1271, Val Loss: 0.3647\n",
      "Epoch [44/50], Train Loss: 0.1263, Val Loss: 0.3630\n",
      "Epoch [45/50], Train Loss: 0.1255, Val Loss: 0.3614\n",
      "Epoch [46/50], Train Loss: 0.1247, Val Loss: 0.3599\n",
      "Epoch [47/50], Train Loss: 0.1239, Val Loss: 0.3583\n",
      "Epoch [48/50], Train Loss: 0.1231, Val Loss: 0.3567\n",
      "Epoch [49/50], Train Loss: 0.1223, Val Loss: 0.3552\n",
      "Epoch [50/50], Train Loss: 0.1215, Val Loss: 0.3536\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=64, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1322, Val Loss: 0.3494\n",
      "Epoch [2/50], Train Loss: 0.1315, Val Loss: 0.3474\n",
      "Epoch [3/50], Train Loss: 0.1313, Val Loss: 0.3454\n",
      "Epoch [4/50], Train Loss: 0.1292, Val Loss: 0.3435\n",
      "Epoch [5/50], Train Loss: 0.1279, Val Loss: 0.3415\n",
      "Epoch [6/50], Train Loss: 0.1259, Val Loss: 0.3396\n",
      "Epoch [7/50], Train Loss: 0.1257, Val Loss: 0.3376\n",
      "Epoch [8/50], Train Loss: 0.1256, Val Loss: 0.3357\n",
      "Epoch [9/50], Train Loss: 0.1256, Val Loss: 0.3338\n",
      "Epoch [10/50], Train Loss: 0.1233, Val Loss: 0.3319\n",
      "Epoch [11/50], Train Loss: 0.1233, Val Loss: 0.3300\n",
      "Epoch [12/50], Train Loss: 0.1204, Val Loss: 0.3281\n",
      "Epoch [13/50], Train Loss: 0.1204, Val Loss: 0.3263\n",
      "Epoch [14/50], Train Loss: 0.1194, Val Loss: 0.3244\n",
      "Epoch [15/50], Train Loss: 0.1188, Val Loss: 0.3226\n",
      "Epoch [16/50], Train Loss: 0.1177, Val Loss: 0.3208\n",
      "Epoch [17/50], Train Loss: 0.1169, Val Loss: 0.3190\n",
      "Epoch [18/50], Train Loss: 0.1158, Val Loss: 0.3172\n",
      "Epoch [19/50], Train Loss: 0.1156, Val Loss: 0.3154\n",
      "Epoch [20/50], Train Loss: 0.1152, Val Loss: 0.3136\n",
      "Epoch [21/50], Train Loss: 0.1126, Val Loss: 0.3119\n",
      "Epoch [22/50], Train Loss: 0.1137, Val Loss: 0.3101\n",
      "Epoch [23/50], Train Loss: 0.1122, Val Loss: 0.3084\n",
      "Epoch [24/50], Train Loss: 0.1104, Val Loss: 0.3067\n",
      "Epoch [25/50], Train Loss: 0.1090, Val Loss: 0.3050\n",
      "Epoch [26/50], Train Loss: 0.1092, Val Loss: 0.3033\n",
      "Epoch [27/50], Train Loss: 0.1083, Val Loss: 0.3016\n",
      "Epoch [28/50], Train Loss: 0.1078, Val Loss: 0.2999\n",
      "Epoch [29/50], Train Loss: 0.1071, Val Loss: 0.2983\n",
      "Epoch [30/50], Train Loss: 0.1059, Val Loss: 0.2967\n",
      "Epoch [31/50], Train Loss: 0.1050, Val Loss: 0.2950\n",
      "Epoch [32/50], Train Loss: 0.1048, Val Loss: 0.2934\n",
      "Epoch [33/50], Train Loss: 0.1034, Val Loss: 0.2917\n",
      "Epoch [34/50], Train Loss: 0.1033, Val Loss: 0.2901\n",
      "Epoch [35/50], Train Loss: 0.1014, Val Loss: 0.2885\n",
      "Epoch [36/50], Train Loss: 0.1015, Val Loss: 0.2870\n",
      "Epoch [37/50], Train Loss: 0.1008, Val Loss: 0.2854\n",
      "Epoch [38/50], Train Loss: 0.0996, Val Loss: 0.2838\n",
      "Epoch [39/50], Train Loss: 0.1001, Val Loss: 0.2823\n",
      "Epoch [40/50], Train Loss: 0.0978, Val Loss: 0.2807\n",
      "Epoch [41/50], Train Loss: 0.0988, Val Loss: 0.2792\n",
      "Epoch [42/50], Train Loss: 0.0969, Val Loss: 0.2777\n",
      "Epoch [43/50], Train Loss: 0.0961, Val Loss: 0.2762\n",
      "Epoch [44/50], Train Loss: 0.0946, Val Loss: 0.2747\n",
      "Epoch [45/50], Train Loss: 0.0942, Val Loss: 0.2732\n",
      "Epoch [46/50], Train Loss: 0.0939, Val Loss: 0.2717\n",
      "Epoch [47/50], Train Loss: 0.0936, Val Loss: 0.2703\n",
      "Epoch [48/50], Train Loss: 0.0923, Val Loss: 0.2688\n",
      "Epoch [49/50], Train Loss: 0.0921, Val Loss: 0.2674\n",
      "Epoch [50/50], Train Loss: 0.0926, Val Loss: 0.2659\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=64, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1515, Val Loss: 0.3689\n",
      "Epoch [2/50], Train Loss: 0.1503, Val Loss: 0.3671\n",
      "Epoch [3/50], Train Loss: 0.1501, Val Loss: 0.3653\n",
      "Epoch [4/50], Train Loss: 0.1503, Val Loss: 0.3636\n",
      "Epoch [5/50], Train Loss: 0.1487, Val Loss: 0.3618\n",
      "Epoch [6/50], Train Loss: 0.1494, Val Loss: 0.3601\n",
      "Epoch [7/50], Train Loss: 0.1468, Val Loss: 0.3583\n",
      "Epoch [8/50], Train Loss: 0.1466, Val Loss: 0.3566\n",
      "Epoch [9/50], Train Loss: 0.1438, Val Loss: 0.3549\n",
      "Epoch [10/50], Train Loss: 0.1433, Val Loss: 0.3532\n",
      "Epoch [11/50], Train Loss: 0.1433, Val Loss: 0.3515\n",
      "Epoch [12/50], Train Loss: 0.1417, Val Loss: 0.3499\n",
      "Epoch [13/50], Train Loss: 0.1422, Val Loss: 0.3482\n",
      "Epoch [14/50], Train Loss: 0.1413, Val Loss: 0.3465\n",
      "Epoch [15/50], Train Loss: 0.1397, Val Loss: 0.3449\n",
      "Epoch [16/50], Train Loss: 0.1394, Val Loss: 0.3433\n",
      "Epoch [17/50], Train Loss: 0.1386, Val Loss: 0.3417\n",
      "Epoch [18/50], Train Loss: 0.1361, Val Loss: 0.3401\n",
      "Epoch [19/50], Train Loss: 0.1357, Val Loss: 0.3384\n",
      "Epoch [20/50], Train Loss: 0.1358, Val Loss: 0.3369\n",
      "Epoch [21/50], Train Loss: 0.1336, Val Loss: 0.3353\n",
      "Epoch [22/50], Train Loss: 0.1344, Val Loss: 0.3337\n",
      "Epoch [23/50], Train Loss: 0.1336, Val Loss: 0.3322\n",
      "Epoch [24/50], Train Loss: 0.1305, Val Loss: 0.3306\n",
      "Epoch [25/50], Train Loss: 0.1328, Val Loss: 0.3291\n",
      "Epoch [26/50], Train Loss: 0.1311, Val Loss: 0.3275\n",
      "Epoch [27/50], Train Loss: 0.1309, Val Loss: 0.3260\n",
      "Epoch [28/50], Train Loss: 0.1287, Val Loss: 0.3245\n",
      "Epoch [29/50], Train Loss: 0.1274, Val Loss: 0.3230\n",
      "Epoch [30/50], Train Loss: 0.1283, Val Loss: 0.3216\n",
      "Epoch [31/50], Train Loss: 0.1273, Val Loss: 0.3201\n",
      "Epoch [32/50], Train Loss: 0.1261, Val Loss: 0.3187\n",
      "Epoch [33/50], Train Loss: 0.1259, Val Loss: 0.3172\n",
      "Epoch [34/50], Train Loss: 0.1250, Val Loss: 0.3158\n",
      "Epoch [35/50], Train Loss: 0.1238, Val Loss: 0.3143\n",
      "Epoch [36/50], Train Loss: 0.1231, Val Loss: 0.3129\n",
      "Epoch [37/50], Train Loss: 0.1221, Val Loss: 0.3115\n",
      "Epoch [38/50], Train Loss: 0.1214, Val Loss: 0.3101\n",
      "Epoch [39/50], Train Loss: 0.1208, Val Loss: 0.3087\n",
      "Epoch [40/50], Train Loss: 0.1206, Val Loss: 0.3073\n",
      "Epoch [41/50], Train Loss: 0.1181, Val Loss: 0.3060\n",
      "Epoch [42/50], Train Loss: 0.1204, Val Loss: 0.3046\n",
      "Epoch [43/50], Train Loss: 0.1176, Val Loss: 0.3032\n",
      "Epoch [44/50], Train Loss: 0.1175, Val Loss: 0.3019\n",
      "Epoch [45/50], Train Loss: 0.1171, Val Loss: 0.3005\n",
      "Epoch [46/50], Train Loss: 0.1155, Val Loss: 0.2992\n",
      "Epoch [47/50], Train Loss: 0.1141, Val Loss: 0.2979\n",
      "Epoch [48/50], Train Loss: 0.1169, Val Loss: 0.2966\n",
      "Epoch [49/50], Train Loss: 0.1147, Val Loss: 0.2952\n",
      "Epoch [50/50], Train Loss: 0.1144, Val Loss: 0.2939\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=64, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1853, Val Loss: 0.4511\n",
      "Epoch [2/50], Train Loss: 0.1835, Val Loss: 0.4483\n",
      "Epoch [3/50], Train Loss: 0.1818, Val Loss: 0.4455\n",
      "Epoch [4/50], Train Loss: 0.1801, Val Loss: 0.4427\n",
      "Epoch [5/50], Train Loss: 0.1785, Val Loss: 0.4400\n",
      "Epoch [6/50], Train Loss: 0.1768, Val Loss: 0.4373\n",
      "Epoch [7/50], Train Loss: 0.1752, Val Loss: 0.4346\n",
      "Epoch [8/50], Train Loss: 0.1736, Val Loss: 0.4319\n",
      "Epoch [9/50], Train Loss: 0.1721, Val Loss: 0.4293\n",
      "Epoch [10/50], Train Loss: 0.1705, Val Loss: 0.4267\n",
      "Epoch [11/50], Train Loss: 0.1690, Val Loss: 0.4242\n",
      "Epoch [12/50], Train Loss: 0.1674, Val Loss: 0.4216\n",
      "Epoch [13/50], Train Loss: 0.1659, Val Loss: 0.4191\n",
      "Epoch [14/50], Train Loss: 0.1645, Val Loss: 0.4166\n",
      "Epoch [15/50], Train Loss: 0.1630, Val Loss: 0.4141\n",
      "Epoch [16/50], Train Loss: 0.1615, Val Loss: 0.4117\n",
      "Epoch [17/50], Train Loss: 0.1601, Val Loss: 0.4093\n",
      "Epoch [18/50], Train Loss: 0.1587, Val Loss: 0.4069\n",
      "Epoch [19/50], Train Loss: 0.1573, Val Loss: 0.4045\n",
      "Epoch [20/50], Train Loss: 0.1559, Val Loss: 0.4021\n",
      "Epoch [21/50], Train Loss: 0.1546, Val Loss: 0.3998\n",
      "Epoch [22/50], Train Loss: 0.1532, Val Loss: 0.3975\n",
      "Epoch [23/50], Train Loss: 0.1519, Val Loss: 0.3952\n",
      "Epoch [24/50], Train Loss: 0.1506, Val Loss: 0.3929\n",
      "Epoch [25/50], Train Loss: 0.1493, Val Loss: 0.3906\n",
      "Epoch [26/50], Train Loss: 0.1480, Val Loss: 0.3884\n",
      "Epoch [27/50], Train Loss: 0.1467, Val Loss: 0.3862\n",
      "Epoch [28/50], Train Loss: 0.1454, Val Loss: 0.3840\n",
      "Epoch [29/50], Train Loss: 0.1442, Val Loss: 0.3818\n",
      "Epoch [30/50], Train Loss: 0.1430, Val Loss: 0.3796\n",
      "Epoch [31/50], Train Loss: 0.1418, Val Loss: 0.3775\n",
      "Epoch [32/50], Train Loss: 0.1406, Val Loss: 0.3754\n",
      "Epoch [33/50], Train Loss: 0.1394, Val Loss: 0.3733\n",
      "Epoch [34/50], Train Loss: 0.1382, Val Loss: 0.3712\n",
      "Epoch [35/50], Train Loss: 0.1370, Val Loss: 0.3691\n",
      "Epoch [36/50], Train Loss: 0.1359, Val Loss: 0.3671\n",
      "Epoch [37/50], Train Loss: 0.1347, Val Loss: 0.3650\n",
      "Epoch [38/50], Train Loss: 0.1336, Val Loss: 0.3630\n",
      "Epoch [39/50], Train Loss: 0.1325, Val Loss: 0.3610\n",
      "Epoch [40/50], Train Loss: 0.1314, Val Loss: 0.3590\n",
      "Epoch [41/50], Train Loss: 0.1303, Val Loss: 0.3570\n",
      "Epoch [42/50], Train Loss: 0.1292, Val Loss: 0.3551\n",
      "Epoch [43/50], Train Loss: 0.1282, Val Loss: 0.3531\n",
      "Epoch [44/50], Train Loss: 0.1271, Val Loss: 0.3512\n",
      "Epoch [45/50], Train Loss: 0.1260, Val Loss: 0.3493\n",
      "Epoch [46/50], Train Loss: 0.1250, Val Loss: 0.3474\n",
      "Epoch [47/50], Train Loss: 0.1240, Val Loss: 0.3455\n",
      "Epoch [48/50], Train Loss: 0.1230, Val Loss: 0.3436\n",
      "Epoch [49/50], Train Loss: 0.1220, Val Loss: 0.3418\n",
      "Epoch [50/50], Train Loss: 0.1210, Val Loss: 0.3399\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=64, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1651, Val Loss: 0.4228\n",
      "Epoch [2/50], Train Loss: 0.1642, Val Loss: 0.4207\n",
      "Epoch [3/50], Train Loss: 0.1636, Val Loss: 0.4185\n",
      "Epoch [4/50], Train Loss: 0.1628, Val Loss: 0.4164\n",
      "Epoch [5/50], Train Loss: 0.1616, Val Loss: 0.4143\n",
      "Epoch [6/50], Train Loss: 0.1606, Val Loss: 0.4122\n",
      "Epoch [7/50], Train Loss: 0.1581, Val Loss: 0.4102\n",
      "Epoch [8/50], Train Loss: 0.1579, Val Loss: 0.4082\n",
      "Epoch [9/50], Train Loss: 0.1561, Val Loss: 0.4061\n",
      "Epoch [10/50], Train Loss: 0.1545, Val Loss: 0.4042\n",
      "Epoch [11/50], Train Loss: 0.1547, Val Loss: 0.4022\n",
      "Epoch [12/50], Train Loss: 0.1525, Val Loss: 0.4002\n",
      "Epoch [13/50], Train Loss: 0.1511, Val Loss: 0.3983\n",
      "Epoch [14/50], Train Loss: 0.1501, Val Loss: 0.3964\n",
      "Epoch [15/50], Train Loss: 0.1505, Val Loss: 0.3945\n",
      "Epoch [16/50], Train Loss: 0.1488, Val Loss: 0.3926\n",
      "Epoch [17/50], Train Loss: 0.1481, Val Loss: 0.3907\n",
      "Epoch [18/50], Train Loss: 0.1464, Val Loss: 0.3889\n",
      "Epoch [19/50], Train Loss: 0.1461, Val Loss: 0.3870\n",
      "Epoch [20/50], Train Loss: 0.1455, Val Loss: 0.3851\n",
      "Epoch [21/50], Train Loss: 0.1434, Val Loss: 0.3834\n",
      "Epoch [22/50], Train Loss: 0.1421, Val Loss: 0.3816\n",
      "Epoch [23/50], Train Loss: 0.1417, Val Loss: 0.3798\n",
      "Epoch [24/50], Train Loss: 0.1409, Val Loss: 0.3780\n",
      "Epoch [25/50], Train Loss: 0.1393, Val Loss: 0.3763\n",
      "Epoch [26/50], Train Loss: 0.1394, Val Loss: 0.3745\n",
      "Epoch [27/50], Train Loss: 0.1380, Val Loss: 0.3728\n",
      "Epoch [28/50], Train Loss: 0.1381, Val Loss: 0.3711\n",
      "Epoch [29/50], Train Loss: 0.1362, Val Loss: 0.3694\n",
      "Epoch [30/50], Train Loss: 0.1348, Val Loss: 0.3677\n",
      "Epoch [31/50], Train Loss: 0.1342, Val Loss: 0.3660\n",
      "Epoch [32/50], Train Loss: 0.1350, Val Loss: 0.3643\n",
      "Epoch [33/50], Train Loss: 0.1343, Val Loss: 0.3627\n",
      "Epoch [34/50], Train Loss: 0.1321, Val Loss: 0.3610\n",
      "Epoch [35/50], Train Loss: 0.1310, Val Loss: 0.3594\n",
      "Epoch [36/50], Train Loss: 0.1304, Val Loss: 0.3578\n",
      "Epoch [37/50], Train Loss: 0.1290, Val Loss: 0.3562\n",
      "Epoch [38/50], Train Loss: 0.1288, Val Loss: 0.3546\n",
      "Epoch [39/50], Train Loss: 0.1269, Val Loss: 0.3530\n",
      "Epoch [40/50], Train Loss: 0.1272, Val Loss: 0.3514\n",
      "Epoch [41/50], Train Loss: 0.1263, Val Loss: 0.3499\n",
      "Epoch [42/50], Train Loss: 0.1260, Val Loss: 0.3483\n",
      "Epoch [43/50], Train Loss: 0.1239, Val Loss: 0.3468\n",
      "Epoch [44/50], Train Loss: 0.1231, Val Loss: 0.3453\n",
      "Epoch [45/50], Train Loss: 0.1235, Val Loss: 0.3438\n",
      "Epoch [46/50], Train Loss: 0.1217, Val Loss: 0.3423\n",
      "Epoch [47/50], Train Loss: 0.1216, Val Loss: 0.3408\n",
      "Epoch [48/50], Train Loss: 0.1218, Val Loss: 0.3393\n",
      "Epoch [49/50], Train Loss: 0.1206, Val Loss: 0.3378\n",
      "Epoch [50/50], Train Loss: 0.1191, Val Loss: 0.3363\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=64, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1014, Val Loss: 0.3013\n",
      "Epoch [2/50], Train Loss: 0.1045, Val Loss: 0.2996\n",
      "Epoch [3/50], Train Loss: 0.1030, Val Loss: 0.2979\n",
      "Epoch [4/50], Train Loss: 0.1019, Val Loss: 0.2963\n",
      "Epoch [5/50], Train Loss: 0.1014, Val Loss: 0.2947\n",
      "Epoch [6/50], Train Loss: 0.0990, Val Loss: 0.2931\n",
      "Epoch [7/50], Train Loss: 0.0980, Val Loss: 0.2916\n",
      "Epoch [8/50], Train Loss: 0.0996, Val Loss: 0.2900\n",
      "Epoch [9/50], Train Loss: 0.0976, Val Loss: 0.2884\n",
      "Epoch [10/50], Train Loss: 0.0981, Val Loss: 0.2869\n",
      "Epoch [11/50], Train Loss: 0.0973, Val Loss: 0.2853\n",
      "Epoch [12/50], Train Loss: 0.0968, Val Loss: 0.2838\n",
      "Epoch [13/50], Train Loss: 0.0966, Val Loss: 0.2823\n",
      "Epoch [14/50], Train Loss: 0.0960, Val Loss: 0.2808\n",
      "Epoch [15/50], Train Loss: 0.0944, Val Loss: 0.2793\n",
      "Epoch [16/50], Train Loss: 0.0930, Val Loss: 0.2779\n",
      "Epoch [17/50], Train Loss: 0.0915, Val Loss: 0.2764\n",
      "Epoch [18/50], Train Loss: 0.0941, Val Loss: 0.2749\n",
      "Epoch [19/50], Train Loss: 0.0916, Val Loss: 0.2735\n",
      "Epoch [20/50], Train Loss: 0.0907, Val Loss: 0.2720\n",
      "Epoch [21/50], Train Loss: 0.0920, Val Loss: 0.2706\n",
      "Epoch [22/50], Train Loss: 0.0909, Val Loss: 0.2692\n",
      "Epoch [23/50], Train Loss: 0.0891, Val Loss: 0.2678\n",
      "Epoch [24/50], Train Loss: 0.0879, Val Loss: 0.2664\n",
      "Epoch [25/50], Train Loss: 0.0873, Val Loss: 0.2651\n",
      "Epoch [26/50], Train Loss: 0.0895, Val Loss: 0.2637\n",
      "Epoch [27/50], Train Loss: 0.0877, Val Loss: 0.2623\n",
      "Epoch [28/50], Train Loss: 0.0886, Val Loss: 0.2610\n",
      "Epoch [29/50], Train Loss: 0.0864, Val Loss: 0.2597\n",
      "Epoch [30/50], Train Loss: 0.0862, Val Loss: 0.2584\n",
      "Epoch [31/50], Train Loss: 0.0867, Val Loss: 0.2571\n",
      "Epoch [32/50], Train Loss: 0.0848, Val Loss: 0.2558\n",
      "Epoch [33/50], Train Loss: 0.0837, Val Loss: 0.2545\n",
      "Epoch [34/50], Train Loss: 0.0854, Val Loss: 0.2532\n",
      "Epoch [35/50], Train Loss: 0.0824, Val Loss: 0.2520\n",
      "Epoch [36/50], Train Loss: 0.0852, Val Loss: 0.2507\n",
      "Epoch [37/50], Train Loss: 0.0817, Val Loss: 0.2495\n",
      "Epoch [38/50], Train Loss: 0.0828, Val Loss: 0.2482\n",
      "Epoch [39/50], Train Loss: 0.0821, Val Loss: 0.2470\n",
      "Epoch [40/50], Train Loss: 0.0813, Val Loss: 0.2458\n",
      "Epoch [41/50], Train Loss: 0.0814, Val Loss: 0.2446\n",
      "Epoch [42/50], Train Loss: 0.0825, Val Loss: 0.2435\n",
      "Epoch [43/50], Train Loss: 0.0819, Val Loss: 0.2423\n",
      "Epoch [44/50], Train Loss: 0.0811, Val Loss: 0.2411\n",
      "Epoch [45/50], Train Loss: 0.0796, Val Loss: 0.2399\n",
      "Epoch [46/50], Train Loss: 0.0802, Val Loss: 0.2388\n",
      "Epoch [47/50], Train Loss: 0.0803, Val Loss: 0.2376\n",
      "Epoch [48/50], Train Loss: 0.0801, Val Loss: 0.2365\n",
      "Epoch [49/50], Train Loss: 0.0785, Val Loss: 0.2354\n",
      "Epoch [50/50], Train Loss: 0.0778, Val Loss: 0.2342\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=64, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1423, Val Loss: 0.3875\n",
      "Epoch [2/50], Train Loss: 0.1407, Val Loss: 0.3845\n",
      "Epoch [3/50], Train Loss: 0.1392, Val Loss: 0.3815\n",
      "Epoch [4/50], Train Loss: 0.1376, Val Loss: 0.3785\n",
      "Epoch [5/50], Train Loss: 0.1361, Val Loss: 0.3756\n",
      "Epoch [6/50], Train Loss: 0.1346, Val Loss: 0.3727\n",
      "Epoch [7/50], Train Loss: 0.1331, Val Loss: 0.3698\n",
      "Epoch [8/50], Train Loss: 0.1316, Val Loss: 0.3670\n",
      "Epoch [9/50], Train Loss: 0.1301, Val Loss: 0.3641\n",
      "Epoch [10/50], Train Loss: 0.1287, Val Loss: 0.3614\n",
      "Epoch [11/50], Train Loss: 0.1273, Val Loss: 0.3586\n",
      "Epoch [12/50], Train Loss: 0.1259, Val Loss: 0.3559\n",
      "Epoch [13/50], Train Loss: 0.1245, Val Loss: 0.3531\n",
      "Epoch [14/50], Train Loss: 0.1231, Val Loss: 0.3504\n",
      "Epoch [15/50], Train Loss: 0.1218, Val Loss: 0.3478\n",
      "Epoch [16/50], Train Loss: 0.1205, Val Loss: 0.3451\n",
      "Epoch [17/50], Train Loss: 0.1191, Val Loss: 0.3425\n",
      "Epoch [18/50], Train Loss: 0.1178, Val Loss: 0.3399\n",
      "Epoch [19/50], Train Loss: 0.1166, Val Loss: 0.3374\n",
      "Epoch [20/50], Train Loss: 0.1153, Val Loss: 0.3348\n",
      "Epoch [21/50], Train Loss: 0.1141, Val Loss: 0.3323\n",
      "Epoch [22/50], Train Loss: 0.1128, Val Loss: 0.3298\n",
      "Epoch [23/50], Train Loss: 0.1116, Val Loss: 0.3273\n",
      "Epoch [24/50], Train Loss: 0.1104, Val Loss: 0.3249\n",
      "Epoch [25/50], Train Loss: 0.1092, Val Loss: 0.3224\n",
      "Epoch [26/50], Train Loss: 0.1081, Val Loss: 0.3200\n",
      "Epoch [27/50], Train Loss: 0.1069, Val Loss: 0.3177\n",
      "Epoch [28/50], Train Loss: 0.1058, Val Loss: 0.3153\n",
      "Epoch [29/50], Train Loss: 0.1046, Val Loss: 0.3130\n",
      "Epoch [30/50], Train Loss: 0.1035, Val Loss: 0.3106\n",
      "Epoch [31/50], Train Loss: 0.1024, Val Loss: 0.3083\n",
      "Epoch [32/50], Train Loss: 0.1013, Val Loss: 0.3060\n",
      "Epoch [33/50], Train Loss: 0.1003, Val Loss: 0.3038\n",
      "Epoch [34/50], Train Loss: 0.0992, Val Loss: 0.3015\n",
      "Epoch [35/50], Train Loss: 0.0982, Val Loss: 0.2993\n",
      "Epoch [36/50], Train Loss: 0.0971, Val Loss: 0.2971\n",
      "Epoch [37/50], Train Loss: 0.0961, Val Loss: 0.2950\n",
      "Epoch [38/50], Train Loss: 0.0951, Val Loss: 0.2928\n",
      "Epoch [39/50], Train Loss: 0.0941, Val Loss: 0.2906\n",
      "Epoch [40/50], Train Loss: 0.0932, Val Loss: 0.2885\n",
      "Epoch [41/50], Train Loss: 0.0922, Val Loss: 0.2864\n",
      "Epoch [42/50], Train Loss: 0.0912, Val Loss: 0.2843\n",
      "Epoch [43/50], Train Loss: 0.0903, Val Loss: 0.2823\n",
      "Epoch [44/50], Train Loss: 0.0894, Val Loss: 0.2802\n",
      "Epoch [45/50], Train Loss: 0.0885, Val Loss: 0.2782\n",
      "Epoch [46/50], Train Loss: 0.0876, Val Loss: 0.2762\n",
      "Epoch [47/50], Train Loss: 0.0867, Val Loss: 0.2742\n",
      "Epoch [48/50], Train Loss: 0.0858, Val Loss: 0.2722\n",
      "Epoch [49/50], Train Loss: 0.0849, Val Loss: 0.2702\n",
      "Epoch [50/50], Train Loss: 0.0841, Val Loss: 0.2683\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=64, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1614, Val Loss: 0.3661\n",
      "Epoch [2/50], Train Loss: 0.1583, Val Loss: 0.3635\n",
      "Epoch [3/50], Train Loss: 0.1566, Val Loss: 0.3608\n",
      "Epoch [4/50], Train Loss: 0.1556, Val Loss: 0.3583\n",
      "Epoch [5/50], Train Loss: 0.1530, Val Loss: 0.3557\n",
      "Epoch [6/50], Train Loss: 0.1507, Val Loss: 0.3532\n",
      "Epoch [7/50], Train Loss: 0.1504, Val Loss: 0.3507\n",
      "Epoch [8/50], Train Loss: 0.1479, Val Loss: 0.3483\n",
      "Epoch [9/50], Train Loss: 0.1458, Val Loss: 0.3459\n",
      "Epoch [10/50], Train Loss: 0.1464, Val Loss: 0.3435\n",
      "Epoch [11/50], Train Loss: 0.1434, Val Loss: 0.3411\n",
      "Epoch [12/50], Train Loss: 0.1419, Val Loss: 0.3388\n",
      "Epoch [13/50], Train Loss: 0.1417, Val Loss: 0.3365\n",
      "Epoch [14/50], Train Loss: 0.1403, Val Loss: 0.3343\n",
      "Epoch [15/50], Train Loss: 0.1390, Val Loss: 0.3320\n",
      "Epoch [16/50], Train Loss: 0.1365, Val Loss: 0.3298\n",
      "Epoch [17/50], Train Loss: 0.1343, Val Loss: 0.3276\n",
      "Epoch [18/50], Train Loss: 0.1333, Val Loss: 0.3254\n",
      "Epoch [19/50], Train Loss: 0.1318, Val Loss: 0.3233\n",
      "Epoch [20/50], Train Loss: 0.1312, Val Loss: 0.3211\n",
      "Epoch [21/50], Train Loss: 0.1304, Val Loss: 0.3190\n",
      "Epoch [22/50], Train Loss: 0.1289, Val Loss: 0.3169\n",
      "Epoch [23/50], Train Loss: 0.1264, Val Loss: 0.3149\n",
      "Epoch [24/50], Train Loss: 0.1256, Val Loss: 0.3129\n",
      "Epoch [25/50], Train Loss: 0.1248, Val Loss: 0.3109\n",
      "Epoch [26/50], Train Loss: 0.1240, Val Loss: 0.3089\n",
      "Epoch [27/50], Train Loss: 0.1218, Val Loss: 0.3069\n",
      "Epoch [28/50], Train Loss: 0.1210, Val Loss: 0.3050\n",
      "Epoch [29/50], Train Loss: 0.1200, Val Loss: 0.3031\n",
      "Epoch [30/50], Train Loss: 0.1195, Val Loss: 0.3011\n",
      "Epoch [31/50], Train Loss: 0.1181, Val Loss: 0.2993\n",
      "Epoch [32/50], Train Loss: 0.1171, Val Loss: 0.2974\n",
      "Epoch [33/50], Train Loss: 0.1153, Val Loss: 0.2955\n",
      "Epoch [34/50], Train Loss: 0.1149, Val Loss: 0.2937\n",
      "Epoch [35/50], Train Loss: 0.1146, Val Loss: 0.2919\n",
      "Epoch [36/50], Train Loss: 0.1127, Val Loss: 0.2901\n",
      "Epoch [37/50], Train Loss: 0.1110, Val Loss: 0.2884\n",
      "Epoch [38/50], Train Loss: 0.1124, Val Loss: 0.2866\n",
      "Epoch [39/50], Train Loss: 0.1098, Val Loss: 0.2849\n",
      "Epoch [40/50], Train Loss: 0.1103, Val Loss: 0.2832\n",
      "Epoch [41/50], Train Loss: 0.1079, Val Loss: 0.2814\n",
      "Epoch [42/50], Train Loss: 0.1059, Val Loss: 0.2798\n",
      "Epoch [43/50], Train Loss: 0.1059, Val Loss: 0.2781\n",
      "Epoch [44/50], Train Loss: 0.1046, Val Loss: 0.2764\n",
      "Epoch [45/50], Train Loss: 0.1042, Val Loss: 0.2748\n",
      "Epoch [46/50], Train Loss: 0.1033, Val Loss: 0.2732\n",
      "Epoch [47/50], Train Loss: 0.1019, Val Loss: 0.2715\n",
      "Epoch [48/50], Train Loss: 0.1013, Val Loss: 0.2700\n",
      "Epoch [49/50], Train Loss: 0.1013, Val Loss: 0.2684\n",
      "Epoch [50/50], Train Loss: 0.1006, Val Loss: 0.2668\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=64, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1902, Val Loss: 0.3926\n",
      "Epoch [2/50], Train Loss: 0.1884, Val Loss: 0.3889\n",
      "Epoch [3/50], Train Loss: 0.1864, Val Loss: 0.3853\n",
      "Epoch [4/50], Train Loss: 0.1860, Val Loss: 0.3817\n",
      "Epoch [5/50], Train Loss: 0.1775, Val Loss: 0.3783\n",
      "Epoch [6/50], Train Loss: 0.1786, Val Loss: 0.3749\n",
      "Epoch [7/50], Train Loss: 0.1753, Val Loss: 0.3715\n",
      "Epoch [8/50], Train Loss: 0.1739, Val Loss: 0.3682\n",
      "Epoch [9/50], Train Loss: 0.1691, Val Loss: 0.3650\n",
      "Epoch [10/50], Train Loss: 0.1728, Val Loss: 0.3617\n",
      "Epoch [11/50], Train Loss: 0.1706, Val Loss: 0.3585\n",
      "Epoch [12/50], Train Loss: 0.1666, Val Loss: 0.3554\n",
      "Epoch [13/50], Train Loss: 0.1646, Val Loss: 0.3523\n",
      "Epoch [14/50], Train Loss: 0.1635, Val Loss: 0.3492\n",
      "Epoch [15/50], Train Loss: 0.1604, Val Loss: 0.3462\n",
      "Epoch [16/50], Train Loss: 0.1579, Val Loss: 0.3433\n",
      "Epoch [17/50], Train Loss: 0.1570, Val Loss: 0.3405\n",
      "Epoch [18/50], Train Loss: 0.1549, Val Loss: 0.3377\n",
      "Epoch [19/50], Train Loss: 0.1506, Val Loss: 0.3349\n",
      "Epoch [20/50], Train Loss: 0.1521, Val Loss: 0.3322\n",
      "Epoch [21/50], Train Loss: 0.1531, Val Loss: 0.3295\n",
      "Epoch [22/50], Train Loss: 0.1498, Val Loss: 0.3268\n",
      "Epoch [23/50], Train Loss: 0.1458, Val Loss: 0.3241\n",
      "Epoch [24/50], Train Loss: 0.1470, Val Loss: 0.3215\n",
      "Epoch [25/50], Train Loss: 0.1449, Val Loss: 0.3190\n",
      "Epoch [26/50], Train Loss: 0.1436, Val Loss: 0.3164\n",
      "Epoch [27/50], Train Loss: 0.1420, Val Loss: 0.3140\n",
      "Epoch [28/50], Train Loss: 0.1368, Val Loss: 0.3116\n",
      "Epoch [29/50], Train Loss: 0.1364, Val Loss: 0.3091\n",
      "Epoch [30/50], Train Loss: 0.1377, Val Loss: 0.3068\n",
      "Epoch [31/50], Train Loss: 0.1343, Val Loss: 0.3044\n",
      "Epoch [32/50], Train Loss: 0.1346, Val Loss: 0.3021\n",
      "Epoch [33/50], Train Loss: 0.1306, Val Loss: 0.2998\n",
      "Epoch [34/50], Train Loss: 0.1318, Val Loss: 0.2976\n",
      "Epoch [35/50], Train Loss: 0.1279, Val Loss: 0.2954\n",
      "Epoch [36/50], Train Loss: 0.1275, Val Loss: 0.2932\n",
      "Epoch [37/50], Train Loss: 0.1268, Val Loss: 0.2910\n",
      "Epoch [38/50], Train Loss: 0.1241, Val Loss: 0.2889\n",
      "Epoch [39/50], Train Loss: 0.1279, Val Loss: 0.2868\n",
      "Epoch [40/50], Train Loss: 0.1231, Val Loss: 0.2847\n",
      "Epoch [41/50], Train Loss: 0.1232, Val Loss: 0.2827\n",
      "Epoch [42/50], Train Loss: 0.1220, Val Loss: 0.2807\n",
      "Epoch [43/50], Train Loss: 0.1200, Val Loss: 0.2787\n",
      "Epoch [44/50], Train Loss: 0.1162, Val Loss: 0.2768\n",
      "Epoch [45/50], Train Loss: 0.1186, Val Loss: 0.2748\n",
      "Epoch [46/50], Train Loss: 0.1169, Val Loss: 0.2729\n",
      "Epoch [47/50], Train Loss: 0.1178, Val Loss: 0.2709\n",
      "Epoch [48/50], Train Loss: 0.1152, Val Loss: 0.2691\n",
      "Epoch [49/50], Train Loss: 0.1116, Val Loss: 0.2672\n",
      "Epoch [50/50], Train Loss: 0.1118, Val Loss: 0.2654\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=128, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1081, Val Loss: 0.3080\n",
      "Epoch [2/50], Train Loss: 0.1074, Val Loss: 0.3064\n",
      "Epoch [3/50], Train Loss: 0.1066, Val Loss: 0.3048\n",
      "Epoch [4/50], Train Loss: 0.1059, Val Loss: 0.3032\n",
      "Epoch [5/50], Train Loss: 0.1051, Val Loss: 0.3017\n",
      "Epoch [6/50], Train Loss: 0.1044, Val Loss: 0.3001\n",
      "Epoch [7/50], Train Loss: 0.1037, Val Loss: 0.2986\n",
      "Epoch [8/50], Train Loss: 0.1030, Val Loss: 0.2970\n",
      "Epoch [9/50], Train Loss: 0.1022, Val Loss: 0.2955\n",
      "Epoch [10/50], Train Loss: 0.1015, Val Loss: 0.2939\n",
      "Epoch [11/50], Train Loss: 0.1008, Val Loss: 0.2924\n",
      "Epoch [12/50], Train Loss: 0.1001, Val Loss: 0.2909\n",
      "Epoch [13/50], Train Loss: 0.0995, Val Loss: 0.2894\n",
      "Epoch [14/50], Train Loss: 0.0988, Val Loss: 0.2879\n",
      "Epoch [15/50], Train Loss: 0.0981, Val Loss: 0.2864\n",
      "Epoch [16/50], Train Loss: 0.0974, Val Loss: 0.2850\n",
      "Epoch [17/50], Train Loss: 0.0968, Val Loss: 0.2835\n",
      "Epoch [18/50], Train Loss: 0.0961, Val Loss: 0.2820\n",
      "Epoch [19/50], Train Loss: 0.0954, Val Loss: 0.2806\n",
      "Epoch [20/50], Train Loss: 0.0948, Val Loss: 0.2791\n",
      "Epoch [21/50], Train Loss: 0.0941, Val Loss: 0.2777\n",
      "Epoch [22/50], Train Loss: 0.0935, Val Loss: 0.2763\n",
      "Epoch [23/50], Train Loss: 0.0929, Val Loss: 0.2749\n",
      "Epoch [24/50], Train Loss: 0.0922, Val Loss: 0.2735\n",
      "Epoch [25/50], Train Loss: 0.0916, Val Loss: 0.2721\n",
      "Epoch [26/50], Train Loss: 0.0910, Val Loss: 0.2707\n",
      "Epoch [27/50], Train Loss: 0.0904, Val Loss: 0.2693\n",
      "Epoch [28/50], Train Loss: 0.0898, Val Loss: 0.2679\n",
      "Epoch [29/50], Train Loss: 0.0892, Val Loss: 0.2665\n",
      "Epoch [30/50], Train Loss: 0.0886, Val Loss: 0.2652\n",
      "Epoch [31/50], Train Loss: 0.0880, Val Loss: 0.2638\n",
      "Epoch [32/50], Train Loss: 0.0874, Val Loss: 0.2625\n",
      "Epoch [33/50], Train Loss: 0.0868, Val Loss: 0.2611\n",
      "Epoch [34/50], Train Loss: 0.0862, Val Loss: 0.2598\n",
      "Epoch [35/50], Train Loss: 0.0857, Val Loss: 0.2585\n",
      "Epoch [36/50], Train Loss: 0.0851, Val Loss: 0.2571\n",
      "Epoch [37/50], Train Loss: 0.0845, Val Loss: 0.2558\n",
      "Epoch [38/50], Train Loss: 0.0840, Val Loss: 0.2545\n",
      "Epoch [39/50], Train Loss: 0.0834, Val Loss: 0.2533\n",
      "Epoch [40/50], Train Loss: 0.0829, Val Loss: 0.2520\n",
      "Epoch [41/50], Train Loss: 0.0823, Val Loss: 0.2507\n",
      "Epoch [42/50], Train Loss: 0.0818, Val Loss: 0.2494\n",
      "Epoch [43/50], Train Loss: 0.0812, Val Loss: 0.2481\n",
      "Epoch [44/50], Train Loss: 0.0807, Val Loss: 0.2469\n",
      "Epoch [45/50], Train Loss: 0.0802, Val Loss: 0.2456\n",
      "Epoch [46/50], Train Loss: 0.0796, Val Loss: 0.2444\n",
      "Epoch [47/50], Train Loss: 0.0791, Val Loss: 0.2431\n",
      "Epoch [48/50], Train Loss: 0.0786, Val Loss: 0.2419\n",
      "Epoch [49/50], Train Loss: 0.0781, Val Loss: 0.2407\n",
      "Epoch [50/50], Train Loss: 0.0776, Val Loss: 0.2395\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=128, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1727, Val Loss: 0.4483\n",
      "Epoch [2/50], Train Loss: 0.1717, Val Loss: 0.4458\n",
      "Epoch [3/50], Train Loss: 0.1710, Val Loss: 0.4433\n",
      "Epoch [4/50], Train Loss: 0.1694, Val Loss: 0.4408\n",
      "Epoch [5/50], Train Loss: 0.1682, Val Loss: 0.4384\n",
      "Epoch [6/50], Train Loss: 0.1666, Val Loss: 0.4359\n",
      "Epoch [7/50], Train Loss: 0.1660, Val Loss: 0.4335\n",
      "Epoch [8/50], Train Loss: 0.1643, Val Loss: 0.4311\n",
      "Epoch [9/50], Train Loss: 0.1632, Val Loss: 0.4287\n",
      "Epoch [10/50], Train Loss: 0.1620, Val Loss: 0.4264\n",
      "Epoch [11/50], Train Loss: 0.1611, Val Loss: 0.4240\n",
      "Epoch [12/50], Train Loss: 0.1594, Val Loss: 0.4217\n",
      "Epoch [13/50], Train Loss: 0.1588, Val Loss: 0.4194\n",
      "Epoch [14/50], Train Loss: 0.1575, Val Loss: 0.4170\n",
      "Epoch [15/50], Train Loss: 0.1554, Val Loss: 0.4148\n",
      "Epoch [16/50], Train Loss: 0.1552, Val Loss: 0.4125\n",
      "Epoch [17/50], Train Loss: 0.1540, Val Loss: 0.4102\n",
      "Epoch [18/50], Train Loss: 0.1533, Val Loss: 0.4080\n",
      "Epoch [19/50], Train Loss: 0.1523, Val Loss: 0.4057\n",
      "Epoch [20/50], Train Loss: 0.1512, Val Loss: 0.4035\n",
      "Epoch [21/50], Train Loss: 0.1499, Val Loss: 0.4013\n",
      "Epoch [22/50], Train Loss: 0.1489, Val Loss: 0.3991\n",
      "Epoch [23/50], Train Loss: 0.1475, Val Loss: 0.3969\n",
      "Epoch [24/50], Train Loss: 0.1461, Val Loss: 0.3947\n",
      "Epoch [25/50], Train Loss: 0.1456, Val Loss: 0.3926\n",
      "Epoch [26/50], Train Loss: 0.1443, Val Loss: 0.3904\n",
      "Epoch [27/50], Train Loss: 0.1432, Val Loss: 0.3883\n",
      "Epoch [28/50], Train Loss: 0.1420, Val Loss: 0.3862\n",
      "Epoch [29/50], Train Loss: 0.1405, Val Loss: 0.3841\n",
      "Epoch [30/50], Train Loss: 0.1405, Val Loss: 0.3820\n",
      "Epoch [31/50], Train Loss: 0.1391, Val Loss: 0.3799\n",
      "Epoch [32/50], Train Loss: 0.1381, Val Loss: 0.3778\n",
      "Epoch [33/50], Train Loss: 0.1372, Val Loss: 0.3758\n",
      "Epoch [34/50], Train Loss: 0.1360, Val Loss: 0.3738\n",
      "Epoch [35/50], Train Loss: 0.1356, Val Loss: 0.3718\n",
      "Epoch [36/50], Train Loss: 0.1344, Val Loss: 0.3697\n",
      "Epoch [37/50], Train Loss: 0.1341, Val Loss: 0.3677\n",
      "Epoch [38/50], Train Loss: 0.1322, Val Loss: 0.3657\n",
      "Epoch [39/50], Train Loss: 0.1311, Val Loss: 0.3637\n",
      "Epoch [40/50], Train Loss: 0.1303, Val Loss: 0.3617\n",
      "Epoch [41/50], Train Loss: 0.1296, Val Loss: 0.3598\n",
      "Epoch [42/50], Train Loss: 0.1288, Val Loss: 0.3578\n",
      "Epoch [43/50], Train Loss: 0.1274, Val Loss: 0.3559\n",
      "Epoch [44/50], Train Loss: 0.1266, Val Loss: 0.3540\n",
      "Epoch [45/50], Train Loss: 0.1256, Val Loss: 0.3520\n",
      "Epoch [46/50], Train Loss: 0.1250, Val Loss: 0.3501\n",
      "Epoch [47/50], Train Loss: 0.1245, Val Loss: 0.3482\n",
      "Epoch [48/50], Train Loss: 0.1237, Val Loss: 0.3463\n",
      "Epoch [49/50], Train Loss: 0.1220, Val Loss: 0.3445\n",
      "Epoch [50/50], Train Loss: 0.1216, Val Loss: 0.3426\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=128, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1297, Val Loss: 0.3691\n",
      "Epoch [2/50], Train Loss: 0.1283, Val Loss: 0.3673\n",
      "Epoch [3/50], Train Loss: 0.1283, Val Loss: 0.3654\n",
      "Epoch [4/50], Train Loss: 0.1264, Val Loss: 0.3635\n",
      "Epoch [5/50], Train Loss: 0.1254, Val Loss: 0.3617\n",
      "Epoch [6/50], Train Loss: 0.1266, Val Loss: 0.3599\n",
      "Epoch [7/50], Train Loss: 0.1245, Val Loss: 0.3581\n",
      "Epoch [8/50], Train Loss: 0.1235, Val Loss: 0.3563\n",
      "Epoch [9/50], Train Loss: 0.1225, Val Loss: 0.3546\n",
      "Epoch [10/50], Train Loss: 0.1229, Val Loss: 0.3528\n",
      "Epoch [11/50], Train Loss: 0.1224, Val Loss: 0.3510\n",
      "Epoch [12/50], Train Loss: 0.1191, Val Loss: 0.3493\n",
      "Epoch [13/50], Train Loss: 0.1198, Val Loss: 0.3476\n",
      "Epoch [14/50], Train Loss: 0.1183, Val Loss: 0.3459\n",
      "Epoch [15/50], Train Loss: 0.1172, Val Loss: 0.3442\n",
      "Epoch [16/50], Train Loss: 0.1179, Val Loss: 0.3425\n",
      "Epoch [17/50], Train Loss: 0.1146, Val Loss: 0.3408\n",
      "Epoch [18/50], Train Loss: 0.1147, Val Loss: 0.3391\n",
      "Epoch [19/50], Train Loss: 0.1145, Val Loss: 0.3374\n",
      "Epoch [20/50], Train Loss: 0.1148, Val Loss: 0.3358\n",
      "Epoch [21/50], Train Loss: 0.1124, Val Loss: 0.3342\n",
      "Epoch [22/50], Train Loss: 0.1123, Val Loss: 0.3325\n",
      "Epoch [23/50], Train Loss: 0.1119, Val Loss: 0.3309\n",
      "Epoch [24/50], Train Loss: 0.1111, Val Loss: 0.3293\n",
      "Epoch [25/50], Train Loss: 0.1112, Val Loss: 0.3277\n",
      "Epoch [26/50], Train Loss: 0.1107, Val Loss: 0.3261\n",
      "Epoch [27/50], Train Loss: 0.1086, Val Loss: 0.3246\n",
      "Epoch [28/50], Train Loss: 0.1086, Val Loss: 0.3230\n",
      "Epoch [29/50], Train Loss: 0.1090, Val Loss: 0.3214\n",
      "Epoch [30/50], Train Loss: 0.1077, Val Loss: 0.3199\n",
      "Epoch [31/50], Train Loss: 0.1059, Val Loss: 0.3184\n",
      "Epoch [32/50], Train Loss: 0.1068, Val Loss: 0.3168\n",
      "Epoch [33/50], Train Loss: 0.1064, Val Loss: 0.3153\n",
      "Epoch [34/50], Train Loss: 0.1038, Val Loss: 0.3138\n",
      "Epoch [35/50], Train Loss: 0.1035, Val Loss: 0.3123\n",
      "Epoch [36/50], Train Loss: 0.1030, Val Loss: 0.3108\n",
      "Epoch [37/50], Train Loss: 0.1010, Val Loss: 0.3093\n",
      "Epoch [38/50], Train Loss: 0.1015, Val Loss: 0.3079\n",
      "Epoch [39/50], Train Loss: 0.1010, Val Loss: 0.3064\n",
      "Epoch [40/50], Train Loss: 0.1013, Val Loss: 0.3050\n",
      "Epoch [41/50], Train Loss: 0.1008, Val Loss: 0.3035\n",
      "Epoch [42/50], Train Loss: 0.0989, Val Loss: 0.3021\n",
      "Epoch [43/50], Train Loss: 0.0990, Val Loss: 0.3006\n",
      "Epoch [44/50], Train Loss: 0.0975, Val Loss: 0.2992\n",
      "Epoch [45/50], Train Loss: 0.0987, Val Loss: 0.2978\n",
      "Epoch [46/50], Train Loss: 0.0955, Val Loss: 0.2964\n",
      "Epoch [47/50], Train Loss: 0.0970, Val Loss: 0.2950\n",
      "Epoch [48/50], Train Loss: 0.0973, Val Loss: 0.2936\n",
      "Epoch [49/50], Train Loss: 0.0953, Val Loss: 0.2923\n",
      "Epoch [50/50], Train Loss: 0.0954, Val Loss: 0.2909\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=128, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1297, Val Loss: 0.3490\n",
      "Epoch [2/50], Train Loss: 0.1286, Val Loss: 0.3467\n",
      "Epoch [3/50], Train Loss: 0.1275, Val Loss: 0.3445\n",
      "Epoch [4/50], Train Loss: 0.1264, Val Loss: 0.3423\n",
      "Epoch [5/50], Train Loss: 0.1253, Val Loss: 0.3401\n",
      "Epoch [6/50], Train Loss: 0.1243, Val Loss: 0.3379\n",
      "Epoch [7/50], Train Loss: 0.1233, Val Loss: 0.3358\n",
      "Epoch [8/50], Train Loss: 0.1222, Val Loss: 0.3337\n",
      "Epoch [9/50], Train Loss: 0.1212, Val Loss: 0.3315\n",
      "Epoch [10/50], Train Loss: 0.1202, Val Loss: 0.3294\n",
      "Epoch [11/50], Train Loss: 0.1192, Val Loss: 0.3273\n",
      "Epoch [12/50], Train Loss: 0.1182, Val Loss: 0.3253\n",
      "Epoch [13/50], Train Loss: 0.1172, Val Loss: 0.3232\n",
      "Epoch [14/50], Train Loss: 0.1162, Val Loss: 0.3212\n",
      "Epoch [15/50], Train Loss: 0.1153, Val Loss: 0.3192\n",
      "Epoch [16/50], Train Loss: 0.1143, Val Loss: 0.3172\n",
      "Epoch [17/50], Train Loss: 0.1134, Val Loss: 0.3152\n",
      "Epoch [18/50], Train Loss: 0.1124, Val Loss: 0.3132\n",
      "Epoch [19/50], Train Loss: 0.1115, Val Loss: 0.3112\n",
      "Epoch [20/50], Train Loss: 0.1106, Val Loss: 0.3093\n",
      "Epoch [21/50], Train Loss: 0.1097, Val Loss: 0.3074\n",
      "Epoch [22/50], Train Loss: 0.1088, Val Loss: 0.3054\n",
      "Epoch [23/50], Train Loss: 0.1079, Val Loss: 0.3036\n",
      "Epoch [24/50], Train Loss: 0.1070, Val Loss: 0.3017\n",
      "Epoch [25/50], Train Loss: 0.1062, Val Loss: 0.2998\n",
      "Epoch [26/50], Train Loss: 0.1053, Val Loss: 0.2979\n",
      "Epoch [27/50], Train Loss: 0.1045, Val Loss: 0.2961\n",
      "Epoch [28/50], Train Loss: 0.1036, Val Loss: 0.2942\n",
      "Epoch [29/50], Train Loss: 0.1028, Val Loss: 0.2924\n",
      "Epoch [30/50], Train Loss: 0.1019, Val Loss: 0.2906\n",
      "Epoch [31/50], Train Loss: 0.1011, Val Loss: 0.2888\n",
      "Epoch [32/50], Train Loss: 0.1003, Val Loss: 0.2870\n",
      "Epoch [33/50], Train Loss: 0.0995, Val Loss: 0.2853\n",
      "Epoch [34/50], Train Loss: 0.0987, Val Loss: 0.2835\n",
      "Epoch [35/50], Train Loss: 0.0979, Val Loss: 0.2818\n",
      "Epoch [36/50], Train Loss: 0.0971, Val Loss: 0.2800\n",
      "Epoch [37/50], Train Loss: 0.0963, Val Loss: 0.2783\n",
      "Epoch [38/50], Train Loss: 0.0956, Val Loss: 0.2766\n",
      "Epoch [39/50], Train Loss: 0.0948, Val Loss: 0.2749\n",
      "Epoch [40/50], Train Loss: 0.0941, Val Loss: 0.2732\n",
      "Epoch [41/50], Train Loss: 0.0933, Val Loss: 0.2715\n",
      "Epoch [42/50], Train Loss: 0.0926, Val Loss: 0.2699\n",
      "Epoch [43/50], Train Loss: 0.0918, Val Loss: 0.2682\n",
      "Epoch [44/50], Train Loss: 0.0911, Val Loss: 0.2666\n",
      "Epoch [45/50], Train Loss: 0.0904, Val Loss: 0.2650\n",
      "Epoch [46/50], Train Loss: 0.0897, Val Loss: 0.2633\n",
      "Epoch [47/50], Train Loss: 0.0890, Val Loss: 0.2617\n",
      "Epoch [48/50], Train Loss: 0.0883, Val Loss: 0.2601\n",
      "Epoch [49/50], Train Loss: 0.0876, Val Loss: 0.2585\n",
      "Epoch [50/50], Train Loss: 0.0869, Val Loss: 0.2570\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=128, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1368, Val Loss: 0.3779\n",
      "Epoch [2/50], Train Loss: 0.1347, Val Loss: 0.3752\n",
      "Epoch [3/50], Train Loss: 0.1336, Val Loss: 0.3726\n",
      "Epoch [4/50], Train Loss: 0.1322, Val Loss: 0.3700\n",
      "Epoch [5/50], Train Loss: 0.1315, Val Loss: 0.3674\n",
      "Epoch [6/50], Train Loss: 0.1300, Val Loss: 0.3648\n",
      "Epoch [7/50], Train Loss: 0.1291, Val Loss: 0.3623\n",
      "Epoch [8/50], Train Loss: 0.1280, Val Loss: 0.3597\n",
      "Epoch [9/50], Train Loss: 0.1262, Val Loss: 0.3572\n",
      "Epoch [10/50], Train Loss: 0.1250, Val Loss: 0.3548\n",
      "Epoch [11/50], Train Loss: 0.1244, Val Loss: 0.3523\n",
      "Epoch [12/50], Train Loss: 0.1229, Val Loss: 0.3499\n",
      "Epoch [13/50], Train Loss: 0.1216, Val Loss: 0.3475\n",
      "Epoch [14/50], Train Loss: 0.1201, Val Loss: 0.3451\n",
      "Epoch [15/50], Train Loss: 0.1187, Val Loss: 0.3428\n",
      "Epoch [16/50], Train Loss: 0.1177, Val Loss: 0.3405\n",
      "Epoch [17/50], Train Loss: 0.1169, Val Loss: 0.3382\n",
      "Epoch [18/50], Train Loss: 0.1156, Val Loss: 0.3359\n",
      "Epoch [19/50], Train Loss: 0.1143, Val Loss: 0.3336\n",
      "Epoch [20/50], Train Loss: 0.1142, Val Loss: 0.3314\n",
      "Epoch [21/50], Train Loss: 0.1119, Val Loss: 0.3292\n",
      "Epoch [22/50], Train Loss: 0.1116, Val Loss: 0.3270\n",
      "Epoch [23/50], Train Loss: 0.1105, Val Loss: 0.3248\n",
      "Epoch [24/50], Train Loss: 0.1087, Val Loss: 0.3227\n",
      "Epoch [25/50], Train Loss: 0.1083, Val Loss: 0.3205\n",
      "Epoch [26/50], Train Loss: 0.1075, Val Loss: 0.3184\n",
      "Epoch [27/50], Train Loss: 0.1068, Val Loss: 0.3163\n",
      "Epoch [28/50], Train Loss: 0.1054, Val Loss: 0.3142\n",
      "Epoch [29/50], Train Loss: 0.1037, Val Loss: 0.3122\n",
      "Epoch [30/50], Train Loss: 0.1029, Val Loss: 0.3102\n",
      "Epoch [31/50], Train Loss: 0.1029, Val Loss: 0.3081\n",
      "Epoch [32/50], Train Loss: 0.1009, Val Loss: 0.3061\n",
      "Epoch [33/50], Train Loss: 0.1013, Val Loss: 0.3042\n",
      "Epoch [34/50], Train Loss: 0.0998, Val Loss: 0.3022\n",
      "Epoch [35/50], Train Loss: 0.0980, Val Loss: 0.3003\n",
      "Epoch [36/50], Train Loss: 0.0984, Val Loss: 0.2983\n",
      "Epoch [37/50], Train Loss: 0.0977, Val Loss: 0.2964\n",
      "Epoch [38/50], Train Loss: 0.0963, Val Loss: 0.2945\n",
      "Epoch [39/50], Train Loss: 0.0950, Val Loss: 0.2927\n",
      "Epoch [40/50], Train Loss: 0.0950, Val Loss: 0.2908\n",
      "Epoch [41/50], Train Loss: 0.0942, Val Loss: 0.2889\n",
      "Epoch [42/50], Train Loss: 0.0923, Val Loss: 0.2871\n",
      "Epoch [43/50], Train Loss: 0.0919, Val Loss: 0.2853\n",
      "Epoch [44/50], Train Loss: 0.0909, Val Loss: 0.2835\n",
      "Epoch [45/50], Train Loss: 0.0909, Val Loss: 0.2817\n",
      "Epoch [46/50], Train Loss: 0.0896, Val Loss: 0.2799\n",
      "Epoch [47/50], Train Loss: 0.0890, Val Loss: 0.2781\n",
      "Epoch [48/50], Train Loss: 0.0881, Val Loss: 0.2764\n",
      "Epoch [49/50], Train Loss: 0.0873, Val Loss: 0.2746\n",
      "Epoch [50/50], Train Loss: 0.0868, Val Loss: 0.2729\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=128, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1392, Val Loss: 0.3698\n",
      "Epoch [2/50], Train Loss: 0.1395, Val Loss: 0.3669\n",
      "Epoch [3/50], Train Loss: 0.1380, Val Loss: 0.3642\n",
      "Epoch [4/50], Train Loss: 0.1364, Val Loss: 0.3614\n",
      "Epoch [5/50], Train Loss: 0.1334, Val Loss: 0.3587\n",
      "Epoch [6/50], Train Loss: 0.1349, Val Loss: 0.3561\n",
      "Epoch [7/50], Train Loss: 0.1333, Val Loss: 0.3535\n",
      "Epoch [8/50], Train Loss: 0.1297, Val Loss: 0.3509\n",
      "Epoch [9/50], Train Loss: 0.1291, Val Loss: 0.3483\n",
      "Epoch [10/50], Train Loss: 0.1271, Val Loss: 0.3457\n",
      "Epoch [11/50], Train Loss: 0.1276, Val Loss: 0.3432\n",
      "Epoch [12/50], Train Loss: 0.1274, Val Loss: 0.3408\n",
      "Epoch [13/50], Train Loss: 0.1237, Val Loss: 0.3383\n",
      "Epoch [14/50], Train Loss: 0.1258, Val Loss: 0.3359\n",
      "Epoch [15/50], Train Loss: 0.1210, Val Loss: 0.3335\n",
      "Epoch [16/50], Train Loss: 0.1217, Val Loss: 0.3312\n",
      "Epoch [17/50], Train Loss: 0.1189, Val Loss: 0.3289\n",
      "Epoch [18/50], Train Loss: 0.1193, Val Loss: 0.3266\n",
      "Epoch [19/50], Train Loss: 0.1188, Val Loss: 0.3243\n",
      "Epoch [20/50], Train Loss: 0.1176, Val Loss: 0.3220\n",
      "Epoch [21/50], Train Loss: 0.1157, Val Loss: 0.3198\n",
      "Epoch [22/50], Train Loss: 0.1150, Val Loss: 0.3176\n",
      "Epoch [23/50], Train Loss: 0.1125, Val Loss: 0.3155\n",
      "Epoch [24/50], Train Loss: 0.1108, Val Loss: 0.3133\n",
      "Epoch [25/50], Train Loss: 0.1118, Val Loss: 0.3112\n",
      "Epoch [26/50], Train Loss: 0.1095, Val Loss: 0.3091\n",
      "Epoch [27/50], Train Loss: 0.1088, Val Loss: 0.3070\n",
      "Epoch [28/50], Train Loss: 0.1086, Val Loss: 0.3050\n",
      "Epoch [29/50], Train Loss: 0.1073, Val Loss: 0.3030\n",
      "Epoch [30/50], Train Loss: 0.1071, Val Loss: 0.3009\n",
      "Epoch [31/50], Train Loss: 0.1046, Val Loss: 0.2990\n",
      "Epoch [32/50], Train Loss: 0.1031, Val Loss: 0.2970\n",
      "Epoch [33/50], Train Loss: 0.1031, Val Loss: 0.2950\n",
      "Epoch [34/50], Train Loss: 0.1024, Val Loss: 0.2932\n",
      "Epoch [35/50], Train Loss: 0.1013, Val Loss: 0.2913\n",
      "Epoch [36/50], Train Loss: 0.1011, Val Loss: 0.2894\n",
      "Epoch [37/50], Train Loss: 0.0990, Val Loss: 0.2875\n",
      "Epoch [38/50], Train Loss: 0.1000, Val Loss: 0.2857\n",
      "Epoch [39/50], Train Loss: 0.0971, Val Loss: 0.2839\n",
      "Epoch [40/50], Train Loss: 0.0973, Val Loss: 0.2821\n",
      "Epoch [41/50], Train Loss: 0.0973, Val Loss: 0.2803\n",
      "Epoch [42/50], Train Loss: 0.0965, Val Loss: 0.2785\n",
      "Epoch [43/50], Train Loss: 0.0945, Val Loss: 0.2767\n",
      "Epoch [44/50], Train Loss: 0.0952, Val Loss: 0.2750\n",
      "Epoch [45/50], Train Loss: 0.0928, Val Loss: 0.2733\n",
      "Epoch [46/50], Train Loss: 0.0922, Val Loss: 0.2716\n",
      "Epoch [47/50], Train Loss: 0.0928, Val Loss: 0.2699\n",
      "Epoch [48/50], Train Loss: 0.0910, Val Loss: 0.2682\n",
      "Epoch [49/50], Train Loss: 0.0907, Val Loss: 0.2666\n",
      "Epoch [50/50], Train Loss: 0.0893, Val Loss: 0.2650\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=128, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.2028, Val Loss: 0.4847\n",
      "Epoch [2/50], Train Loss: 0.2003, Val Loss: 0.4804\n",
      "Epoch [3/50], Train Loss: 0.1978, Val Loss: 0.4763\n",
      "Epoch [4/50], Train Loss: 0.1954, Val Loss: 0.4723\n",
      "Epoch [5/50], Train Loss: 0.1931, Val Loss: 0.4683\n",
      "Epoch [6/50], Train Loss: 0.1908, Val Loss: 0.4644\n",
      "Epoch [7/50], Train Loss: 0.1886, Val Loss: 0.4606\n",
      "Epoch [8/50], Train Loss: 0.1864, Val Loss: 0.4569\n",
      "Epoch [9/50], Train Loss: 0.1842, Val Loss: 0.4532\n",
      "Epoch [10/50], Train Loss: 0.1821, Val Loss: 0.4496\n",
      "Epoch [11/50], Train Loss: 0.1801, Val Loss: 0.4461\n",
      "Epoch [12/50], Train Loss: 0.1780, Val Loss: 0.4426\n",
      "Epoch [13/50], Train Loss: 0.1761, Val Loss: 0.4393\n",
      "Epoch [14/50], Train Loss: 0.1741, Val Loss: 0.4359\n",
      "Epoch [15/50], Train Loss: 0.1722, Val Loss: 0.4326\n",
      "Epoch [16/50], Train Loss: 0.1704, Val Loss: 0.4294\n",
      "Epoch [17/50], Train Loss: 0.1686, Val Loss: 0.4262\n",
      "Epoch [18/50], Train Loss: 0.1668, Val Loss: 0.4231\n",
      "Epoch [19/50], Train Loss: 0.1650, Val Loss: 0.4200\n",
      "Epoch [20/50], Train Loss: 0.1633, Val Loss: 0.4170\n",
      "Epoch [21/50], Train Loss: 0.1616, Val Loss: 0.4140\n",
      "Epoch [22/50], Train Loss: 0.1600, Val Loss: 0.4111\n",
      "Epoch [23/50], Train Loss: 0.1584, Val Loss: 0.4083\n",
      "Epoch [24/50], Train Loss: 0.1568, Val Loss: 0.4054\n",
      "Epoch [25/50], Train Loss: 0.1552, Val Loss: 0.4026\n",
      "Epoch [26/50], Train Loss: 0.1537, Val Loss: 0.3999\n",
      "Epoch [27/50], Train Loss: 0.1522, Val Loss: 0.3972\n",
      "Epoch [28/50], Train Loss: 0.1507, Val Loss: 0.3945\n",
      "Epoch [29/50], Train Loss: 0.1492, Val Loss: 0.3918\n",
      "Epoch [30/50], Train Loss: 0.1478, Val Loss: 0.3893\n",
      "Epoch [31/50], Train Loss: 0.1464, Val Loss: 0.3867\n",
      "Epoch [32/50], Train Loss: 0.1450, Val Loss: 0.3842\n",
      "Epoch [33/50], Train Loss: 0.1436, Val Loss: 0.3817\n",
      "Epoch [34/50], Train Loss: 0.1423, Val Loss: 0.3792\n",
      "Epoch [35/50], Train Loss: 0.1409, Val Loss: 0.3768\n",
      "Epoch [36/50], Train Loss: 0.1396, Val Loss: 0.3744\n",
      "Epoch [37/50], Train Loss: 0.1383, Val Loss: 0.3720\n",
      "Epoch [38/50], Train Loss: 0.1371, Val Loss: 0.3697\n",
      "Epoch [39/50], Train Loss: 0.1358, Val Loss: 0.3674\n",
      "Epoch [40/50], Train Loss: 0.1346, Val Loss: 0.3651\n",
      "Epoch [41/50], Train Loss: 0.1334, Val Loss: 0.3628\n",
      "Epoch [42/50], Train Loss: 0.1322, Val Loss: 0.3606\n",
      "Epoch [43/50], Train Loss: 0.1310, Val Loss: 0.3584\n",
      "Epoch [44/50], Train Loss: 0.1299, Val Loss: 0.3562\n",
      "Epoch [45/50], Train Loss: 0.1288, Val Loss: 0.3541\n",
      "Epoch [46/50], Train Loss: 0.1276, Val Loss: 0.3520\n",
      "Epoch [47/50], Train Loss: 0.1265, Val Loss: 0.3499\n",
      "Epoch [48/50], Train Loss: 0.1254, Val Loss: 0.3478\n",
      "Epoch [49/50], Train Loss: 0.1244, Val Loss: 0.3458\n",
      "Epoch [50/50], Train Loss: 0.1233, Val Loss: 0.3437\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=128, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1235, Val Loss: 0.3350\n",
      "Epoch [2/50], Train Loss: 0.1217, Val Loss: 0.3324\n",
      "Epoch [3/50], Train Loss: 0.1198, Val Loss: 0.3300\n",
      "Epoch [4/50], Train Loss: 0.1190, Val Loss: 0.3275\n",
      "Epoch [5/50], Train Loss: 0.1183, Val Loss: 0.3251\n",
      "Epoch [6/50], Train Loss: 0.1153, Val Loss: 0.3227\n",
      "Epoch [7/50], Train Loss: 0.1152, Val Loss: 0.3203\n",
      "Epoch [8/50], Train Loss: 0.1147, Val Loss: 0.3180\n",
      "Epoch [9/50], Train Loss: 0.1123, Val Loss: 0.3157\n",
      "Epoch [10/50], Train Loss: 0.1116, Val Loss: 0.3133\n",
      "Epoch [11/50], Train Loss: 0.1108, Val Loss: 0.3111\n",
      "Epoch [12/50], Train Loss: 0.1104, Val Loss: 0.3088\n",
      "Epoch [13/50], Train Loss: 0.1081, Val Loss: 0.3066\n",
      "Epoch [14/50], Train Loss: 0.1072, Val Loss: 0.3044\n",
      "Epoch [15/50], Train Loss: 0.1053, Val Loss: 0.3022\n",
      "Epoch [16/50], Train Loss: 0.1048, Val Loss: 0.3000\n",
      "Epoch [17/50], Train Loss: 0.1034, Val Loss: 0.2979\n",
      "Epoch [18/50], Train Loss: 0.1024, Val Loss: 0.2958\n",
      "Epoch [19/50], Train Loss: 0.1021, Val Loss: 0.2937\n",
      "Epoch [20/50], Train Loss: 0.1003, Val Loss: 0.2916\n",
      "Epoch [21/50], Train Loss: 0.0993, Val Loss: 0.2896\n",
      "Epoch [22/50], Train Loss: 0.0987, Val Loss: 0.2876\n",
      "Epoch [23/50], Train Loss: 0.0971, Val Loss: 0.2856\n",
      "Epoch [24/50], Train Loss: 0.0973, Val Loss: 0.2836\n",
      "Epoch [25/50], Train Loss: 0.0955, Val Loss: 0.2816\n",
      "Epoch [26/50], Train Loss: 0.0938, Val Loss: 0.2797\n",
      "Epoch [27/50], Train Loss: 0.0941, Val Loss: 0.2778\n",
      "Epoch [28/50], Train Loss: 0.0930, Val Loss: 0.2759\n",
      "Epoch [29/50], Train Loss: 0.0914, Val Loss: 0.2740\n",
      "Epoch [30/50], Train Loss: 0.0901, Val Loss: 0.2722\n",
      "Epoch [31/50], Train Loss: 0.0900, Val Loss: 0.2703\n",
      "Epoch [32/50], Train Loss: 0.0894, Val Loss: 0.2685\n",
      "Epoch [33/50], Train Loss: 0.0879, Val Loss: 0.2667\n",
      "Epoch [34/50], Train Loss: 0.0875, Val Loss: 0.2649\n",
      "Epoch [35/50], Train Loss: 0.0858, Val Loss: 0.2631\n",
      "Epoch [36/50], Train Loss: 0.0858, Val Loss: 0.2614\n",
      "Epoch [37/50], Train Loss: 0.0843, Val Loss: 0.2596\n",
      "Epoch [38/50], Train Loss: 0.0846, Val Loss: 0.2579\n",
      "Epoch [39/50], Train Loss: 0.0836, Val Loss: 0.2562\n",
      "Epoch [40/50], Train Loss: 0.0828, Val Loss: 0.2545\n",
      "Epoch [41/50], Train Loss: 0.0818, Val Loss: 0.2529\n",
      "Epoch [42/50], Train Loss: 0.0816, Val Loss: 0.2512\n",
      "Epoch [43/50], Train Loss: 0.0794, Val Loss: 0.2496\n",
      "Epoch [44/50], Train Loss: 0.0800, Val Loss: 0.2480\n",
      "Epoch [45/50], Train Loss: 0.0798, Val Loss: 0.2464\n",
      "Epoch [46/50], Train Loss: 0.0781, Val Loss: 0.2448\n",
      "Epoch [47/50], Train Loss: 0.0774, Val Loss: 0.2432\n",
      "Epoch [48/50], Train Loss: 0.0774, Val Loss: 0.2416\n",
      "Epoch [49/50], Train Loss: 0.0765, Val Loss: 0.2401\n",
      "Epoch [50/50], Train Loss: 0.0763, Val Loss: 0.2386\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=128, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1793, Val Loss: 0.4404\n",
      "Epoch [2/50], Train Loss: 0.1788, Val Loss: 0.4368\n",
      "Epoch [3/50], Train Loss: 0.1764, Val Loss: 0.4332\n",
      "Epoch [4/50], Train Loss: 0.1723, Val Loss: 0.4298\n",
      "Epoch [5/50], Train Loss: 0.1744, Val Loss: 0.4263\n",
      "Epoch [6/50], Train Loss: 0.1702, Val Loss: 0.4229\n",
      "Epoch [7/50], Train Loss: 0.1705, Val Loss: 0.4196\n",
      "Epoch [8/50], Train Loss: 0.1686, Val Loss: 0.4163\n",
      "Epoch [9/50], Train Loss: 0.1674, Val Loss: 0.4130\n",
      "Epoch [10/50], Train Loss: 0.1622, Val Loss: 0.4099\n",
      "Epoch [11/50], Train Loss: 0.1629, Val Loss: 0.4067\n",
      "Epoch [12/50], Train Loss: 0.1615, Val Loss: 0.4035\n",
      "Epoch [13/50], Train Loss: 0.1564, Val Loss: 0.4005\n",
      "Epoch [14/50], Train Loss: 0.1582, Val Loss: 0.3974\n",
      "Epoch [15/50], Train Loss: 0.1547, Val Loss: 0.3944\n",
      "Epoch [16/50], Train Loss: 0.1541, Val Loss: 0.3914\n",
      "Epoch [17/50], Train Loss: 0.1501, Val Loss: 0.3885\n",
      "Epoch [18/50], Train Loss: 0.1508, Val Loss: 0.3856\n",
      "Epoch [19/50], Train Loss: 0.1487, Val Loss: 0.3828\n",
      "Epoch [20/50], Train Loss: 0.1479, Val Loss: 0.3799\n",
      "Epoch [21/50], Train Loss: 0.1470, Val Loss: 0.3771\n",
      "Epoch [22/50], Train Loss: 0.1434, Val Loss: 0.3743\n",
      "Epoch [23/50], Train Loss: 0.1440, Val Loss: 0.3715\n",
      "Epoch [24/50], Train Loss: 0.1413, Val Loss: 0.3688\n",
      "Epoch [25/50], Train Loss: 0.1417, Val Loss: 0.3661\n",
      "Epoch [26/50], Train Loss: 0.1390, Val Loss: 0.3635\n",
      "Epoch [27/50], Train Loss: 0.1383, Val Loss: 0.3608\n",
      "Epoch [28/50], Train Loss: 0.1375, Val Loss: 0.3583\n",
      "Epoch [29/50], Train Loss: 0.1359, Val Loss: 0.3557\n",
      "Epoch [30/50], Train Loss: 0.1351, Val Loss: 0.3532\n",
      "Epoch [31/50], Train Loss: 0.1337, Val Loss: 0.3507\n",
      "Epoch [32/50], Train Loss: 0.1320, Val Loss: 0.3482\n",
      "Epoch [33/50], Train Loss: 0.1295, Val Loss: 0.3457\n",
      "Epoch [34/50], Train Loss: 0.1298, Val Loss: 0.3433\n",
      "Epoch [35/50], Train Loss: 0.1303, Val Loss: 0.3409\n",
      "Epoch [36/50], Train Loss: 0.1276, Val Loss: 0.3385\n",
      "Epoch [37/50], Train Loss: 0.1248, Val Loss: 0.3361\n",
      "Epoch [38/50], Train Loss: 0.1253, Val Loss: 0.3338\n",
      "Epoch [39/50], Train Loss: 0.1235, Val Loss: 0.3315\n",
      "Epoch [40/50], Train Loss: 0.1233, Val Loss: 0.3292\n",
      "Epoch [41/50], Train Loss: 0.1216, Val Loss: 0.3270\n",
      "Epoch [42/50], Train Loss: 0.1219, Val Loss: 0.3247\n",
      "Epoch [43/50], Train Loss: 0.1213, Val Loss: 0.3225\n",
      "Epoch [44/50], Train Loss: 0.1188, Val Loss: 0.3203\n",
      "Epoch [45/50], Train Loss: 0.1196, Val Loss: 0.3181\n",
      "Epoch [46/50], Train Loss: 0.1144, Val Loss: 0.3159\n",
      "Epoch [47/50], Train Loss: 0.1150, Val Loss: 0.3138\n",
      "Epoch [48/50], Train Loss: 0.1126, Val Loss: 0.3117\n",
      "Epoch [49/50], Train Loss: 0.1134, Val Loss: 0.3096\n",
      "Epoch [50/50], Train Loss: 0.1133, Val Loss: 0.3075\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=16, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1997, Val Loss: 0.4498\n",
      "Epoch [2/50], Train Loss: 0.1818, Val Loss: 0.4204\n",
      "Epoch [3/50], Train Loss: 0.1659, Val Loss: 0.3930\n",
      "Epoch [4/50], Train Loss: 0.1508, Val Loss: 0.3664\n",
      "Epoch [5/50], Train Loss: 0.1364, Val Loss: 0.3401\n",
      "Epoch [6/50], Train Loss: 0.1225, Val Loss: 0.3139\n",
      "Epoch [7/50], Train Loss: 0.1091, Val Loss: 0.2880\n",
      "Epoch [8/50], Train Loss: 0.0964, Val Loss: 0.2627\n",
      "Epoch [9/50], Train Loss: 0.0846, Val Loss: 0.2382\n",
      "Epoch [10/50], Train Loss: 0.0738, Val Loss: 0.2151\n",
      "Epoch [11/50], Train Loss: 0.0643, Val Loss: 0.1938\n",
      "Epoch [12/50], Train Loss: 0.0562, Val Loss: 0.1748\n",
      "Epoch [13/50], Train Loss: 0.0496, Val Loss: 0.1584\n",
      "Epoch [14/50], Train Loss: 0.0444, Val Loss: 0.1447\n",
      "Epoch [15/50], Train Loss: 0.0406, Val Loss: 0.1335\n",
      "Epoch [16/50], Train Loss: 0.0378, Val Loss: 0.1247\n",
      "Epoch [17/50], Train Loss: 0.0357, Val Loss: 0.1177\n",
      "Epoch [18/50], Train Loss: 0.0343, Val Loss: 0.1122\n",
      "Epoch [19/50], Train Loss: 0.0332, Val Loss: 0.1078\n",
      "Epoch [20/50], Train Loss: 0.0323, Val Loss: 0.1042\n",
      "Epoch [21/50], Train Loss: 0.0317, Val Loss: 0.1011\n",
      "Epoch [22/50], Train Loss: 0.0311, Val Loss: 0.0985\n",
      "Epoch [23/50], Train Loss: 0.0306, Val Loss: 0.0962\n",
      "Epoch [24/50], Train Loss: 0.0302, Val Loss: 0.0941\n",
      "Epoch [25/50], Train Loss: 0.0298, Val Loss: 0.0922\n",
      "Epoch [26/50], Train Loss: 0.0294, Val Loss: 0.0905\n",
      "Epoch [27/50], Train Loss: 0.0290, Val Loss: 0.0888\n",
      "Epoch [28/50], Train Loss: 0.0287, Val Loss: 0.0872\n",
      "Epoch [29/50], Train Loss: 0.0283, Val Loss: 0.0857\n",
      "Epoch [30/50], Train Loss: 0.0280, Val Loss: 0.0842\n",
      "Epoch [31/50], Train Loss: 0.0276, Val Loss: 0.0827\n",
      "Epoch [32/50], Train Loss: 0.0273, Val Loss: 0.0813\n",
      "Epoch [33/50], Train Loss: 0.0269, Val Loss: 0.0800\n",
      "Epoch [34/50], Train Loss: 0.0266, Val Loss: 0.0786\n",
      "Epoch [35/50], Train Loss: 0.0262, Val Loss: 0.0773\n",
      "Epoch [36/50], Train Loss: 0.0259, Val Loss: 0.0760\n",
      "Epoch [37/50], Train Loss: 0.0256, Val Loss: 0.0747\n",
      "Epoch [38/50], Train Loss: 0.0252, Val Loss: 0.0734\n",
      "Epoch [39/50], Train Loss: 0.0249, Val Loss: 0.0721\n",
      "Epoch [40/50], Train Loss: 0.0245, Val Loss: 0.0709\n",
      "Epoch [41/50], Train Loss: 0.0242, Val Loss: 0.0696\n",
      "Epoch [42/50], Train Loss: 0.0239, Val Loss: 0.0684\n",
      "Epoch [43/50], Train Loss: 0.0235, Val Loss: 0.0671\n",
      "Epoch [44/50], Train Loss: 0.0232, Val Loss: 0.0659\n",
      "Epoch [45/50], Train Loss: 0.0228, Val Loss: 0.0646\n",
      "Epoch [46/50], Train Loss: 0.0225, Val Loss: 0.0634\n",
      "Epoch [47/50], Train Loss: 0.0221, Val Loss: 0.0622\n",
      "Epoch [48/50], Train Loss: 0.0218, Val Loss: 0.0610\n",
      "Epoch [49/50], Train Loss: 0.0214, Val Loss: 0.0597\n",
      "Epoch [50/50], Train Loss: 0.0211, Val Loss: 0.0585\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=16, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.2581, Val Loss: 0.5641\n",
      "Epoch [2/50], Train Loss: 0.2329, Val Loss: 0.5260\n",
      "Epoch [3/50], Train Loss: 0.2163, Val Loss: 0.4933\n",
      "Epoch [4/50], Train Loss: 0.1983, Val Loss: 0.4641\n",
      "Epoch [5/50], Train Loss: 0.1846, Val Loss: 0.4374\n",
      "Epoch [6/50], Train Loss: 0.1709, Val Loss: 0.4126\n",
      "Epoch [7/50], Train Loss: 0.1587, Val Loss: 0.3893\n",
      "Epoch [8/50], Train Loss: 0.1466, Val Loss: 0.3669\n",
      "Epoch [9/50], Train Loss: 0.1370, Val Loss: 0.3455\n",
      "Epoch [10/50], Train Loss: 0.1260, Val Loss: 0.3250\n",
      "Epoch [11/50], Train Loss: 0.1187, Val Loss: 0.3051\n",
      "Epoch [12/50], Train Loss: 0.1082, Val Loss: 0.2861\n",
      "Epoch [13/50], Train Loss: 0.0995, Val Loss: 0.2677\n",
      "Epoch [14/50], Train Loss: 0.0943, Val Loss: 0.2499\n",
      "Epoch [15/50], Train Loss: 0.0839, Val Loss: 0.2329\n",
      "Epoch [16/50], Train Loss: 0.0788, Val Loss: 0.2169\n",
      "Epoch [17/50], Train Loss: 0.0717, Val Loss: 0.2017\n",
      "Epoch [18/50], Train Loss: 0.0667, Val Loss: 0.1877\n",
      "Epoch [19/50], Train Loss: 0.0627, Val Loss: 0.1746\n",
      "Epoch [20/50], Train Loss: 0.0570, Val Loss: 0.1625\n",
      "Epoch [21/50], Train Loss: 0.0551, Val Loss: 0.1516\n",
      "Epoch [22/50], Train Loss: 0.0517, Val Loss: 0.1418\n",
      "Epoch [23/50], Train Loss: 0.0482, Val Loss: 0.1329\n",
      "Epoch [24/50], Train Loss: 0.0460, Val Loss: 0.1248\n",
      "Epoch [25/50], Train Loss: 0.0442, Val Loss: 0.1176\n",
      "Epoch [26/50], Train Loss: 0.0426, Val Loss: 0.1114\n",
      "Epoch [27/50], Train Loss: 0.0437, Val Loss: 0.1059\n",
      "Epoch [28/50], Train Loss: 0.0412, Val Loss: 0.1012\n",
      "Epoch [29/50], Train Loss: 0.0412, Val Loss: 0.0971\n",
      "Epoch [30/50], Train Loss: 0.0407, Val Loss: 0.0930\n",
      "Epoch [31/50], Train Loss: 0.0396, Val Loss: 0.0900\n",
      "Epoch [32/50], Train Loss: 0.0389, Val Loss: 0.0869\n",
      "Epoch [33/50], Train Loss: 0.0385, Val Loss: 0.0842\n",
      "Epoch [34/50], Train Loss: 0.0377, Val Loss: 0.0819\n",
      "Epoch [35/50], Train Loss: 0.0367, Val Loss: 0.0797\n",
      "Epoch [36/50], Train Loss: 0.0377, Val Loss: 0.0776\n",
      "Epoch [37/50], Train Loss: 0.0354, Val Loss: 0.0755\n",
      "Epoch [38/50], Train Loss: 0.0367, Val Loss: 0.0737\n",
      "Epoch [39/50], Train Loss: 0.0357, Val Loss: 0.0720\n",
      "Epoch [40/50], Train Loss: 0.0354, Val Loss: 0.0702\n",
      "Epoch [41/50], Train Loss: 0.0329, Val Loss: 0.0687\n",
      "Epoch [42/50], Train Loss: 0.0330, Val Loss: 0.0671\n",
      "Epoch [43/50], Train Loss: 0.0336, Val Loss: 0.0656\n",
      "Epoch [44/50], Train Loss: 0.0320, Val Loss: 0.0639\n",
      "Epoch [45/50], Train Loss: 0.0315, Val Loss: 0.0625\n",
      "Epoch [46/50], Train Loss: 0.0320, Val Loss: 0.0612\n",
      "Epoch [47/50], Train Loss: 0.0320, Val Loss: 0.0595\n",
      "Epoch [48/50], Train Loss: 0.0305, Val Loss: 0.0577\n",
      "Epoch [49/50], Train Loss: 0.0305, Val Loss: 0.0563\n",
      "Epoch [50/50], Train Loss: 0.0296, Val Loss: 0.0546\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=16, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.2511, Val Loss: 0.5492\n",
      "Epoch [2/50], Train Loss: 0.2321, Val Loss: 0.5177\n",
      "Epoch [3/50], Train Loss: 0.2166, Val Loss: 0.4883\n",
      "Epoch [4/50], Train Loss: 0.2005, Val Loss: 0.4604\n",
      "Epoch [5/50], Train Loss: 0.1846, Val Loss: 0.4341\n",
      "Epoch [6/50], Train Loss: 0.1719, Val Loss: 0.4085\n",
      "Epoch [7/50], Train Loss: 0.1638, Val Loss: 0.3831\n",
      "Epoch [8/50], Train Loss: 0.1490, Val Loss: 0.3596\n",
      "Epoch [9/50], Train Loss: 0.1408, Val Loss: 0.3374\n",
      "Epoch [10/50], Train Loss: 0.1304, Val Loss: 0.3160\n",
      "Epoch [11/50], Train Loss: 0.1238, Val Loss: 0.2955\n",
      "Epoch [12/50], Train Loss: 0.1140, Val Loss: 0.2764\n",
      "Epoch [13/50], Train Loss: 0.1088, Val Loss: 0.2585\n",
      "Epoch [14/50], Train Loss: 0.1033, Val Loss: 0.2426\n",
      "Epoch [15/50], Train Loss: 0.0953, Val Loss: 0.2273\n",
      "Epoch [16/50], Train Loss: 0.0917, Val Loss: 0.2137\n",
      "Epoch [17/50], Train Loss: 0.0854, Val Loss: 0.2008\n",
      "Epoch [18/50], Train Loss: 0.0816, Val Loss: 0.1888\n",
      "Epoch [19/50], Train Loss: 0.0786, Val Loss: 0.1778\n",
      "Epoch [20/50], Train Loss: 0.0751, Val Loss: 0.1674\n",
      "Epoch [21/50], Train Loss: 0.0719, Val Loss: 0.1576\n",
      "Epoch [22/50], Train Loss: 0.0703, Val Loss: 0.1488\n",
      "Epoch [23/50], Train Loss: 0.0680, Val Loss: 0.1400\n",
      "Epoch [24/50], Train Loss: 0.0639, Val Loss: 0.1328\n",
      "Epoch [25/50], Train Loss: 0.0630, Val Loss: 0.1260\n",
      "Epoch [26/50], Train Loss: 0.0618, Val Loss: 0.1193\n",
      "Epoch [27/50], Train Loss: 0.0599, Val Loss: 0.1141\n",
      "Epoch [28/50], Train Loss: 0.0573, Val Loss: 0.1093\n",
      "Epoch [29/50], Train Loss: 0.0564, Val Loss: 0.1045\n",
      "Epoch [30/50], Train Loss: 0.0538, Val Loss: 0.0997\n",
      "Epoch [31/50], Train Loss: 0.0529, Val Loss: 0.0960\n",
      "Epoch [32/50], Train Loss: 0.0530, Val Loss: 0.0930\n",
      "Epoch [33/50], Train Loss: 0.0509, Val Loss: 0.0898\n",
      "Epoch [34/50], Train Loss: 0.0499, Val Loss: 0.0864\n",
      "Epoch [35/50], Train Loss: 0.0475, Val Loss: 0.0840\n",
      "Epoch [36/50], Train Loss: 0.0466, Val Loss: 0.0807\n",
      "Epoch [37/50], Train Loss: 0.0473, Val Loss: 0.0788\n",
      "Epoch [38/50], Train Loss: 0.0464, Val Loss: 0.0763\n",
      "Epoch [39/50], Train Loss: 0.0449, Val Loss: 0.0737\n",
      "Epoch [40/50], Train Loss: 0.0436, Val Loss: 0.0711\n",
      "Epoch [41/50], Train Loss: 0.0446, Val Loss: 0.0684\n",
      "Epoch [42/50], Train Loss: 0.0450, Val Loss: 0.0665\n",
      "Epoch [43/50], Train Loss: 0.0438, Val Loss: 0.0647\n",
      "Epoch [44/50], Train Loss: 0.0450, Val Loss: 0.0618\n",
      "Epoch [45/50], Train Loss: 0.0415, Val Loss: 0.0593\n",
      "Epoch [46/50], Train Loss: 0.0403, Val Loss: 0.0577\n",
      "Epoch [47/50], Train Loss: 0.0404, Val Loss: 0.0560\n",
      "Epoch [48/50], Train Loss: 0.0392, Val Loss: 0.0547\n",
      "Epoch [49/50], Train Loss: 0.0381, Val Loss: 0.0535\n",
      "Epoch [50/50], Train Loss: 0.0388, Val Loss: 0.0524\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=16, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0970, Val Loss: 0.2318\n",
      "Epoch [2/50], Train Loss: 0.0777, Val Loss: 0.2022\n",
      "Epoch [3/50], Train Loss: 0.0640, Val Loss: 0.1766\n",
      "Epoch [4/50], Train Loss: 0.0543, Val Loss: 0.1552\n",
      "Epoch [5/50], Train Loss: 0.0476, Val Loss: 0.1380\n",
      "Epoch [6/50], Train Loss: 0.0429, Val Loss: 0.1245\n",
      "Epoch [7/50], Train Loss: 0.0396, Val Loss: 0.1141\n",
      "Epoch [8/50], Train Loss: 0.0373, Val Loss: 0.1062\n",
      "Epoch [9/50], Train Loss: 0.0356, Val Loss: 0.1000\n",
      "Epoch [10/50], Train Loss: 0.0343, Val Loss: 0.0953\n",
      "Epoch [11/50], Train Loss: 0.0333, Val Loss: 0.0915\n",
      "Epoch [12/50], Train Loss: 0.0325, Val Loss: 0.0886\n",
      "Epoch [13/50], Train Loss: 0.0318, Val Loss: 0.0861\n",
      "Epoch [14/50], Train Loss: 0.0311, Val Loss: 0.0839\n",
      "Epoch [15/50], Train Loss: 0.0305, Val Loss: 0.0821\n",
      "Epoch [16/50], Train Loss: 0.0299, Val Loss: 0.0804\n",
      "Epoch [17/50], Train Loss: 0.0293, Val Loss: 0.0789\n",
      "Epoch [18/50], Train Loss: 0.0287, Val Loss: 0.0774\n",
      "Epoch [19/50], Train Loss: 0.0281, Val Loss: 0.0759\n",
      "Epoch [20/50], Train Loss: 0.0275, Val Loss: 0.0745\n",
      "Epoch [21/50], Train Loss: 0.0269, Val Loss: 0.0730\n",
      "Epoch [22/50], Train Loss: 0.0262, Val Loss: 0.0715\n",
      "Epoch [23/50], Train Loss: 0.0255, Val Loss: 0.0701\n",
      "Epoch [24/50], Train Loss: 0.0248, Val Loss: 0.0686\n",
      "Epoch [25/50], Train Loss: 0.0241, Val Loss: 0.0670\n",
      "Epoch [26/50], Train Loss: 0.0233, Val Loss: 0.0654\n",
      "Epoch [27/50], Train Loss: 0.0225, Val Loss: 0.0638\n",
      "Epoch [28/50], Train Loss: 0.0217, Val Loss: 0.0621\n",
      "Epoch [29/50], Train Loss: 0.0209, Val Loss: 0.0605\n",
      "Epoch [30/50], Train Loss: 0.0201, Val Loss: 0.0587\n",
      "Epoch [31/50], Train Loss: 0.0192, Val Loss: 0.0569\n",
      "Epoch [32/50], Train Loss: 0.0184, Val Loss: 0.0551\n",
      "Epoch [33/50], Train Loss: 0.0175, Val Loss: 0.0532\n",
      "Epoch [34/50], Train Loss: 0.0166, Val Loss: 0.0513\n",
      "Epoch [35/50], Train Loss: 0.0158, Val Loss: 0.0494\n",
      "Epoch [36/50], Train Loss: 0.0150, Val Loss: 0.0474\n",
      "Epoch [37/50], Train Loss: 0.0141, Val Loss: 0.0454\n",
      "Epoch [38/50], Train Loss: 0.0133, Val Loss: 0.0434\n",
      "Epoch [39/50], Train Loss: 0.0126, Val Loss: 0.0413\n",
      "Epoch [40/50], Train Loss: 0.0119, Val Loss: 0.0393\n",
      "Epoch [41/50], Train Loss: 0.0112, Val Loss: 0.0372\n",
      "Epoch [42/50], Train Loss: 0.0105, Val Loss: 0.0352\n",
      "Epoch [43/50], Train Loss: 0.0099, Val Loss: 0.0332\n",
      "Epoch [44/50], Train Loss: 0.0093, Val Loss: 0.0313\n",
      "Epoch [45/50], Train Loss: 0.0088, Val Loss: 0.0294\n",
      "Epoch [46/50], Train Loss: 0.0083, Val Loss: 0.0277\n",
      "Epoch [47/50], Train Loss: 0.0078, Val Loss: 0.0259\n",
      "Epoch [48/50], Train Loss: 0.0074, Val Loss: 0.0243\n",
      "Epoch [49/50], Train Loss: 0.0070, Val Loss: 0.0228\n",
      "Epoch [50/50], Train Loss: 0.0066, Val Loss: 0.0215\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=16, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1978, Val Loss: 0.3653\n",
      "Epoch [2/50], Train Loss: 0.1615, Val Loss: 0.3153\n",
      "Epoch [3/50], Train Loss: 0.1327, Val Loss: 0.2729\n",
      "Epoch [4/50], Train Loss: 0.1097, Val Loss: 0.2365\n",
      "Epoch [5/50], Train Loss: 0.0934, Val Loss: 0.2044\n",
      "Epoch [6/50], Train Loss: 0.0794, Val Loss: 0.1773\n",
      "Epoch [7/50], Train Loss: 0.0696, Val Loss: 0.1555\n",
      "Epoch [8/50], Train Loss: 0.0625, Val Loss: 0.1376\n",
      "Epoch [9/50], Train Loss: 0.0592, Val Loss: 0.1240\n",
      "Epoch [10/50], Train Loss: 0.0536, Val Loss: 0.1136\n",
      "Epoch [11/50], Train Loss: 0.0526, Val Loss: 0.1060\n",
      "Epoch [12/50], Train Loss: 0.0508, Val Loss: 0.0999\n",
      "Epoch [13/50], Train Loss: 0.0492, Val Loss: 0.0953\n",
      "Epoch [14/50], Train Loss: 0.0486, Val Loss: 0.0917\n",
      "Epoch [15/50], Train Loss: 0.0470, Val Loss: 0.0885\n",
      "Epoch [16/50], Train Loss: 0.0468, Val Loss: 0.0855\n",
      "Epoch [17/50], Train Loss: 0.0460, Val Loss: 0.0832\n",
      "Epoch [18/50], Train Loss: 0.0449, Val Loss: 0.0808\n",
      "Epoch [19/50], Train Loss: 0.0440, Val Loss: 0.0787\n",
      "Epoch [20/50], Train Loss: 0.0441, Val Loss: 0.0765\n",
      "Epoch [21/50], Train Loss: 0.0426, Val Loss: 0.0742\n",
      "Epoch [22/50], Train Loss: 0.0403, Val Loss: 0.0716\n",
      "Epoch [23/50], Train Loss: 0.0411, Val Loss: 0.0700\n",
      "Epoch [24/50], Train Loss: 0.0409, Val Loss: 0.0679\n",
      "Epoch [25/50], Train Loss: 0.0391, Val Loss: 0.0657\n",
      "Epoch [26/50], Train Loss: 0.0381, Val Loss: 0.0641\n",
      "Epoch [27/50], Train Loss: 0.0372, Val Loss: 0.0627\n",
      "Epoch [28/50], Train Loss: 0.0372, Val Loss: 0.0606\n",
      "Epoch [29/50], Train Loss: 0.0364, Val Loss: 0.0578\n",
      "Epoch [30/50], Train Loss: 0.0367, Val Loss: 0.0560\n",
      "Epoch [31/50], Train Loss: 0.0352, Val Loss: 0.0547\n",
      "Epoch [32/50], Train Loss: 0.0330, Val Loss: 0.0528\n",
      "Epoch [33/50], Train Loss: 0.0338, Val Loss: 0.0515\n",
      "Epoch [34/50], Train Loss: 0.0335, Val Loss: 0.0502\n",
      "Epoch [35/50], Train Loss: 0.0318, Val Loss: 0.0483\n",
      "Epoch [36/50], Train Loss: 0.0318, Val Loss: 0.0467\n",
      "Epoch [37/50], Train Loss: 0.0301, Val Loss: 0.0455\n",
      "Epoch [38/50], Train Loss: 0.0316, Val Loss: 0.0443\n",
      "Epoch [39/50], Train Loss: 0.0310, Val Loss: 0.0433\n",
      "Epoch [40/50], Train Loss: 0.0300, Val Loss: 0.0418\n",
      "Epoch [41/50], Train Loss: 0.0309, Val Loss: 0.0412\n",
      "Epoch [42/50], Train Loss: 0.0303, Val Loss: 0.0399\n",
      "Epoch [43/50], Train Loss: 0.0289, Val Loss: 0.0385\n",
      "Epoch [44/50], Train Loss: 0.0294, Val Loss: 0.0381\n",
      "Epoch [45/50], Train Loss: 0.0287, Val Loss: 0.0372\n",
      "Epoch [46/50], Train Loss: 0.0275, Val Loss: 0.0360\n",
      "Epoch [47/50], Train Loss: 0.0274, Val Loss: 0.0355\n",
      "Epoch [48/50], Train Loss: 0.0297, Val Loss: 0.0348\n",
      "Epoch [49/50], Train Loss: 0.0276, Val Loss: 0.0341\n",
      "Epoch [50/50], Train Loss: 0.0261, Val Loss: 0.0332\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=16, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.2080, Val Loss: 0.3627\n",
      "Epoch [2/50], Train Loss: 0.1832, Val Loss: 0.3300\n",
      "Epoch [3/50], Train Loss: 0.1692, Val Loss: 0.3027\n",
      "Epoch [4/50], Train Loss: 0.1487, Val Loss: 0.2787\n",
      "Epoch [5/50], Train Loss: 0.1393, Val Loss: 0.2580\n",
      "Epoch [6/50], Train Loss: 0.1306, Val Loss: 0.2395\n",
      "Epoch [7/50], Train Loss: 0.1206, Val Loss: 0.2217\n",
      "Epoch [8/50], Train Loss: 0.1134, Val Loss: 0.2060\n",
      "Epoch [9/50], Train Loss: 0.1100, Val Loss: 0.1934\n",
      "Epoch [10/50], Train Loss: 0.1000, Val Loss: 0.1802\n",
      "Epoch [11/50], Train Loss: 0.0938, Val Loss: 0.1681\n",
      "Epoch [12/50], Train Loss: 0.0916, Val Loss: 0.1568\n",
      "Epoch [13/50], Train Loss: 0.0894, Val Loss: 0.1468\n",
      "Epoch [14/50], Train Loss: 0.0818, Val Loss: 0.1382\n",
      "Epoch [15/50], Train Loss: 0.0797, Val Loss: 0.1307\n",
      "Epoch [16/50], Train Loss: 0.0786, Val Loss: 0.1232\n",
      "Epoch [17/50], Train Loss: 0.0727, Val Loss: 0.1160\n",
      "Epoch [18/50], Train Loss: 0.0739, Val Loss: 0.1099\n",
      "Epoch [19/50], Train Loss: 0.0714, Val Loss: 0.1052\n",
      "Epoch [20/50], Train Loss: 0.0679, Val Loss: 0.0983\n",
      "Epoch [21/50], Train Loss: 0.0661, Val Loss: 0.0935\n",
      "Epoch [22/50], Train Loss: 0.0632, Val Loss: 0.0875\n",
      "Epoch [23/50], Train Loss: 0.0630, Val Loss: 0.0827\n",
      "Epoch [24/50], Train Loss: 0.0597, Val Loss: 0.0783\n",
      "Epoch [25/50], Train Loss: 0.0578, Val Loss: 0.0760\n",
      "Epoch [26/50], Train Loss: 0.0581, Val Loss: 0.0728\n",
      "Epoch [27/50], Train Loss: 0.0578, Val Loss: 0.0679\n",
      "Epoch [28/50], Train Loss: 0.0569, Val Loss: 0.0641\n",
      "Epoch [29/50], Train Loss: 0.0544, Val Loss: 0.0619\n",
      "Epoch [30/50], Train Loss: 0.0548, Val Loss: 0.0598\n",
      "Epoch [31/50], Train Loss: 0.0520, Val Loss: 0.0568\n",
      "Epoch [32/50], Train Loss: 0.0523, Val Loss: 0.0554\n",
      "Epoch [33/50], Train Loss: 0.0499, Val Loss: 0.0533\n",
      "Epoch [34/50], Train Loss: 0.0506, Val Loss: 0.0527\n",
      "Epoch [35/50], Train Loss: 0.0504, Val Loss: 0.0523\n",
      "Epoch [36/50], Train Loss: 0.0482, Val Loss: 0.0485\n",
      "Epoch [37/50], Train Loss: 0.0455, Val Loss: 0.0471\n",
      "Epoch [38/50], Train Loss: 0.0462, Val Loss: 0.0468\n",
      "Epoch [39/50], Train Loss: 0.0459, Val Loss: 0.0477\n",
      "Epoch [40/50], Train Loss: 0.0469, Val Loss: 0.0458\n",
      "Epoch [41/50], Train Loss: 0.0430, Val Loss: 0.0450\n",
      "Epoch [42/50], Train Loss: 0.0443, Val Loss: 0.0428\n",
      "Epoch [43/50], Train Loss: 0.0433, Val Loss: 0.0420\n",
      "Epoch [44/50], Train Loss: 0.0412, Val Loss: 0.0410\n",
      "Epoch [45/50], Train Loss: 0.0418, Val Loss: 0.0402\n",
      "Epoch [46/50], Train Loss: 0.0409, Val Loss: 0.0405\n",
      "Epoch [47/50], Train Loss: 0.0433, Val Loss: 0.0396\n",
      "Epoch [48/50], Train Loss: 0.0416, Val Loss: 0.0378\n",
      "Epoch [49/50], Train Loss: 0.0395, Val Loss: 0.0378\n",
      "Epoch [50/50], Train Loss: 0.0393, Val Loss: 0.0348\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=16, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1735, Val Loss: 0.4078\n",
      "Epoch [2/50], Train Loss: 0.1423, Val Loss: 0.3575\n",
      "Epoch [3/50], Train Loss: 0.1169, Val Loss: 0.3130\n",
      "Epoch [4/50], Train Loss: 0.0953, Val Loss: 0.2727\n",
      "Epoch [5/50], Train Loss: 0.0773, Val Loss: 0.2363\n",
      "Epoch [6/50], Train Loss: 0.0631, Val Loss: 0.2044\n",
      "Epoch [7/50], Train Loss: 0.0525, Val Loss: 0.1775\n",
      "Epoch [8/50], Train Loss: 0.0451, Val Loss: 0.1558\n",
      "Epoch [9/50], Train Loss: 0.0403, Val Loss: 0.1391\n",
      "Epoch [10/50], Train Loss: 0.0373, Val Loss: 0.1264\n",
      "Epoch [11/50], Train Loss: 0.0355, Val Loss: 0.1170\n",
      "Epoch [12/50], Train Loss: 0.0344, Val Loss: 0.1101\n",
      "Epoch [13/50], Train Loss: 0.0337, Val Loss: 0.1049\n",
      "Epoch [14/50], Train Loss: 0.0331, Val Loss: 0.1009\n",
      "Epoch [15/50], Train Loss: 0.0326, Val Loss: 0.0976\n",
      "Epoch [16/50], Train Loss: 0.0320, Val Loss: 0.0948\n",
      "Epoch [17/50], Train Loss: 0.0315, Val Loss: 0.0924\n",
      "Epoch [18/50], Train Loss: 0.0309, Val Loss: 0.0900\n",
      "Epoch [19/50], Train Loss: 0.0301, Val Loss: 0.0875\n",
      "Epoch [20/50], Train Loss: 0.0292, Val Loss: 0.0849\n",
      "Epoch [21/50], Train Loss: 0.0282, Val Loss: 0.0820\n",
      "Epoch [22/50], Train Loss: 0.0268, Val Loss: 0.0786\n",
      "Epoch [23/50], Train Loss: 0.0251, Val Loss: 0.0746\n",
      "Epoch [24/50], Train Loss: 0.0228, Val Loss: 0.0699\n",
      "Epoch [25/50], Train Loss: 0.0198, Val Loss: 0.0645\n",
      "Epoch [26/50], Train Loss: 0.0165, Val Loss: 0.0586\n",
      "Epoch [27/50], Train Loss: 0.0137, Val Loss: 0.0527\n",
      "Epoch [28/50], Train Loss: 0.0121, Val Loss: 0.0477\n",
      "Epoch [29/50], Train Loss: 0.0112, Val Loss: 0.0441\n",
      "Epoch [30/50], Train Loss: 0.0107, Val Loss: 0.0419\n",
      "Epoch [31/50], Train Loss: 0.0104, Val Loss: 0.0407\n",
      "Epoch [32/50], Train Loss: 0.0101, Val Loss: 0.0399\n",
      "Epoch [33/50], Train Loss: 0.0098, Val Loss: 0.0394\n",
      "Epoch [34/50], Train Loss: 0.0095, Val Loss: 0.0391\n",
      "Epoch [35/50], Train Loss: 0.0093, Val Loss: 0.0389\n",
      "Epoch [36/50], Train Loss: 0.0090, Val Loss: 0.0389\n",
      "Epoch [37/50], Train Loss: 0.0087, Val Loss: 0.0389\n",
      "Epoch [38/50], Train Loss: 0.0084, Val Loss: 0.0389\n",
      "Epoch [39/50], Train Loss: 0.0082, Val Loss: 0.0389\n",
      "Epoch [40/50], Train Loss: 0.0079, Val Loss: 0.0391\n",
      "Epoch [41/50], Train Loss: 0.0077, Val Loss: 0.0391\n",
      "Epoch [42/50], Train Loss: 0.0074, Val Loss: 0.0393\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=16, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1944, Val Loss: 0.4587\n",
      "Epoch [2/50], Train Loss: 0.1691, Val Loss: 0.4256\n",
      "Epoch [3/50], Train Loss: 0.1537, Val Loss: 0.3949\n",
      "Epoch [4/50], Train Loss: 0.1359, Val Loss: 0.3655\n",
      "Epoch [5/50], Train Loss: 0.1245, Val Loss: 0.3361\n",
      "Epoch [6/50], Train Loss: 0.1103, Val Loss: 0.3077\n",
      "Epoch [7/50], Train Loss: 0.1017, Val Loss: 0.2794\n",
      "Epoch [8/50], Train Loss: 0.0903, Val Loss: 0.2523\n",
      "Epoch [9/50], Train Loss: 0.0822, Val Loss: 0.2286\n",
      "Epoch [10/50], Train Loss: 0.0753, Val Loss: 0.2057\n",
      "Epoch [11/50], Train Loss: 0.0705, Val Loss: 0.1869\n",
      "Epoch [12/50], Train Loss: 0.0701, Val Loss: 0.1723\n",
      "Epoch [13/50], Train Loss: 0.0645, Val Loss: 0.1605\n",
      "Epoch [14/50], Train Loss: 0.0640, Val Loss: 0.1522\n",
      "Epoch [15/50], Train Loss: 0.0642, Val Loss: 0.1458\n",
      "Epoch [16/50], Train Loss: 0.0619, Val Loss: 0.1405\n",
      "Epoch [17/50], Train Loss: 0.0606, Val Loss: 0.1364\n",
      "Epoch [18/50], Train Loss: 0.0613, Val Loss: 0.1349\n",
      "Epoch [19/50], Train Loss: 0.0593, Val Loss: 0.1325\n",
      "Epoch [20/50], Train Loss: 0.0618, Val Loss: 0.1306\n",
      "Epoch [21/50], Train Loss: 0.0575, Val Loss: 0.1284\n",
      "Epoch [22/50], Train Loss: 0.0596, Val Loss: 0.1271\n",
      "Epoch [23/50], Train Loss: 0.0580, Val Loss: 0.1261\n",
      "Epoch [24/50], Train Loss: 0.0585, Val Loss: 0.1262\n",
      "Epoch [25/50], Train Loss: 0.0560, Val Loss: 0.1264\n",
      "Epoch [26/50], Train Loss: 0.0567, Val Loss: 0.1259\n",
      "Epoch [27/50], Train Loss: 0.0567, Val Loss: 0.1240\n",
      "Epoch [28/50], Train Loss: 0.0566, Val Loss: 0.1236\n",
      "Epoch [29/50], Train Loss: 0.0543, Val Loss: 0.1225\n",
      "Epoch [30/50], Train Loss: 0.0547, Val Loss: 0.1211\n",
      "Epoch [31/50], Train Loss: 0.0545, Val Loss: 0.1199\n",
      "Epoch [32/50], Train Loss: 0.0532, Val Loss: 0.1187\n",
      "Epoch [33/50], Train Loss: 0.0529, Val Loss: 0.1174\n",
      "Epoch [34/50], Train Loss: 0.0511, Val Loss: 0.1142\n",
      "Epoch [35/50], Train Loss: 0.0510, Val Loss: 0.1121\n",
      "Epoch [36/50], Train Loss: 0.0497, Val Loss: 0.1099\n",
      "Epoch [37/50], Train Loss: 0.0489, Val Loss: 0.1079\n",
      "Epoch [38/50], Train Loss: 0.0501, Val Loss: 0.1045\n",
      "Epoch [39/50], Train Loss: 0.0478, Val Loss: 0.1010\n",
      "Epoch [40/50], Train Loss: 0.0462, Val Loss: 0.0978\n",
      "Epoch [41/50], Train Loss: 0.0460, Val Loss: 0.0944\n",
      "Epoch [42/50], Train Loss: 0.0439, Val Loss: 0.0890\n",
      "Epoch [43/50], Train Loss: 0.0437, Val Loss: 0.0836\n",
      "Epoch [44/50], Train Loss: 0.0411, Val Loss: 0.0783\n",
      "Epoch [45/50], Train Loss: 0.0402, Val Loss: 0.0734\n",
      "Epoch [46/50], Train Loss: 0.0371, Val Loss: 0.0658\n",
      "Epoch [47/50], Train Loss: 0.0373, Val Loss: 0.0595\n",
      "Epoch [48/50], Train Loss: 0.0342, Val Loss: 0.0527\n",
      "Epoch [49/50], Train Loss: 0.0324, Val Loss: 0.0478\n",
      "Epoch [50/50], Train Loss: 0.0325, Val Loss: 0.0418\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=16, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1145, Val Loss: 0.2154\n",
      "Epoch [2/50], Train Loss: 0.1010, Val Loss: 0.1970\n",
      "Epoch [3/50], Train Loss: 0.0973, Val Loss: 0.1847\n",
      "Epoch [4/50], Train Loss: 0.0868, Val Loss: 0.1751\n",
      "Epoch [5/50], Train Loss: 0.0859, Val Loss: 0.1686\n",
      "Epoch [6/50], Train Loss: 0.0844, Val Loss: 0.1629\n",
      "Epoch [7/50], Train Loss: 0.0850, Val Loss: 0.1587\n",
      "Epoch [8/50], Train Loss: 0.0798, Val Loss: 0.1554\n",
      "Epoch [9/50], Train Loss: 0.0756, Val Loss: 0.1516\n",
      "Epoch [10/50], Train Loss: 0.0714, Val Loss: 0.1487\n",
      "Epoch [11/50], Train Loss: 0.0714, Val Loss: 0.1469\n",
      "Epoch [12/50], Train Loss: 0.0692, Val Loss: 0.1452\n",
      "Epoch [13/50], Train Loss: 0.0704, Val Loss: 0.1438\n",
      "Epoch [14/50], Train Loss: 0.0675, Val Loss: 0.1422\n",
      "Epoch [15/50], Train Loss: 0.0671, Val Loss: 0.1404\n",
      "Epoch [16/50], Train Loss: 0.0636, Val Loss: 0.1390\n",
      "Epoch [17/50], Train Loss: 0.0687, Val Loss: 0.1392\n",
      "Epoch [18/50], Train Loss: 0.0634, Val Loss: 0.1385\n",
      "Epoch [19/50], Train Loss: 0.0631, Val Loss: 0.1366\n",
      "Epoch [20/50], Train Loss: 0.0591, Val Loss: 0.1353\n",
      "Epoch [21/50], Train Loss: 0.0611, Val Loss: 0.1337\n",
      "Epoch [22/50], Train Loss: 0.0603, Val Loss: 0.1314\n",
      "Epoch [23/50], Train Loss: 0.0621, Val Loss: 0.1312\n",
      "Epoch [24/50], Train Loss: 0.0595, Val Loss: 0.1305\n",
      "Epoch [25/50], Train Loss: 0.0596, Val Loss: 0.1284\n",
      "Epoch [26/50], Train Loss: 0.0572, Val Loss: 0.1273\n",
      "Epoch [27/50], Train Loss: 0.0586, Val Loss: 0.1264\n",
      "Epoch [28/50], Train Loss: 0.0572, Val Loss: 0.1261\n",
      "Epoch [29/50], Train Loss: 0.0556, Val Loss: 0.1255\n",
      "Epoch [30/50], Train Loss: 0.0540, Val Loss: 0.1238\n",
      "Epoch [31/50], Train Loss: 0.0549, Val Loss: 0.1223\n",
      "Epoch [32/50], Train Loss: 0.0544, Val Loss: 0.1217\n",
      "Epoch [33/50], Train Loss: 0.0549, Val Loss: 0.1208\n",
      "Epoch [34/50], Train Loss: 0.0534, Val Loss: 0.1197\n",
      "Epoch [35/50], Train Loss: 0.0484, Val Loss: 0.1185\n",
      "Epoch [36/50], Train Loss: 0.0511, Val Loss: 0.1169\n",
      "Epoch [37/50], Train Loss: 0.0514, Val Loss: 0.1156\n",
      "Epoch [38/50], Train Loss: 0.0518, Val Loss: 0.1151\n",
      "Epoch [39/50], Train Loss: 0.0502, Val Loss: 0.1144\n",
      "Epoch [40/50], Train Loss: 0.0514, Val Loss: 0.1130\n",
      "Epoch [41/50], Train Loss: 0.0507, Val Loss: 0.1122\n",
      "Epoch [42/50], Train Loss: 0.0468, Val Loss: 0.1101\n",
      "Epoch [43/50], Train Loss: 0.0474, Val Loss: 0.1086\n",
      "Epoch [44/50], Train Loss: 0.0479, Val Loss: 0.1074\n",
      "Epoch [45/50], Train Loss: 0.0468, Val Loss: 0.1064\n",
      "Epoch [46/50], Train Loss: 0.0453, Val Loss: 0.1037\n",
      "Epoch [47/50], Train Loss: 0.0462, Val Loss: 0.1035\n",
      "Epoch [48/50], Train Loss: 0.0445, Val Loss: 0.1016\n",
      "Epoch [49/50], Train Loss: 0.0437, Val Loss: 0.0989\n",
      "Epoch [50/50], Train Loss: 0.0442, Val Loss: 0.0970\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=32, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0874, Val Loss: 0.2835\n",
      "Epoch [2/50], Train Loss: 0.0716, Val Loss: 0.2412\n",
      "Epoch [3/50], Train Loss: 0.0596, Val Loss: 0.2038\n",
      "Epoch [4/50], Train Loss: 0.0498, Val Loss: 0.1699\n",
      "Epoch [5/50], Train Loss: 0.0420, Val Loss: 0.1402\n",
      "Epoch [6/50], Train Loss: 0.0364, Val Loss: 0.1160\n",
      "Epoch [7/50], Train Loss: 0.0326, Val Loss: 0.0981\n",
      "Epoch [8/50], Train Loss: 0.0302, Val Loss: 0.0854\n",
      "Epoch [9/50], Train Loss: 0.0284, Val Loss: 0.0764\n",
      "Epoch [10/50], Train Loss: 0.0269, Val Loss: 0.0695\n",
      "Epoch [11/50], Train Loss: 0.0256, Val Loss: 0.0638\n",
      "Epoch [12/50], Train Loss: 0.0243, Val Loss: 0.0587\n",
      "Epoch [13/50], Train Loss: 0.0229, Val Loss: 0.0539\n",
      "Epoch [14/50], Train Loss: 0.0215, Val Loss: 0.0492\n",
      "Epoch [15/50], Train Loss: 0.0201, Val Loss: 0.0444\n",
      "Epoch [16/50], Train Loss: 0.0185, Val Loss: 0.0395\n",
      "Epoch [17/50], Train Loss: 0.0170, Val Loss: 0.0346\n",
      "Epoch [18/50], Train Loss: 0.0154, Val Loss: 0.0299\n",
      "Epoch [19/50], Train Loss: 0.0141, Val Loss: 0.0259\n",
      "Epoch [20/50], Train Loss: 0.0133, Val Loss: 0.0231\n",
      "Epoch [21/50], Train Loss: 0.0128, Val Loss: 0.0215\n",
      "Epoch [22/50], Train Loss: 0.0124, Val Loss: 0.0206\n",
      "Epoch [23/50], Train Loss: 0.0121, Val Loss: 0.0201\n",
      "Epoch [24/50], Train Loss: 0.0117, Val Loss: 0.0197\n",
      "Epoch [25/50], Train Loss: 0.0114, Val Loss: 0.0194\n",
      "Epoch [26/50], Train Loss: 0.0111, Val Loss: 0.0191\n",
      "Epoch [27/50], Train Loss: 0.0108, Val Loss: 0.0189\n",
      "Epoch [28/50], Train Loss: 0.0105, Val Loss: 0.0187\n",
      "Epoch [29/50], Train Loss: 0.0102, Val Loss: 0.0185\n",
      "Epoch [30/50], Train Loss: 0.0099, Val Loss: 0.0184\n",
      "Epoch [31/50], Train Loss: 0.0096, Val Loss: 0.0183\n",
      "Epoch [32/50], Train Loss: 0.0093, Val Loss: 0.0182\n",
      "Epoch [33/50], Train Loss: 0.0090, Val Loss: 0.0181\n",
      "Epoch [34/50], Train Loss: 0.0088, Val Loss: 0.0180\n",
      "Epoch [35/50], Train Loss: 0.0085, Val Loss: 0.0179\n",
      "Epoch [36/50], Train Loss: 0.0082, Val Loss: 0.0179\n",
      "Epoch [37/50], Train Loss: 0.0079, Val Loss: 0.0178\n",
      "Epoch [38/50], Train Loss: 0.0077, Val Loss: 0.0177\n",
      "Epoch [39/50], Train Loss: 0.0074, Val Loss: 0.0177\n",
      "Epoch [40/50], Train Loss: 0.0071, Val Loss: 0.0176\n",
      "Epoch [41/50], Train Loss: 0.0068, Val Loss: 0.0175\n",
      "Epoch [42/50], Train Loss: 0.0066, Val Loss: 0.0174\n",
      "Epoch [43/50], Train Loss: 0.0063, Val Loss: 0.0173\n",
      "Epoch [44/50], Train Loss: 0.0060, Val Loss: 0.0172\n",
      "Epoch [45/50], Train Loss: 0.0057, Val Loss: 0.0170\n",
      "Epoch [46/50], Train Loss: 0.0054, Val Loss: 0.0169\n",
      "Epoch [47/50], Train Loss: 0.0052, Val Loss: 0.0167\n",
      "Epoch [48/50], Train Loss: 0.0049, Val Loss: 0.0166\n",
      "Epoch [49/50], Train Loss: 0.0046, Val Loss: 0.0164\n",
      "Epoch [50/50], Train Loss: 0.0044, Val Loss: 0.0162\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=32, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1655, Val Loss: 0.4063\n",
      "Epoch [2/50], Train Loss: 0.1378, Val Loss: 0.3579\n",
      "Epoch [3/50], Train Loss: 0.1174, Val Loss: 0.3136\n",
      "Epoch [4/50], Train Loss: 0.0984, Val Loss: 0.2697\n",
      "Epoch [5/50], Train Loss: 0.0812, Val Loss: 0.2271\n",
      "Epoch [6/50], Train Loss: 0.0678, Val Loss: 0.1883\n",
      "Epoch [7/50], Train Loss: 0.0551, Val Loss: 0.1556\n",
      "Epoch [8/50], Train Loss: 0.0472, Val Loss: 0.1298\n",
      "Epoch [9/50], Train Loss: 0.0428, Val Loss: 0.1114\n",
      "Epoch [10/50], Train Loss: 0.0396, Val Loss: 0.0991\n",
      "Epoch [11/50], Train Loss: 0.0387, Val Loss: 0.0903\n",
      "Epoch [12/50], Train Loss: 0.0357, Val Loss: 0.0841\n",
      "Epoch [13/50], Train Loss: 0.0356, Val Loss: 0.0800\n",
      "Epoch [14/50], Train Loss: 0.0343, Val Loss: 0.0758\n",
      "Epoch [15/50], Train Loss: 0.0333, Val Loss: 0.0722\n",
      "Epoch [16/50], Train Loss: 0.0326, Val Loss: 0.0695\n",
      "Epoch [17/50], Train Loss: 0.0312, Val Loss: 0.0664\n",
      "Epoch [18/50], Train Loss: 0.0303, Val Loss: 0.0637\n",
      "Epoch [19/50], Train Loss: 0.0292, Val Loss: 0.0611\n",
      "Epoch [20/50], Train Loss: 0.0287, Val Loss: 0.0588\n",
      "Epoch [21/50], Train Loss: 0.0274, Val Loss: 0.0564\n",
      "Epoch [22/50], Train Loss: 0.0272, Val Loss: 0.0544\n",
      "Epoch [23/50], Train Loss: 0.0263, Val Loss: 0.0518\n",
      "Epoch [24/50], Train Loss: 0.0251, Val Loss: 0.0494\n",
      "Epoch [25/50], Train Loss: 0.0247, Val Loss: 0.0476\n",
      "Epoch [26/50], Train Loss: 0.0238, Val Loss: 0.0449\n",
      "Epoch [27/50], Train Loss: 0.0232, Val Loss: 0.0425\n",
      "Epoch [28/50], Train Loss: 0.0214, Val Loss: 0.0410\n",
      "Epoch [29/50], Train Loss: 0.0214, Val Loss: 0.0389\n",
      "Epoch [30/50], Train Loss: 0.0201, Val Loss: 0.0373\n",
      "Epoch [31/50], Train Loss: 0.0195, Val Loss: 0.0354\n",
      "Epoch [32/50], Train Loss: 0.0188, Val Loss: 0.0333\n",
      "Epoch [33/50], Train Loss: 0.0173, Val Loss: 0.0312\n",
      "Epoch [34/50], Train Loss: 0.0171, Val Loss: 0.0293\n",
      "Epoch [35/50], Train Loss: 0.0160, Val Loss: 0.0273\n",
      "Epoch [36/50], Train Loss: 0.0155, Val Loss: 0.0248\n",
      "Epoch [37/50], Train Loss: 0.0140, Val Loss: 0.0224\n",
      "Epoch [38/50], Train Loss: 0.0127, Val Loss: 0.0207\n",
      "Epoch [39/50], Train Loss: 0.0120, Val Loss: 0.0190\n",
      "Epoch [40/50], Train Loss: 0.0106, Val Loss: 0.0169\n",
      "Epoch [41/50], Train Loss: 0.0105, Val Loss: 0.0151\n",
      "Epoch [42/50], Train Loss: 0.0103, Val Loss: 0.0141\n",
      "Epoch [43/50], Train Loss: 0.0098, Val Loss: 0.0128\n",
      "Epoch [44/50], Train Loss: 0.0098, Val Loss: 0.0119\n",
      "Epoch [45/50], Train Loss: 0.0096, Val Loss: 0.0105\n",
      "Epoch [46/50], Train Loss: 0.0091, Val Loss: 0.0107\n",
      "Epoch [47/50], Train Loss: 0.0092, Val Loss: 0.0102\n",
      "Epoch [48/50], Train Loss: 0.0089, Val Loss: 0.0094\n",
      "Epoch [49/50], Train Loss: 0.0084, Val Loss: 0.0088\n",
      "Epoch [50/50], Train Loss: 0.0086, Val Loss: 0.0087\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=32, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0934, Val Loss: 0.1782\n",
      "Epoch [2/50], Train Loss: 0.0791, Val Loss: 0.1518\n",
      "Epoch [3/50], Train Loss: 0.0708, Val Loss: 0.1303\n",
      "Epoch [4/50], Train Loss: 0.0603, Val Loss: 0.1134\n",
      "Epoch [5/50], Train Loss: 0.0564, Val Loss: 0.1008\n",
      "Epoch [6/50], Train Loss: 0.0528, Val Loss: 0.0907\n",
      "Epoch [7/50], Train Loss: 0.0505, Val Loss: 0.0844\n",
      "Epoch [8/50], Train Loss: 0.0492, Val Loss: 0.0791\n",
      "Epoch [9/50], Train Loss: 0.0457, Val Loss: 0.0751\n",
      "Epoch [10/50], Train Loss: 0.0440, Val Loss: 0.0719\n",
      "Epoch [11/50], Train Loss: 0.0421, Val Loss: 0.0685\n",
      "Epoch [12/50], Train Loss: 0.0418, Val Loss: 0.0656\n",
      "Epoch [13/50], Train Loss: 0.0414, Val Loss: 0.0631\n",
      "Epoch [14/50], Train Loss: 0.0374, Val Loss: 0.0605\n",
      "Epoch [15/50], Train Loss: 0.0394, Val Loss: 0.0575\n",
      "Epoch [16/50], Train Loss: 0.0369, Val Loss: 0.0543\n",
      "Epoch [17/50], Train Loss: 0.0354, Val Loss: 0.0528\n",
      "Epoch [18/50], Train Loss: 0.0355, Val Loss: 0.0507\n",
      "Epoch [19/50], Train Loss: 0.0337, Val Loss: 0.0478\n",
      "Epoch [20/50], Train Loss: 0.0325, Val Loss: 0.0453\n",
      "Epoch [21/50], Train Loss: 0.0313, Val Loss: 0.0432\n",
      "Epoch [22/50], Train Loss: 0.0298, Val Loss: 0.0402\n",
      "Epoch [23/50], Train Loss: 0.0293, Val Loss: 0.0378\n",
      "Epoch [24/50], Train Loss: 0.0288, Val Loss: 0.0355\n",
      "Epoch [25/50], Train Loss: 0.0262, Val Loss: 0.0334\n",
      "Epoch [26/50], Train Loss: 0.0260, Val Loss: 0.0313\n",
      "Epoch [27/50], Train Loss: 0.0254, Val Loss: 0.0289\n",
      "Epoch [28/50], Train Loss: 0.0237, Val Loss: 0.0276\n",
      "Epoch [29/50], Train Loss: 0.0242, Val Loss: 0.0255\n",
      "Epoch [30/50], Train Loss: 0.0226, Val Loss: 0.0234\n",
      "Epoch [31/50], Train Loss: 0.0219, Val Loss: 0.0220\n",
      "Epoch [32/50], Train Loss: 0.0223, Val Loss: 0.0212\n",
      "Epoch [33/50], Train Loss: 0.0205, Val Loss: 0.0209\n",
      "Epoch [34/50], Train Loss: 0.0192, Val Loss: 0.0194\n",
      "Epoch [35/50], Train Loss: 0.0194, Val Loss: 0.0182\n",
      "Epoch [36/50], Train Loss: 0.0188, Val Loss: 0.0178\n",
      "Epoch [37/50], Train Loss: 0.0173, Val Loss: 0.0171\n",
      "Epoch [38/50], Train Loss: 0.0182, Val Loss: 0.0159\n",
      "Epoch [39/50], Train Loss: 0.0180, Val Loss: 0.0138\n",
      "Epoch [40/50], Train Loss: 0.0165, Val Loss: 0.0144\n",
      "Epoch [41/50], Train Loss: 0.0165, Val Loss: 0.0151\n",
      "Epoch [42/50], Train Loss: 0.0168, Val Loss: 0.0136\n",
      "Epoch [43/50], Train Loss: 0.0160, Val Loss: 0.0131\n",
      "Epoch [44/50], Train Loss: 0.0159, Val Loss: 0.0129\n",
      "Epoch [45/50], Train Loss: 0.0157, Val Loss: 0.0122\n",
      "Epoch [46/50], Train Loss: 0.0144, Val Loss: 0.0120\n",
      "Epoch [47/50], Train Loss: 0.0151, Val Loss: 0.0118\n",
      "Epoch [48/50], Train Loss: 0.0145, Val Loss: 0.0118\n",
      "Epoch [49/50], Train Loss: 0.0146, Val Loss: 0.0113\n",
      "Epoch [50/50], Train Loss: 0.0140, Val Loss: 0.0100\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=32, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1296, Val Loss: 0.3120\n",
      "Epoch [2/50], Train Loss: 0.0798, Val Loss: 0.2215\n",
      "Epoch [3/50], Train Loss: 0.0498, Val Loss: 0.1561\n",
      "Epoch [4/50], Train Loss: 0.0386, Val Loss: 0.1202\n",
      "Epoch [5/50], Train Loss: 0.0363, Val Loss: 0.1029\n",
      "Epoch [6/50], Train Loss: 0.0355, Val Loss: 0.0937\n",
      "Epoch [7/50], Train Loss: 0.0347, Val Loss: 0.0876\n",
      "Epoch [8/50], Train Loss: 0.0340, Val Loss: 0.0829\n",
      "Epoch [9/50], Train Loss: 0.0332, Val Loss: 0.0788\n",
      "Epoch [10/50], Train Loss: 0.0324, Val Loss: 0.0750\n",
      "Epoch [11/50], Train Loss: 0.0314, Val Loss: 0.0712\n",
      "Epoch [12/50], Train Loss: 0.0303, Val Loss: 0.0671\n",
      "Epoch [13/50], Train Loss: 0.0292, Val Loss: 0.0628\n",
      "Epoch [14/50], Train Loss: 0.0279, Val Loss: 0.0581\n",
      "Epoch [15/50], Train Loss: 0.0266, Val Loss: 0.0531\n",
      "Epoch [16/50], Train Loss: 0.0250, Val Loss: 0.0477\n",
      "Epoch [17/50], Train Loss: 0.0234, Val Loss: 0.0421\n",
      "Epoch [18/50], Train Loss: 0.0217, Val Loss: 0.0365\n",
      "Epoch [19/50], Train Loss: 0.0201, Val Loss: 0.0313\n",
      "Epoch [20/50], Train Loss: 0.0188, Val Loss: 0.0268\n",
      "Epoch [21/50], Train Loss: 0.0176, Val Loss: 0.0232\n",
      "Epoch [22/50], Train Loss: 0.0167, Val Loss: 0.0206\n",
      "Epoch [23/50], Train Loss: 0.0157, Val Loss: 0.0186\n",
      "Epoch [24/50], Train Loss: 0.0149, Val Loss: 0.0171\n",
      "Epoch [25/50], Train Loss: 0.0141, Val Loss: 0.0161\n",
      "Epoch [26/50], Train Loss: 0.0134, Val Loss: 0.0155\n",
      "Epoch [27/50], Train Loss: 0.0127, Val Loss: 0.0152\n",
      "Epoch [28/50], Train Loss: 0.0121, Val Loss: 0.0152\n",
      "Epoch [29/50], Train Loss: 0.0116, Val Loss: 0.0154\n",
      "Epoch [30/50], Train Loss: 0.0111, Val Loss: 0.0157\n",
      "Epoch [31/50], Train Loss: 0.0107, Val Loss: 0.0161\n",
      "Epoch [32/50], Train Loss: 0.0103, Val Loss: 0.0165\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=32, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1312, Val Loss: 0.3210\n",
      "Epoch [2/50], Train Loss: 0.0960, Val Loss: 0.2569\n",
      "Epoch [3/50], Train Loss: 0.0713, Val Loss: 0.2019\n",
      "Epoch [4/50], Train Loss: 0.0550, Val Loss: 0.1562\n",
      "Epoch [5/50], Train Loss: 0.0447, Val Loss: 0.1216\n",
      "Epoch [6/50], Train Loss: 0.0414, Val Loss: 0.0990\n",
      "Epoch [7/50], Train Loss: 0.0386, Val Loss: 0.0844\n",
      "Epoch [8/50], Train Loss: 0.0357, Val Loss: 0.0734\n",
      "Epoch [9/50], Train Loss: 0.0341, Val Loss: 0.0650\n",
      "Epoch [10/50], Train Loss: 0.0330, Val Loss: 0.0578\n",
      "Epoch [11/50], Train Loss: 0.0304, Val Loss: 0.0510\n",
      "Epoch [12/50], Train Loss: 0.0290, Val Loss: 0.0447\n",
      "Epoch [13/50], Train Loss: 0.0282, Val Loss: 0.0382\n",
      "Epoch [14/50], Train Loss: 0.0258, Val Loss: 0.0331\n",
      "Epoch [15/50], Train Loss: 0.0239, Val Loss: 0.0276\n",
      "Epoch [16/50], Train Loss: 0.0231, Val Loss: 0.0228\n",
      "Epoch [17/50], Train Loss: 0.0219, Val Loss: 0.0199\n",
      "Epoch [18/50], Train Loss: 0.0205, Val Loss: 0.0171\n",
      "Epoch [19/50], Train Loss: 0.0199, Val Loss: 0.0151\n",
      "Epoch [20/50], Train Loss: 0.0196, Val Loss: 0.0139\n",
      "Epoch [21/50], Train Loss: 0.0183, Val Loss: 0.0136\n",
      "Epoch [22/50], Train Loss: 0.0178, Val Loss: 0.0132\n",
      "Epoch [23/50], Train Loss: 0.0172, Val Loss: 0.0127\n",
      "Epoch [24/50], Train Loss: 0.0163, Val Loss: 0.0132\n",
      "Epoch [25/50], Train Loss: 0.0159, Val Loss: 0.0126\n",
      "Epoch [26/50], Train Loss: 0.0157, Val Loss: 0.0121\n",
      "Epoch [27/50], Train Loss: 0.0147, Val Loss: 0.0116\n",
      "Epoch [28/50], Train Loss: 0.0145, Val Loss: 0.0110\n",
      "Epoch [29/50], Train Loss: 0.0142, Val Loss: 0.0098\n",
      "Epoch [30/50], Train Loss: 0.0125, Val Loss: 0.0102\n",
      "Epoch [31/50], Train Loss: 0.0121, Val Loss: 0.0099\n",
      "Epoch [32/50], Train Loss: 0.0128, Val Loss: 0.0090\n",
      "Epoch [33/50], Train Loss: 0.0118, Val Loss: 0.0087\n",
      "Epoch [34/50], Train Loss: 0.0105, Val Loss: 0.0090\n",
      "Epoch [35/50], Train Loss: 0.0101, Val Loss: 0.0080\n",
      "Epoch [36/50], Train Loss: 0.0099, Val Loss: 0.0073\n",
      "Epoch [37/50], Train Loss: 0.0089, Val Loss: 0.0075\n",
      "Epoch [38/50], Train Loss: 0.0091, Val Loss: 0.0069\n",
      "Epoch [39/50], Train Loss: 0.0090, Val Loss: 0.0071\n",
      "Epoch [40/50], Train Loss: 0.0087, Val Loss: 0.0072\n",
      "Epoch [41/50], Train Loss: 0.0090, Val Loss: 0.0065\n",
      "Epoch [42/50], Train Loss: 0.0085, Val Loss: 0.0066\n",
      "Epoch [43/50], Train Loss: 0.0081, Val Loss: 0.0064\n",
      "Epoch [44/50], Train Loss: 0.0081, Val Loss: 0.0067\n",
      "Epoch [45/50], Train Loss: 0.0079, Val Loss: 0.0066\n",
      "Epoch [46/50], Train Loss: 0.0081, Val Loss: 0.0063\n",
      "Epoch [47/50], Train Loss: 0.0080, Val Loss: 0.0059\n",
      "Epoch [48/50], Train Loss: 0.0082, Val Loss: 0.0060\n",
      "Epoch [49/50], Train Loss: 0.0077, Val Loss: 0.0063\n",
      "Epoch [50/50], Train Loss: 0.0082, Val Loss: 0.0059\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=32, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1271, Val Loss: 0.3057\n",
      "Epoch [2/50], Train Loss: 0.0966, Val Loss: 0.2594\n",
      "Epoch [3/50], Train Loss: 0.0820, Val Loss: 0.2265\n",
      "Epoch [4/50], Train Loss: 0.0749, Val Loss: 0.1998\n",
      "Epoch [5/50], Train Loss: 0.0679, Val Loss: 0.1758\n",
      "Epoch [6/50], Train Loss: 0.0611, Val Loss: 0.1574\n",
      "Epoch [7/50], Train Loss: 0.0587, Val Loss: 0.1433\n",
      "Epoch [8/50], Train Loss: 0.0528, Val Loss: 0.1312\n",
      "Epoch [9/50], Train Loss: 0.0516, Val Loss: 0.1210\n",
      "Epoch [10/50], Train Loss: 0.0483, Val Loss: 0.1114\n",
      "Epoch [11/50], Train Loss: 0.0480, Val Loss: 0.1039\n",
      "Epoch [12/50], Train Loss: 0.0476, Val Loss: 0.0972\n",
      "Epoch [13/50], Train Loss: 0.0424, Val Loss: 0.0901\n",
      "Epoch [14/50], Train Loss: 0.0415, Val Loss: 0.0847\n",
      "Epoch [15/50], Train Loss: 0.0408, Val Loss: 0.0782\n",
      "Epoch [16/50], Train Loss: 0.0376, Val Loss: 0.0717\n",
      "Epoch [17/50], Train Loss: 0.0342, Val Loss: 0.0663\n",
      "Epoch [18/50], Train Loss: 0.0333, Val Loss: 0.0591\n",
      "Epoch [19/50], Train Loss: 0.0323, Val Loss: 0.0534\n",
      "Epoch [20/50], Train Loss: 0.0306, Val Loss: 0.0466\n",
      "Epoch [21/50], Train Loss: 0.0286, Val Loss: 0.0400\n",
      "Epoch [22/50], Train Loss: 0.0275, Val Loss: 0.0351\n",
      "Epoch [23/50], Train Loss: 0.0259, Val Loss: 0.0303\n",
      "Epoch [24/50], Train Loss: 0.0251, Val Loss: 0.0270\n",
      "Epoch [25/50], Train Loss: 0.0236, Val Loss: 0.0254\n",
      "Epoch [26/50], Train Loss: 0.0229, Val Loss: 0.0241\n",
      "Epoch [27/50], Train Loss: 0.0215, Val Loss: 0.0209\n",
      "Epoch [28/50], Train Loss: 0.0213, Val Loss: 0.0188\n",
      "Epoch [29/50], Train Loss: 0.0212, Val Loss: 0.0171\n",
      "Epoch [30/50], Train Loss: 0.0212, Val Loss: 0.0181\n",
      "Epoch [31/50], Train Loss: 0.0205, Val Loss: 0.0168\n",
      "Epoch [32/50], Train Loss: 0.0207, Val Loss: 0.0148\n",
      "Epoch [33/50], Train Loss: 0.0189, Val Loss: 0.0167\n",
      "Epoch [34/50], Train Loss: 0.0187, Val Loss: 0.0151\n",
      "Epoch [35/50], Train Loss: 0.0183, Val Loss: 0.0143\n",
      "Epoch [36/50], Train Loss: 0.0175, Val Loss: 0.0139\n",
      "Epoch [37/50], Train Loss: 0.0174, Val Loss: 0.0143\n",
      "Epoch [38/50], Train Loss: 0.0178, Val Loss: 0.0150\n",
      "Epoch [39/50], Train Loss: 0.0180, Val Loss: 0.0122\n",
      "Epoch [40/50], Train Loss: 0.0166, Val Loss: 0.0129\n",
      "Epoch [41/50], Train Loss: 0.0171, Val Loss: 0.0128\n",
      "Epoch [42/50], Train Loss: 0.0157, Val Loss: 0.0133\n",
      "Epoch [43/50], Train Loss: 0.0159, Val Loss: 0.0115\n",
      "Epoch [44/50], Train Loss: 0.0161, Val Loss: 0.0112\n",
      "Epoch [45/50], Train Loss: 0.0154, Val Loss: 0.0113\n",
      "Epoch [46/50], Train Loss: 0.0156, Val Loss: 0.0121\n",
      "Epoch [47/50], Train Loss: 0.0151, Val Loss: 0.0105\n",
      "Epoch [48/50], Train Loss: 0.0152, Val Loss: 0.0118\n",
      "Epoch [49/50], Train Loss: 0.0151, Val Loss: 0.0110\n",
      "Epoch [50/50], Train Loss: 0.0148, Val Loss: 0.0089\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=32, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0999, Val Loss: 0.2356\n",
      "Epoch [2/50], Train Loss: 0.0773, Val Loss: 0.2129\n",
      "Epoch [3/50], Train Loss: 0.0633, Val Loss: 0.1869\n",
      "Epoch [4/50], Train Loss: 0.0522, Val Loss: 0.1606\n",
      "Epoch [5/50], Train Loss: 0.0435, Val Loss: 0.1361\n",
      "Epoch [6/50], Train Loss: 0.0378, Val Loss: 0.1164\n",
      "Epoch [7/50], Train Loss: 0.0348, Val Loss: 0.1032\n",
      "Epoch [8/50], Train Loss: 0.0334, Val Loss: 0.0953\n",
      "Epoch [9/50], Train Loss: 0.0323, Val Loss: 0.0902\n",
      "Epoch [10/50], Train Loss: 0.0310, Val Loss: 0.0860\n",
      "Epoch [11/50], Train Loss: 0.0295, Val Loss: 0.0817\n",
      "Epoch [12/50], Train Loss: 0.0277, Val Loss: 0.0767\n",
      "Epoch [13/50], Train Loss: 0.0255, Val Loss: 0.0707\n",
      "Epoch [14/50], Train Loss: 0.0229, Val Loss: 0.0634\n",
      "Epoch [15/50], Train Loss: 0.0197, Val Loss: 0.0551\n",
      "Epoch [16/50], Train Loss: 0.0164, Val Loss: 0.0469\n",
      "Epoch [17/50], Train Loss: 0.0137, Val Loss: 0.0400\n",
      "Epoch [18/50], Train Loss: 0.0119, Val Loss: 0.0347\n",
      "Epoch [19/50], Train Loss: 0.0105, Val Loss: 0.0307\n",
      "Epoch [20/50], Train Loss: 0.0092, Val Loss: 0.0277\n",
      "Epoch [21/50], Train Loss: 0.0080, Val Loss: 0.0255\n",
      "Epoch [22/50], Train Loss: 0.0070, Val Loss: 0.0239\n",
      "Epoch [23/50], Train Loss: 0.0062, Val Loss: 0.0227\n",
      "Epoch [24/50], Train Loss: 0.0054, Val Loss: 0.0218\n",
      "Epoch [25/50], Train Loss: 0.0049, Val Loss: 0.0211\n",
      "Epoch [26/50], Train Loss: 0.0044, Val Loss: 0.0205\n",
      "Epoch [27/50], Train Loss: 0.0040, Val Loss: 0.0200\n",
      "Epoch [28/50], Train Loss: 0.0037, Val Loss: 0.0196\n",
      "Epoch [29/50], Train Loss: 0.0035, Val Loss: 0.0192\n",
      "Epoch [30/50], Train Loss: 0.0033, Val Loss: 0.0189\n",
      "Epoch [31/50], Train Loss: 0.0032, Val Loss: 0.0186\n",
      "Epoch [32/50], Train Loss: 0.0031, Val Loss: 0.0183\n",
      "Epoch [33/50], Train Loss: 0.0030, Val Loss: 0.0180\n",
      "Epoch [34/50], Train Loss: 0.0029, Val Loss: 0.0177\n",
      "Epoch [35/50], Train Loss: 0.0028, Val Loss: 0.0174\n",
      "Epoch [36/50], Train Loss: 0.0028, Val Loss: 0.0171\n",
      "Epoch [37/50], Train Loss: 0.0027, Val Loss: 0.0169\n",
      "Epoch [38/50], Train Loss: 0.0027, Val Loss: 0.0166\n",
      "Epoch [39/50], Train Loss: 0.0027, Val Loss: 0.0164\n",
      "Epoch [40/50], Train Loss: 0.0026, Val Loss: 0.0161\n",
      "Epoch [41/50], Train Loss: 0.0026, Val Loss: 0.0159\n",
      "Epoch [42/50], Train Loss: 0.0026, Val Loss: 0.0156\n",
      "Epoch [43/50], Train Loss: 0.0026, Val Loss: 0.0154\n",
      "Epoch [44/50], Train Loss: 0.0025, Val Loss: 0.0152\n",
      "Epoch [45/50], Train Loss: 0.0025, Val Loss: 0.0150\n",
      "Epoch [46/50], Train Loss: 0.0025, Val Loss: 0.0148\n",
      "Epoch [47/50], Train Loss: 0.0025, Val Loss: 0.0146\n",
      "Epoch [48/50], Train Loss: 0.0024, Val Loss: 0.0144\n",
      "Epoch [49/50], Train Loss: 0.0024, Val Loss: 0.0142\n",
      "Epoch [50/50], Train Loss: 0.0024, Val Loss: 0.0140\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=32, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0845, Val Loss: 0.2376\n",
      "Epoch [2/50], Train Loss: 0.0570, Val Loss: 0.1669\n",
      "Epoch [3/50], Train Loss: 0.0467, Val Loss: 0.1262\n",
      "Epoch [4/50], Train Loss: 0.0432, Val Loss: 0.1037\n",
      "Epoch [5/50], Train Loss: 0.0408, Val Loss: 0.0925\n",
      "Epoch [6/50], Train Loss: 0.0408, Val Loss: 0.0862\n",
      "Epoch [7/50], Train Loss: 0.0396, Val Loss: 0.0790\n",
      "Epoch [8/50], Train Loss: 0.0356, Val Loss: 0.0722\n",
      "Epoch [9/50], Train Loss: 0.0345, Val Loss: 0.0654\n",
      "Epoch [10/50], Train Loss: 0.0312, Val Loss: 0.0571\n",
      "Epoch [11/50], Train Loss: 0.0297, Val Loss: 0.0501\n",
      "Epoch [12/50], Train Loss: 0.0253, Val Loss: 0.0406\n",
      "Epoch [13/50], Train Loss: 0.0233, Val Loss: 0.0337\n",
      "Epoch [14/50], Train Loss: 0.0200, Val Loss: 0.0275\n",
      "Epoch [15/50], Train Loss: 0.0197, Val Loss: 0.0252\n",
      "Epoch [16/50], Train Loss: 0.0175, Val Loss: 0.0222\n",
      "Epoch [17/50], Train Loss: 0.0158, Val Loss: 0.0192\n",
      "Epoch [18/50], Train Loss: 0.0150, Val Loss: 0.0182\n",
      "Epoch [19/50], Train Loss: 0.0133, Val Loss: 0.0161\n",
      "Epoch [20/50], Train Loss: 0.0119, Val Loss: 0.0157\n",
      "Epoch [21/50], Train Loss: 0.0117, Val Loss: 0.0153\n",
      "Epoch [22/50], Train Loss: 0.0110, Val Loss: 0.0143\n",
      "Epoch [23/50], Train Loss: 0.0118, Val Loss: 0.0118\n",
      "Epoch [24/50], Train Loss: 0.0109, Val Loss: 0.0109\n",
      "Epoch [25/50], Train Loss: 0.0112, Val Loss: 0.0109\n",
      "Epoch [26/50], Train Loss: 0.0103, Val Loss: 0.0102\n",
      "Epoch [27/50], Train Loss: 0.0103, Val Loss: 0.0104\n",
      "Epoch [28/50], Train Loss: 0.0105, Val Loss: 0.0095\n",
      "Epoch [29/50], Train Loss: 0.0102, Val Loss: 0.0087\n",
      "Epoch [30/50], Train Loss: 0.0096, Val Loss: 0.0097\n",
      "Epoch [31/50], Train Loss: 0.0098, Val Loss: 0.0089\n",
      "Epoch [32/50], Train Loss: 0.0099, Val Loss: 0.0081\n",
      "Epoch [33/50], Train Loss: 0.0097, Val Loss: 0.0076\n",
      "Epoch [34/50], Train Loss: 0.0098, Val Loss: 0.0071\n",
      "Epoch [35/50], Train Loss: 0.0098, Val Loss: 0.0070\n",
      "Epoch [36/50], Train Loss: 0.0092, Val Loss: 0.0069\n",
      "Epoch [37/50], Train Loss: 0.0095, Val Loss: 0.0072\n",
      "Epoch [38/50], Train Loss: 0.0093, Val Loss: 0.0069\n",
      "Epoch [39/50], Train Loss: 0.0094, Val Loss: 0.0068\n",
      "Epoch [40/50], Train Loss: 0.0091, Val Loss: 0.0062\n",
      "Epoch [41/50], Train Loss: 0.0092, Val Loss: 0.0072\n",
      "Epoch [42/50], Train Loss: 0.0082, Val Loss: 0.0064\n",
      "Epoch [43/50], Train Loss: 0.0086, Val Loss: 0.0049\n",
      "Epoch [44/50], Train Loss: 0.0084, Val Loss: 0.0067\n",
      "Epoch [45/50], Train Loss: 0.0083, Val Loss: 0.0067\n",
      "Epoch [46/50], Train Loss: 0.0088, Val Loss: 0.0043\n",
      "Epoch [47/50], Train Loss: 0.0082, Val Loss: 0.0063\n",
      "Epoch [48/50], Train Loss: 0.0086, Val Loss: 0.0066\n",
      "Epoch [49/50], Train Loss: 0.0083, Val Loss: 0.0046\n",
      "Epoch [50/50], Train Loss: 0.0082, Val Loss: 0.0053\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=32, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.2764, Val Loss: 0.4688\n",
      "Epoch [2/50], Train Loss: 0.1788, Val Loss: 0.3103\n",
      "Epoch [3/50], Train Loss: 0.1321, Val Loss: 0.2082\n",
      "Epoch [4/50], Train Loss: 0.1088, Val Loss: 0.1537\n",
      "Epoch [5/50], Train Loss: 0.0997, Val Loss: 0.1271\n",
      "Epoch [6/50], Train Loss: 0.0903, Val Loss: 0.1118\n",
      "Epoch [7/50], Train Loss: 0.0861, Val Loss: 0.0955\n",
      "Epoch [8/50], Train Loss: 0.0796, Val Loss: 0.0789\n",
      "Epoch [9/50], Train Loss: 0.0740, Val Loss: 0.0701\n",
      "Epoch [10/50], Train Loss: 0.0707, Val Loss: 0.0611\n",
      "Epoch [11/50], Train Loss: 0.0657, Val Loss: 0.0505\n",
      "Epoch [12/50], Train Loss: 0.0640, Val Loss: 0.0466\n",
      "Epoch [13/50], Train Loss: 0.0622, Val Loss: 0.0410\n",
      "Epoch [14/50], Train Loss: 0.0576, Val Loss: 0.0410\n",
      "Epoch [15/50], Train Loss: 0.0548, Val Loss: 0.0343\n",
      "Epoch [16/50], Train Loss: 0.0531, Val Loss: 0.0366\n",
      "Epoch [17/50], Train Loss: 0.0523, Val Loss: 0.0330\n",
      "Epoch [18/50], Train Loss: 0.0520, Val Loss: 0.0329\n",
      "Epoch [19/50], Train Loss: 0.0491, Val Loss: 0.0284\n",
      "Epoch [20/50], Train Loss: 0.0471, Val Loss: 0.0295\n",
      "Epoch [21/50], Train Loss: 0.0480, Val Loss: 0.0284\n",
      "Epoch [22/50], Train Loss: 0.0435, Val Loss: 0.0262\n",
      "Epoch [23/50], Train Loss: 0.0436, Val Loss: 0.0231\n",
      "Epoch [24/50], Train Loss: 0.0439, Val Loss: 0.0246\n",
      "Epoch [25/50], Train Loss: 0.0422, Val Loss: 0.0230\n",
      "Epoch [26/50], Train Loss: 0.0416, Val Loss: 0.0248\n",
      "Epoch [27/50], Train Loss: 0.0395, Val Loss: 0.0205\n",
      "Epoch [28/50], Train Loss: 0.0389, Val Loss: 0.0240\n",
      "Epoch [29/50], Train Loss: 0.0373, Val Loss: 0.0236\n",
      "Epoch [30/50], Train Loss: 0.0376, Val Loss: 0.0212\n",
      "Epoch [31/50], Train Loss: 0.0380, Val Loss: 0.0221\n",
      "Epoch [32/50], Train Loss: 0.0367, Val Loss: 0.0199\n",
      "Epoch [33/50], Train Loss: 0.0340, Val Loss: 0.0177\n",
      "Epoch [34/50], Train Loss: 0.0331, Val Loss: 0.0180\n",
      "Epoch [35/50], Train Loss: 0.0336, Val Loss: 0.0150\n",
      "Epoch [36/50], Train Loss: 0.0334, Val Loss: 0.0178\n",
      "Epoch [37/50], Train Loss: 0.0315, Val Loss: 0.0162\n",
      "Epoch [38/50], Train Loss: 0.0314, Val Loss: 0.0156\n",
      "Epoch [39/50], Train Loss: 0.0315, Val Loss: 0.0141\n",
      "Epoch [40/50], Train Loss: 0.0306, Val Loss: 0.0147\n",
      "Epoch [41/50], Train Loss: 0.0305, Val Loss: 0.0147\n",
      "Epoch [42/50], Train Loss: 0.0293, Val Loss: 0.0124\n",
      "Epoch [43/50], Train Loss: 0.0292, Val Loss: 0.0137\n",
      "Epoch [44/50], Train Loss: 0.0293, Val Loss: 0.0111\n",
      "Epoch [45/50], Train Loss: 0.0269, Val Loss: 0.0119\n",
      "Epoch [46/50], Train Loss: 0.0273, Val Loss: 0.0115\n",
      "Epoch [47/50], Train Loss: 0.0279, Val Loss: 0.0139\n",
      "Epoch [48/50], Train Loss: 0.0263, Val Loss: 0.0140\n",
      "Epoch [49/50], Train Loss: 0.0257, Val Loss: 0.0105\n",
      "Epoch [50/50], Train Loss: 0.0262, Val Loss: 0.0131\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=64, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1046, Val Loss: 0.2582\n",
      "Epoch [2/50], Train Loss: 0.0741, Val Loss: 0.1903\n",
      "Epoch [3/50], Train Loss: 0.0471, Val Loss: 0.1179\n",
      "Epoch [4/50], Train Loss: 0.0311, Val Loss: 0.0746\n",
      "Epoch [5/50], Train Loss: 0.0304, Val Loss: 0.0673\n",
      "Epoch [6/50], Train Loss: 0.0294, Val Loss: 0.0643\n",
      "Epoch [7/50], Train Loss: 0.0279, Val Loss: 0.0603\n",
      "Epoch [8/50], Train Loss: 0.0266, Val Loss: 0.0564\n",
      "Epoch [9/50], Train Loss: 0.0253, Val Loss: 0.0524\n",
      "Epoch [10/50], Train Loss: 0.0239, Val Loss: 0.0482\n",
      "Epoch [11/50], Train Loss: 0.0224, Val Loss: 0.0438\n",
      "Epoch [12/50], Train Loss: 0.0209, Val Loss: 0.0392\n",
      "Epoch [13/50], Train Loss: 0.0192, Val Loss: 0.0343\n",
      "Epoch [14/50], Train Loss: 0.0175, Val Loss: 0.0293\n",
      "Epoch [15/50], Train Loss: 0.0156, Val Loss: 0.0244\n",
      "Epoch [16/50], Train Loss: 0.0137, Val Loss: 0.0199\n",
      "Epoch [17/50], Train Loss: 0.0121, Val Loss: 0.0168\n",
      "Epoch [18/50], Train Loss: 0.0111, Val Loss: 0.0151\n",
      "Epoch [19/50], Train Loss: 0.0104, Val Loss: 0.0141\n",
      "Epoch [20/50], Train Loss: 0.0097, Val Loss: 0.0132\n",
      "Epoch [21/50], Train Loss: 0.0091, Val Loss: 0.0123\n",
      "Epoch [22/50], Train Loss: 0.0084, Val Loss: 0.0115\n",
      "Epoch [23/50], Train Loss: 0.0076, Val Loss: 0.0107\n",
      "Epoch [24/50], Train Loss: 0.0068, Val Loss: 0.0099\n",
      "Epoch [25/50], Train Loss: 0.0060, Val Loss: 0.0091\n",
      "Epoch [26/50], Train Loss: 0.0051, Val Loss: 0.0085\n",
      "Epoch [27/50], Train Loss: 0.0041, Val Loss: 0.0078\n",
      "Epoch [28/50], Train Loss: 0.0033, Val Loss: 0.0073\n",
      "Epoch [29/50], Train Loss: 0.0027, Val Loss: 0.0066\n",
      "Epoch [30/50], Train Loss: 0.0024, Val Loss: 0.0061\n",
      "Epoch [31/50], Train Loss: 0.0023, Val Loss: 0.0059\n",
      "Epoch [32/50], Train Loss: 0.0023, Val Loss: 0.0059\n",
      "Epoch [33/50], Train Loss: 0.0023, Val Loss: 0.0058\n",
      "Epoch [34/50], Train Loss: 0.0023, Val Loss: 0.0056\n",
      "Epoch [35/50], Train Loss: 0.0022, Val Loss: 0.0056\n",
      "Epoch [36/50], Train Loss: 0.0022, Val Loss: 0.0056\n",
      "Epoch [37/50], Train Loss: 0.0022, Val Loss: 0.0054\n",
      "Epoch [38/50], Train Loss: 0.0022, Val Loss: 0.0054\n",
      "Epoch [39/50], Train Loss: 0.0022, Val Loss: 0.0054\n",
      "Epoch [40/50], Train Loss: 0.0022, Val Loss: 0.0053\n",
      "Epoch [41/50], Train Loss: 0.0022, Val Loss: 0.0052\n",
      "Epoch [42/50], Train Loss: 0.0021, Val Loss: 0.0051\n",
      "Epoch [43/50], Train Loss: 0.0021, Val Loss: 0.0051\n",
      "Epoch [44/50], Train Loss: 0.0021, Val Loss: 0.0050\n",
      "Epoch [45/50], Train Loss: 0.0021, Val Loss: 0.0049\n",
      "Epoch [46/50], Train Loss: 0.0021, Val Loss: 0.0049\n",
      "Epoch [47/50], Train Loss: 0.0021, Val Loss: 0.0049\n",
      "Epoch [48/50], Train Loss: 0.0021, Val Loss: 0.0048\n",
      "Epoch [49/50], Train Loss: 0.0021, Val Loss: 0.0047\n",
      "Epoch [50/50], Train Loss: 0.0021, Val Loss: 0.0047\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=64, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1089, Val Loss: 0.3004\n",
      "Epoch [2/50], Train Loss: 0.0786, Val Loss: 0.2272\n",
      "Epoch [3/50], Train Loss: 0.0530, Val Loss: 0.1492\n",
      "Epoch [4/50], Train Loss: 0.0391, Val Loss: 0.0962\n",
      "Epoch [5/50], Train Loss: 0.0371, Val Loss: 0.0836\n",
      "Epoch [6/50], Train Loss: 0.0356, Val Loss: 0.0779\n",
      "Epoch [7/50], Train Loss: 0.0321, Val Loss: 0.0697\n",
      "Epoch [8/50], Train Loss: 0.0312, Val Loss: 0.0631\n",
      "Epoch [9/50], Train Loss: 0.0295, Val Loss: 0.0579\n",
      "Epoch [10/50], Train Loss: 0.0275, Val Loss: 0.0520\n",
      "Epoch [11/50], Train Loss: 0.0252, Val Loss: 0.0454\n",
      "Epoch [12/50], Train Loss: 0.0233, Val Loss: 0.0394\n",
      "Epoch [13/50], Train Loss: 0.0223, Val Loss: 0.0334\n",
      "Epoch [14/50], Train Loss: 0.0202, Val Loss: 0.0285\n",
      "Epoch [15/50], Train Loss: 0.0183, Val Loss: 0.0229\n",
      "Epoch [16/50], Train Loss: 0.0162, Val Loss: 0.0187\n",
      "Epoch [17/50], Train Loss: 0.0151, Val Loss: 0.0166\n",
      "Epoch [18/50], Train Loss: 0.0139, Val Loss: 0.0147\n",
      "Epoch [19/50], Train Loss: 0.0132, Val Loss: 0.0128\n",
      "Epoch [20/50], Train Loss: 0.0130, Val Loss: 0.0121\n",
      "Epoch [21/50], Train Loss: 0.0122, Val Loss: 0.0118\n",
      "Epoch [22/50], Train Loss: 0.0110, Val Loss: 0.0106\n",
      "Epoch [23/50], Train Loss: 0.0103, Val Loss: 0.0106\n",
      "Epoch [24/50], Train Loss: 0.0096, Val Loss: 0.0094\n",
      "Epoch [25/50], Train Loss: 0.0093, Val Loss: 0.0093\n",
      "Epoch [26/50], Train Loss: 0.0085, Val Loss: 0.0086\n",
      "Epoch [27/50], Train Loss: 0.0078, Val Loss: 0.0090\n",
      "Epoch [28/50], Train Loss: 0.0075, Val Loss: 0.0086\n",
      "Epoch [29/50], Train Loss: 0.0067, Val Loss: 0.0085\n",
      "Epoch [30/50], Train Loss: 0.0063, Val Loss: 0.0082\n",
      "Epoch [31/50], Train Loss: 0.0058, Val Loss: 0.0077\n",
      "Epoch [32/50], Train Loss: 0.0058, Val Loss: 0.0072\n",
      "Epoch [33/50], Train Loss: 0.0049, Val Loss: 0.0062\n",
      "Epoch [34/50], Train Loss: 0.0051, Val Loss: 0.0064\n",
      "Epoch [35/50], Train Loss: 0.0051, Val Loss: 0.0068\n",
      "Epoch [36/50], Train Loss: 0.0048, Val Loss: 0.0063\n",
      "Epoch [37/50], Train Loss: 0.0049, Val Loss: 0.0058\n",
      "Epoch [38/50], Train Loss: 0.0050, Val Loss: 0.0059\n",
      "Epoch [39/50], Train Loss: 0.0047, Val Loss: 0.0052\n",
      "Epoch [40/50], Train Loss: 0.0048, Val Loss: 0.0061\n",
      "Epoch [41/50], Train Loss: 0.0046, Val Loss: 0.0055\n",
      "Epoch [42/50], Train Loss: 0.0044, Val Loss: 0.0054\n",
      "Epoch [43/50], Train Loss: 0.0045, Val Loss: 0.0054\n",
      "Epoch [44/50], Train Loss: 0.0045, Val Loss: 0.0051\n",
      "Epoch [45/50], Train Loss: 0.0046, Val Loss: 0.0054\n",
      "Epoch [46/50], Train Loss: 0.0045, Val Loss: 0.0055\n",
      "Epoch [47/50], Train Loss: 0.0045, Val Loss: 0.0050\n",
      "Epoch [48/50], Train Loss: 0.0042, Val Loss: 0.0049\n",
      "Epoch [49/50], Train Loss: 0.0044, Val Loss: 0.0050\n",
      "Epoch [50/50], Train Loss: 0.0044, Val Loss: 0.0054\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=64, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1235, Val Loss: 0.3058\n",
      "Epoch [2/50], Train Loss: 0.0790, Val Loss: 0.2103\n",
      "Epoch [3/50], Train Loss: 0.0569, Val Loss: 0.1429\n",
      "Epoch [4/50], Train Loss: 0.0471, Val Loss: 0.1062\n",
      "Epoch [5/50], Train Loss: 0.0453, Val Loss: 0.0905\n",
      "Epoch [6/50], Train Loss: 0.0432, Val Loss: 0.0822\n",
      "Epoch [7/50], Train Loss: 0.0416, Val Loss: 0.0747\n",
      "Epoch [8/50], Train Loss: 0.0398, Val Loss: 0.0669\n",
      "Epoch [9/50], Train Loss: 0.0367, Val Loss: 0.0594\n",
      "Epoch [10/50], Train Loss: 0.0346, Val Loss: 0.0522\n",
      "Epoch [11/50], Train Loss: 0.0324, Val Loss: 0.0446\n",
      "Epoch [12/50], Train Loss: 0.0303, Val Loss: 0.0383\n",
      "Epoch [13/50], Train Loss: 0.0271, Val Loss: 0.0327\n",
      "Epoch [14/50], Train Loss: 0.0261, Val Loss: 0.0246\n",
      "Epoch [15/50], Train Loss: 0.0225, Val Loss: 0.0195\n",
      "Epoch [16/50], Train Loss: 0.0206, Val Loss: 0.0150\n",
      "Epoch [17/50], Train Loss: 0.0191, Val Loss: 0.0128\n",
      "Epoch [18/50], Train Loss: 0.0174, Val Loss: 0.0130\n",
      "Epoch [19/50], Train Loss: 0.0172, Val Loss: 0.0096\n",
      "Epoch [20/50], Train Loss: 0.0159, Val Loss: 0.0099\n",
      "Epoch [21/50], Train Loss: 0.0148, Val Loss: 0.0073\n",
      "Epoch [22/50], Train Loss: 0.0148, Val Loss: 0.0069\n",
      "Epoch [23/50], Train Loss: 0.0141, Val Loss: 0.0067\n",
      "Epoch [24/50], Train Loss: 0.0131, Val Loss: 0.0061\n",
      "Epoch [25/50], Train Loss: 0.0128, Val Loss: 0.0051\n",
      "Epoch [26/50], Train Loss: 0.0118, Val Loss: 0.0062\n",
      "Epoch [27/50], Train Loss: 0.0115, Val Loss: 0.0057\n",
      "Epoch [28/50], Train Loss: 0.0118, Val Loss: 0.0058\n",
      "Epoch [29/50], Train Loss: 0.0125, Val Loss: 0.0044\n",
      "Epoch [30/50], Train Loss: 0.0117, Val Loss: 0.0048\n",
      "Epoch [31/50], Train Loss: 0.0116, Val Loss: 0.0053\n",
      "Epoch [32/50], Train Loss: 0.0114, Val Loss: 0.0039\n",
      "Epoch [33/50], Train Loss: 0.0112, Val Loss: 0.0049\n",
      "Epoch [34/50], Train Loss: 0.0109, Val Loss: 0.0059\n",
      "Epoch [35/50], Train Loss: 0.0110, Val Loss: 0.0038\n",
      "Epoch [36/50], Train Loss: 0.0102, Val Loss: 0.0049\n",
      "Epoch [37/50], Train Loss: 0.0104, Val Loss: 0.0042\n",
      "Epoch [38/50], Train Loss: 0.0109, Val Loss: 0.0037\n",
      "Epoch [39/50], Train Loss: 0.0101, Val Loss: 0.0046\n",
      "Epoch [40/50], Train Loss: 0.0096, Val Loss: 0.0047\n",
      "Epoch [41/50], Train Loss: 0.0096, Val Loss: 0.0035\n",
      "Epoch [42/50], Train Loss: 0.0096, Val Loss: 0.0033\n",
      "Epoch [43/50], Train Loss: 0.0094, Val Loss: 0.0039\n",
      "Epoch [44/50], Train Loss: 0.0093, Val Loss: 0.0041\n",
      "Epoch [45/50], Train Loss: 0.0090, Val Loss: 0.0036\n",
      "Epoch [46/50], Train Loss: 0.0092, Val Loss: 0.0041\n",
      "Epoch [47/50], Train Loss: 0.0087, Val Loss: 0.0032\n",
      "Epoch [48/50], Train Loss: 0.0088, Val Loss: 0.0037\n",
      "Epoch [49/50], Train Loss: 0.0089, Val Loss: 0.0034\n",
      "Epoch [50/50], Train Loss: 0.0085, Val Loss: 0.0028\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=64, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0881, Val Loss: 0.2325\n",
      "Epoch [2/50], Train Loss: 0.0418, Val Loss: 0.1192\n",
      "Epoch [3/50], Train Loss: 0.0353, Val Loss: 0.0858\n",
      "Epoch [4/50], Train Loss: 0.0354, Val Loss: 0.0781\n",
      "Epoch [5/50], Train Loss: 0.0330, Val Loss: 0.0710\n",
      "Epoch [6/50], Train Loss: 0.0306, Val Loss: 0.0636\n",
      "Epoch [7/50], Train Loss: 0.0281, Val Loss: 0.0558\n",
      "Epoch [8/50], Train Loss: 0.0254, Val Loss: 0.0473\n",
      "Epoch [9/50], Train Loss: 0.0223, Val Loss: 0.0380\n",
      "Epoch [10/50], Train Loss: 0.0185, Val Loss: 0.0277\n",
      "Epoch [11/50], Train Loss: 0.0139, Val Loss: 0.0176\n",
      "Epoch [12/50], Train Loss: 0.0089, Val Loss: 0.0133\n",
      "Epoch [13/50], Train Loss: 0.0073, Val Loss: 0.0127\n",
      "Epoch [14/50], Train Loss: 0.0049, Val Loss: 0.0101\n",
      "Epoch [15/50], Train Loss: 0.0037, Val Loss: 0.0081\n",
      "Epoch [16/50], Train Loss: 0.0030, Val Loss: 0.0073\n",
      "Epoch [17/50], Train Loss: 0.0029, Val Loss: 0.0073\n",
      "Epoch [18/50], Train Loss: 0.0028, Val Loss: 0.0070\n",
      "Epoch [19/50], Train Loss: 0.0028, Val Loss: 0.0070\n",
      "Epoch [20/50], Train Loss: 0.0027, Val Loss: 0.0070\n",
      "Epoch [21/50], Train Loss: 0.0026, Val Loss: 0.0068\n",
      "Epoch [22/50], Train Loss: 0.0026, Val Loss: 0.0067\n",
      "Epoch [23/50], Train Loss: 0.0026, Val Loss: 0.0067\n",
      "Epoch [24/50], Train Loss: 0.0026, Val Loss: 0.0068\n",
      "Epoch [25/50], Train Loss: 0.0025, Val Loss: 0.0064\n",
      "Epoch [26/50], Train Loss: 0.0026, Val Loss: 0.0063\n",
      "Epoch [27/50], Train Loss: 0.0025, Val Loss: 0.0066\n",
      "Epoch [28/50], Train Loss: 0.0025, Val Loss: 0.0066\n",
      "Epoch [29/50], Train Loss: 0.0025, Val Loss: 0.0059\n",
      "Epoch [30/50], Train Loss: 0.0025, Val Loss: 0.0060\n",
      "Epoch [31/50], Train Loss: 0.0026, Val Loss: 0.0067\n",
      "Epoch [32/50], Train Loss: 0.0024, Val Loss: 0.0063\n",
      "Epoch [33/50], Train Loss: 0.0025, Val Loss: 0.0054\n",
      "Epoch [34/50], Train Loss: 0.0025, Val Loss: 0.0057\n",
      "Epoch [35/50], Train Loss: 0.0027, Val Loss: 0.0072\n",
      "Epoch [36/50], Train Loss: 0.0024, Val Loss: 0.0059\n",
      "Epoch [37/50], Train Loss: 0.0026, Val Loss: 0.0049\n",
      "Epoch [38/50], Train Loss: 0.0025, Val Loss: 0.0055\n",
      "Epoch [39/50], Train Loss: 0.0029, Val Loss: 0.0077\n",
      "Epoch [40/50], Train Loss: 0.0024, Val Loss: 0.0050\n",
      "Epoch [41/50], Train Loss: 0.0028, Val Loss: 0.0044\n",
      "Epoch [42/50], Train Loss: 0.0025, Val Loss: 0.0056\n",
      "Epoch [43/50], Train Loss: 0.0031, Val Loss: 0.0080\n",
      "Epoch [44/50], Train Loss: 0.0025, Val Loss: 0.0040\n",
      "Epoch [45/50], Train Loss: 0.0029, Val Loss: 0.0040\n",
      "Epoch [46/50], Train Loss: 0.0026, Val Loss: 0.0065\n",
      "Epoch [47/50], Train Loss: 0.0028, Val Loss: 0.0069\n",
      "Epoch [48/50], Train Loss: 0.0027, Val Loss: 0.0033\n",
      "Epoch [49/50], Train Loss: 0.0027, Val Loss: 0.0039\n",
      "Epoch [50/50], Train Loss: 0.0027, Val Loss: 0.0076\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=64, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1105, Val Loss: 0.2838\n",
      "Epoch [2/50], Train Loss: 0.0642, Val Loss: 0.1795\n",
      "Epoch [3/50], Train Loss: 0.0392, Val Loss: 0.1006\n",
      "Epoch [4/50], Train Loss: 0.0369, Val Loss: 0.0755\n",
      "Epoch [5/50], Train Loss: 0.0360, Val Loss: 0.0701\n",
      "Epoch [6/50], Train Loss: 0.0320, Val Loss: 0.0636\n",
      "Epoch [7/50], Train Loss: 0.0282, Val Loss: 0.0546\n",
      "Epoch [8/50], Train Loss: 0.0241, Val Loss: 0.0439\n",
      "Epoch [9/50], Train Loss: 0.0202, Val Loss: 0.0343\n",
      "Epoch [10/50], Train Loss: 0.0184, Val Loss: 0.0256\n",
      "Epoch [11/50], Train Loss: 0.0167, Val Loss: 0.0217\n",
      "Epoch [12/50], Train Loss: 0.0142, Val Loss: 0.0181\n",
      "Epoch [13/50], Train Loss: 0.0125, Val Loss: 0.0147\n",
      "Epoch [14/50], Train Loss: 0.0108, Val Loss: 0.0111\n",
      "Epoch [15/50], Train Loss: 0.0094, Val Loss: 0.0101\n",
      "Epoch [16/50], Train Loss: 0.0093, Val Loss: 0.0089\n",
      "Epoch [17/50], Train Loss: 0.0086, Val Loss: 0.0083\n",
      "Epoch [18/50], Train Loss: 0.0086, Val Loss: 0.0087\n",
      "Epoch [19/50], Train Loss: 0.0079, Val Loss: 0.0088\n",
      "Epoch [20/50], Train Loss: 0.0076, Val Loss: 0.0074\n",
      "Epoch [21/50], Train Loss: 0.0078, Val Loss: 0.0069\n",
      "Epoch [22/50], Train Loss: 0.0081, Val Loss: 0.0082\n",
      "Epoch [23/50], Train Loss: 0.0074, Val Loss: 0.0063\n",
      "Epoch [24/50], Train Loss: 0.0079, Val Loss: 0.0061\n",
      "Epoch [25/50], Train Loss: 0.0077, Val Loss: 0.0065\n",
      "Epoch [26/50], Train Loss: 0.0072, Val Loss: 0.0063\n",
      "Epoch [27/50], Train Loss: 0.0073, Val Loss: 0.0054\n",
      "Epoch [28/50], Train Loss: 0.0072, Val Loss: 0.0055\n",
      "Epoch [29/50], Train Loss: 0.0074, Val Loss: 0.0071\n",
      "Epoch [30/50], Train Loss: 0.0069, Val Loss: 0.0061\n",
      "Epoch [31/50], Train Loss: 0.0068, Val Loss: 0.0052\n",
      "Epoch [32/50], Train Loss: 0.0071, Val Loss: 0.0065\n",
      "Epoch [33/50], Train Loss: 0.0068, Val Loss: 0.0060\n",
      "Epoch [34/50], Train Loss: 0.0070, Val Loss: 0.0047\n",
      "Epoch [35/50], Train Loss: 0.0066, Val Loss: 0.0038\n",
      "Epoch [36/50], Train Loss: 0.0069, Val Loss: 0.0063\n",
      "Epoch [37/50], Train Loss: 0.0063, Val Loss: 0.0057\n",
      "Epoch [38/50], Train Loss: 0.0065, Val Loss: 0.0040\n",
      "Epoch [39/50], Train Loss: 0.0062, Val Loss: 0.0032\n",
      "Epoch [40/50], Train Loss: 0.0062, Val Loss: 0.0063\n",
      "Epoch [41/50], Train Loss: 0.0062, Val Loss: 0.0067\n",
      "Epoch [42/50], Train Loss: 0.0064, Val Loss: 0.0033\n",
      "Epoch [43/50], Train Loss: 0.0063, Val Loss: 0.0027\n",
      "Epoch [44/50], Train Loss: 0.0067, Val Loss: 0.0082\n",
      "Epoch [45/50], Train Loss: 0.0066, Val Loss: 0.0072\n",
      "Epoch [46/50], Train Loss: 0.0067, Val Loss: 0.0019\n",
      "Epoch [47/50], Train Loss: 0.0070, Val Loss: 0.0019\n",
      "Epoch [48/50], Train Loss: 0.0075, Val Loss: 0.0137\n",
      "Epoch [49/50], Train Loss: 0.0062, Val Loss: 0.0042\n",
      "Epoch [50/50], Train Loss: 0.0077, Val Loss: 0.0026\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=64, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0958, Val Loss: 0.2305\n",
      "Epoch [2/50], Train Loss: 0.0652, Val Loss: 0.1553\n",
      "Epoch [3/50], Train Loss: 0.0494, Val Loss: 0.1052\n",
      "Epoch [4/50], Train Loss: 0.0487, Val Loss: 0.0860\n",
      "Epoch [5/50], Train Loss: 0.0455, Val Loss: 0.0713\n",
      "Epoch [6/50], Train Loss: 0.0422, Val Loss: 0.0591\n",
      "Epoch [7/50], Train Loss: 0.0382, Val Loss: 0.0462\n",
      "Epoch [8/50], Train Loss: 0.0333, Val Loss: 0.0316\n",
      "Epoch [9/50], Train Loss: 0.0292, Val Loss: 0.0202\n",
      "Epoch [10/50], Train Loss: 0.0260, Val Loss: 0.0153\n",
      "Epoch [11/50], Train Loss: 0.0257, Val Loss: 0.0136\n",
      "Epoch [12/50], Train Loss: 0.0227, Val Loss: 0.0131\n",
      "Epoch [13/50], Train Loss: 0.0210, Val Loss: 0.0108\n",
      "Epoch [14/50], Train Loss: 0.0196, Val Loss: 0.0093\n",
      "Epoch [15/50], Train Loss: 0.0185, Val Loss: 0.0083\n",
      "Epoch [16/50], Train Loss: 0.0158, Val Loss: 0.0065\n",
      "Epoch [17/50], Train Loss: 0.0157, Val Loss: 0.0059\n",
      "Epoch [18/50], Train Loss: 0.0156, Val Loss: 0.0072\n",
      "Epoch [19/50], Train Loss: 0.0143, Val Loss: 0.0074\n",
      "Epoch [20/50], Train Loss: 0.0145, Val Loss: 0.0061\n",
      "Epoch [21/50], Train Loss: 0.0142, Val Loss: 0.0078\n",
      "Epoch [22/50], Train Loss: 0.0138, Val Loss: 0.0048\n",
      "Epoch [23/50], Train Loss: 0.0130, Val Loss: 0.0066\n",
      "Epoch [24/50], Train Loss: 0.0129, Val Loss: 0.0075\n",
      "Epoch [25/50], Train Loss: 0.0126, Val Loss: 0.0063\n",
      "Epoch [26/50], Train Loss: 0.0128, Val Loss: 0.0059\n",
      "Epoch [27/50], Train Loss: 0.0120, Val Loss: 0.0041\n",
      "Epoch [28/50], Train Loss: 0.0125, Val Loss: 0.0053\n",
      "Epoch [29/50], Train Loss: 0.0120, Val Loss: 0.0073\n",
      "Epoch [30/50], Train Loss: 0.0116, Val Loss: 0.0056\n",
      "Epoch [31/50], Train Loss: 0.0115, Val Loss: 0.0054\n",
      "Epoch [32/50], Train Loss: 0.0117, Val Loss: 0.0069\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=64, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1191, Val Loss: 0.2650\n",
      "Epoch [2/50], Train Loss: 0.0507, Val Loss: 0.1372\n",
      "Epoch [3/50], Train Loss: 0.0386, Val Loss: 0.0979\n",
      "Epoch [4/50], Train Loss: 0.0402, Val Loss: 0.0937\n",
      "Epoch [5/50], Train Loss: 0.0387, Val Loss: 0.0908\n",
      "Epoch [6/50], Train Loss: 0.0369, Val Loss: 0.0868\n",
      "Epoch [7/50], Train Loss: 0.0350, Val Loss: 0.0822\n",
      "Epoch [8/50], Train Loss: 0.0329, Val Loss: 0.0764\n",
      "Epoch [9/50], Train Loss: 0.0302, Val Loss: 0.0690\n",
      "Epoch [10/50], Train Loss: 0.0266, Val Loss: 0.0592\n",
      "Epoch [11/50], Train Loss: 0.0219, Val Loss: 0.0461\n",
      "Epoch [12/50], Train Loss: 0.0154, Val Loss: 0.0300\n",
      "Epoch [13/50], Train Loss: 0.0081, Val Loss: 0.0176\n",
      "Epoch [14/50], Train Loss: 0.0062, Val Loss: 0.0174\n",
      "Epoch [15/50], Train Loss: 0.0043, Val Loss: 0.0120\n",
      "Epoch [16/50], Train Loss: 0.0044, Val Loss: 0.0153\n",
      "Epoch [17/50], Train Loss: 0.0035, Val Loss: 0.0124\n",
      "Epoch [18/50], Train Loss: 0.0035, Val Loss: 0.0128\n",
      "Epoch [19/50], Train Loss: 0.0032, Val Loss: 0.0116\n",
      "Epoch [20/50], Train Loss: 0.0034, Val Loss: 0.0126\n",
      "Epoch [21/50], Train Loss: 0.0030, Val Loss: 0.0109\n",
      "Epoch [22/50], Train Loss: 0.0032, Val Loss: 0.0117\n",
      "Epoch [23/50], Train Loss: 0.0030, Val Loss: 0.0109\n",
      "Epoch [24/50], Train Loss: 0.0030, Val Loss: 0.0110\n",
      "Epoch [25/50], Train Loss: 0.0029, Val Loss: 0.0103\n",
      "Epoch [26/50], Train Loss: 0.0030, Val Loss: 0.0106\n",
      "Epoch [27/50], Train Loss: 0.0028, Val Loss: 0.0101\n",
      "Epoch [28/50], Train Loss: 0.0028, Val Loss: 0.0100\n",
      "Epoch [29/50], Train Loss: 0.0028, Val Loss: 0.0098\n",
      "Epoch [30/50], Train Loss: 0.0027, Val Loss: 0.0097\n",
      "Epoch [31/50], Train Loss: 0.0027, Val Loss: 0.0095\n",
      "Epoch [32/50], Train Loss: 0.0027, Val Loss: 0.0093\n",
      "Epoch [33/50], Train Loss: 0.0027, Val Loss: 0.0093\n",
      "Epoch [34/50], Train Loss: 0.0026, Val Loss: 0.0091\n",
      "Epoch [35/50], Train Loss: 0.0026, Val Loss: 0.0090\n",
      "Epoch [36/50], Train Loss: 0.0026, Val Loss: 0.0089\n",
      "Epoch [37/50], Train Loss: 0.0025, Val Loss: 0.0089\n",
      "Epoch [38/50], Train Loss: 0.0025, Val Loss: 0.0087\n",
      "Epoch [39/50], Train Loss: 0.0025, Val Loss: 0.0086\n",
      "Epoch [40/50], Train Loss: 0.0025, Val Loss: 0.0087\n",
      "Epoch [41/50], Train Loss: 0.0025, Val Loss: 0.0085\n",
      "Epoch [42/50], Train Loss: 0.0024, Val Loss: 0.0083\n",
      "Epoch [43/50], Train Loss: 0.0024, Val Loss: 0.0084\n",
      "Epoch [44/50], Train Loss: 0.0024, Val Loss: 0.0084\n",
      "Epoch [45/50], Train Loss: 0.0024, Val Loss: 0.0082\n",
      "Epoch [46/50], Train Loss: 0.0024, Val Loss: 0.0081\n",
      "Epoch [47/50], Train Loss: 0.0024, Val Loss: 0.0083\n",
      "Epoch [48/50], Train Loss: 0.0024, Val Loss: 0.0081\n",
      "Epoch [49/50], Train Loss: 0.0023, Val Loss: 0.0078\n",
      "Epoch [50/50], Train Loss: 0.0023, Val Loss: 0.0079\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=64, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0675, Val Loss: 0.1511\n",
      "Epoch [2/50], Train Loss: 0.0439, Val Loss: 0.0946\n",
      "Epoch [3/50], Train Loss: 0.0399, Val Loss: 0.0762\n",
      "Epoch [4/50], Train Loss: 0.0382, Val Loss: 0.0632\n",
      "Epoch [5/50], Train Loss: 0.0338, Val Loss: 0.0470\n",
      "Epoch [6/50], Train Loss: 0.0280, Val Loss: 0.0288\n",
      "Epoch [7/50], Train Loss: 0.0215, Val Loss: 0.0174\n",
      "Epoch [8/50], Train Loss: 0.0191, Val Loss: 0.0182\n",
      "Epoch [9/50], Train Loss: 0.0168, Val Loss: 0.0160\n",
      "Epoch [10/50], Train Loss: 0.0149, Val Loss: 0.0144\n",
      "Epoch [11/50], Train Loss: 0.0138, Val Loss: 0.0141\n",
      "Epoch [12/50], Train Loss: 0.0105, Val Loss: 0.0107\n",
      "Epoch [13/50], Train Loss: 0.0092, Val Loss: 0.0055\n",
      "Epoch [14/50], Train Loss: 0.0082, Val Loss: 0.0063\n",
      "Epoch [15/50], Train Loss: 0.0086, Val Loss: 0.0099\n",
      "Epoch [16/50], Train Loss: 0.0078, Val Loss: 0.0048\n",
      "Epoch [17/50], Train Loss: 0.0075, Val Loss: 0.0051\n",
      "Epoch [18/50], Train Loss: 0.0083, Val Loss: 0.0116\n",
      "Epoch [19/50], Train Loss: 0.0075, Val Loss: 0.0039\n",
      "Epoch [20/50], Train Loss: 0.0081, Val Loss: 0.0031\n",
      "Epoch [21/50], Train Loss: 0.0101, Val Loss: 0.0156\n",
      "Epoch [22/50], Train Loss: 0.0086, Val Loss: 0.0039\n",
      "Epoch [23/50], Train Loss: 0.0084, Val Loss: 0.0032\n",
      "Epoch [24/50], Train Loss: 0.0089, Val Loss: 0.0129\n",
      "Epoch [25/50], Train Loss: 0.0072, Val Loss: 0.0051\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=64, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1134, Val Loss: 0.2254\n",
      "Epoch [2/50], Train Loss: 0.0704, Val Loss: 0.1377\n",
      "Epoch [3/50], Train Loss: 0.0607, Val Loss: 0.1067\n",
      "Epoch [4/50], Train Loss: 0.0602, Val Loss: 0.0960\n",
      "Epoch [5/50], Train Loss: 0.0536, Val Loss: 0.0839\n",
      "Epoch [6/50], Train Loss: 0.0492, Val Loss: 0.0709\n",
      "Epoch [7/50], Train Loss: 0.0449, Val Loss: 0.0586\n",
      "Epoch [8/50], Train Loss: 0.0406, Val Loss: 0.0415\n",
      "Epoch [9/50], Train Loss: 0.0353, Val Loss: 0.0264\n",
      "Epoch [10/50], Train Loss: 0.0308, Val Loss: 0.0178\n",
      "Epoch [11/50], Train Loss: 0.0293, Val Loss: 0.0131\n",
      "Epoch [12/50], Train Loss: 0.0270, Val Loss: 0.0144\n",
      "Epoch [13/50], Train Loss: 0.0241, Val Loss: 0.0129\n",
      "Epoch [14/50], Train Loss: 0.0239, Val Loss: 0.0112\n",
      "Epoch [15/50], Train Loss: 0.0245, Val Loss: 0.0092\n",
      "Epoch [16/50], Train Loss: 0.0217, Val Loss: 0.0103\n",
      "Epoch [17/50], Train Loss: 0.0212, Val Loss: 0.0090\n",
      "Epoch [18/50], Train Loss: 0.0204, Val Loss: 0.0070\n",
      "Epoch [19/50], Train Loss: 0.0206, Val Loss: 0.0096\n",
      "Epoch [20/50], Train Loss: 0.0191, Val Loss: 0.0141\n",
      "Epoch [21/50], Train Loss: 0.0179, Val Loss: 0.0085\n",
      "Epoch [22/50], Train Loss: 0.0188, Val Loss: 0.0054\n",
      "Epoch [23/50], Train Loss: 0.0192, Val Loss: 0.0112\n",
      "Epoch [24/50], Train Loss: 0.0172, Val Loss: 0.0110\n",
      "Epoch [25/50], Train Loss: 0.0179, Val Loss: 0.0058\n",
      "Epoch [26/50], Train Loss: 0.0174, Val Loss: 0.0065\n",
      "Epoch [27/50], Train Loss: 0.0167, Val Loss: 0.0115\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=128, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0954, Val Loss: 0.1978\n",
      "Epoch [2/50], Train Loss: 0.0417, Val Loss: 0.0713\n",
      "Epoch [3/50], Train Loss: 0.0357, Val Loss: 0.0590\n",
      "Epoch [4/50], Train Loss: 0.0327, Val Loss: 0.0533\n",
      "Epoch [5/50], Train Loss: 0.0296, Val Loss: 0.0452\n",
      "Epoch [6/50], Train Loss: 0.0269, Val Loss: 0.0371\n",
      "Epoch [7/50], Train Loss: 0.0241, Val Loss: 0.0278\n",
      "Epoch [8/50], Train Loss: 0.0209, Val Loss: 0.0176\n",
      "Epoch [9/50], Train Loss: 0.0171, Val Loss: 0.0088\n",
      "Epoch [10/50], Train Loss: 0.0146, Val Loss: 0.0061\n",
      "Epoch [11/50], Train Loss: 0.0142, Val Loss: 0.0106\n",
      "Epoch [12/50], Train Loss: 0.0111, Val Loss: 0.0068\n",
      "Epoch [13/50], Train Loss: 0.0096, Val Loss: 0.0062\n",
      "Epoch [14/50], Train Loss: 0.0077, Val Loss: 0.0060\n",
      "Epoch [15/50], Train Loss: 0.0067, Val Loss: 0.0072\n",
      "Epoch [16/50], Train Loss: 0.0046, Val Loss: 0.0070\n",
      "Epoch [17/50], Train Loss: 0.0034, Val Loss: 0.0044\n",
      "Epoch [18/50], Train Loss: 0.0030, Val Loss: 0.0026\n",
      "Epoch [19/50], Train Loss: 0.0031, Val Loss: 0.0063\n",
      "Epoch [20/50], Train Loss: 0.0027, Val Loss: 0.0057\n",
      "Epoch [21/50], Train Loss: 0.0027, Val Loss: 0.0031\n",
      "Epoch [22/50], Train Loss: 0.0026, Val Loss: 0.0030\n",
      "Epoch [23/50], Train Loss: 0.0028, Val Loss: 0.0064\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=128, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0911, Val Loss: 0.2171\n",
      "Epoch [2/50], Train Loss: 0.0420, Val Loss: 0.0882\n",
      "Epoch [3/50], Train Loss: 0.0355, Val Loss: 0.0654\n",
      "Epoch [4/50], Train Loss: 0.0329, Val Loss: 0.0568\n",
      "Epoch [5/50], Train Loss: 0.0296, Val Loss: 0.0478\n",
      "Epoch [6/50], Train Loss: 0.0264, Val Loss: 0.0398\n",
      "Epoch [7/50], Train Loss: 0.0238, Val Loss: 0.0320\n",
      "Epoch [8/50], Train Loss: 0.0202, Val Loss: 0.0235\n",
      "Epoch [9/50], Train Loss: 0.0169, Val Loss: 0.0166\n",
      "Epoch [10/50], Train Loss: 0.0145, Val Loss: 0.0121\n",
      "Epoch [11/50], Train Loss: 0.0121, Val Loss: 0.0096\n",
      "Epoch [12/50], Train Loss: 0.0093, Val Loss: 0.0070\n",
      "Epoch [13/50], Train Loss: 0.0060, Val Loss: 0.0064\n",
      "Epoch [14/50], Train Loss: 0.0061, Val Loss: 0.0068\n",
      "Epoch [15/50], Train Loss: 0.0053, Val Loss: 0.0030\n",
      "Epoch [16/50], Train Loss: 0.0054, Val Loss: 0.0059\n",
      "Epoch [17/50], Train Loss: 0.0048, Val Loss: 0.0058\n",
      "Epoch [18/50], Train Loss: 0.0044, Val Loss: 0.0025\n",
      "Epoch [19/50], Train Loss: 0.0050, Val Loss: 0.0038\n",
      "Epoch [20/50], Train Loss: 0.0048, Val Loss: 0.0072\n",
      "Epoch [21/50], Train Loss: 0.0046, Val Loss: 0.0031\n",
      "Epoch [22/50], Train Loss: 0.0046, Val Loss: 0.0026\n",
      "Epoch [23/50], Train Loss: 0.0045, Val Loss: 0.0066\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=128, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1021, Val Loss: 0.2588\n",
      "Epoch [2/50], Train Loss: 0.0575, Val Loss: 0.1482\n",
      "Epoch [3/50], Train Loss: 0.0439, Val Loss: 0.1012\n",
      "Epoch [4/50], Train Loss: 0.0411, Val Loss: 0.0867\n",
      "Epoch [5/50], Train Loss: 0.0366, Val Loss: 0.0723\n",
      "Epoch [6/50], Train Loss: 0.0338, Val Loss: 0.0629\n",
      "Epoch [7/50], Train Loss: 0.0303, Val Loss: 0.0504\n",
      "Epoch [8/50], Train Loss: 0.0270, Val Loss: 0.0399\n",
      "Epoch [9/50], Train Loss: 0.0223, Val Loss: 0.0256\n",
      "Epoch [10/50], Train Loss: 0.0171, Val Loss: 0.0176\n",
      "Epoch [11/50], Train Loss: 0.0152, Val Loss: 0.0121\n",
      "Epoch [12/50], Train Loss: 0.0128, Val Loss: 0.0063\n",
      "Epoch [13/50], Train Loss: 0.0115, Val Loss: 0.0064\n",
      "Epoch [14/50], Train Loss: 0.0114, Val Loss: 0.0105\n",
      "Epoch [15/50], Train Loss: 0.0105, Val Loss: 0.0042\n",
      "Epoch [16/50], Train Loss: 0.0098, Val Loss: 0.0039\n",
      "Epoch [17/50], Train Loss: 0.0098, Val Loss: 0.0087\n",
      "Epoch [18/50], Train Loss: 0.0092, Val Loss: 0.0049\n",
      "Epoch [19/50], Train Loss: 0.0091, Val Loss: 0.0040\n",
      "Epoch [20/50], Train Loss: 0.0090, Val Loss: 0.0068\n",
      "Epoch [21/50], Train Loss: 0.0085, Val Loss: 0.0060\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=128, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0567, Val Loss: 0.0734\n",
      "Epoch [2/50], Train Loss: 0.0445, Val Loss: 0.0661\n",
      "Epoch [3/50], Train Loss: 0.0356, Val Loss: 0.0540\n",
      "Epoch [4/50], Train Loss: 0.0308, Val Loss: 0.0393\n",
      "Epoch [5/50], Train Loss: 0.0259, Val Loss: 0.0239\n",
      "Epoch [6/50], Train Loss: 0.0193, Val Loss: 0.0087\n",
      "Epoch [7/50], Train Loss: 0.0118, Val Loss: 0.0059\n",
      "Epoch [8/50], Train Loss: 0.0115, Val Loss: 0.0113\n",
      "Epoch [9/50], Train Loss: 0.0067, Val Loss: 0.0075\n",
      "Epoch [10/50], Train Loss: 0.0048, Val Loss: 0.0051\n",
      "Epoch [11/50], Train Loss: 0.0033, Val Loss: 0.0038\n",
      "Epoch [12/50], Train Loss: 0.0028, Val Loss: 0.0038\n",
      "Epoch [13/50], Train Loss: 0.0028, Val Loss: 0.0078\n",
      "Epoch [14/50], Train Loss: 0.0024, Val Loss: 0.0055\n",
      "Epoch [15/50], Train Loss: 0.0026, Val Loss: 0.0039\n",
      "Epoch [16/50], Train Loss: 0.0026, Val Loss: 0.0039\n",
      "Epoch [17/50], Train Loss: 0.0026, Val Loss: 0.0072\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=128, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0651, Val Loss: 0.0765\n",
      "Epoch [2/50], Train Loss: 0.0487, Val Loss: 0.0690\n",
      "Epoch [3/50], Train Loss: 0.0398, Val Loss: 0.0545\n",
      "Epoch [4/50], Train Loss: 0.0339, Val Loss: 0.0335\n",
      "Epoch [5/50], Train Loss: 0.0287, Val Loss: 0.0116\n",
      "Epoch [6/50], Train Loss: 0.0188, Val Loss: 0.0053\n",
      "Epoch [7/50], Train Loss: 0.0195, Val Loss: 0.0163\n",
      "Epoch [8/50], Train Loss: 0.0155, Val Loss: 0.0078\n",
      "Epoch [9/50], Train Loss: 0.0118, Val Loss: 0.0066\n",
      "Epoch [10/50], Train Loss: 0.0090, Val Loss: 0.0088\n",
      "Epoch [11/50], Train Loss: 0.0066, Val Loss: 0.0037\n",
      "Epoch [12/50], Train Loss: 0.0062, Val Loss: 0.0024\n",
      "Epoch [13/50], Train Loss: 0.0059, Val Loss: 0.0059\n",
      "Epoch [14/50], Train Loss: 0.0061, Val Loss: 0.0100\n",
      "Epoch [15/50], Train Loss: 0.0062, Val Loss: 0.0025\n",
      "Epoch [16/50], Train Loss: 0.0068, Val Loss: 0.0021\n",
      "Epoch [17/50], Train Loss: 0.0069, Val Loss: 0.0130\n",
      "Epoch [18/50], Train Loss: 0.0061, Val Loss: 0.0025\n",
      "Epoch [19/50], Train Loss: 0.0074, Val Loss: 0.0027\n",
      "Epoch [20/50], Train Loss: 0.0061, Val Loss: 0.0089\n",
      "Epoch [21/50], Train Loss: 0.0056, Val Loss: 0.0063\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=128, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0781, Val Loss: 0.1198\n",
      "Epoch [2/50], Train Loss: 0.0485, Val Loss: 0.0686\n",
      "Epoch [3/50], Train Loss: 0.0438, Val Loss: 0.0550\n",
      "Epoch [4/50], Train Loss: 0.0366, Val Loss: 0.0394\n",
      "Epoch [5/50], Train Loss: 0.0317, Val Loss: 0.0178\n",
      "Epoch [6/50], Train Loss: 0.0246, Val Loss: 0.0066\n",
      "Epoch [7/50], Train Loss: 0.0205, Val Loss: 0.0178\n",
      "Epoch [8/50], Train Loss: 0.0166, Val Loss: 0.0066\n",
      "Epoch [9/50], Train Loss: 0.0138, Val Loss: 0.0055\n",
      "Epoch [10/50], Train Loss: 0.0130, Val Loss: 0.0071\n",
      "Epoch [11/50], Train Loss: 0.0127, Val Loss: 0.0066\n",
      "Epoch [12/50], Train Loss: 0.0130, Val Loss: 0.0055\n",
      "Epoch [13/50], Train Loss: 0.0128, Val Loss: 0.0083\n",
      "Epoch [14/50], Train Loss: 0.0121, Val Loss: 0.0032\n",
      "Epoch [15/50], Train Loss: 0.0123, Val Loss: 0.0057\n",
      "Epoch [16/50], Train Loss: 0.0119, Val Loss: 0.0134\n",
      "Epoch [17/50], Train Loss: 0.0111, Val Loss: 0.0023\n",
      "Epoch [18/50], Train Loss: 0.0120, Val Loss: 0.0029\n",
      "Epoch [19/50], Train Loss: 0.0116, Val Loss: 0.0156\n",
      "Epoch [20/50], Train Loss: 0.0107, Val Loss: 0.0024\n",
      "Epoch [21/50], Train Loss: 0.0127, Val Loss: 0.0041\n",
      "Epoch [22/50], Train Loss: 0.0100, Val Loss: 0.0080\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=128, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0536, Val Loss: 0.0598\n",
      "Epoch [2/50], Train Loss: 0.0517, Val Loss: 0.0836\n",
      "Epoch [3/50], Train Loss: 0.0361, Val Loss: 0.0561\n",
      "Epoch [4/50], Train Loss: 0.0328, Val Loss: 0.0351\n",
      "Epoch [5/50], Train Loss: 0.0242, Val Loss: 0.0087\n",
      "Epoch [6/50], Train Loss: 0.0145, Val Loss: 0.0042\n",
      "Epoch [7/50], Train Loss: 0.0134, Val Loss: 0.0238\n",
      "Epoch [8/50], Train Loss: 0.0102, Val Loss: 0.0062\n",
      "Epoch [9/50], Train Loss: 0.0069, Val Loss: 0.0148\n",
      "Epoch [10/50], Train Loss: 0.0048, Val Loss: 0.0117\n",
      "Epoch [11/50], Train Loss: 0.0049, Val Loss: 0.0034\n",
      "Epoch [12/50], Train Loss: 0.0031, Val Loss: 0.0080\n",
      "Epoch [13/50], Train Loss: 0.0030, Val Loss: 0.0092\n",
      "Epoch [14/50], Train Loss: 0.0033, Val Loss: 0.0051\n",
      "Epoch [15/50], Train Loss: 0.0030, Val Loss: 0.0069\n",
      "Epoch [16/50], Train Loss: 0.0032, Val Loss: 0.0080\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=128, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0458, Val Loss: 0.0375\n",
      "Epoch [2/50], Train Loss: 0.0670, Val Loss: 0.0994\n",
      "Epoch [3/50], Train Loss: 0.0382, Val Loss: 0.0584\n",
      "Epoch [4/50], Train Loss: 0.0383, Val Loss: 0.0436\n",
      "Epoch [5/50], Train Loss: 0.0319, Val Loss: 0.0209\n",
      "Epoch [6/50], Train Loss: 0.0231, Val Loss: 0.0048\n",
      "Epoch [7/50], Train Loss: 0.0185, Val Loss: 0.0062\n",
      "Epoch [8/50], Train Loss: 0.0139, Val Loss: 0.0102\n",
      "Epoch [9/50], Train Loss: 0.0095, Val Loss: 0.0120\n",
      "Epoch [10/50], Train Loss: 0.0099, Val Loss: 0.0073\n",
      "Epoch [11/50], Train Loss: 0.0082, Val Loss: 0.0027\n",
      "Epoch [12/50], Train Loss: 0.0066, Val Loss: 0.0069\n",
      "Epoch [13/50], Train Loss: 0.0065, Val Loss: 0.0079\n",
      "Epoch [14/50], Train Loss: 0.0071, Val Loss: 0.0028\n",
      "Epoch [15/50], Train Loss: 0.0067, Val Loss: 0.0021\n",
      "Epoch [16/50], Train Loss: 0.0083, Val Loss: 0.0154\n",
      "Epoch [17/50], Train Loss: 0.0068, Val Loss: 0.0017\n",
      "Epoch [18/50], Train Loss: 0.0084, Val Loss: 0.0026\n",
      "Epoch [19/50], Train Loss: 0.0085, Val Loss: 0.0180\n",
      "Epoch [20/50], Train Loss: 0.0068, Val Loss: 0.0024\n",
      "Epoch [21/50], Train Loss: 0.0090, Val Loss: 0.0046\n",
      "Epoch [22/50], Train Loss: 0.0060, Val Loss: 0.0089\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=128, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0796, Val Loss: 0.1062\n",
      "Epoch [2/50], Train Loss: 0.0595, Val Loss: 0.0872\n",
      "Epoch [3/50], Train Loss: 0.0515, Val Loss: 0.0715\n",
      "Epoch [4/50], Train Loss: 0.0465, Val Loss: 0.0569\n",
      "Epoch [5/50], Train Loss: 0.0402, Val Loss: 0.0300\n",
      "Epoch [6/50], Train Loss: 0.0326, Val Loss: 0.0121\n",
      "Epoch [7/50], Train Loss: 0.0269, Val Loss: 0.0086\n",
      "Epoch [8/50], Train Loss: 0.0232, Val Loss: 0.0166\n",
      "Epoch [9/50], Train Loss: 0.0185, Val Loss: 0.0069\n",
      "Epoch [10/50], Train Loss: 0.0195, Val Loss: 0.0081\n",
      "Epoch [11/50], Train Loss: 0.0153, Val Loss: 0.0048\n",
      "Epoch [12/50], Train Loss: 0.0142, Val Loss: 0.0032\n",
      "Epoch [13/50], Train Loss: 0.0141, Val Loss: 0.0063\n",
      "Epoch [14/50], Train Loss: 0.0138, Val Loss: 0.0041\n",
      "Epoch [15/50], Train Loss: 0.0146, Val Loss: 0.0018\n",
      "Epoch [16/50], Train Loss: 0.0138, Val Loss: 0.0058\n",
      "Epoch [17/50], Train Loss: 0.0127, Val Loss: 0.0069\n",
      "Epoch [18/50], Train Loss: 0.0128, Val Loss: 0.0063\n",
      "Epoch [19/50], Train Loss: 0.0129, Val Loss: 0.0034\n",
      "Epoch [20/50], Train Loss: 0.0112, Val Loss: 0.0069\n",
      "Early stopping triggered.\n",
      "Best Parameters: (0.005, 'adam', 16, 1, 0.0), Best Validation Loss: 0.0010\n"
     ]
    }
   ],
   "source": [
    "# Transform to PyTorch Tensor\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "# Vanilla RNN Model\n",
    "class MyRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout):\n",
    "        super(MyRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.dropout(out[:, -1, :])  # Dropout on the last hidden state\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Hyperparameter Grid\n",
    "param_grid = {\n",
    "    \"learning_rate\": [0.005, 0.001, 0.0005, 0.0001],\n",
    "    \"optimizer\": [\"adam\", \"sgd\", \"adamw\"],\n",
    "    \"hidden_size\": [16, 32, 64, 128],\n",
    "    \"num_layers\": [1, 2, 3],\n",
    "    \"dropout\": [0.0, 0.2, 0.5]\n",
    "}\n",
    "\n",
    "# Generate all combinations of hyperparameters\n",
    "param_combinations = list(product(*param_grid.values()))\n",
    "best_val_loss = float(\"inf\")\n",
    "best_params_RNN = None\n",
    "\n",
    "# Grid Search\n",
    "for params in param_combinations:\n",
    "    learning_rate, optimizer_name, hidden_size, num_layers, dropout = params\n",
    "\n",
    "    print(f\"Testing parameters: lr={learning_rate}, optimizer={optimizer_name}, hidden_size={hidden_size}, num_layers={num_layers}, dropout={dropout}\")\n",
    "\n",
    "    # Initialize model\n",
    "    model = MyRNN(input_size=5, hidden_size=hidden_size, num_layers=num_layers, output_size=5, dropout=dropout).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Initialize optimizer\n",
    "    if optimizer_name == \"adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    elif optimizer_name == \"sgd\":\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    elif optimizer_name == \"adamw\":\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Early Stopping\n",
    "    patience = 5\n",
    "    early_stop_counter = 0\n",
    "    best_model_val_loss = float(\"inf\")\n",
    "\n",
    "    # Training Loop\n",
    "    for epoch in range(50):  # Max epochs\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for i in range(0, len(X_train), 32):  # Batch size = 32\n",
    "            x_batch = X_train[i:i+32]\n",
    "            y_batch = y_train[i:i+32]\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(x_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss /= len(X_train) / 32  # Average train loss\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val)\n",
    "            val_loss = criterion(val_outputs, y_val).item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/50], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early Stopping Check\n",
    "        if val_loss < best_model_val_loss:\n",
    "            best_model_val_loss = val_loss\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            if early_stop_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    # Update best parameters if current combination is better\n",
    "    if best_model_val_loss < best_val_loss:\n",
    "        best_val_loss = best_model_val_loss\n",
    "        best_params_RNN = params\n",
    "        # Save best model\n",
    "        torch.save(model.state_dict(), \"best_model_RNN.pth\")\n",
    "\n",
    "print(f\"Best Parameters: {best_params_RNN}, Best Validation Loss: {best_val_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open - MSE: 0.0006, MAE: 0.0210, R: 0.8555\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA/IAAAIjCAYAAACgdyAGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAADxBUlEQVR4nOzdd3hT1RsH8G+6Jy2jlDLLXrIRZIOyBRUEZYiAghMBERWUjYI/B6CIgANZiqgslSV7L9l7lJYyStlllM7c3x8vtzdpkzRJk6bj+3mePjdNbm5O0iS97znveY9OURQFRERERERERJQruLm6AURERERERERkPQbyRERERERERLkIA3kiIiIiIiKiXISBPBEREREREVEuwkCeiIiIiIiIKBdhIE9ERERERESUizCQJyIiIiIiIspFGMgTERERERER5SIM5ImIiIiIiIhyEQbyREREThIeHo5+/fql/b5582bodDps3rzZZW1KL30biYiIKOdjIE9ERHnS3LlzodPp0n58fHxQqVIlDBo0CLGxsa5unk1WrVqFcePGuboZTtGyZUujv5O5n5zy/Hfs2IEuXbogNDQU3t7eCA8Px+uvv47o6GhXN42IiPIRD1c3gIiIyJkmTJiAsmXLIiEhAdu3b8fMmTOxatUqHDt2DH5+ftnalubNm+Phw4fw8vKy6X6rVq3CjBkzckww60gff/wxBgwYkPb7vn378M033+Cjjz5C1apV066vWbOmK5pnZPr06RgyZAjKlSuHd955B2FhYTh58iR+/PFHLF68GKtWrULjxo1d3UwiIsoHGMgTEVGe1qFDB9SvXx8AMGDAABQuXBhTpkzBihUr0LNnT5P3efDgAfz9/R3eFjc3N/j4+Dj8uLlZmzZtjH738fHBN998gzZt2qBly5Zm7+esv5E5O3bswNChQ9G0aVOsWbPGqBPozTffRJMmTdCtWzccP34cBQsWzLZ2ERFR/sTUeiIiyleefPJJAEBkZCQAoF+/fggICEBERAQ6duyIwMBA9O7dGwCg1+sxbdo0VK9eHT4+PggNDcXrr7+O27dvGx1TURR88sknKFmyJPz8/NCqVSscP348w2ObmyO/Z88edOzYEQULFoS/vz9q1qyJr7/+Oq19M2bMAACjVHOVo9uYXnJyMgoVKoT+/ftnuO3u3bvw8fHB8OHD066bPn06qlevDj8/PxQsWBD169fHr7/+munjWDJu3DjodDqcOHECvXr1QsGCBdG0aVMAkppvKuDv168fwsPDja6z9rUyZeLEidDpdJg3b16GTI7y5cvj888/R0xMDGbPnm3UhoCAAJw/fx7t2rWDv78/ihcvjgkTJkBRFLvaFh4ejk6dOmH79u1o0KABfHx8UK5cOcyfPz/T50BERHkHA3kiIspXIiIiAACFCxdOuy4lJQXt2rVD0aJF8eWXX+L5558HALz++ut4//330aRJE3z99dfo378/fvnlF7Rr1w7Jyclp9x8zZgxGjx6NWrVq4YsvvkC5cuXQtm1bPHjwINP2rFu3Ds2bN8eJEycwZMgQfPXVV2jVqhX++eeftDaoo9YLFixI+1E5u42enp7o0qULli9fjqSkJKPbli9fjsTERPTo0QMA8MMPP2Dw4MGoVq0apk2bhvHjx6N27drYs2dPpq+DNbp37474+HhMmjQJAwcOtPn+1r5W6cXHx2PDhg1o1qwZypYta3KfF198Ed7e3ml/N1Vqairat2+P0NBQfP7556hXrx7Gjh2LsWPH2t22c+fOoVu3bmjTpg2++uorFCxYEP369bOqY4aIiPIIhYiIKA/6+eefFQDK+vXrlevXrysXL15UfvvtN6Vw4cKKr6+vcunSJUVRFKVv374KAGXEiBFG99+2bZsCQPnll1+Mrl+zZo3R9deuXVO8vLyUp59+WtHr9Wn7ffTRRwoApW/fvmnXbdq0SQGgbNq0SVEURUlJSVHKli2rlClTRrl9+7bR4xge6+2331ZM/ct2RhtNWbt2rQJA+fvvv42u79ixo1KuXLm035999lmlevXqFo+VmT/++MPoNVIURRk7dqwCQOnZs2eG/Vu0aKG0aNEiw/V9+/ZVypQpk/a7ta+VKYcOHVIAKEOGDLHY9po1ayqFChUyagMA5Z133km7Tq/XK08//bTi5eWlXL9+3ea2lSlTRgGgbN26Ne26a9euKd7e3sp7771nsX1ERJR3cESeiIjytNatWyMkJASlSpVCjx49EBAQgGXLlqFEiRJG+7355ptGv//xxx8ICgpCmzZtcOPGjbSfevXqISAgAJs2bQIArF+/HklJSXjnnXeMUt6HDh2aadsOHjyIyMhIDB06FMHBwUa3GR7LnOxoIyDTEYoUKYLFixenXXf79m2sW7cOL774Ytp1wcHBuHTpEvbt22fVcW31xhtv2H1fa18rU+7duwcACAwMtPgYgYGBuHv3bobrBw0alHZZp9Nh0KBBSEpKwvr16+1qW7Vq1dCsWbO030NCQlC5cmWcP38+8xeCiIjyBBa7IyKiPG3GjBmoVKkSPDw8EBoaisqVK8PNzbgf28PDAyVLljS67uzZs4iLi0PRokVNHvfatWsAgAsXLgAAKlasaHR7SEhIpkXP1DT/xx57zPonlM1tBOT1ef755/Hrr78iMTER3t7eWLp0KZKTk40C+Q8//BDr169HgwYNUKFCBbRt2xa9evVCkyZN7Hp+6ZlLa7eGta+VKWoArwb05ty7dy9DsO/m5oZy5coZXVepUiUAQFRUlF1tK126dIZ9ChYsaNVcfyIiyhsYyBMRUZ7WoEGDtKr15nh7e2cI7vV6PYoWLYpffvnF5H1CQkIc1kZ7ZWcbe/TogdmzZ2P16tV47rnn8Pvvv6NKlSqoVatW2j5Vq1bF6dOn8c8//2DNmjVYsmQJvvvuO4wZMwbjx4/Pcht8fX0zXKfT6TIUjgNkbrqhrLxWFSpUgIeHB44cOWJ2n8TERJw+fTrT95optrbN3d3d5H6mXgciIsqbGMgTERGZUL58eaxfvx5NmjQxGUCqypQpA0BGVQ1HXq9fv57pCGn58uUBAMeOHUPr1q3N7mcuzT472qhq3rw5wsLCsHjxYjRt2hQbN27Exx9/nGE/f39/vPjii3jxxReRlJSErl274tNPP8XIkSOdsvRewYIFTaaUq1kIKmtfK1P8/f3RqlUrbNy4ERcuXEh7PQ39/vvvSExMRKdOnYyu1+v1OH/+fNooPACcOXMGANKq6melbURElD9xjjwREZEJL7zwAlJTUzFx4sQMt6WkpODOnTsAZA6+p6cnpk+fbjQiOm3atEwfo27duihbtiymTZuWdjyV4bHU9dLT75MdbVS5ubmhW7du+Pvvv7FgwQKkpKQYpdUDwM2bN41+9/LyQrVq1aAoisWq8FlRvnx5nDp1CtevX0+77vDhw9ixY4fRfta+VuaMGjUKiqKgX79+ePjwodFtkZGR+OCDDxAWFobXX389w32//fbbtMuKouDbb7+Fp6cnnnrqKYe0jYiI8h+OyBMREZnQokULvP7665g8eTIOHTqEtm3bwtPTE2fPnsUff/yBr7/+Gt26dUNISAiGDx+OyZMno1OnTujYsSMOHjyI1atXo0iRIhYfw83NDTNnzkTnzp1Ru3Zt9O/fH2FhYTh16hSOHz+OtWvXAgDq1asHABg8eDDatWsHd3d39OjRI1vaaOjFF1/E9OnTMXbsWNSoUQNVq1Y1ur1t27YoVqwYmjRpgtDQUJw8eRLffvstnn766UwLxdnrlVdewZQpU9CuXTu8+uqruHbtGmbNmoXq1asbFZ6z9rUyp3nz5vjyyy8xbNgw1KxZE/369Uv7W/3www/Q6/VYtWpVhpoDPj4+WLNmDfr27YuGDRti9erVWLlyJT766KO0lPmsto2IiPIh1xXMJyIich51+bl9+/ZZ3K9v376Kv7+/2du///57pV69eoqvr68SGBio1KhRQ/nggw+UK1eupO2TmpqqjB8/XgkLC1N8fX2Vli1bKseOHVPKlCljcfk51fbt25U2bdoogYGBir+/v1KzZk1l+vTpabenpKQo77zzjhISEqLodLoMS9E5so2W6PV6pVSpUgoA5ZNPPslw++zZs5XmzZsrhQsXVry9vZXy5csr77//vhIXF2fV8RXF8vJz6nJt6S1cuFApV66c4uXlpdSuXVtZu3ZthuXnVNa8VpZs3bpVefbZZ5UiRYoonp6eSunSpZWBAwcqUVFRGfZV31sRERFK27ZtFT8/PyU0NFQZO3askpqaalfbypQpozz99NMZ7mtuGT4iIsqbdIrCyihEREREjtavXz/8+eefuH//vqubQkREeQznyBMRERERERHlIgzkiYiIiIiIiHIRBvJEREREREREuQjnyBMRERERERHlIhyRJyIiIiIiIspFGMgTERERERER5SIerm5ATqTX63HlyhUEBgZCp9O5ujlERERERESUxymKgnv37qF48eJwc7M85s5A3oQrV66gVKlSrm4GERERERER5TMXL15EyZIlLe7DQN6EwMBAAPICFihQwMWtISIiIiIiorzu7t27KFWqVFo8agkDeRPUdPoCBQowkCciIiIiIqJsY830bha7IyIiIiIiIspFGMgTERERERER5SIM5ImIiIiIiIhyEc6Rt5OiKEhJSUFqaqqrm0J5mLu7Ozw8PLgMIhERERERpWEgb4ekpCTExMQgPj7e1U2hfMDPzw9hYWHw8vJydVOIiIiIiCgHYCBvI71ej8jISLi7u6N48eLw8vLiaCk5haIoSEpKwvXr1xEZGYmKFSvCzY2zYYiIiIiI8jsG8jZKSkqCXq9HqVKl4Ofn5+rmUB7n6+sLT09PXLhwAUlJSfDx8XF1k4iIiIiIyMU4vGcnjoxSduF7jYiIiIiIDDFCICIiIiIiIspFGMgTERERERER5SIM5ImIiIiIiIhyEQby+YBOp7P4M27cuGxtz/Hjx/HCCy8gJCQE3t7eqFSpEsaMGcPl/IiIiIiIiKzAqvX5QExMTNrlxYsXY8yYMTh9+nTadQEBAWmXFUVBamoqPDyc89bYvXs3WrdujdatW2PlypUIDQ3F3r178d5772HDhg3YtGkT10snIiIiIiKygCPyDqAowIMH2f+jKNa1r1ixYmk/QUFB0Ol0ab+fOnUKgYGBWL16NerVqwdvb29s374d/fr1w3PPPWd0nKFDh6Jly5Zpv+v1ekyePBlly5aFr68vatWqhT///NPC66Tg1VdfRdWqVbF06VI0aNAAZcqUQffu3fH3339j165dmDp1atr+Op0OM2fORIcOHeDr64ty5cplOP7FixfxwgsvIDg4GIUKFcKzzz6LqKiotNvV5/Hll18iLCwMhQsXxttvv43k5GTrXjwiIiIiIqIcxqWB/NatW9G5c2cUL14cOp0Oy5cvz/Q+mzdvRt26deHt7Y0KFSpg7ty5GfaZMWMGwsPD4ePjg4YNG2Lv3r2Ob7yB+HggICD7fxyZiT5ixAh89tlnOHnyJGrWrGnVfSZPnoz58+dj1qxZOH78ON5991289NJL2LJli8n9Dx06hBMnTmDYsGEZllSrVasWWrdujUWLFhldP3r0aDz//PM4fPgwevfujR49euDkyZMAgOTkZLRr1w6BgYHYtm0bduzYgYCAALRv3x5JSUlpx9i0aRMiIiKwadMmzJs3D3PnzjX5viEiIiIiIsoNXBrIP3jwALVq1cKMGTOs2j8yMhJPP/00WrVqhUOHDmHo0KEYMGAA1q5dm7bP4sWLMWzYMIwdOxYHDhxArVq10K5dO1y7ds1ZTyNPmDBhAtq0aYPy5cujUKFCme6fmJiISZMmYc6cOWjXrh3KlSuHfv364aWXXsLs2bNN3ufMmTMAgKpVq5q8vWrVqmn7qLp3744BAwagUqVKmDhxIurXr4/p06cDkL+1Xq/Hjz/+iBo1aqBq1ar4+eefER0djc2bN6cdo2DBgvj2229RpUoVdOrUCU8//TQ2bNhgzctCRERERESU47h0jnyHDh3QoUMHq/efNWsWypYti6+++gqABH7bt2/H1KlT0a5dOwDAlClTMHDgQPTv3z/tPitXrsScOXMwYsQIxz8JAH5+wP37Tjl0po/rKPXr17dp/3PnziE+Ph5t2rQxuj4pKQl16tSxeF/F2jkBABo1apTh90OHDgEADh8+jHPnziEwMNBon4SEBERERKT9Xr16dbi7u6f9HhYWhqNHj1rdBiIiIiIicr3r14HYWOCxx1zdEtfLVcXudu3ahdatWxtd165dOwwdOhSABJH79+/HyJEj0253c3ND69atsWvXLrPHTUxMRGJiYtrvd+/etaldOh3g72/TXXIc/3RPwM3NLUPAbTiv/P6jnouVK1eiRIkSRvt5e3ubfIxKlSoBAE6ePGky2D958mTaPta4f/8+6tWrh19++SXDbSEhIWmXPT09jW7T6XTQ6/VWPw4REREREbnec88Bu3YBa9cC6cYT851cVezu6tWrCA0NNbouNDQUd+/excOHD3Hjxg2kpqaa3Ofq1atmjzt58mQEBQWl/ZQqVcop7c9NQkJCjKrdA0gbCQeAatWqwdvbG9HR0ahQoYLRj7nXr3bt2qhSpQqmTp2aIZA+fPgw1q9fj549expdv3v37gy/q6n5devWxdmzZ1G0aNEMbQgKCrL3qRMRERERUQ5z5w6wc6cU/B40CDAYh82XclUg7ywjR45EXFxc2s/Fixdd3SSXe/LJJ/Hff/9h/vz5OHv2LMaOHYtjx46l3R4YGIjhw4fj3Xffxbx58xAREYEDBw5g+vTpmDdvnslj6nQ6/PTTTzhx4gSef/557N27F9HR0fjjjz/QuXNnNGrUKC27QvXHH39gzpw5OHPmDMaOHYu9e/di0KBBAIDevXujSJEiePbZZ7Ft2zZERkZi8+bNGDx4MC5duuS014aIiIiIiLKX4fjemTPAlCmua0tOkKsC+WLFiiE2NtboutjYWBQoUAC+vr4oUqQI3N3dTe5TrFgxs8f19vZGgQIFjH7yu3bt2mH06NH44IMP8Pjjj+PevXt4+eWXjfaZOHEiRo8ejcmTJ6Nq1apo3749Vq5cibJly5o9buPGjbF79264u7ujQ4cOqFChAkaOHIm+ffti3bp1GdLyx48fj99++w01a9bE/PnzsWjRIlSrVg0A4Ofnh61bt6J06dLo2rUrqlatildffRUJCQn8GxIRERER5SE7dshWndU7cSIQHe269riaTrGl8pgT6XQ6LFu2LMPa5YY+/PBDrFq1yqhQWa9evXDr1i2sWbMGANCwYUM0aNAgrbK5Xq9H6dKlMWjQIKuL3d29exdBQUGIi4vLEBAmJCQgMjISZcuWhY+Pj43PkmxhzXsiP+B7joiIiIjyu6eeAjZuBGbNAn75Bdi2DejaFViyxNUtcxxLcWh6Lh2Rv3//Pg4dOpQ29zoyMhKHDh1C9KOulZEjRxqNAr/xxhs4f/48PvjgA5w6dQrfffcdfv/9d7z77rtp+wwbNgw//PAD5s2bh5MnT+LNN9/EgwcP0qrYExERERERUe6RkgLs2SOXmzQBZswA3N2BpUul8F1+5NKq9f/99x9atWqV9vuwYcMAAH379sXcuXMRExOTFtQDQNmyZbFy5Uq8++67+Prrr1GyZEn8+OOPaUvPAcCLL76I69evY8yYMbh69Spq166NNWvWZCiAR0RERERERDnf0aPAgwdAgQJAtWqAmxsweDAwdSrwzjtyu5mFs/KsHJNan5MwtZ5yEr7niIiIiCg/mzFDKtW3awc8mlGNu3eBypWBq1eBzz4DPvzQtW10hFyTWk9ERERERERkiVrornFj7boCBYDx4+XysmXZ3yZXYyBPREREREREOdbOnbJt0sT4+po1ZXv1ava2JydgIE9EREREREQ50uXLwIULMi++QQPj29QVxmNjgfw2YZyBPBEREREREeVIu3bJtmZNIDDQ+Da1nnlCgsyZz08YyBMREREREVGOpKbVG86PV/n6ylx5IP+l1zOQJyIiIiIiohzJVKE7Q2p6PQN5oizq168fnnvuubTfW7ZsiaFDh2Z7OzZv3gydToc7d+5k+2MTEREREVHWPHwIHDgglzML5GNjs6dNOQUD+XyiX79+0Ol00Ol08PLyQoUKFTBhwgSkpKQ4/bGXLl2KiRMnWrWvK4LvnTt3omPHjihYsCB8fHxQo0YNTJkyBampqdnWBiIiIiIiMvbff0BKChAWBoSHm96HI/KU57Vv3x4xMTE4e/Ys3nvvPYwbNw5ffPGFyX2TkpIc9riFChVCYPrKFDnEsmXL0KJFC5QsWRKbNm3CqVOnMGTIEHzyySfo0aMHlPxW/pKIiIiIKIcwnB+v05neRy14x0CebKcowIMH2f9jY5Dp7e2NYsWKoUyZMnjzzTfRunVr/PXXXwC0dPhPP/0UxYsXR+XKlQEAFy9exAsvvIDg4GAUKlQIzz77LKKiotKOmZqaimHDhiE4OBiFCxfGBx98kCH4TZ9an5iYiA8//BClSpWCt7c3KlSogJ9++glRUVFo1aoVAKBgwYLQ6XTo168fAECv12Py5MkoW7YsfH19UatWLfz5559Gj7Nq1SpUqlQJvr6+aNWqlVE7TXnw4AEGDhyIZ555Bt9//z1q166N8PBwDBgwAPPmzcOff/6J33//HQAQFRUFnU6H3377DY0bN4aPjw8ee+wxbNmyxeiYx44dQ4cOHRAQEIDQ0FD06dMHN27cMHotBg8ejA8++ACFChVCsWLFMG7cOIvtJCIiIiLKjywVulNxRJ7sFx8PBARk/098fJaa7evrazTyvmHDBpw+fRrr1q3DP//8g+TkZLRr1w6BgYHYtm0bduzYgYCAALRv3z7tfl999RXmzp2LOXPmYPv27bh16xaWLVtm8XFffvllLFq0CN988w1OnjyJ2bNnIyAgAKVKlcKSJUsAAKdPn0ZMTAy+/vprAMDkyZMxf/58zJo1C8ePH8e7776Ll156KS2QvnjxIrp27YrOnTvj0KFDGDBgAEaMGGGxHf/++y9u3ryJ4cOHZ7itc+fOqFSpEhYtWmR0/fvvv4/33nsPBw8eRKNGjdC5c2fcvHkTAHDnzh08+eSTqFOnDv777z+sWbMGsbGxeOGFF4yOMW/ePPj7+2PPnj34/PPPMWHCBKxbt85iW4mIiIiI8hNFsS2Qz29z5D1c3QDKfoqiYMOGDVi7di3eeeedtOv9/f3x448/wsvLCwCwcOFC6PV6/Pjjj9A9ymX5+eefERwcjM2bN6Nt27aYNm0aRo4cia5duwIAZs2ahbVr15p97DNnzuD333/HunXr0Lp1awBAuXLl0m4vVKgQAKBo0aIIDg4GICP4kyZNwvr169GoUaO0+2zfvh2zZ89GixYtMHPmTJQvXx5fffUVAKBy5co4evQo/ve//1lsCwBUrVrV5O1VqlRJ20c1aNAgPP/88wCAmTNnYs2aNfjpp5/wwQcf4Ntvv0WdOnUwadKktP3nzJmDUqVK4cyZM6hUqRIAoGbNmhg7diwAoGLFivj222+xYcMGtGnTxmxbiYiIiIjyk7NngRs3AG9voE4d8/vl1xF5BvKO4OcH3L/vmse1wT///IOAgAAkJydDr9ejV69eRmndNWrUSAviAeDw4cM4d+5chvntCQkJiIiIQFxcHGJiYtCwYcO02zw8PFC/fn2zc8sPHToEd3d3tGjRwup2nzt3DvHx8RkC3aSkJNR59Kk+efKkUTsApAX9mbFlHrzhMdXnevLkSQDyem3atAkBAQEZ7hcREWEUyBsKCwvDtWvXrG4DEREREVFepy47V7++BPPm5Nc58gzkHUGnA/z9Xd2KTLVq1QozZ86El5cXihcvDg8P4z+/f7rncP/+fdSrVw+//PJLhmOFhITY1QZfX1+b73P/USfJypUrUaJECaPbvC19qjOhBtYnT55EYxP5OidPnkS1atVsamfnzp1NZgGEhYWlXfb09DS6TafTQa/XW/04RERERER53aZNss1s/M8wtV6vB9zyyeTxfPI0CZBAvUKFCihdunSGIN6UunXr4uzZsyhatCgqVKhg9BMUFISgoCCEhYVhz549afdJSUnB/v37zR6zRo0a0Ov1GYrEqdSMAMOl36pVqwZvb29ER0dnaEepUqUASHr83r17jY61e/dui8+vbdu2KFSoUFo6vqG//voLZ8+eRc+ePc0eU32uamp+3bp1cfz4cYSHh2doZ/pOEiIiIiIiMk1RtED+US1ss4oWlW1qKnDrlnPblZMwkCezevfujSJFiuDZZ5/Ftm3bEBkZic2bN2Pw4MG4dOkSAGDIkCH47LPPsHz5cpw6dQpvvfWWxTXgw8PD0bdvX7zyyitYvnx52jHV6vBlypSBTqfDP//8g+vXr+P+/fsIDAzE8OHD8e6772LevHmIiIjAgQMHMH36dMybNw8A8MYbb+Ds2bN4//33cfr0afz666+YO3euxefn7++P2bNnY8WKFXjttddw5MgRREVF4aeffkK/fv3QrVu3DIXqZsyYgWXLluHUqVN4++23cfv2bbzyyisAgLfffhu3bt1Cz549sW/fPkRERGDt2rXo378/16QnIiIiIrLSuXPApUuAp6flQneA7FOkiFzOT+n1DOTJLD8/P2zduhWlS5dG165dUbVqVbz66qtISEhAgQIFAADvvfce+vTpg759+6JRo0YIDAxEly5dLB535syZ6NatG9566y1UqVIFAwcOxIMHDwAAJUqUwPjx4zFixAiEhoZi0KBBAICJEydi9OjRmDx5MqpWrYr27dtj5cqVKFu2LACgdOnSWLJkCZYvX45atWph1qxZRkXnzOnWrRs2bdqE6OhoNGvWDJUrV8bUqVPx8ccf47fffksr8qf67LPP8Nlnn6FWrVrYvn07/vrrLxR59M1RvHhx7NixA6mpqWjbti1q1KiBoUOHIjg4GG75JceHiIiIiCiL1NH4J56wrixYfpwnr1NsqfSVT9y9exdBQUGIi4tLC1hVCQkJiIyMRNmyZeHj4+OiFlJ2i4qKQtmyZXHw4EHUrl07Wx+b7zkiIiIiyk969gR++w0YMwYYPz7z/Vu3BjZsABYsAF56yfntcxZLcWh6HCYkIiIiIiKiHMGW+fGq/LgEHQN5IiIiIiIiyhFOnZIK9N7eklpvDcPK9fkFl58jskJ4eLhN680TEREREZHt1NH4xo0Ba2eV5sc58hyRJyIiIiIiohxBDeSffNL6+zC1nqzG0VnKLnyvEREREVF+oNcDmzfLZWvnxwMM5MkKnp6eAID4+HgXt4TyC/W9pr73iIiIiIjyouPHgRs3ZMm5xx+3/n6cI0+Zcnd3R3BwMK5duwZA1lpPv9Y4kSMoioL4+Hhcu3YNwcHBcHd3d3WTiIiIiIicRk2rb9oU8PKy/n7qHPkbN4DkZCA/jH8xkLdDsUddPmowT+RMwcHBae85IiIiIqK8ytZl51SFCwPu7kBqKnD9OlC8uOPbltMwkLeDTqdDWFgYihYtiuTkZFc3h/IwT09PjsQTERERUZ6n1wNbtshlWwN5d3egaFEgJkbmyTOQJ4vc3d0ZZBEREREREWXR4cPA7dtAYCBQr57t9y9WTAL5/DJPnsXuiIiIiIiIyKXUtPpmzQAPO4ab89ta8gzkiYiIiIiIyKU2bpStrWn1qvy2BB0DeSIiIiIiInKpvXtl27y5ffdnIE9ERERERESUTR48kGrzAFCpkn3HUFPrOUeeiIiIiIiIyMmio2UbFAQEB9t3DI7IExEREREREWWTqCjZhofbfwwG8kRERERERETZRA3ky5Sx/xgM5ImIiIiIiIiyyYULss3KiLw6Rz4uDkhIyHKTcjwG8kREREREROQyjkitDw4GvLzkcn4oeMdAnoiIiIiIiFzGEan1Ol3+Sq9nIE9EREREREQu44gReYCBPBEREREREZHTPXyopcJnNZBX58kzkCciIiIiIiJyEnUN+YAAoGDBrB1LHZHnHHkiIiIiIiIiJzFMq9fpsnYsptYTEREREREROZkjlp5TMZAnIiIiIsrEzJlAWBhw+LCrW0JEuZUjKtarOEeeiIiIiCgTCxfKCfPy5a5uCRHlVo6qWA9wjjwRERERkUWKApw8KZePHHFtW4go93JWar2iZP14ORkDeSIiIiKy2fXrwO3bcpmBPBHZyxmp9fHxwP37WT9eTsZAnoiIiIhsduqUdjkiIu+fNBOR4yUmAleuyGVHjMgHBAD+/nI5r8+TZyBPRERERDZT0+oBSWE9ftx1bSGi3OniRdn6+QFFijjmmPmlcj0DeSIiIiKymeGIPMDK9URkO8O0+qyuIa8KC5NtTIxjjpdTMZAnIiIiIpupgbw6isZ58kRkK0dWrFeVKCHby5cdd8yciIE8EREREdlMTa1//nnZMpAnIls5smK9ioE8EREREZEJ8fHaCfgLL8j2yJG8v9wTETmWM0bkixeXLQN5IiIiIiIDp0/LtkgRoGlTwMMDiIvTClcREVnDkUvPqdQRebUafl7FQJ6IiIiIbKLOj69SBfDyAqpWld+ZXk9EtmBqvf0YyBMRERGRTdT58WoAX7OmbBnIE5G1kpK0YNtZgXxenu7DQJ6IiIiIbGI4Ig8wkCci2126BOj1gI8PULSo446rzpFPSABu33bccXMalwfyM2bMQHh4OHx8fNCwYUPs3bvX7L7JycmYMGECypcvDx8fH9SqVQtr1qwx2mfcuHHQ6XRGP1XU/zJERERElGUckSeirFLT6h25hjwgHQOFCsnlvJxe79JAfvHixRg2bBjGjh2LAwcOoFatWmjXrh2uXbtmcv9Ro0Zh9uzZmD59Ok6cOIE33ngDXbp0wcGDB432q169OmJiYtJ+tm/fnh1Ph4iIiCjPS00FzpyRy+pYSa1asj19WkbBiIgy44yK9ar8UPDOpYH8lClTMHDgQPTv3x/VqlXDrFmz4Ofnhzlz5pjcf8GCBfjoo4/QsWNHlCtXDm+++SY6duyIr776ymg/Dw8PFCtWLO2nSJEi2fF0iIiIiPK8yEiZ2+rjo1WaLlZMKtjr9cDx465tHxHlDs6oWK/KDwXvXBbIJyUlYf/+/WjdurXWGDc3tG7dGrt27TJ5n8TERPj4+Bhd5+vrm2HE/ezZsyhevDjKlSuH3r17Izo62mJbEhMTcffuXaMfIiIiIspInR9fuTLg9uhMUqdjej0R2cYZFetVDOSd6MaNG0hNTUVoaKjR9aGhobh69arJ+7Rr1w5TpkzB2bNnodfrsW7dOixduhQxMTFp+zRs2BBz587FmjVrMHPmTERGRqJZs2a4d++e2bZMnjwZQUFBaT+lSpVyzJMkIiIiymPUQF6dH69iIE9EtnBmar1a8I6BfA7x9ddfo2LFiqhSpQq8vLwwaNAg9O/fH25u2tPo0KEDunfvjpo1a6Jdu3ZYtWoV7ty5g99//93scUeOHIm4uLi0n4sXL2bH0yEiIiLKddRCd+lrCTOQJyJbZEdqPefIO0GRIkXg7u6O2NhYo+tjY2NRrFgxk/cJCQnB8uXL8eDBA1y4cAGnTp1CQEAAypUrZ/ZxgoODUalSJZw7d87sPt7e3ihQoIDRDxERERFllNmI/OHDltduVhTgo4+AGTOc0z4iyvlSUmT5OSCLI/KKAmzbBty8aXQ1U+udyMvLC/Xq1cOGDRvSrtPr9diwYQMaNWpk8b4+Pj4oUaIEUlJSsGTJEjz77LNm971//z4iIiIQFhbmsLYTERER5UeKYn5Evlo1mTN/8yZgZpYkAGDfPmDyZGDIECmaR0TOd+YM8L//ZYh3XebyZVkBw8tLimXabcIEoHlz4NlnjXoQ80Mg7+HKBx82bBj69u2L+vXro0GDBpg2bRoePHiA/v37AwBefvlllChRApMnTwYA7NmzB5cvX0bt2rVx+fJljBs3Dnq9Hh988EHaMYcPH47OnTujTJkyuHLlCsaOHQt3d3f07NnTJc+RiIiIKK+4fh24fVuK21WsaHybry9QqZKM2B85ApgbQ9m0SbapqVIBv3Jl57aZKL9bsgTo1w+4fx/YuBFYs8byuu3x8cCxY5Jdc+SI7Pv440DDhvK5d8Sa72pafenSWtFMm82fD4wbJ5d37ADWrwfatAGgBfLXrgHJyYCnZ1ZamzO5NJB/8cUXcf36dYwZMwZXr15F7dq1sWbNmrQCeNHR0Ubz3xMSEjBq1CicP38eAQEB6NixIxYsWIDg4OC0fS5duoSePXvi5s2bCAkJQdOmTbF7926EhIRk99MjIiIiylPU0fiyZSVwT69mTS2Qb9fO9DHUQB6QUUIG8kTOkZIi01i++EK77t9/gXnzJLBPb9Mm4J13gBMnzE+PKVRIAvr33wdatbK+LdeuyWOrnQMHD8r1dqfVb94MDBigHSQqChg/HmjdGtDpUKSIBO/JyZIhlBdrmesUxdIspvzp7t27CAoKQlxcHOfLExERET0yezbwxhtAx47AypUZb//0U2DUKOCll4AFCzLenpwMFCwIPHggv3/1FTBsmHPbTJQfXbsG9OihdZy9/z4QFCSfz+BgCdYNs2YOHwaaNpVRewAoWhSoVUs651JTgT17gAMHgMRE7T6DB8s0GT8/y205cABo2zZjWr+7u9TKeP11G5/cyZNA48bAnTvACy8AU6cC5csDCQkyKv/UUwCkiF50NLBrF/DEEzY+hovYEoe6dESeiIiIiHIPdUQ+faE7Va1asj182PTt+/ZpQTwgI/JE5FhJSUCzZvL5CggAfv4Z6NZNRuiXLQP27wcGDZKUe0CKzj39tATxrVoBv/5qet56UpKMpv/wA/D998A330ia/vz5Mkpvyu7dQPv2QFycpOW3bSvfE7VqAdWrA/7+Nj65a9eksXfuAI0aAXPnSnrQa69Jg8aPB558EtDpUKKEBPJ5dZ58rlp+joiIiIhcR61Yn77QnapePdkeO6ZVpDa0ebNsvbxke/asQ5tHRJBg+8wZoEABYO9eCeIBwMMD+Okn2S5dCvz5J3D3rsTFly9LwcqlS80Xn/PyAurXl8yc1atlrfYzZ2Rw/L33gPSLhG3ZIlPW4+JktP+//4BvvwUGDgQaNLAjiAeAV16R4hrlygErVmhzfD74QBq4bVvaF01eL3jHQJ6IiIiIrJLZiHxYmIwEKgqweHHG29U0XzWwYCBP5HjHj8u2bt2Mn9VatYARI+Ty228DXbtK4F+sGLBqlaTdW6N9e+mw690b0OuBKVNkxL1pUxmxX7YM6NBBRvmfekpG7rM8YzkxEVi3Ti7/+SdgWAOtRAnpIQBkVB4M5ImIiIiI8OCBpKkC5kfkAUBdKGjRIuPrExOlsDSgzYm9eFEqZBOR46iBfPXqpm8fNUoC/GvXgA0bZI7733/LnHJbFCwILFwo923XTqrP79ghWe5duwIPH8po/z//2Dn6nt6hQ5LfX6QIULt2xttHjJBR+S1bgC1bULy4XH3ligMeOwdiIE9EREREmVLT6osUAQoXNr9ft25SxGr/fuM58Hv3yol90aIyal+woFwfEeG8NhPlRydOyNZcIO/tDcyZI8vIublJ9kz9+vY/XqdOMuJ+8aKsVV+tmlzfrZuk6vv42H9sI3v3yrZBA9Nr4JUsKan3ADBhAkfkiYiIiIjUAnY1a1reLyRECloBxqPyalp9y5bG69Cz4B2RY2U2Ig9IFfedO6UafadOjnnc4sVlqvqxYxI8//67Vg/DIfbskW2DBub3GTlS1p3buBGVHx4CwECeiIiIiPKxQ4dkW6dO5vsapterCx2rgby69nSlSrLlPHkix7l/X5ZUB7SRcXOeeCJrI/Hm6HQS1JsaNM8SNZA3VyIfAEqXBpo0AQCUjJMejcuXte+hvISBPBEREVE+pyjAH39oKbmmqIG8qamp6T33nKTTnj4t90tIkLWcAS2Q54g8keOpBSmLFpVpMHnGrVtaWXxLI/KABPMACj24CEDqe9y758zGuQYDeSIiIqJ8btUq4IUX5McUvd62QD4wUEvXXbRI1pJOTJTK2OpIvBrIc0SeyHGsSavPldT58RUrAoUKWd63ZEkAgFfsRQQFyVV5Mb2egTwRERFRPvfzz7I9fhy4fj3j7VFRMqLl7Q1UrmzdMQ3T6zdulMutWmnptkytJ3K8zArd5VrWzI9XlSol20uX8nTBOwbyRERERPnYrVuyfJRq9+6M+6ij8Y89JnWkrNGxo6wbfekS8N13cp2aVg9oI/KxscDduzY3m4hMyPMj8pbmx6sejcjj4kUG8kRERESUN/3+uyzNrNq5M+M+Bw/K1pq0epWPj6wlDQA3b8rWMJAvUAAIDZXLHJUncgw1kM+s0F2uoijWFbpTqSPyBoF8XlxLnoE8ERERUT42f75sa9SQrVqUzpAt8+MNqen1gAySlS9vfDsL3hE5zv37wIULcjlPjcifPy+9gV5eQK1ame+vBvI3bqB00QQAHJEnIiIiojzk7FkJ3N3cgGnT5Lp9+4DkZOP97A3kn3xSqmcDxvPjVZwnT+Q4asX60FCgcGHXtsWh1LT62rWlUEdmChYEfH0BABV9LwFgIE9EREREeciCBbJt2xZo2RIIDgbi44EjR7R9btyQee6AdYNhhjw8gLfflsuGo/MqjsiTs6SkAIsXA1evurol2SfPzo+3Ja0ekB7DR6Py4R4M5ImIiIgoD9HrtUD+5ZdlVP6JJ+R3w/T6w4dlW6GCLCtnq9GjJSu2Q4eMt3EJOnKW0aOBHj2AYcNc3ZLskyfnxwO2B/JAWiBfXC9ryTOQJyIiIqI8Yds2WVauQAHguefkusaNZWtY8M7etHqVTmd+2Wc1tf7MGalnReQIJ04AX34pl02twpBX5ckR+aQkrdqmNUvPqR5Vri/yUAL5q1eB1FRHN861GMgTERER5UNqkbvu3dOmk6JRI9kajsjbU7HeWmrxuzt3tMr2RFmhKDKdIyVFfo+MzD/LG+bJQP7IESAxUXoDK1Sw/n6PRuQD7lyCu7tkIMXGOqmNLsJAnoiIiCifiY8H/vhDLr/8snZ9gwaSYh8VBcTEyHVZHZG3xM9PKzDN9HpyhEWLgM2bpXNKzQQxrPmQV927B0RHy+U8lVqvptU3aJCxWqYlj0bk3S5dRLFiclVeS69nIE9ERESUz6xYISf+4eFA06ba9QUKAI89Jpd37QIePgROnZLfnRHIAyx4R44TF6fNif/4Yy3DRK3zkJfl+Yr1tsyPB0yuJc9AnoiIiIhytUWLZNunj4zAGzJMrz9+XOaVFikCFC/unLZwCTpylNGjJX26UiVg+HCgZk25Pj8E8nkyrR4wHpG3hRrIX7qUFshfueK4ZuUEDOSJiIiI8hl1kOvppzPeZljwTk2rr1PHtqxWW3BEnuwRHQ2cOydLI964IUXtZsyQ22bMkOXG1eUS80NqfZ4M5G/fBk6flsu2BvKPUutx8ybCi8YDyHsj8h6ubgARERERZZ9r12TUUqfT0ugNqSPy+/drQYGz0uoBLkFHtvvlF+Cll0zf1qMH0Lq1XFYD+aNHJbPE3T172ucKeTKQ37dPtuXLS1qQLYKDAX9/4MEDVPK/DKBingvkOSJPRERElI8cPSrb8uXlPDe9ChXknDkxEVi8WK5zZiBvmFrPJegoMw8fAiNGyGU/P8DLS7stLAz46ivt94oVAR8fKe4YEZHxWElJwKefAn/+KVXNc7MTJ2SbpwrdbdggWzVNyBY6XVp6fTnPvLmWPAN5IiIionxETTNW5w+np9Npo/Lqsl3ODOTLlpV5+g8eaJXyicz57jtJpy9VSpYsTEyUpeYePJB0e8NaDu7uWtaJqXnyP/4IjBolSzDWqAH89lvuXGvcsGJ9nhqRX7VKth072nf/R+n1JRQJ5DlHnoiIiIhyrcwCeUAL5AEZ0VRHzZ3By0uCeYDp9WRZXBwwaZJcHj9e3puABOx+foCHiUnDlubJr1wpW51ORrR79pTAX12aMbdQR+OLFdOW3Mv1Ll4Ejh2TXr62be07RtqI/CVs2AAsX+645uUEDOSJiIiI8hFrAnnDTNYaNUwHSI7EgndkjS+/BG7dAqpWlRUXrKEG8ulH5BMSgE2b5PLWrcCECUDBgrLc4gsvAP/847h2O1uenB+/erVsGzWyv3fiUSDve+MinnxS+57JKxjIExEREeUTKSnaSb+lQL5+fa0wmDPT6lVcgo4yExsLTJkilz/91PrOJXOB/JYtMt++RAmgSRNZui4qCujbV24fMSL3pNnnyUA+q2n1gFa5/uLFrLcnB2IgT0RERJRPnD0rc4r9/bV0dlP8/bUAPjsCeY7IU2Y++USK1jVoADz3nPX3UzusoqNlNTOVOuDboYO2tGKBAsDUqVLw/PhxYOFCR7Tc+dRlIk2tQpErJSYC69fL5awE8gZryedFDOSJiIiI8gm1Yn2NGjL11JIvvwQGDABeftn57eISdGTJ+fPA7Nly+bPPtMDbGsHBQOnScll9/wNaIN++vfH+BQsCI0fK5TFjJAU/J1MUWSoSAB5/3LVtcZht26R6YViYllJhD47IExEREVFeYM38eFXLlsAPPwABAU5tEgAttT4iIvekM1P2GTsWSE6WmmetWtl+//Tp9efPS/aHh4e25ryhd96RlPvoaGDmTPvanJoqVfWdLSJCigB6e+eh1Ho1rd4wXcIe6oj87dvSMZDHMJAnIiIiyifUQL5GDde2I73SpaV6fWJinh08Izs9fCjLwgEyN94e6QP5NWtk27gxEBSUcX9fX2DcOO0x4+JsezxFkYJ5RYtqae/O8t9/sq1dG/D0dO5jZRtHzI8H5I8bGCiX82B6PQN5IiIionzClhH57OTuDpQvL5eZXk+GDh6UIo3FigH16tl3jPSBvOH8eHP69QOqVJFR9S+/tO3x/vwTWLoU0OuBf/+1ubk2UQN5e1+bHCciAjh92ny6hK3ycHo9A3kiIiKifCAuDrhwQS7ntBF5gAXvyLR9+2T7+OP2Z1mrgfyxY5JhvXGj/G4pkPfw0DIApkwBrl617rHu3gWGDNF+N7V+vSOpgXz9+s59nGyj9rI0bWo6XcJWebjgHQN5IiIionxALfRVqpQU9MppuAQdmbJ3r2wbNLD/GOXKAX5+Urhuzhypfl+8eOaZKV26AA0byv5vv21d4bsxY4CYGJkqAjg3kNfrtUJ3eSaQd1RavUoN5DkiT0RERES5UU5Nq1dxRJ5McUQg7+6uZaF89ZVs27fPfIRfpwO++EJWeFi6FGjWTArgmXPgADB9ulz+7jvZnjwJJCXZ33ZLzpwB7t+XOf1VqzrnMbJVfDywaZNcdlQgr6bWc0SeiIiIiHIjdUTeqYG8Xg/MmKFVE7NBThuRVxRXt4Bu3QLOnZPLWR1xVtPr1ekl6ZedM6dZM8n2LlRI0tjr1tWWODeUmgq88YZ8BHr0AF55RTLDU1KAU6ey1nZz1LT6OnVkKkCut3mzpD2ULg1Uq+aYY3JEnoiIiIhys2wZkf/5Z2DQICnZbeMwpDoiHxkpS4250u7dMv1g1izXtiO/UwPVChUkkM4Kw+XI3d2BNm2sv2/btpLCXreuFL9r105S6LduBaKi5P06e7bM5y9QQObU63TaZ81Z6fV5Oq0+K8vOGWKxOyIiIiLKrfT6bBiRv34d+OADuXzvnlalzErFi8s85tRUCeZdacUKKQ7o7IrjZJkj0upVhoF8o0ZAcLBt9w8PB7ZvB/r3l8/TxIlAixZA2bKAjw8weLDs9+mnQFiYXHZ2IJ+nCt2lpgJ//y2XHZVWD7DYHRERERHlXhcuSGzt5aWNfDvc8OGSC61SS4NbSafT2ubq9PoTJ2R7965r25HfOTKQN1ypwVK1ekt8fYGffgLmzgWefFIyBby8JLBPTZXCeG++qe3vzEA+NVXm5AN5JJBfu1YKEBQs6Jhl51RqIH/njhQUyEMYyBMRERHlcWogUa0a4OnphAfYuBGYP1+i8f795boNG2w+TE4peHf8uGwZyLuOojg2kC9QQNZa9/AAnnvO/uPodEDfvvL2PnsWePhQqtTv3y9z593dtX2dGcifOiW14QICtPoSuZpaHbB/f+kxcZTAQPnjA3kuvZ6BPBEREVEe59T58YmJ2jDkm28CI0bI5V27JNKwQU4oePfwIXD+vFy+d8917cjvLl4EYmMl8K5d2zHH/OsvSUd3VB01QCraFysm8+cDAoxve+wx2cbEyMwTR1LT6uvWNe48yJWiorT58W+84fjj59H0egbyRERERHmcUwP5zz6TIfRixYBJk2RYvWRJKXa3c6dNh8oJI/KnTmkV6zki7zpqiYUaNRw3QFu8uPFceWcLCADKl5fLao0KR1ED+Xr1HHtcl5g9Wz50bdo4Z+5PHq1cz0CeiIiIKI9zWqG7M2ckeAeAr7+W9bZ0OplADNicXp8TRuTV+fEAA3lXcmRavSupnzl7A/nUVNMDyXmm0F1iohQeAIwLDDhSHl1LnoE8ERERUR4WH68Fxg4P5D/5REbe27cHunfXrlcDeRsL3qmDcRcvSoq7K6jz4wGpjZWa6pp25Hd5LZC3d578Rx/JgPLUqdp1KSnAoUNyOdcH8kuWyLyDEiWAzp2d8xgckSciIiKi3ObECamqXbQoEBrq4IPv2SPboUON131WA/n//pN13KxUpIgM6isKEBHhuGbawnBEHshzha5zhdRUbcT58cdd25asUqvl2xvIr14t2+HDpbA7IO/RhASp4VahQtbb6FIzZ8r2tdekIIIz5NG15BnIExEREeVhaqztqIJhaR480Ib60x+8VCkZXtfrgS1brD6kTuf69Pr0gTwL3mW/06elA8Xf37GF6VxBHZE/dsz27I6HD7X3o14PvPiizGYxnB/vZm80d/Qo0KQJ0LatbW/yPXuAFi2Ab7+VRmXF0aPA9u1SrW/AgKwdyxIWuyMiIiIiZ0pJcfwxd+yQbZMmDj7wsWMydB4aanqoP4vp9a4oeJeQoGUCqIODnCef/dS0+nr1cn9F9nLlAD8/eW+dO2fbfY8eleC/SBGgcWNJbnnmGa30hF1p9YoiQfjjj0sxynXrpIfAmi+fu3eBF14Atm4F3nlHitNduGBHIx5RR+O7dJFKhM5Spw6weDHw88/OewwXYCBPREREuUZCAvD008CgQa5uieN16waULq0tfeYoaiDftKljj5uWK2yuDPhTT8nWxkDelSPyp0/LIGOhQjJlF2Ag7wp5ZX48IB0R6jJ0tqbXHzgg23r1ZCp5yZLyHv31V7ne5kD++nXpCXjnHSky9+STsiTA6tVynbpcgznDhwPR0bJChZ+ffLZr1JAAObP7pnfvHrBggVx2VpE7VZEi0gGR2+dppMNAnoiIiHKN9etlueEZM7QANS948ABYtkzWm379ddvPic25eFHOu93dgYYNHXPMNIcPy9ZcIN+ypWyPHgWuXbP6sOqIvCsCebXQXbVqMlcfYCDvCmogn1fiLnsL3h08KNu6dSV2Xr4c8PHRbrcpkD90SD6r//wDeHsD33wjX6i//ipzWmbNAr780vz9164FfvhBLi9aJMdr3FgC8ldeAbp2lcKX1oiOlsJ29+8DlSsDrVrZ8ERIxUCeiIiIco01a7TLn37qunY42oED2nTT9euB+fMdc1y1s6N2bZlv7FCZBfIhIdptmzZZfVh1RN4VqfXqfOTq1aWQGMA58tktIUELePPCiDxgfyCvjsjXrSvbevWAOXPkcokSQNmyNhzsgw+kp7BqVekpeecdCeCfe04rif/BB8Aff2S87507wKuvyuXBg6WTrmJFSbH/7DPAy0t6Gd58M/NeyF9/lRdkyxb5Upo2zbhQJlmNgTwREeVIjhqRpLzFMJBfvRrYv991bXGkfftk6+cn23ffBWJjs35cp82PVxQtKrG0pp218+Q3bQKeeAI4fjxtRP7q1ewPog1H5NVAniPy2evwYSA5WfqBypRxdWscw55APjlZ218N5AGgZ09g2zbp8LM6/j1/XubC63TAypUZP7NDhkiADgB9+khwbljh/d13gcuXpUT+pEna9e7uwIcfShDv5ia9DNOmmW7DnTtAr15A794y2f+JJ+SP3b69lU+C0mMgT0REOcr581I/p3hx4/Wcic6elUJknp4yiATknVF5NZX4ww+lLtPt23JunVVOC+SjoiTC9fICqlQxv5+1gfyYMVINe84cBAVJEAdkf3q9OiJfrRoQGCiXGchnr127ZNugQd4ZqFWXoIuKsn41xhMnJFM9KCjjyHvTppY/dhn8+KNs27QxP4w/ZYrMn09MBEaOlIIdLVrIKP3cufLHmDvXdGpPhw5aWv7w4dqaeYB0+i1ZImkuixZJ8D9unPRGlC9vw5Og9BjIExFRjqDXA999JwMF69fLaNwHH7i6VZSTqKPxTZvKoJBOJ/PKjx1zbbscQR2Rb9xYzrnd3aXI8t9/23/Me/e07HeHB/LqgatVk54Vc5o3lydz7pzMizUlNlbrcTh9GoBrCt4ZVhU3TK1nIJ+9Fi+WrVorMS8oVEhbytza7ys1rb5OnSx2aCQna9XaX3vN/H7u7pJW//338rkFJHX+iy/k8rBhlr9Ihg6V9Hu9HujRAzh5UnouOnWSSp5XrsiI/vbtwNixzlszPh9hIE9ERC4XFSUDBW+/LUW/GjeW//GrVsl5BBGgBfLt28s0z+efl98nT3Zdmxzh5k2tUn39+pJG+9578vtbb9kfSO7ZI+fU4eFaBXaHyWx+vKpAAa1i2b//mt7nr7+0uTSPAnlXLEF35oy8XsHBUliMc+Sz3+nTwO7dElP26uXq1jiWren16efH2+2ff6RnPDRURtwt8fICBg6U+esXLgD/+580oG1bYOJEy/fV6aQ3vlkz+dJq3Vo6+latks6+UaPkyT/xRBafEKkYyBMRkUsdOiQnOBs3yio433wjGXcDB8rtI0dmnC+vKJJ2XLSoFk9Q3paQoNVL69BBth9/LNvffnNNhXNH+e8/2VasKEEkIANW5csDly4B779v33G3b5etw0fjAesDeUBG5ADzFfyWLdMuR0YCiYlOH5G/dSvj94o6lad6dYlJOCKf/dS3SPv2EnfmJWogf/Sodfs7LJD//nvZ9utnOXsmvdKlJS1u/36pWO/rm/l9vLwkjT48XEbgHz6U9PzDh6UjwJpjkNUYyBMRkcs8fCijLvfuydJYR45IIV03N2D0aPmfv3NnxvTiadMk4L9+XWrwsDBe3rd1q7xfihfX1mSuXVtiRL1eajPlVqaW2vLz01Z6+v57OTe2lUPmx5v7cGW2hryhvn3lQ71tW9qIe5q7d4ENG+SyuzuQmgpERDh1CboVK4DChYHx442vN5wfD3COfHbT67VlxV9+2bVtcQY1kP/jD+mE3LdPW6kivdRU6eQGshjIX7ggQTgADBiQhQPZICRE5si/8ILMqd+0SVKoyOFcHsjPmDED4eHh8PHxQcOGDbFX/W9mQnJyMiZMmIDy5cvDx8cHtWrVwhrD8rV2HJOIiFznww9lGl2xYpL9V6GCdltYmEy5A4CPPpITG0DOD4YPl8s6nZwjmMvYpbzDMK3ecL6oOio/f775Kdg5nTo/Pv2a2a1ayWcEkKmnUVHWHzMlRVKUATsD+dhY4Nln5cOZPu3l3j2pOghYrlivKlkS6NhRLv/0k/Ftq1ZJRa/KlWUyMACcPp02In/6tOM76tQEgMmTJc5RpQ/kOSKfvTZvlkLpQUGZZ4DnRs2by3vqxg2p8dGggQx6Dx8uGUeGzpwB4uOlQ0/9LNjlp5/kA/TUU8b/YJ2tShUpdtC3b96pWJgDuTSQX7x4MYYNG4axY8fiwIEDqFWrFtq1a4dr166Z3H/UqFGYPXs2pk+fjhMnTuCNN95Aly5dcPDgQbuPSURErrFmDTB9ulyeOxcoUiTjPh98ABQsKCmvv/wiQX+PHjKK8eqrMhoPSLBjbmSD8gY1kFfT6lVPPCHnqCkpWj2n3ERRtEDe1JrZEydKtkpcnGSvJCdbd9yjR4H79yUoql7dxkatXClltv/6C7h2TatGbXhwQNIjTH1wTVFHA+fNk8BdpUbVXbpIMA+kBfL+/lK9X80scBR1fCcpSaYwqAxT6wHOkc9ualr9iy8CPj6ubYszlCghHUe//AJ07w4EBMiKbl99lbHOh5pWX7u2JKrYJSVF6zizVOSOci/FhRo0aKC8/fbbab+npqYqxYsXVyZPnmxy/7CwMOXbb781uq5r165K79697T6moihKQkKCEhcXl/Zz8eJFBYASFxdn71MjIiJFUe7eVZS+fRVlwgRFuXVLu/76dUUpVkxRAEV55x3Lx/jf/2S/0qUVpXx5udysmaIkJirKjRuKEhQk1y1c6MxnQq4UFSV/Y3d3Rbl9O+Pts2fL7c2bZ3vTsuziRe25PXhgep/z57X3+UcfWXfc6dNl//btbWhMfLyiDBokdwQUpVw52Xp7K8rNm9p+330n13foYP2xk5K0D/2ff8p1Dx8qSkCAXLdnj3xRAPKloSjKq6/Kr3362PAcMhEXpyg6nfYUdTpFOXJEURIS5G8AKMqlS7LvmjXye+3ajnt8Mu3ePUXx95fXe8cOV7cmezx8qChTp8pzLlhQ/l+q3ntPrh80KAsPsGKFHKRIEXmDU64QFxdndRzqshH5pKQk7N+/H61bt067zs3NDa1bt8YudQHJdBITE+GTrovO19cX2x9Vc7HnmAAwefJkBAUFpf2UKlUqK0+NiIgemTtXBuDGjAHKlJEU+WvXgNdflyK6VatKUVxLBg2Sgb/oaMnmLVNG5gt7eck81xEjZL9Ro2T5W8p71NH4J57QisEZUpcq371b0lGtdeCA9RWknUUdHX7sMUmjNaVsWW2+/OTJ8nr89x/w9dcyDbVCBZliYJiCbnOhu7g4eYG//VZ+HzJEhqhr1ZIP1sKF2r62FLpTeXpKsS1AW9N6wwZJGyhRQsr1qwtjP5pHrw4i/vGHjMw7wv798jqVLi2joooi30tnzsj0nQIF5PsG4Bz57LR0qaxYUqEC0KiRq1uTPXx8pCZMpUry/lZr0gEOKnSnfmn06wd4e2fhQJRTuSyQv3HjBlJTUxGariRlaGgorl69avI+7dq1w5QpU3D27Fno9XqsW7cOS5cuRUxMjN3HBICRI0ciLi4u7efixYtZfHZERARIoTpAUgjv3ZMgpFQpOWnz9JQUw8yK2Pr5aemvAQFS+C4kRLt98GA58Y6KAmbOdMrTIBczl1avKl9epmEnJWnvucycPStxa+PGjgsS7WFufnx63btLYKso8jo8/rjUkPjjD+ngmjTJuPCjzYXuZs+WXg21UNW0aRJpqNH0999rB7cnkAdkPgwgxbeio7W0+ueek2J4Bqn1UBQ8/rhMwU9IMO5HyAq146RBA+DTTyVt+Z9/gFmz5Hq1Yj3AOfLZSU2rf/nl/DWl2t1dppABwJQp0memKA4I5GNjpf4EoC0BQ3mOy4vd2eLrr79GxYoVUaVKFXh5eWHQoEHo378/3Nyy9jS8vb1RoEABox8iIso6NahatkwqRT/+uDY9duJErbZVZgYMkCB940aZumvIz0+rPv3JJzKwSHlHUhKwfr1cbt/e9D46nTYqv3GjdccdN07mmz94APz+e5abaTdL8+PTmzpVi52Dg6V+3KefyvsekBH6oUNlHu6lSxIkWHNcpKbK+s+AlP83fKF795betuPHJeVBr9fmyNsayFeoIBX8FEVG5f/6S67v0kW2FSvKH/P2beD6deh0pvsRssKw46RiRS3GUZ++WugO4Bz57HLxova57dPHzoMkJ8sI9PjxUkwlF+nTR5JSrlyRDo3ISPk/5uVl/H60ydq18lmtUyeL1fIoJ3NZIF+kSBG4u7sjNjbW6PrY2FgUK1bM5H1CQkKwfPlyPHjwABcuXMCpU6cQEBCAcuXK2X1MIiJyjkuXZNDNzU1GPp95BtizB1i3TtLtbVkb280NeOMN86OW/fpJVu7Nm8Dnnzuk+ZRD7Nwp2ddFi1ru+LElkD92DFi0SPtdXfIqu+n12hrymY3IA9JptWOHpIHfvCk16T76SNLq1Szab77R4uK6daVgXKb++Uei/8KFgZ49jW8LCpL8fUAe5Px56f3w9kbaGnG2UIveffGFrB9ZsKCU8wakw6B0abn8KL1e7Uc4dky+P7LKcEQekGk/hlMaDAsDqoF8YiKn7TjTwoXSSdOihSw/brPVqyV147XXpIeuWjV5Ty1cKGtW5nBeXsB778nlzz/XOptq1rRt2Xcj6pJz5tKYKE9wWSDv5eWFevXqYYO6digAvV6PDRs2oFEmk2N8fHxQokQJpKSkYMmSJXj22WezfEwiInIstTRJrVqSEg/IYFvr1pI+mcVkKiMeHlrV36++As6dM73fjRtAs2Yy/TevO3VKXvsxY6R4cW6lptW3a2f5PdOqlWz/+y/zVOgxYyRwaNlSjrljh8SnzqTXZ6w4f/asjLz5+FhfWd7fX+Ln9K/FgAFSoFqnA9TFfKxOq1eXjxgwwPRcF3VY/LffZC14QCb1e3hY+QAGunaV4F1db6tzZ+NoJd08+eBgrR/BcA6xPa5eldFfnQ6oV0+uCwsDhg3T9jEcAVW/twCOyjuLohin1dvk1ClJS+nYUS6HhEjg6u4u71N1qPurr7T1S3OogQOBQoXkf9eECXKdtRlrGaSmaoG8uTQmyhuyofieWb/99pvi7e2tzJ07Vzlx4oTy2muvKcHBwcrVq1cVRVGUPn36KCNGjEjbf/fu3cqSJUuUiIgIZevWrcqTTz6plC1bVrltUMI2s2Naw5ZqgUREZNq770rBXIOFRJxKr1eUtm3lMVu3lt/T3961q1at+vz57GmXqwwbpj3XFi0U5coVV7fIPvXqyXNYsCDzfdVVDf75x/w++/bJPm5uinLihKK0aSO/jx9vfxtv3lSU48czFobW6xVl5075DISEKEpoqKIcPqzdvmCBPHajRvY/dnpz5mhV2dXi8BYdP669IFFRpvfR6xWlWjXZr1Qp2b7yiv2NHDxYe3MuW2b6tvfeS7tqxw65ys9PUe7csf9h//pLjlOtmvH1cXFSUN/PT1FiY41v8/OT+0RE2P+4ZF5EhLy+Hh7yd7Dajh1yJ0BRPD3l/aK+OS5dUpSJE2WpE8MvQXPv7xxi3DituYCizJxp54H27pUDFCggq0VQrmJLHOrSQF5RFGX69OlK6dKlFS8vL6VBgwbK7t27025r0aKF0vfREiSKoiibN29Wqlatqnh7eyuFCxdW+vTpo1y+fNmmY1qDgTwRUdY1bCjnEr/8kn2Pee6covj4mF6Obt4845OkrARuuUHVqtryWoCiFC2qKBs2uLpVtrl9W+JLQFFM/LvPYOBA2XfYMPP7tGtnvKTZ/Pnye/nyGTt/rBEXpyglS2pLyFWpIh1Gb7+tKGXLGr/n1L/DqVNyXzVmHTzY9se1ZMUKRXn/fSvP4d96Sxrx3HOW91PXyVJ/vv7a/gYeOSJvzMDAjGvuzZghx+/UKe0qw36E776z/2FHj5Zj9OuX8bbLlxXlzJmM16sr5h06ZP/jknlz5tjZmdW/v9yxaVPTfzhFUZSUFEX54QdtXbvAQEWZO9e+D3o2uHFDa6q6IqNd1GUcu3RxaPsoe+SqQD4nYiBPRJQ18fEySAIoSmRk9j72pEnyuCEh2tLXFy7I4IR6wqgukZ1Dz+eyTF133c1NUXbvVpQaNbSgftIkV7fOeuoyyJUqWbf/okWW1/3eulUb/VNHWA3Xr9650/Y2fvhhxmDd8MffX1FeekmeS+3acl2JEpIRor4X03c6ZZs7d7Qnn1kvz40bsp68+sQ2b87aY69dazpS2bBBjl+xotHV06Zpf1t7P7dqJ86MGdbfp1Iluc/WrfY9JlnWt6+8viNH2nAnvV7rPVu7NvP9z51TlMaNtffuCy8oSnKyvU12KjWTzd1d/o/aRX2us2c7tG2UPXLFOvJERJR37d8v84HDwmTd9+z03nsyz/X6deDDD2Vucr9+Mm/6iSdkznVAgMyJVtfazmvUeeWNGgENG0qRsFdf1dbMVpc2yuk2bZKtOv89M+p+hw5JMThDiiJF4QB5LR7VyUVAgEzbBmwvehcZKZXkASnAfumSTE2dMkWWRVy0CLh2TY77zDPAv/8CVasCly8DTz2lzWW3ptCdU8ybJ4XrqlXL/EUuXBh4/nnt95o1s/bYbduaLqmvLkF3/ry2xAVkurO3t/xt9++3/eEUxbYVAlRcgs65Nm+WbYsWNtzp9Gn5sHl7S9GTzJQvD2zdKms0enjIMhVLlpjf//vv5UvBnjdaFg0fLvUeu3fPfGlWk27fltUlACksQnkaA3kiInI4ddm5xo2zf01gLy9ZEhuQFa769pWA0M9PAqoCBeQkCZA4Ji9avVq2ap0jX195LdTf1XplOZ2tgXxoqFY0bssW49vWrpXn7e0NjBplfJu65NXixbZVJ//wQ4k1n3oK6NRJ6mq1bSvruX/9NdCjh3FF9JAQWUqvfHnpBEhIkKLwFSpY/5gOo9cD334rlwcNsu6D+sYbsq1SRQrWOUPx4tK7kpoKRESkXV2okNbh8scfth/2/Hng1i35frClDyIwULYM5B0vKkoWS3B3t6EwIyA9YgDQtKn10a67OzByJDBihPyuLvOQ3vXr0gu3bJn0gn74YbZWvi9eXF4Xw1U1bLJhg3y2q1bN/l50ynYM5ImIyOEMA3lXaNpUWx964ULZTpmiBUx9+8r299+B+Pjsb58zJSXJuRyQceWhpk1lqw7Y5GQ3bwKHD8vlli2tv58a9BsuQ3f1qozCA8BbbwElSxrf58kn5QT61i1g1SrrHmf7dgko3dzkvWVth1Xx4vL3KVVKfq9f37ErOFht3Topm1+ggPWLdzdrJkHU8uXOa5dOp617/ahyvUpdpU5dxt4W6mh87doSzFuLa8k7j9rZVr++8QoBmVq3TrZt29r+oK++Ku+xDRuMOorS/PCD9OapnUmffy7Lf6TvGXSiLHV+q+lYrFafLzCQJyIih1IULZB35cqfn30ma48DsjqRuoIWIPFIeLicnDszJnGFHTvMr7v+xBOyzQ2BvHreXK2ajLRbK/168klJkoFx5Yoca/z4jPdxdwd69ZLL1qTX6/XA0KFy+dVXbc8yL1NG2vfSS7LstUuoaSv9+9sWRbVpo6W/O0u6JehUjz0m2+PHbT9k+vXjrcXUeudRP+M2pdUnJWn5+G3a2P6g4eFaB8BPPxnflpwMfPedXP7uO2DFCul5O3tWehP/9z/bHy87KQoD+XyGgTwRETlURIRkJ3p5AXXruq4dhQoBf/4p2cBz5xqPcri5aWsW57X0esO0+vQjvY8/Lq9DVJSMUudkalq9LaPxgAQFOh1w8iQQEyM1E7Zvl4Bs2TItVTo9dVD6n39kZN6ShQtl+mxgIDBxom3tU1WoIJ0GapZEtlIUmTMMaD0YOYnaUXDqlNHV6rSJ6GjbA2t1RN7WegQM5J1HDeRt+ozv3i09lSEhMlJuDzVda84cCd5Vy5dLAYuiRYEXXpDCFidOaPuPGiW/51THj0v7fXysqx1AuR4DeSIicih1NL5+fZmP7ErNmgEzZ8o5X3pqIL9+vZz75BXp58cbKlBAC4b27Mm+NtnD1vnxqkKFtEyEN97QpoEvWKBlbJtSs6bEBcnJwKxZ5vd78ECm2gJSPM+WbIEcIypK5i54etofDDmTGsinG5EvWFAGSAHb4qmUFK1uma0j8pwj7xwXL0rdAjc3G+fHq2n1rVvbPyelc2cJ1mNjpedO9c03sn39de2fV1CQFL979ll5Iw0eLB1h1ti3D/jkk+ybl7F2rWxbtrSzUh7lNgzkiYjIoVw9P95a5cvLaKher82jz+0uXQKOHZPzW3PTR3NDev21a1r6tK0j8oAW/P/1l2zHjJHBtcy89ZZsR482PeUiOVky0a9ckQzdIUNsb1uOoA5P16rl+t42U9TU+lOnMgRNakfUsWPWH+7ECalXVqCA5c4cUzgi7xzqaHy9etprbBU1kLcnrV7l5SVLmQBa0btDhyR1x8NDK+poaMoU+axs2AAsXZr5Y5w7J1UwR4+WdB+93v72Wotp9fkOA3kiInKo3BLIA1rRu3nzLA+yzJ0rsYU6zzanUs/jGjSQ1cJMUQP5Xbuyp032UE/ya9QAihSx/f7qPHlA6iOMHWvd/QYOlB+9HujZU+oNqBISZPW1P/6QgezZsyWDNVdSA/n69V3bDnMqVpTt7dvAjRtGN9kzT1793NpTWJDF7pzDrvnxt29r792sBPIAMGCAbNeskbka06fL7926aWkfhsqVkwr2ADBsmOUqqQkJUphDfdOsWGH/HBxrPXigTZdhIJ9vMJAnIiKHiYvTRspcWejOWt27SzB28qT5JdnWrpVzvtOnZRninExNq09frd6QGsjv2yeZojmRvWn1qhYtpKBcjRqSbWFt8KbTSY2rzp3lXLxzZxnNffBALv/9t7xfVqywr2B2jmHvhPHs4ucni2kDGdLr7RmRz8rT5Yi8c9i1fvzGjdrSaumXnrBVxYryBaMoUpn+l1/k+nfeMX+fDz+U92V0tFRTNWfoUBnhL1IEmDxZrhs3Tr44nGXzZikEGB5ue9oJ5VoM5ImIyGH27JHzonLlgGLFXN2azAUFSU0jQKZAph+lPnFCbk9Nld9XrpRplTlRcrLM9wcsB/JVq0pwEh9vWzCUnbIayPv7S2br/v22L3fu4QH89pt0eNy+LYNb7drJa+vvL8vTWXp9c7zUVG3CeE4N5AGHVq63qmL9vHkyapouBZpz5B3v8mX5fLq52VjsMSvLzpmiFrGbMUOWnKtXz3IPtJ8fMHWqXP78c9PL1/36q6Tr6HTSOTBihMyrB2SZCmcVy1MXnm/fPovr11FuwkCeiIgcJjel1aumTZP23rkj9ZP+/Veuv34d6NRJTuCbNZO03JSUnDufftcuaWuRInI+ao6bG9CwoVzOifPkY2JkarROZ+NoXToeHpICbw8/P6mBVbmyFOXasUM6fdats79zIcc4fVqqfvv5Sa9OTmWmcn21arKNicl8dQFA4nK1w8rs5yIlRQqcjRmj9SI9whF5x1PT6mvXBoKDbbij+uWc1bR6VZcuUh1T9c47mQfBXbrIP4rERAnQY2K0eVmnTmnrnH78sdbh8OWXUuzj/n3guefkn40jrVmjZRSoVVwpX2AgT0REDpGaqg2Y5KZAvmBBOT9s105GqTt1knOirl2ByEjJLli6VJtS+fPP1hctzk5qWn27dpmnkufkgndqym3t2raPpjtS4cJyflymjFSm37gxd0wXydR//8m2bl3p7cipzFSuDwyUvwlg3aj87dvaFBJTU58BSPn0xES5vGCB0U2cI+94di07FxEhX8ienlnr4TPk46MFviEhwIsvZn4fnU6q23t4SHpO8eJAQIDM42ndWubhtGolqfQqT0/g998lLf/sWdPF9OwVF6dlFgwenEe+pMhaDOSJiCjL7t6V1PSdOyWIfOopV7fINv7+UuH8hRckRf2ll6SAcVCQjMwWKQL06CHnfcePa7GQK6kjjTNnAr17yxawLu07JwfyWU2rd6TwcIkjL1yQuDdPyOnz41VmAnlAS6+3ZmqIWisvONhChobhqP+SJUaFzDgi73h2Fboz7CUOCHBcY95/X0bOv/nG+uqVVavKknRlysg/PHWe0uXL0uv366+Au7vxfUJCpFImID3DjuoZeu89Wa6kfPmcX8SFHI6BPBFRPnT5suNGlaOiZB3glSvlPGjRotxZa8fLS86/1MxId3c571Kzj4OCpGo5AMyZ45o2qg4cAEqVkkGgt96SdsfFydLI1gTyamr96dPWpSdnp5wUyAOy4lROXKHNbjm9Yr1KnSMfEZEhFVkteGfNiLwayFtc/cAwkL9/32jtQcNAPidm4uQ2MTHyvaPTyZQlqzk6rV5VvLhUNO3Rw7b79e8v//wePgTOnJGUqNmzpXK8uQIxDRoAFSpIb/GGDVluOtauBX76SS7PmSM90pSvMJAnIspn/vxTCv5aKrprrR075Nzk2DEgLEzOYdTicbmRuzswa5Z0RmzcmPGcsX9/2S5aJOdvrqAosn75lSsyzfmppySLc/16yTw1nPJpTuHC2gpf2bWknqJIh8Phw+b3iYzUimDZdJJP1klKkmraQM4fkS9RQibEp6YaBdaAfSPyVgXy6oRtg/R6tdidokjWtKtcvy59G8OHu64NjqCukFarlg1TZ44dk6AVcHwgn1VeXvJl2r699AJn1outLg2nzoWyV1ycNt9r8GCgefOsHY9yJQbyRET5zIEDst24MWvH+e8/Wa/7+nWgTh0JCHN6bGANnU4GZ0ydF7VqJdmUcXEZYotss3mzpP17eclA0Pr1sk76U09JYG+t7E6vnz5dpgC0aWM+Tfl//5Ntq1aSAUEOduyYzAUPDpaRwZxM/SACsoyAAcMl6DIbJbcpkFej5H//Ba5eBSCfKbXmhCvnya9eLSPZ06alNS1XUr9vrO6oi42VwiXx8fLFkNMzSTKjpkytWZO1FI/hw5lSTwzkiYjym5s3ZXvyZNaO8803MsDXpo2swZ7VZX1zAzc3oG9fufzzz65pw/jxsh04UAYt7aXWRMqOQP7ECVmCGZCOny++yLhPVJSWJTpmjI0PsHChRHfqaDOZZphWnxuWqFID+fXr5Y3zSNWq0vybN4Fr1ywfItNAXlG0QL5zZ+nh0uvTlvPS6XLGPHk1kyU1NesrZxw6JDGgK6irr9WsacXOCQlS5f3CBRn1/vPPzCt55nQtW8pcneho+/8JHz8O/PijXGZKfb6Wyz8NRERkKzWQv3zZ/hPT+/elJhQggaXTzyNSU+WExVJedjbp10+269fLuVh22rJFfry8ZHnirDAckU+3dLZDJSVJ8cCEBFkBAACmTJGpAYY++USqi7dubWOW6MGDwKuvSoTwzTcOa3eOcPky0K2brHPtiAnaapXG3JI6U7GirBmXmipB3CO+vjIQCWSeXp9pIH/9upS21+nk8fr0kesN0uudHchfvCh9K1OmmN/HsI8qKytnREfLY1WpIoXUs5sayKvLCJqlKMArr8gXVMGCUnXUmnlDOZ2fn1au3970erVIy3PPMaU+n2MgT0SUzxgWN0u3RLPV1MLOFStqAaHTKAowdKgEa506SWToQmXLSoanogDz5mXvY0+YINtXX816BkSNGhIQxcWZLAzuMOPGSaxduLBkbjRuLO8dw9WZIiKAuXPlsvocrXLvniwZpb4n/vlHgr68YsgQ+bANGiQn7VmtTJhbKtYb6tlTto9GyFXqPPnMCt5lGsiro6Lh4fKBePFFKW9/8GDawdV58s4K5L/6Cti/H5g61fTtimLch3nihP21Lfbvl4/IgwfyVN97T1uez9nu3dMyAdQiomZNmCB/cw8P6cTJjRVUzVHT6+0J5JOTtZSMV15xXJsoV2IgT0SUz6gj8oD9mX3z58v25ZcdlKG7bp1UOjPl66+Bb7+Vy5cuZZgv6wpq0buff86+k+Dt26Wugadn1kfjATk/VuM5Z6XXb9+uzXv//nspEP355/L7Tz9p77+JEyW46NDBhmWQFUVK9p89K70awcEyupoT19Szx/btEsS7uUkKxl9/STGKXbvsO566RBaQu+YZq9Uzt20zygc3nCdviRrIFy5sZge1N1Otkl+4MNCxo1x+NCrvzLXkHzzQOrEuXcqYqQJIYsbNm1KMU105w96pPeqIuFpYfcoUyYKJjbXveLZQX+pixTIpdLdundbTN3OmFGPJS9RAfts2SW+zxapVMp8kNNS6JUooT2MgT0SUzxgO6tkTyEdHa0uEqVmoWbJ0qazjW62apEYb5nkvXw4MGyaX1eDjyy9dvg7U88/L+X5kZNbnq1pLHanu3x8oXdoxx1SzKeyNDS25d086evR6qSvQtatc36SJDC7r9dIhceaMlsWszv+3yvz58uK7u8vInRp8/fWXI5+Ga+j1MlQKSGXq3bulOF10tKTSWsrBNufQIektCQ3NXQUtSpUCmjaVy4sXp13tsBH59IE8oH2xLVwIpKY6NbX+l18kK0ZlaqRdTauvUkX6rgDpz7Rn5Qz1O19N9ggMlOk6detKir8zqZ0ImY7Gq5/hl1/WKrPnJRUryjyjpCTbq86qPTh9+khvLOVrDOSJiPKZrI7IL1wocbRawT3LvvtOtgkJcnbZrp0MTe3dC/TqJQ/25psyShMQABw9qi1F5CJ+ftqo+Pjxzs/237VLnr6HBzBypOOOW6+ebK1ZxstWH38sHR1lykhShaHJkyX+/usvyZzW66XOmNUZ36dOaRHN+PES6D3zjPyeFwL5336T939AgPTg1KkjOdEvvigpIO+9J8sX2MIwrT43FLozpKbXG2TjWFu53upA3jC67NRJMjwuXwY2b3ZaIK8oUv4A0NL39+zJuJ+aVl+7tkyvDg+X4H/ZMtsfU/3Or1pVOtf27pU+oitXnN8pqT52pvPj1d6ZvDYSr9Lp7Euvv3YNWLlSLqtpYZSvMZAnIspHHj40HsWxNZA3nBf+8ssOaNC5c8CGDXJiM26czFFdv14mcHfqJI3t2FFG6oODZZ1eQMvPTu/GDekQyAZvvw2EhUm1dbWAcHq3b0swmz79PjVV1lN+910p2vX44+anPyuKVsW9b185iXcUtfjchQuOOyZgVPQb332XcSm5KlVknj+gLYdo9Wh8SooEtPHxsuae2qPSvr3MOzh1Sob5c6uHD7XemhEjZAQdkPzuRYu0ZRPUEv/Wyo3z41XduknPz3//yVQKAJUrS8fW3bsSb5tj14i8t7eW0r9okdPmyO/YARw5Il97o0fLdZZG5GvXNl45Q615Zi293jiQB+Rpq8dz9sfG6hF5NZDPNOLPxexZhm7hQvn+a9Agb782ZDUG8kRE+Uj6YDEiQpaVTi8pSWKG9OsV790rJ3t+ftpczSxRI+D27WUx9IMHJdC4c0fmO9euLaNwagrh0KFyedMmGaE0tGyZrMfWrl22pN77+gKjRsnlTz6RuNLQ0aMy0lWunLxelSpJn0SvXtIB0KKFrAl9/rzEJz17mq7T9skn0rfh6Ql89JFjn4PaKRATY/p9YK9DhySACgiQ5QlNGTdOW/e+a1cZdLbKnj0S/QQFSU6+u7tcHxSkVYPOzaPyX38tKfQlS0pPjyGdTrJTAMmLtiWyzG0V6w0VLSqdNkBaer2Xl2QoA+YzSpKT5asEMBPIx8drvViGgTwAdOki240bnTZHXh2N79VLvrYA6W9Jv4qEOiJfq5Zs1cB740bbOuGio6WfyMtL68QDtNfxUR+J01g1In/jhramYKYRfy7WsqX8IaKirKs2qihaWj1H4+kRBvJERPmIGsiHhMgAn15v+uTtp5/k5LJWLRk1Uqmj8V27aqmgdktK0k5MBg6UbeXK8oCffio9Bf/8Y/xApUppa0sbLka+fLmMoCUlyVC3qfxUJxgwQILhmBjtpByQDpK2bbXXOzlZXufVq6WD5Pp1Kfb08svArFnSKfDvv5KObmjJEm00/rvvjE++HaFwYQmmFcWx82PXrZNty5bSAWFKWJgkWjz+uFYQzyrqhP5WreQghp59Vra5NZC/dg2YNEkuT5qk9XQYatBAgs6HD61fP8xwaYLcVOjOkPq5X7QoraMus3ny6udPpzNTXO3sWTlWoUIZI/0mTaSTKDISpfQSLTtyRP7qVW0Jz7ffluDWz086CwxXE7l/XxKXAC2Qt3flDDWQrlTJeHq1WhDemSPyCQnSaQlkEp+rw/bh4dITmFf5+0tvLmBdev3+/dJj5eOjfRYo32MgT0SUj6jz4wsX1k6mTKXXq4HYtWtywvjDDzJiq05RVUeEsuSvv+QBihWTNHqVOvT8558ywp7e8OGy/eMPyVtfsQLo3l1SDtUcbrXKvZN5eUkiAQB89pmc6F+5IqPQV6/KDIEbN2QkbONGqdw+YYKMsMfGykn4669rKbL/+59Wz+vAAa3m1tChzqn5pNNpo/JRUY477r//yrZtW8v7vfqqNkfXamogb6q8fefOst2xQ8upzk3GjZNIrl49oHdv0/vodNqInFruPDPqfPoyZaQXLzfq0kU+cCdOpA3Bq4G8uRF59S1QqJCWuGHEMK0+fd2AwMC07IWqsZsBODaQ/+EH6eBr3FiyUTw8tD4Ww/T6o0clYA8Lk8QEleFbIP0IvjnmUtvVz9/161oGg6OdOSPtLFhQmy1iUn5Iq1fZMk9e7fTu0kWmmRGBgTwRUb6iBvKFCpkP5PV6GdQGgIYN5WTztdckOL19W2LrVq0c0JgffpDtK6+YH7Y1pVYtiRDVcuhqEN+jh1YE7/ffs2c9JQAvvSSJBLduyeh527bSv1C+vDSncGFJJGjVShIPRo+WLGHDp9yjB/D++3L5lVfkfs88I4Ou7dsbJx84mqMD+fh4WTkNMJ9WbzdFsRzIly4t0zH0eq0oVG5x44Y27/2rr2QytDkvvSS379iR+TCqXq8VIOjWzTFtdYXgYC3wWboUgFbwztyIvF3z4w09mqpRLnozAMcF8snJwOzZcvntt7XrGzSQrWFCkeH8eEPPPy99DZGR0jFoDXOp7YGB2nJ0zkqvN+xEsFhrUd1R/ePmZer7ecsWWYfQnIQE4Ndf5TLT6skAA3kionxETTW1NCJ/4oQE/H5+EtBPnCjXb9sm2z59zIxu2SIyUhu2tWeo+YMPtEYlJ0ta/YIF0vPwxBNyndpR4GQeHtrScF9/LUFF8eKS1ZA+89uSyZMl8I2Pl+D98mX5GxmWCHAGRwfy27bJDIdSpaSDw6Gio2Ueg4eHVnI/PTW9fsUKBz+4k82bJy9cvXpayq05xYvLm0S9nyVLlkjtiYAArTBgbtW6tWwfDVkbptabGpV2VCBf4txmAI6bI79ihXy+ixY1rjXSsKFsDUfk1UBeTatX+flpMd3o0daVBUlf6M6Qml7vrEDe0mMbUXtl8kMgX7myfAEnJVlegmDFCkmVKFUq71byJ7swkCciykesGZHfskW2jRtLJuuoUZIFHxgo8VO/fg5oiFrkrm1bmfBpqyef1AK57t1lMWY12h00SLazZklAnw26ddNOtAsVkj4KW5+Wu7sE7eo8+EKFgL//zljx3dEcHcir/TNt2jhhlbPdu2Vbq5bp+eOAtgzd2rXZtoJBlimKzLsAtJUZMqN+EOfPN10lEZBMFbUc+nvvWYhocwn1M79/P6AoKF9ePjfx8aYTcLIcyD+aJx9wPQplEOWwEXm1j3HgQCmQr1JH5I8c0VYXMVx6Lr2PPpKp1nv3piUpmKUolqvGO7vgnfrYVi89lx9S63U67XP87rsZq8sCkgZiuGxJlnvRKS+xK5Dftm0bXnrpJTRq1AiXH635sWDBAmxXc+mIiChHMjUif/q0cRygptUbDgp27iznvPv3O2CUNTlZmxRubdCSnk4nZ66//mocxAMSVRctKkNe2TQq6+YmWdFdu0oga+9gUqFCwKpVMnd8zRpJz3c2Rwfyan2FzObH28VSWr2qTh2p+B4fL4UJbLF1qwyRqh0G2WXrVkmRDwjQ1kzPTOfOMuH40iVZwtGU+fPlA164MDBsmOPa6yq1asmHLTYWuHwZHh5a1oupYo0WA3m9XisAaC6QDwhImyffAlscEsgrijbi3r278W2lSsn88ZQUSaJITZWgHsg4Ig/Ivuqf9eOPMy5zaSg2VgZ13dy00XdDzi54Z9WIfH6pWG9o5EjppblxQ+ZVGaZWKIqkXZw5I99pQ4a4rJmUM9kcyC9ZsgTt2rWDr68vDh48iMRH69XExcVhklpplYiIciTDEfmyZWU0KCFBW8JIUbQR+fTZvcWLAzVrOqARK1fKyEPRolpxMnuULi1BT/r59d7eWgdBNhW9A2SwcMkS8xnf1qpcWRIWsmuVsDJlZOuIteRjYqQ4l06nrRbmUNYE8jqdNipvS0fOv//KGmBLl8rcVVNVIK116ZIMt1pbPV+dMN2rl/XLQfj4yP6A6aJ3CQlSPA+QYEFdQy038/PTRmofLT9ZqpT8anMgb7gWm6X0mUcFQVphk0MC+WvXtIA6faeoTmecXn/unDTR11cbMU9v+HB5fqdPa/XQTFHfzuXKyVsnPWeOyKekaB0EFgfa80vFekNeXrI+vLe3FL2bOVO77auv5PvI01OKv+b2jBpyOJsD+U8++QSzZs3CDz/8AE+Dk6cmTZrgwIEDDm0cERE5luGIvLu7NgqjnuSdOSMjN97eWppnll29KvP/Pv5YhmnVkvf9+8tJjDO8/ro8wS1bJLK0ZN48mQt78KBz2pLTpJtMrI7IX74sUzWzQi26VbeuE845ExK0v5GlQB7QAvnFi7UCiJasWiX3SUiQXOU7dySYj4mxr61vvCG9Mc8+K+kVliZX37ihrUNma4aKmpa7bFnGcuOzZ0t0W6IE8NZbth03JzNMr4cMVAJ2BPJqWn3FipaLUDyaJ98Smx0yR179ri1b1nRAbVjwTp0fX7Om+YzqAgVk+hMgK2jEx5vez1JaPaAF8mfOWDff3hYREZKI5eendbyYlJ/S6g1Vr66twTl8uPTKbN4MfPihXPfNN1oPD5EBmwP506dPo3nz5hmuDwoKwh1nrVlBREQOYbj8HJBxnryaVv/EE8ZzN+22e7ecaXftKutir1snc/4KFwbefNMBD2BGyZLAc8/J5e++M7/fr79KMLRli6TkO6qaVU6jKDLhvmZN+cPWrCkdKlOnIuTEFgT4pDhkLXnD+fEOt3+/RAOhoVrvgzmtWknAFxcnReGGDNEmHaf399+ypFNiomzPnpWo5sIFWRbx/n3b2rlli2SduLnJEOucOZIXvWOH6f3nz9eK3NmazlGvnlR8S0iQQCAiQjpq7t0DPv1U9hkzRoZ084p0gbwaGF66lHFXqwJ5c2n1qsaNoXh4IBwXEBIfZTF93RqZpZgbjsir8+NNpdUbeuMN+UjExEjMZ8/jli8vb9e4OMev3GjYiWBpMYZ8VbE+vXfekWKODx/KMiYvviif5Zdflo5pIhNsDuSLFSuGc+fOZbh++/btKKdW6CEiohzJMLUeyBjIm0urt9sPP8hEz/BwqU4/a5acgF+5ouV0O4ta9G7+fNNnpqtXa9kBXl7A+fPaffKSXbuA5s1lxPnoUclzPXpUXpdhw6Br1RI/+cgaWObmycfGZj5aryjaiLxTAnnDtPrMquh5eUn5fPXv+c03MlfhyBEJzM+fl+N9953MiU9Kko6cxYtl0vXq1bLe+oEDckJtbfSmKNqKCq+/LqNqZcrIKg3Nm0t1MsMX0p4id4YMi2V99pksCB4QANSoIYuCV6iQ95arUhdbf1Twzu7UemsD+YAAKPVlnosjRuUze1j16Z0/r32eTBW6M+Ttra0u8tlnWuaVIXNLz6l8fbVOEXvT6/ftk9kpagdE+sdmxXoL3NxkikzBgpKKce2adLjOnOmEqqGUZyg2mjRpklKtWjVl9+7dSmBgoLJt2zZl4cKFSkhIiPLNN9/YergcKS4uTgGgxMXFubopREQOVayYogCKcvCg/P7bb/J7o0aKotcrSsmS8vuGDQ54sIQERQkKkgNu3uyAA9pIr1eU6tXl8UNCFOXbbxUlKUlu275dUXx95bZevRRlyxZFcXOT33/9Nfvb6gxxcYrStas8J0BRfHwUZcQIRTl5UlH++ktRxo9XlDZtFAVQrvqWUQBF+fHHjIc5eFBR3N0VpWdPyw935Ig8jK+v/OkdTn0u//ufbfdbtUpRQkO118HUT48eipKcbHy/3bu198gbb1j3WL//Lvv7+yvK1aty3Z07itK3r/ZYdeooyvHjctvmzXJdQICi3L1r2/NS3bunKK+9Jsf19jZ+XosW2XfMnOzBA+2zeumS8uef2ndYeuHhctuuXSaO06KF3LhgQeaPOXKkogDKz+irREVlrfmPPnLKTz+Z36dyZeM/444dmR83NVVRataU/YcPz3i7+t2/Z4/5Y7RuLfv8/HPmj2dKly7G/09UvXvL9ZMmZXKAokVlx3377GtAXrB4sbwGQUGKcu6cq1tDLmBLHGpzIK/X65VPPvlE8ff3V3Q6naLT6RQfHx9l1KhRdjU2J2IgT0R5kV6vKJ6eco5w4YJcd/iw/B4crCgREXLZ01POlbNs2TI5YIkScpbpCvv2KUrFitoZccWKijJ9ujxhQFE6dNCC+zFj5LoCBRQlMtI17XWk99+X5+PmpiivvqooFy9m3CcuLu21KYzriql/5ZMmaYcxdQjVl1/Kfu3bO+4ppNHrFSUsTB5g61bb7x8bqyjPPKO9D3x9Jcpr0EBRPv44YxCvWr5cCxoPHbL8GImJilKhguw7dmzG2//8U1EKF5bbvb0VZdo06UAAJBB3hJQURTlzRlGWLpW2G0ZTecljj8nrtmKFsnu3XCxZMuNuAQFy29mzJo6hdu5YEzT++6+iAEokyihHjmSt6WpnqaXg/OWXtbeqTmd9H8/KlXIfPz9FuXVLu/72be14lk5t33xT9vnoI+sez1BKitZvCyjK2rXabXXrynXLllk4wPXr2p3v3bO9AXnJunXS4Ur5klMDeVViYqJy/PhxZc+ePcq9PPaBYyBPRHnRvXvaedL9+3Ldw4danDJ5smwbN3bQA77wghxw2DAHHdBOSUmKMmOGjMobDnM1aWLcY5GcLENJ6otgLrjLDe7ckQ4JQFGWLLG8b6VKigIobbFG6dMn483PP6+9ZJ9+av4w7drJPlOmZK3pJkVFycE9PLLWyxQbKx8EWwJc9X38+uuW95s+XfYLDTUfeV25Ij0d6TMC/vvP+vaQluEwZoxy+bJcdHc3/sg+fKi9vLdvp7v/rVvajdZEyffvK0nwUBRA+e+P83Y3++5d7WENA+30vv1W269CBeuPr9dro/JffKFdv3On+c4OQ1OmyH7duln/mKo9e4zf0o0bS3tSU7XEltOnLRxgyxbZKTzc9gcnykNsiUPtWkceALy8vFCtWjU0aNAAAflliQgiolxMnR/v7S3VgwGpmqyuvKRO1XXI/Pj796WIGGD9utjO4ukpVbsjIoDRo+XJ16sn7VNfCEAqV//yi5SB3rkT+OQT17U5q77/XooKVqumFf0z59Gk3Pr4z+Qc+Uc1xQBI3TZFybhPQoJWX8Gp8+Nr1TL+m9mqaFGZQ27LnFO14vvChVIJzJS7d4EJE+Ty2LHml5ALC5MK+TNmaAXo7Clyl98ZFLwLDZWPbmqqLJChUr/v3N2BoKB091fXjy9Rwrrl/vz9cdxfysl77dxsd7PVhw0NlanQ5hgWKM9sfrwhnU5banz6dK20Q2YV61XqKib2zJFX5/M3biz/V3bulOsMV/mzWEorP8+PJ7KTzYF8q1at8OSTT5r9ISKinEktgFSokHEco57cRUbK1sTCJLZbsULO3ipUyDlBSmCgBFu3b8vaTqbOpMuWlYJ8gFT9NlHcNcdLSgKmTZPLw4dnUiYaaX+fetifIZC/eVMrgOfvL30h27ZlPMRff0kwHxbmpPNwa9aPd5bmzaVD5MEDYMEC0/t8+aUUl6tUSYo6WqLTSefAwYPAu+/K8odkG4NA3t0dKF5cfjUseGdY6C5Dv421he4MHC3cEgAQuH+zzc1VqUXfMnvYmjW1lTltCeQBoFcvec7R0cDy5caPm1kgb7iWvKkOO0vUQL5XL6miDwDjxmmdCJUqWV7lL98uPUeUBTYH8rVr10atWrXSfqpVq4akpCQcOHAANWrUcEYbiYjIAdIvPacyPLlzdweaNHHAgy1aJNuePXNexV0vL/OLMgPS5vbtZTjro4+yr12O8uuvsipAWJicVWfGYEQ+/VryBw7Itnx5LbHi55+N756YCIwcKZcHDnTSn9uVgbwaeANS5T59hHP2rATygCyx6Olp3XErVwamTOEIpD1q15YOqqtXgStXTFaut1ix/swZ2apD0FY4E9YSAFD42Gbbo9xH1P6DzAJqLy+tQ7VpU9sew8dHW9lT7c/LrGK9qmxZ+WqMj5evEGvFx2urK7ZuLQs3qKPy6nJ4mVasz89LzxHZyeZAfurUqUY/3377LbZv346hQ4fC09p/XkRElO0MR+QNGZ5g1a1rXaapRTdvAmvXymVXp9Xb6/PPJYD74w9g925Xt8Z6er0WVA4dKvMoMlOnDhSdDqVxEYX114zW41bT6uvV01Yx+/13GC3B9c03slRW8eLaymsO9fChjF4DrgnkAaBPH0lJOHlSm0MASD53377SxqeeArp2dU378hs/P+2La/9+k2vJWwzkz5+XbYUKVj/k5TKNkQRPBN6KluFuO1i9DBtkdcgNG+yb6vTmm9KftGOHLAlnbWq9p6c21cqW9Prt26UDsGRJ6RsJC9OWPlf/FWQ60M7UeiKb2T1HPr2XXnoJc+bMcdThiIjIwawZkXfI/PglS2Q0u3Zt685Yc6IaNbT1ud9/3/QI3Nmzdp/QO83q1XJCHBionUlnJjAQusqVAWRMrzcM5Bs1kkHk+HgJ5gFZ6lhdv3rSJIl1He7AAXk/hYYC4eFOeAArFCggwTwgo/KqKVMkWyAwUAoI5LTsk7zMIL3e5hH5iAjZWpy0bcy7kD+iEC6/XLhgU1NV1qbWAxIM2ztjNSwM6NFDLk+erDXXmq9jNb1eTVqwxoYNsm3dWvsIfPihjMqrLD72jRvyZQLYNN2BKL9zWCC/a9cu+Bh+YomIKEdRA3lLI/IOmR9vmFafm02YIAXJtm+XOf+GfvpJXrjKlaVAXk7xxReyff11ExW+LHgUFKUveGcYyOt0wCuvyO9qv/2YMTI6X6+eFuc6nGFavSsDZTVfedkyyTs+cUKKJwLA1KlA6dKua1t+ZBDIlywpF60O5NUReRsC+QIFgKsoJr8YVtWzUnKyVnIjO/o31aJ3y5ZJP2SRIkBISOb3s6fgnTo/vnVr7TrDUXkgkxF5NWUgPFyKURKRVWwO5Lt27Wr006VLFzzxxBPo378/Xre295+IiLKdmlqffkQ+KEhOwMqWBVq2zOKDXL6spR6rQ0K5VcmSUowMkOGl5GQ5Ix4zRgqapaZKhbeXXpJR+9RU17Z371557T08tLN4az2aJ18P+9NG727d0gog1q0r2z59ZA7tzp3An38CP/wg10+dmnlNPZspinSYjB8vvzukeEMW1KwpE5ZTUqQgYt++UiCgY0eth4Oyj4kReatS6+/c0b4M1TxyKwQGArEIlV9iY21ubkSEvHX8/ZHW8eBM9eoZz6+3tvPAsOCdNW7c0Ga+PPWU8W0ffiivW0hIJuUImFZPZBdL9SNNCkrXw+/m5obKlStjwoQJaNu2rcMaRkREjmUutR4A1i2+Bf2GTXCbcVaGjc6eBWJipOywNQXTVIsXSwDWpEneGKH88ENZyu3MGWDmTEnzVquMjxolz/XTT2Ve+pEjwG+/WV5XypnU0fhevWyPFAwK3i2LkqvUQnflymlPKSxM4ta//5aH0euBbt2AZs2y3nwjMTFSOW/lSvm9aVPgtdcc/CB2eOstydD45BP52wcHS28GU+qzn1rwLiYG5XxjAIRZNyKv9k4VLWpTQZACBbIWyBsWys+ut8vQofJ2BWwP5K1Nrd+0ST4Kjz0GFCtmfFtYGHD4sPyZLJbrYCBPZBebA/mf05erJSKiXMFcsTsAQNOmcFMncBoaMgR45hnr0h1TU4G5c+Vybk+rVxUoIOuCv/OONsrt7i4jsuoyYzVrSiW4f/8FHn8cWLfOppE+hzh5UmoTALLknK1q14ai06Gkchl3z1wFUMword7QK69IIJ+cLNW1//e/LLU8oz//lJzcW7fkAT79VDIjLK00kF26dpUAUJ3PO326tvYZZS9/f4mKT5xA6ev7AXRCTIy8Lz09LQTydsyPB+Sr4FwWUuttKXTnKM8+C5QpI3PkrV3VTR05j4iQr/TMPnam0uoNWfVVqKbWc+k5Ips4OhGOiIhyKLMj8omJ2lnmiy/KKPwvv8jQzI0bEqxYY84c4OhROeN98UVHNdv1Xn9dG6by95co1nCt8BdekPLQZcrI2a9TSrdnYtIkGRZ79lkp1GergAA8DJcIIzhCInhzgfzTT2tzbYcOtTkesuzXX4Hu3SWIr1NH0gKGD88ZQTwgw4rqXPnnngN693Zpc/K9R2/OoIj98PSUj4C6bJrZQN6O+fFA1kfkbSl0Z7VLl6TypJnl8Dw8pG+1d2/g5ZetO2SpUtJ/lpRkXS3PzAL5TCkKR+SJ7GTViHzBggWhszIP6JY65ENERDmK2RF5dXTRw0MCKXWys6LI/O8vvgDeflvOZC0dXF1MfMIEMxWmcilPT0mZ//prGZVXJ4wbql0b+OcfCaKXLJHpCdYubfXggeSxXrig/dy6JQHsY49lfv9z5+TvBmjF1+ygq1cPiDyB0jf2Izn5abOBvKenrCX/779ZeriM9uzR5pq/9ZZMvPfycuADOMioUZJ58dRTTKl3tXr1gAUL4HZACt5FRkpsW6aMFYF8+fI2PZSjUusdOiLfs6e29ttLL5ncpWVLM7VPkpPluyc+Xn48PYEyZeDuLl9dJ07IDCtLI+qRkfJyenhkoVDqkSPyP8jbmyPyRDayKpCfNm2ak5tBRETOZnZEXj0pLVrUuGJZjx4yF/jUKWDaNCnyZs6oUfIAjz0mQX9eU7euNjfenMcek+HqlStlzvysWab3S06WwnQbNshw1q5dUgUrvWPHZBHozILFyZNlsnqHDhmjbht4N60P/LkA9ZT/cOyYFu+Y6rd4+mn5cZiLF2WEOzER6NxZskAcXj3PQTw8HPzkyW6GBe8qSGB58aL0QTo6tT4rxe4UxQmB/Pnz2gT4RYvMBvIZ3LsHNGigNcjQ998DAweiYkUtkLdU/kpddu6JJ2wqN2BM7YR8+mnAz8/OgxDlT1YF8n379nV2O4iIyIn0egsj8upJafpKRe7ukmbfo4esl/3OO6YLuR04oAWt06dLoJNfffihBPJz58prl/41/ftvyXG9c8f4+pAQWXqpTBn5mT1bctuXLZN52eZERQHz58vlLA6Pu9XXlqCbuVSuK1vWTE0FR3rwQKYEXL0qGQ2//JJzg3jKWerUke+bK1fwRO1j2IrHcPGiDDAnJMgujkytN1p+TlGszsi4fFniZ3d3mxMBzPvtN+3yunXynRIcnPn91q83DuLd3GTB9/h4YPBgoHFjVKwoKe6ZFbzLclq9Xq8tV8ppKkQ2y9J/yoSEBNy9e9foh4iIcp67d+WcCbAwIh8amvGO3bvLSHNcnKQ6p6fXA4MGyUltjx4OWL8ul2vaVNY7T0wEvvnG+LazZ+Vk9c4d+SN07y4Be0SEpJbu3Qv88YeM5g8bJvcZNcrysnaffSaj+a1by+NmRe3aSIUbiiMGW3+TicZZGOC3jl4vy7gdPCidGX/9lYWhPcp3/P2BTp0AAB1vSMbMxYvaaLy3t+ySJiUFaesrZiW1PilJvhOtpMbNFSpkMlskJUW+B/73P6B9e8lXj4nJuJ+iSIcXIIF4cjKwYoV1jdm2TbYDBsj3VEqK9DK0aye9Hz17ompZ6QWxtASdXq+NyNsdyO/YIX+wAgVkOQwisonNgfyDBw8waNAgFC1aFP7+/ihYsKDRDxER5TxqWr2/v4llgCwF8m5u2jre06ZpB1ItXCip4f7+2vJn+ZlOJ6PyAPDdd9KDAsgJ8gsvyAlzs2Zycv7777KkmqmRwWHDZCj85EnthD29S5dksjrgmMnq/v64WlDyfguckwnyTg3kr18H+vWTmgKensDSpZKVQGSLfv0AAI+fXAB3pODSJeO0eqNB84sXpWPM21vWRrNBgQJAInwQh0e1QmxIr8+0Yv29e/L9UKgQ0LAhMGIEsHatBN2ffJJx/6NHJffd21tG0QFZ7cEaW7fK9qmnpFdBp5Pv+blzZXrV0aNou14KdloakY+KktfZ21sy9e2iptU//7xkBRCRTWwO5D/44ANs3LgRM2fOhLe3N3788UeMHz8exYsXx3w1vY+IiHIUi0vPWQrkAZm7XLu2nGx+8glw6JCkiM+YoVVoHz3a9rXL86rOnaU0dVyczDkFJDA/dEgii0WLJHC1JChITuYBWf4uKSnjPp9/Ltc3b56FSlPGbpaV9eTrwUGBvKlq2ikpMgWjUiVgwQK57vvvJZuByFYdOwIhIfC7F4t2WGs0Im92fnzZsjZP31BH9q/asQRdphXrFy+WbJx79+Sz/8wzwHvvyW0//aSV4lcZzitXV9D499/MswTu3pXsF0A6FA0VK5a2fGjJZdPxNP5BVJQM9puijtZXqJD515lJSUnSmQkAvXrZcQAisjmQ//vvv/Hdd9/h+eefh4eHB5o1a4ZRo0Zh0qRJ+MXcqAEREbmU2UJ3QOaBfPpR+Tp15ERz0CC5b6VKss43CTc34P335fLUqRKszpwpI18LFwIlSlh3nLffllHDqCjghx+Mb4uM1K5zYOn4pBoSyNfHfwBMF7qz2ogRMlxXtSrQrZvUDPjpJzno4MEyxaBWLWDLlrRRVSKbeXqmza/uh7mWA3k758cD8rE2LHinj7F+RD7TQnfr1sl22DD5sl6xQjKcmjSR9Pcvv9T2NZxX3rOnLNlWtaoExn/9ZbkhO3fK/cuVM/091KGDrCkJYC76IyQ1BpGRpg917pxs1ZU5bbZunfQwh4YCrVrZeRCi/M3mQP7WrVso9+gLsECBAmnLzTVt2hRb1XQdIiLKUbI0Ig/IKLNavrhoURmqfe45Oen799+cuUyYK/XuDRQvLiNp6gLOH30k81Ct5eenBekTJ0pRuLt35brq1SVd/4knJEXWQbwayxB8PexHeBnFdMePNWJipEBicrJEMUuWSGfQgAGSFlyokHRu7N/vsGwCyscedQQ9g7+QFHsLly/L1Y5aek5Vu7YWyE8aEotff7VcwkJlcUTecLJ5ly5SEQ+Qjr9Ro+TyrFkyFQWQYDw6WnoV1NUTuneX7R9/WG6Iep5u6TP32WdArVooghv4Hq+ZTa9XA3lrV9nMQM0q6NFDe85EZBObA/ly5coh8lH3XJUqVfD7o7SYv//+G8HWVMskIqJsl6UReUBOKteskdGh2Fjgv/+kovrUqVJlnYx5extnKTRvLiPStnr1VUkDjo2V0bcKFWR6w8OHUtxu/nyHrmUe8lQtpMAdYbiKNtWvZH4Hc2bMkCD+iSeko2fKFFkjvmlTYMgQmXz7xhs8gSfHqFULSu3a8EYSeuA3HDkiVztq6TnVqlVAuUaSWq+7dhW9e8tCC+rjmXLnjpaFbzKQP3RIvqADAmR+vKF27YD69eXzrhYbVUfju3YFfH3lshrIr11rOb3emkDe2zttJYyOWIWIU6Zz6w1T6212/z6wfLlcZlo9kd1sDuT79++Pw4cPAwBGjBiBGTNmwMfHB++++y7eV1MJbTBjxgyEh4fDx8cHDRs2xN69ey3uP23aNFSuXBm+vr4oVaoU3n33XSSoa4wAGDduHHQ6ndFPFbOTkoiI8ocsj8gDEjBy5N16r70mdQNKlJCTb3uW5fPy0qY1/P23jMpVqiSF4XbsyEJeq2mhZf1wTFcDAPB0wBb7DvLwobYc4fDhQJs20qnx009SvGvaNDM9SkT20z1aKrkf5qZNA3dkaj0gsXa9jvI9+XS9WBQsKKPt6iITpqhp9SVKSMG8DNQ13Fq2zDjZ3HBU/ttvZXULU/PKq1eXXoKkJPmeMOXhQ6mID2SeBVOjBpI9fOAOPW4cvGhylyyNyP/1lyx3V6EC8PjjdhyAiAAbAvnhw4fj1KlTePfddzH4UYXM1q1b49SpU/j1119x8OBBDBkyxKYHX7x4MYYNG4axY8fiwIEDqFWrFtq1a4dr166Z3P/XX3/FiBEjMHbsWJw8eRI//fQTFi9ejI8++shov+rVqyMmJibtZ/v27Ta1i4gorzE7Ip+crN2YWSBPtilQQM7yT52SNHt79eol6fOlSkk6+rFjkoLrwJF4lZsb8F8hSf9vcGOlfQdZsEDeU+HhMv2CKDv06oUUnQcaYB/cTh4H4PjUegBp35O1w2KxaZNctWeP+RT7R2Nfmc+Pb9PG9O2dOwM1a0ohvG7dpABA0aLAk09q++h0mafX79kj3/fFi2fekaHT4UFIOADg4cmoDDenpmovZaZ9ibGxEvUbFr5U0+p79XLK9xhRfmF1IL9ixQpUr14djRs3xpw5c/DgwQMAQJkyZdC1a1fUrFnT5gefMmUKBg4ciP79+6NatWqYNWsW/Pz8MGfOHJP779y5E02aNEGvXr0QHh6Otm3bomfPnhlG8T08PFCsWLG0nyIZvsmJiPIXs4G8Ou/S3Z2jpM4QECA/WeHuLqN20dGSjm5XiWjrNZgg63IXO7TGugnAhvR6LQV4yBCmzlP2KVoUh0vInPE+iqwpb3T6d/u25LkDMl3FXsW0qvWPPSYf7/v3tXnw6e3aJdsnnjBx48OH2rru5gJ5Nzfg44/lsrrvCy9kzPAxTK9Xl700ZJhWb0XwrJQJBwDoojJWu4uOlj4Bb+9MFiu5cwd47DGJ9sPDgYEDJW1/7Vq5vWfPTNtBROZZHcifPXsWmzZtQqVKlTBkyBAUK1YMr7zyCnbu3GnXAyclJWH//v1o3bq11hg3N7Ru3Rq71G+9dBo3boz9+/enBe7nz5/HqlWr0LFjxwxtLV68OMqVK4fevXsjOjraYlsSExNx9+5dox8iorzEbGq9OnkzJMTm5Zgob6r52hNAwYLQ3boF7N5t253XrpUMhMBAmRNPlI1ONOwHAOgDWVPeKJBX58cXKyaFJO2lZi7FxsLdXaawAzLgbYp6mty4sYkbd+yQuiPFi1tYmw6yzrrh7abmlT/2GFC5shzvn38y3m7N/HgDPlWksyPodiQePjS+TU2rL1cuk38bP/+sLSEQHQ38+CPQt68sQVm3ruXnTESZsumsrXnz5pg7dy6uXr2Kr7/+GmfPnkXTpk1RtWpVfPnll4iNtX4pjhs3biA1NRWh6VI5Q0NDcdXM2py9evXChAkT0LRpU3h6eqJ8+fJo2bKlUWp9w4YNMXfuXKxZswYzZ85EZGQkmjVrhnv37plty+TJkxEUFJT2U6pUKaufBxFRbmB2RN7a+fGUf3h4AO3by2VTAYElU6bIduBAMxOCiZwnvkVHXEcRhOEq2uJf40DeEWn1gFEgD0VJG2k3Fchfv64VhTM5Im+YVm9plNzdXRuVL1/e9MF0Okm9B4DffjO+LTlZSw1Iv368GT5VJZAPR1RaH4jKqqXn9HopeglIls7q1VIro3p1if7fe8+qdhCReXYNv/j7++OVV17Btm3bcObMGXTt2hWTJ09G6dKlHd0+I5s3b8akSZPw3Xff4cCBA1i6dClWrlyJiRMnpu3ToUMHdO/eHTVr1kS7du2watUq3LlzJ626vikjR45EXFxc2s/Fi6YLexAR5VZmR+TVQF5NFyUCtGWtVtowT/7oUZkC4OYGvPOOc9pFZEGJsl74BbKm/CuYYzqQt7PQXRo1kE9KAu7cSSs0byp5RY2dq1UDChY0cSy10J25tHpDvXsDv/wia8ybC/rVVPW//5YS+6oDB6S4XKFC0hgr6MqGAwDKIjLDEnRWVaxfs0ayIIKDpWOvfXvp6Dt2TDoWWK2eKMuylEf54MEDbNu2DVu2bMHt27fT1pe3RpEiReDu7p5hFD82NhbFzJxQjh49Gn369MGAAQNQo0YNdOnSBZMmTcLkyZOh1+tN3ic4OBiVKlXCObX70ARvb28UKFDA6IeIKC/hiDzZpH17CciPHpWUWGuoc+Off17mwxJls5IlgTmQKR3PYgUKpxoUT87i0nNpfHyAoCC5HBubFsgfPy5z5Q1ZTKu/cQNp5fUNppmapdNJ8Fu9uvl9qlcHhg6Vy6++qqW1q2n1zZpZP4XqUR2BsohMC9xVVlWsnz5dtq+8Avj7G9/GaVxEDmHXJ2n79u145ZVXEBYWhsGDB6NSpUrYtm0bTpqr9GGCl5cX6tWrhw0bNqRdp9frsWHDBjRq1MjkfeLj4+GW7sPv/qiQjmJYDdPA/fv3ERERgbCwMKvbRkSUl6SmajWeGMiTVQoXlnXqAeORPXMuXJDRQkDSZ4lcoFQp4ChqYg8awBMp8P19nnajo0bkAaP0+rAweVy9HvjvP+PdLAbyGzZIJfcaNRz7/TtpkpTIv3oVePNNeQwb58cDSAvkiyMGkSeMJ8lnmlp/9qyMyOt00gYicgqrA/mYmBh89tlnqFKlCpo3b45Tp05hypQpiImJwZw5c9CkSRObH3zYsGH44YcfMG/ePJw8eRJvvvkmHjx4gP79+wMAXn75ZYwcOTJt/86dO2PmzJn47bffEBkZiXXr1mH06NHo3LlzWkA/fPhwbNmyBVFRUdi5cye6dOkCd3d39GRlTCLKp27f1i5nSO9kIE/mqOn1mc2Tv3FDRvCTkoAmTbQOAKJsVqgQ4OsL/ICBcsWPP2rLnjlqjjxgVLkegMl58klJwL59ctlkIG9LWr0tfH2BhQul1sWff8pykGq1e1sC+UKFkOwjK27cO65l5aSmaskNZkfk1bnxHTrYudA8EVnDI/NdRKlSpVC4cGH06dMHr776KqqaXRDTei+++CKuX7+OMWPG4OrVq6hduzbWrFmTVgAvOjraaAR+1KhR0Ol0GDVqFC5fvoyQkBB07twZn376ado+ly5dQs+ePXHz5k2EhISgadOm2L17N0JCQrLcXiKi3EidHx8UlHHFIgbyZNbTTwMffQRs3CjLZPn6Ztzn3j2gY0epVF+ypLY+NJEL6HQyOr74zIv4xm0o/M6ckSD2iSe0KSIOHpEHgIYNZfl2w0D+0CEgIUE6FypVSnd/Rcl8/fisqFsXGDsWGD1a5qcnJck6ebVrW38MnQ7JJcvC89xRKOcjAVQGAFy6JIfz8pLXOoP796VaPcBaGUROZnUg//vvv+OZZ56BR4azwKwZNGgQBg0aZPK2zZs3G/3u4eGBsWPHYuzYsWaP91v6Sp1ERPmcOj8+Q6E7gIE8mVejhpypX7wIbNokAbuhxESgSxcZdixcGPj3X8DJRW+JMlOyJHDmTCC2hPVEh8s/Aj/8IMu76fXSGeWIwp4mAnlACt4pinQoGKbVZ6hNFxEh01G8vKyuIm+zESOkWKVaha9JExM9uZZ5ViwLnDuKoNuRuHtXFqIwXHruUTKssQULZB37ihWBtm2z9hyIyCKrU+u7du3q8CCeiIicz2yhO4CBPJmn05lPr09NlSraGzZIIatVq2ReLpGLqaPEux8bIBf+/BPYv18ulytneZk3a6UL5OvWlaA2JkZGrIFM5sero/GNG2csBOcoHh7A/PmAn5/8bkeHgWfFcACyBJ0awFusWK8owLffyuW332ZROyIn4yeMiCiPM7v0XEqKVtWYgTyZYrgMnTrXOCoK6NoVWLIE8PQEli8HGjRwVQuJjKj9Sfr6DSSrJCEBUKdgOiKtHsgwR97PD6hZU67as0c+Kjt2yO/ZOj8+vYoVZbpLp07AgAG239+gcr26BJ3FivWbNgEnTkjnRN++9rWZiKzGIXYiojzO7Ij8jRtyxunmBuMFl4keefJJWW4rOloWxf77b1lmLjFRhiB/+cW6pbOIssmgQUCZMkCHDjqg6ABgyBBZRhFwXCCfbkQekGn4Bw9KIN+gAXDlinxEHn883X0VRSs+16qVY9pjybPPyo89DAL5NY9G4i1WrF+6VLa9esn68UTkVByRJyLK49QRebNLzxUpYmayI+V7fn4SzAOSmvvZZxLEt2ola2117+7a9hGl4+8P9OjxaKn3l14CvL21Gx1RsR4wGcgbzpNX0+rr1NEy29NcuABcvy7ZLHXqOKY9zvIokA9HVFpKvcXUerWcvfpiEJFTMZAnIsrjzBa7e5QWyrR6skhNr9frZRhuxQqZG29LBWwiVyhUCHj+ee13R6fWx8amTTlRY9f9+7Vl202m1atr0tWqJdkuOVl4OAAgBDdw8eR96PWZLD0XGSnbRx0ARORcNqfWP3jwAJ999hk2bNiAa9euQa/XG91+Xl2nk4iIcoRMR+QZyJMl/fsDZ87Imftrr0mlbaLcYuBAbVlERwXyRYvKNikJuHMHKFgQlSpJFkBcnPZwJgP5vXtlmxvqShQogJSgQvCIu4XkM5G4dKkGEhMlmSDDAhV6vRbIO+p1JiKLbA7kBwwYgC1btqBPnz4ICwuDzhHVP4mIyGnMjsirgbwjlmOivMvXF5g2zdWtILJPixYyR/z+fTMTu+3g46NF7bGxQMGCcHOTUfl//5WrAaBRIxP3zU2BPABdubLAwVsoeDcKe/bUACAD7hkWsoqJkY4Nd3dZA5CInM7mQH716tVYuXIlmjRp4oz2EBGRg5ktdscReSLK63Q6WVnB0YoVk4j96lWgShUAWiAPACVKaEvhpUlJkdoSgIkqeDmTe7lw4OB+lEUkVq+W60ym1asZuWXK2LxePRHZx+Y58gULFkShDMM6RESUU5ldfo6BPBGRfSwUvAMkrT5D0urJk0B8PBAYCFSu7Pw2OoJB5XqLgTznxxNlO5sD+YkTJ2LMmDGIj493RnuIiMjBOCJPRORgJgJ5w2x5i/Pj69fPPSuFGFSuV+ujmpyhoI7Ic348UbaxOfflq6++QkREBEJDQxEeHg5PT0+j2w8cOOCwxhERUdZcuyZTQ3U6E1PhGcgTEdlH/UJVo1sAISFSjP7oUeCpp0zcJ5fNjweQVrm+LCLTruKIPFHOYHMg/9xzzzmhGURE5AzqeWPVqkBAQLobGcgTEdnHxIg8IKszXrkC1Khh4j7q0nO5ZH48AKPUekABoLM8R56BPFG2sTmQHzt2rDPaQURETmB2ACg1Fbh+XS4zkCciso2ZQL5MGfnJ4OFD4MgRuZwLR+SDcBcFcRv3PAqpVxnj0nNE2c7mOfIAcOfOHfz4448YOXIkbj2qonTgwAFcvnzZoY0jIqKs2bNHthnOG2/elHV/dTrJByUiIuuZSK236OBB6UAtVix3Lc/m6wvl0XMNRxTCw00UpU9IANQYgCPyRNnG5hH5I0eOoHXr1ggKCkJUVBQGDhyIQoUKYenSpYiOjsb8+fOd0U4iIrKRomgj8obVlAFoo0iFC3OpICIiW5kZkTfLMD0qQzn7nE0XHg5cvYqyiER8hboZd7hwQbYBAUCRItnaNqL8zOYR+WHDhqFfv344e/YsfHx80q7v2LEjtm7d6tDGERGR/c6eBe7cAXx8TMzXVEeRmFZPRGQ7w0BeUTLfPzcWulMZzJO3WLG+bNlc10lBlJvZPAyzb98+zJ49O8P1JUqUwFVr04uIiMjp1PPGunWBdAuMsNAdEVFWFC0q2+Rk4PZtoFAhy/urX8i5qdCdymAJumLNTdzO+fFELmFzIO/t7Y27d+9muP7MmTMI4TxLIqIcw+IAEAN5IiL7+fgAwcGS9hQbazmQv3kTiIiQy/XrZ0frHOtRdbuBT0XCu5uJ21mxnsglbE6tf+aZZzBhwgQkJycDAHQ6HaKjo/Hhhx/i+eefd3gDiYjIPmYL3QEM5ImIssraefL//SfbihUzH7nPiR4F6N5XIk3fzhF5IpewOZD/6quvcP/+fRQtWhQPHz5EixYtUKFCBQQGBuLTTz91RhuJiMhGiYnAoUNyOUOhO0A78VQrLxMRkW3UQD6zqaW5eX48oI20R0WZrgfAEXkil7A5tT4oKAjr1q3D9u3bceTIEdy/fx9169ZF69atndE+IiKyw+HDQFKSFKU3eW7FEXkioqxRO0IzG5HP7YF8qVJSxO7hQ+DaNeP/G4qiBfIckSfKVnavOdS0aVM0bdrUkW0hIiID0dGyMlzx4rbfN9OVjhjIExFljTWp9YbrgObGQncA4OUFlCwJXLwoafSG/zdu3wbU2lmP5tITUfawObUeADZs2IBOnTqhfPnyKF++PDp16oT169c7um1ERPnWjz8CFSrIaPqnn8roui3U+fEm0+oBBvJERFllTWp9dLSMYnt4ALVrZ0uznMIwvd6QOj++WDHAzy9bm0SU39kcyH/33Xdo3749AgMDMWTIEAwZMgQFChRAx44dMWPGDGe0kYgo30hKAt5+Gxg4UFY1SkoCRo0C6tXTgnNrWMzk1OvlxBJgIE9EZC81lXzJEuDIkYy3KwrwxRdyuWZNwNc3+9rmaOpzTf88OT+eyGVsDuQnTZqEqVOnYtGiRRg8eDAGDx6MX3/9FVOnTsWkSZOc0UYionwhNhZo3Rr47jtJh584EVi4EChSBDh2DGjUCBgyBHjwwPJxbt8GzpyRyyYD+Vu3gNRUuayuhUxERLbp2hVo3lxSy9u3By5cML59zBhAHeT68MPsb58jtWkj2yVLjAvesWI9kcvYHMjfuXMH7du3z3B927ZtERcX55BGERHlN8eOyfLC27YBBQoAK1bISHzv3sDJk0CfPnLu9M03QMuWlqdk7tsn2/LlpdhdBuqdCxUCPD0d/VSIiPIHb2/5sn7sMSAmRoL5mzflti++AD75RC5/+y3wwguua6cjdO4sz/fMGeDoUe16jsgTuYxd68gvW7Ysw/UrVqxAp06dHNIoIqL8JDkZ6NULuHQJqFxZUug7d9ZuL1IEmD8fWLNGAvP//gOeeAI4dcr08TItkMz58UREjhEcDKxeLcXgTp2SL++vvwY++EBunzxZ5kvldoGBQMeOcvn337XrOSJP5DI2V62vVq0aPv30U2zevBmNGjUCAOzevRs7duzAe++9h2+++SZt38GDBzuupUREedQ338gAR6FCMiIfEmJ6v3btgF27gA4dgIgIoHFjGQxq1sx4v0wL3amFmRjIExFlXcmSwNq1QNOm8iW9a5dcP2KE/OQV3bsDy5ZJID9xoswB44g8kcvoFMVwokvmylr5QdXpdDivfrhzmbt37yIoKAhxcXEoUKCAq5tDRHnYxYtA1aoy7/2nn4BXXsn8Ptevy6DPnj2yKtCsWZJ67+Eh6ffFikktu507ZV59Bl99BQwfDrz4IvDbbw5/TkRE+dL27TKXPCEBeOstSak3uf5nLnXvntRVSUgADh4EatSQAn7JyVLNvkwZV7eQKNezJQ61eUQ+Uk2hISKiLBs6VIL4Jk2Afv2su09ICLBxI/DSSzI48sorwNixUum+TRttpaM6dcwcYPVq2das6YBnQEREAGREfscO4MQJmS+Vl4J4QEuvX7oU+OMPSSNLTpZ/OCVLurp1RPmOXevIA8CNGzdw48YNR7aFiChfWblSzofc3YGZMwE3G76R/fzkPGrCBJk3f/GiFEhWR+Br1QJ8fEzc8dIl6QUAgJ49s/wciIjIQN260stqyxd6btK9u2x//12bH1+mjPwjI6JsZdO3zJ07d/D222+jSJEiCA0NRWhoKIoUKYJBgwbhzp07TmoiEVHeEx8PDBokl999VzIUbeXuDoweLbH5woUyqq9KP28+zaJFkn/ftCnnNBIRkW06dZJe4nPnpCcaYKE7IhexOrX+1q1baNSoES5fvozevXujatWqAIATJ05g7ty52LBhA3bu3ImCBQs6rbFERHnFp5/KlMJSpSQtPit8fGSZut69gSNHZJqm2cH2hQtl26dP1h6UiIjyn4AA4OmnZT35H3+U69gpTOQSVgfyEyZMgJeXFyIiIhCartLxhAkT0LZtW0yYMAFTp051eCOJiPKS69dliWFAKtYHBDju2DVrWpj6fuSI/Hh5aemRREREtnjhBQnk4+Pld47IE7mE1an1y5cvx5dffpkhiAeAYsWK4fPPPze5vjwRERnbtEnqA9WoATz7bDY+8C+/yPbppwFmTxERkT2eflqq1as4Ik/kElYH8jExMahevbrZ2x977DFcVdcmJiIiszZvlu2TT2ZjUWO9XgvkX3opmx6UiIjyHH9/CeZVHJEncgmrA/kiRYogKirK7O2RkZEoVKiQI9pERJSnqYH8/9u787ioqv+P468BAUEWxQXB3DW33HJB1MotwdI0rbQyl9TKrCzbtFKzLNN+mS2WfQuXFsu03WxxSXO3NEvL3BdMcVcURRDu74/DDIyAItsw8H4+Hjzmzr13Lmcab/qe8znntGtXgL902TL47z8oXdosHyQiIpJTd9yRtq0eeRGXyHaQj4yM5NlnnyUxMTHDsfPnzzN69GiioqLytHEiIkXNoUOwZYvpib/++gL8xR99ZB5vvz2LdelERESy6eaboV49s0SKOvJEXOKKJrtr3rw5tWvXZtiwYdStWxfLstiyZQvvvPMO58+f5yP7PxRFRCRTy5aZx0aNcvBvn+3b4YcfoHdvyGS+kiydOwfz5pltzVYvIiK55ecHmzeDxxWtZC0ieSjbQf6qq65i9erVPPjgg4waNQrLsgCw2WzceOONvP3221SuXDnfGipSUHbtMn83detWgOOXpdjIUVn99u0wfrxZOi4lBV5+2Wx36pS913/3HZw+DVWrOi82LyIiklMK8SIule0gD1C9enV++OEHTpw4wfbt2wGoVauWxsZLkXH4MISHw9Gj8MQTaUuEieSVbAd5yzI1+BMnpgV4gAoVTH1+587w7LNmEfoSl/hfeXIyTJ9utu++W//wEhERESkCcvQvujJlytCyZUtatmypEC9FyrBhJsQD/N//waRJrm2PFC2XHR9/5ozpPR82DGrVggYN4MMPTYi/+WZYtw727IH77jNBf/x4M/V9TEzGa1kWfPstNG4MP/1k9t19d36+PREREREpIFfUIy9SlH3+uRlGXKIEDB4M06bB009DuXJw772ubp0UBZccHz9nDvTvD+fPp+3z8oIuXeC556BFi7T9771nAvyQIbB8uSmZb9AAWrc2pfPlypny+5Urzfllypie/fr18/X9iYiIiEjBUJAXwZTUDxtmtp95BsaNg8BA0yM/ZIgJXT16uLSJUgRcsqz+tddMiK9SxfS+d+kC7duDv3/mF+vdG5o1gwEDTGDfvNn8/O9/aef4+sKjj8JTT5ll50RERESkSFCQFyGtpL5RIzPsGOCVV8y+6dOhTx/4+ecCXi5Mipwsg/y+ffDbb6bmft267M9IX6sWrFhhavZXrzaBftUq2LHDfPM0diyEheXdGxARERGRQkFBXoq99CX1M2eCt7fZb7OZCuZjx+Cbb6BvX/j3X7PiisiVio29xPj4r782j23bXtmycnYhISa4q2xEREREpFjQ9MVSrB096lxS37Sp8/ESJWD2bDMEOSZGs9hLzl1yfPyXX5rHnj0LtE0iIiIi4p4U5KVY+/RTE+YbNEgrqb+Yn19agJ84MfMJwkUuJ8uy+sOHzYR1ALfeWoAtEhERERF3pSAvxdqvv5rHu+5KK6nPzG23wXXXwblzZiZ7kStlD/Lt21904NtvzfJyzZqZ0g8RERERkctQkJdiy7LSgvzlJrGz2eCNN8zjp5+mreolkh2xsWZ+BZvNfCHkRGX1IiIiInKFFOSl2Nq2zVQ1+/g4L9GdlaZNYdAgsz18uOlEFckO+/j4xo0vGh9/6hQsWmS2FeRFREREJJsU5KXYsg9LDg83YT47xo8368uvXw+zZuVf26RoyXJ8/Pz5kJQE9etD3boF3CoRERERcVcK8lJsZbesPr2QEBg92myPGgWnT+d9u6ToWbLEPGYI8iqrFxEREZEcUJCXYsveI59hzPJ775mdkZFmlruBA+Gxx8wi4MAjj0CtWnDoEERHF2ybxf3s32+GcXh4wA03pDtw9iz88IPZVpAXERERkStQwtUNEHGFfftgzx7w9ISIiHQHjhwxA+DPn8/4ok8+gb//xrt8eZ54Ah54AN5+2wR7D30lJlmw98Y3awalS6c78NNPZhmEatWgSZOCb5iIiIiIuC3FDymW7L3x114LAQHpDvzvfybEN2pkBsG//TZMmGDGLx85AkOHgmXRt68JZTt3pnWqimTGHuQ7drzoQPqyeputQNskIiIiIu5NQV6KpUzL6hMTYepUs/3UU9CvHwwbBiNHmt74EiXgiy/gs88oVSptBvu33irQposbsay0IN+hQ7oDZ8/Cd9+ZbZXVi4iIiMgVcnmQnzp1KtWqVaNkyZKEh4ezbt26S54/ZcoU6tSpg6+vL5UrV+axxx4jISEhV9eU4ifTie7mzoWDByE0FG6/3fkF116bNsvdsGFw8CAPPmg6Un/6CbZuLZBmi5vZsQNiYsDbG9q0SXfg2WfN0nNVq140tkNERERE5PJcGuTnzJnDiBEjGDt2LBs2bKBx48ZERkZy+PDhTM+fPXs2I0eOZOzYsWzZsoXo6GjmzJnDM888k+NrSvFz5Ihj3jratk3daVkwZYrZfvBBk7wuNmqUGeh84gQMGUKN6hZdu5pDb7+d360Wd2TvjY+IAD+/1J2//gpvvGG2p03TBAsiIiIicsVc+i/IyZMnM2TIEAYOHEj9+vWZNm0afn5+TJ8+PdPzV61aRZs2bbjrrruoVq0anTt35s4773Tqcb/Sa0rxs2KFeWzQAMqWTd25ejX8/rtZUP7++zN/oZeXGTfv7Q3ffw8zZ/LII+bQzJkQF5ffLRd3s3ixeXSU1Z85Y1ZBsCwYPBiiolzWNhERERFxXy4L8omJiaxfv55OnTqlNcbDg06dOrF69epMX9O6dWvWr1/vCO67du1iwYIF3HTTTTm+JsD58+eJi4tz+pGiK9OyentvfN++UL581i9u0ABefNFsDx9Ox3oHqFfP5LOZM/OhsVLoWRYkJWXcn5ICv/xith0T3Y0cCbt2QeXK8NprBdZGERERESlaXBbkjx49SnJyMiEhIU77Q0JCiI2NzfQ1d911Fy+88AJt27bFy8uLmjVr0q5dO0dpfU6uCTBhwgSCgoIcP5UrV87lu5PCzD7RnSPI791rJrEDs/Tc5Tz+ODRvDqdPY/twFg89ZHa//bYJb1K8jBkD/v7w44/O+zdvhqNHoVQpaNEC0z1vn0xx+nQIDCzwtoqIiIhI0eBWgzOXLl3Kyy+/zDvvvMOGDRv48ssv+f7773nR3kOaQ6NGjeLUqVOOn5iYmDxqsRQ2cXHwxx9m2zFj/dSpJoF36AANG17+Ip6eZhk6gI8/pt89FoGBsH07/PxzvjRbCqljx0zHemKiGZERH592zF5Wf9114J0QB/fea3Y88ACkqxoSEREREblSLgvy5cqVw9PTk0OHDjntP3ToEBUrVsz0NaNHj+aee+5h8ODBNGzYkFtvvZWXX36ZCRMmkJKSkqNrAvj4+BAYGOj0I0XTqlUms9eoAZUqYZLX+++bg48+mv0L9eplxtP/8w/+O/90ZDRNele8vPsunDtntvftgxdeSDvmtH78Cy+YE6pVg0mTCrqZIiIiIlLEuCzIe3t706xZMxbbu62AlJQUFi9eTEQWyzGdPXsWj4tmePb09ATAsqwcXVOKlwxl9S+9BCdPQs2acPPN2b9QUBB062a2P/7Y0UH/ww9wiVEcUoQkJKR9cdO3r3mcPBk2bYILF2DZMrOvY5sEU0oPZrb6gICCb6yIiIiIFCkuLa0fMWIE77//PrNmzWLLli0MHTqU+Ph4Bg4cCEC/fv0YNWqU4/xu3brx7rvv8tlnn7F7924WLlzI6NGj6datmyPQX+6aUnzt3Qtffmm2r78eM+vdK6+YHRMnXvkyYPb0Nns2V9dMJiLC9PbPnp356fHxZsLyUaPMEuLi3j79FA4dMpUd06fDrbeaAD90KPz2G5w+DWXKQOO935olC6+66sq+LBIRERERyUIJV/7y3r17c+TIEcaMGUNsbCxNmjThxx9/dExWt2/fPqce+Oeeew6bzcZzzz3Hf//9R/ny5enWrRsvvfRStq8p2bN0KQQHQ6NGrm5J7iUmmp7SF14wZdABARDV6iRE3WOmHB840JTKX6kuXUxSO3gQli6lX7+OrF4NH34II0ZkPH3atLSZ7adPhwkTYMAALSNemL32Grzzjll1sG3btP2WZf5MATzyiFmZ8I03zBwJK1emTaHQvj14zIg2TwYMMPMriIiIiIjkks2yLMvVjShs4uLiCAoK4tSpU8VyvPz27VC3rsmoBw6YZdPd1bJl8OCD8M8/5vkNN5hgVn/8XaZLtWZNM/tdTsudH3gA3nsPBgzg+GszCA01Xxxs3AiNG6edlpRkxuXv32++IDl+3Oxv0QLefBNatcrV25R8cOYMhIWZnvWgIFPAYf9i66efzBLw/v4QEwOlS5v9r70GTzyRdo2ZL+yj/9hqJvnv2GH+vImIiIiIZOJKcqj6AiWDBQtMifixY2ljyt3R//4H7dqZEF++vOkp/+UXqP/HJybEe3rCxx/nbszy3Xebxy++INj3nGPY/EcfOZ/2+ecmxIeEwJ498H//Z37tb79B69YmGErh8sknJsSDGQoRFQW7d5vn9iXgBw1KC/FgeufTV7HcdHimCfHt2inEi4iIiEieUZCXDNKvh/39965rR27Mn59W3jxwIGzdCvfcA7a9e0wXPZgFwHPbFd6mDVStahLfd9/Rr5/Z/cknZrw0mBxnD34PP2wC/OOPw7Zt0L07+FlnGPGY5ThfXM+yTOUGmD8mDRuaERSRkWZZuYULzZCI4cOdX+flZYZQeHpCndoplJs/wxwYNKhg34CIiIiIFGkK8uLk3Lm02bbBPYP8unXQu7epKrj3XoiONsMEHOPh4+JMN/gzz+T+l3l4pPXKf/IJUVFQrpyZuX7RIrP7l19M9b6vr6nEt6tYET6r8QxnCOD3LX6cuaoOdO4MgwenLUIuLrFqFfz1l/nMHn3UfLlVrZoZdhIZac7p2ROqV8/42ogIM3P9ivFLse3ZA4GB5mQRERERkTyiIC9Oli83Yb5CBShRwvQa79jh6lZl386d0LUrnD1rSqGnTQObLfXgvHlmFj9fX1P7XiKP5nq0B/kFC/A+fYw77zRPP/zQPNp74++9F8qWTfe65GRKfmTWsPclgdKHtpmu3uho6NEDjh7Nm/bJFbP3xt91l/kSKCzMDH8oVw6Sk82xxx/P+vX16kG5b6LTLuLnl78NFhEREZFiRUFenNjHanftCtddZ7bdpVf+yBET3o8cgaZNzbh0L6/UgwkJ8OSTZvvpp83Mc3mlfn3zCy9cgLlzHeX1X30Fq1ebOQdsNnjssYtet24dHD2KFRREx6u20p4lfHPrTHO9M2fSvgGQAnX4MMyda7btwzMArr4afvjBhPlbbrnMqIwTJ+CLL8z2vffmW1tFREREpHhSkBcn9vHxUVFpS167S5Dv189UD1StatrsNIfd5MlmIfmrrkoL9HnJ3iv/zjs0a5REvXrmuwP7qnY9e2Yy19n8+QDYoqK47/+uZint6buwPydGpq5t/9ZbJlVKgYqONqsMtGwJzZo5H2ve3IyV/+aby1zk00/h/Hm45hrzIhERERGRPKQgLw4xMWaGdw8P6NQpLcgvW2Y6iAuzpCRTlQ6mJzw0NN3BAwfg5ZfN9sSJ+VPm3K+fWVdu0yZs//eqo1f+4EHzmGkZdmqQp1s3br/dLEV35gw8t7qrCX/x8fDqq3nfVslScrJZTRDS5kS8WLZGZEyfbh4HDUo3tkNEREREJG8oyIuDvaw+PNyMC65Tx1SgJyamTdxWWMXEmBDm4+O8fjsAzz5rQnGrVjgGsOe18uXhjTfM9rhx9G/xjyO/tW5tJkBzsnevmU3NwwOiovDwgEmTzKH3/mdj/30vmCdTp5qZ86RA/PCD+WiCg+GOO3J4kYkTYf16M67DXqkhIiIiIpKHFOTFwV5Wb5+V22Zzn/L6XbvMY40aJhs7/P47zJxptqdMyd/e0bvvNv/BEhMJffZebrnZzIqW6eT49v+grVs7ZsBr187MTZCcDA9/H2W+eDh3zgRDKRD2Se7uvdfMiXhFLAvGjoWRI83z5583X/CIiIiIiOQxBXkBzDxt9l73qKi0/fYgv2CBySmF1c6d5tFpDjvLMmuHAfTta0oN8pPNZuqyAwNh7Vo+i5jCX3+l/Td0kq6sPr2JE80a5F9/Y2PNTam98u++a4YHXMLy5aYHee/ePHgfxdSePWlfZqVfJjBbLMtMovhC6mc2YULeLG8oIiIiIpKJPFp/S9zd2rVw6pQpKU4/N9cNN5gh5QcOwMaNZnL2wih9j7zD/PmwcqV5AxMmFExDKlUyE+sNHkzJF5+j4W3dgKudz4mPhyVLzHbXrk6H6teHESPM0Pje73diV0RbPFevMO1/661Mf2VMjFmt7vhxM3579uy8f1vFwapVJo+3apVuYsLdu82yhT4+ZvbEgADw90/btv+88ELa5/PGG/DIIy57HyIiIiJS9CnIC5A2Pv7GG02PMP/9B0lJlKxWjU6d4NtvTTV4YQ/yTjPDf/65ebz/fjNbfUG5916YM8fMvjdokJktMH29/6JFZkbz6tXNguMXGTvWLH+2Z4+N98Jf4EE6wP/+Z2bbr1LF6dwLF8wy5cePm+eff24yf9Wq+fkGi6a//zaPjRql2/nAA/Dzz9m7gM0G06bBfffledtERERERNJTab0AzsvOERtrAmb16lC3Li/Gj6Aji/j5u/MubeOlZOiRT0pKG4fes2fBNsZmg/ffNz23K1bAuHHOx+1l9V27Zjpmv1QpU00P8PCX7TndrJ2ZcXDgQDOAPp1x48yvCAiAa681h6dMyfu3VBzYg3yDBqk7kpLMmAUwC8ffdBNcdx00aWK+MapQIW0gfWCgmYtBIV5ERERECoDNsgrzyGfXiIuLIygoiFOnThEYGOjq5uS7o0dNJrEs0xEf9kM0DB6c4bxTBGJ9O5/S3a5zQSuzZllmlv1Tp2Dz5tQgtnQptG8P5cqZLyY8PQu+YR9+CP37m+05c8wg9pQUUx1w8KDp6b3xxixffued8Nln0LP+v8zb2xxbfLxJ7mPGALB4sXm5ZZlly8uUMV/ElCplyu3LlCmIN1l01K4NO3aYgomOHYHffjOLyZcpY24Sjyy+97xwwXwIXl4F2l4RERERKVquJIeqR15YuNDkkEaNICwMswYXwBNPmBrvgQM5VqICQcRxatzrLm1rZk6cMCEeTBEBAN98Yx67dnVNiAeztrx9Afn+/c0M+hs2mBDv7w/XX3/Jl0+ZAqVLw5f/1OWn7qld9OPGwdKlHDpk5u+zLPOdS58+0LkzNGxohuDb10KX7Dl3Lm3CREeP/MqV5rF166xDPJiJCRTiRURERKQAKciLo9I7MhJTTmwfE3zHHXDbbTB9Op8NMlPah2343iTnQsReVh8aaua1w7LSgvwtt7isXYCZhr5LF0hIgO7dTck9mNTt43PJl4aEpK0t3+vre1hcZSCkpHDiprvo0+EwsbEmdNqXr7fZzHcvAFOnJHFh4mtp8wTIJf37r/ljExxs/rsDaUG+TRuXtUtEREREJDMK8sXciRPw5Zdmu2dPTHg5fdqsf92smeO8zo835C8a4mUlcip6Xravv2ED/PFHHjf6IhmWnvvnHzPbuI/PJUvXC4Snp6l7r1fPTP3/v/+Z/RctO5eVQYPMsOyzZ+GWfW/xN/Upc+4gI/+5B7+SKcyZk/rlRao+faBFyD4+P3Q9JUY+Yda2P3YsH95Y0ZJ+fLzNhkn1CvIiIiIiUkgpyBdzs2ebzuJrrkldZn3BAnOgSxencuLateHXyn0BOP3ux9m69rFjJoS2a2dKl/NLhonu7L3xnTqZEnZXCwoy0/7bB63bbOa/bzZ4eMBXX8GMGTB5Win+fOZzkrx8ieRndnZ9hAY+O5zO9174PctONyWCNWbHhQvw9dd5+GaKpgwT3e3ZY4ZAeHlBixauapaIiIiISKYU5Isxy0qr9B4yJLUn0j4+/qabMpwfcN+dpGDjql2/wt69l73+4sWmJzkuzkwill8yLD337bfm0dVl9enVqmXWI/f1NSHeUb99eWXLwoABZhW9u15qgNe0twGoOG+q+Yalfn0YORIeewy6dsX37HE2eDRnGvebC8ydmw9vqGjJEOTtvfHXXps2M72IiIiISCGhIF+MbdgAf/5pKtD79gX27TPTvnt4ZFqSfvMDlVlmawfAoddnX/b6CxembW/dmjdtzoxTaX1sLKxda3Zks3y9wHToYMrr7RUDOTVwoJkRv2NHM9Hali1mLL593blHHmHOwyuYzAjzfPHitIXmJVNZBnmV1YuIiIhIIaQgX4zZe+N79jSTfDl64yMiUnc4K1cONjU25fV8/JHp0s+CZRVckHcqrf/uO/OkZUsz+11hU7q0Cd+5YbPBPfeYddKOHDFj8O+6yyw7MG8evPEGDz3uww7b1fxJI5XXX8bZs2ZKBUgX5FesMI9t27qkTSIiIiIil6IgX0zFx5vx8WDK6oG08fGZlNXbVXm0Fwn4EHJsCykbNmZ53vbtztX327blrr1ZSUw0a6ZDaml9YSyrz0+lS5sZ7j75xJRX9OoFQOXKZs6DudxuzlN5fZa2bDFfPJUrBxUqACdPpnXRt27tyqaJiIiIiGRKQb6YmjvXTE5fsybccANw/rwpwYZLBvmo3kH84GVC8v6JWU96Z++Nty+vnV898vv2QUqKGcYc4h9veqnBLPVWzHXtmi7IL1qk8vosZCirX73aJPtata5oLgMRERERkYKiIF9M2cvqBw9OnZx++XLTTR8aCo0bZ/m6kiXhv3amvD5w/mxITs70PPtS9Len5sitWy9ZiZ9j6cfH2xYtNFPwV6+eLpUVX127wjbqsMnW0JTX53ZsfhGl8fEiIiIi4m4U5Iuhf/6BVavMEuf9+6fuTL/snM12ydc3fjqKYwRT+lwsCQuWZDielAS//GK2hw41lzt50gznzmtO4+PtZfXdu1/2PRQHjRqZEvvPLZXXX4qCvIiIiIi4GwX5Yig62jx27ZpuPrhsjI+3a9PemwX+vQE4+GrG8vq1a03ZftmyZohxlSpmf36Mk7cH+Vo1UmD+fPOksM1W7yI2Wybl9SdOuLZRhZBTkE9KSlv1QEFeRERERAopBfli5vx5s3IZpJvkbtcuU/teogR06nTZa3h4wNlepry+0so5sGaN03H7+PhOncy5deqY5/kxTt5eWt/Se6Pp8vf3h+uuy/tf5Ka6doWt1GVriQYmpKq83smZM7Bnj9lu0ADYuBHOnYMyZaBuXRe2TEREREQkawryxczGjXD0qJmhOzIydad92bk2bSAoKFvXaTcygu/oinfKeZKiujp1t9vHx9uXor/6avO4dStm8foRI9LSUy7Ze+SbHE79pR06pM2wJ7RvbyYCnH0htVd+3jzXNqiQ2bLFPFaoYO4JR1l969apk0eIiIiIiBQ++pdqMbNjh3ls0CDdcub2IN+lS7avU6eujQV9P+U3muN16hgpkVFw6BAnT8K6deYce5CvUwdKco7wr0aa9d1ffx1eeinX78Wy0oJ85X9Tg3znzrm+blHi62sqIxzl9T//bCYsEEDj40VERETEPSnIFzP2UvSaNVN3JCbC0qVm29FFnz0T3vJnUMj37KQGHnt2w8038+uCM6SkmPBuHxsfnrSCjTSh146JabPc22fDy4WjR81YfD/i8duwwuxUkM+ga1fYQn12+dY35fX2SQHFOchbFqxI/XOkIC8iIiIihZiCfDGTIcivWWOWnStf3kxzfgVKl4YJ0RWI4keOUA7Wr6fmyNu4hw+ZWHYSPPYYdO9OixHXUYdtHCCU5JkfmZLlnTshJiZX78XeG9+r7DJsSUlQrZpZ+1uc3HyzefziXOqGfTI3cQ7yu3dDbKwZmtGihUvbJSIiIiJyKQryxUyGIH/xzHRX6OaboU3/2nRlPudsvjSI+YkP6U/3VU/DlCmO3t8ZnoOpzz/sbtMXmjUzL162LFfvxR7ku5VMV1avZecyqFQJmjaFv2hodtjTqzgHefsXHE2bmjEJIiIiIiKFlIJ8MZNlkLcPaM+B11+HmNBwbrW+ZC0tWWTrRFKfe+Cpp2DyZFizhikN3ucUpc2Ed+3amRfaS/pzyB7kW5/R+PjL6doV/qG+efLPP65tTCFx+jTs22e2GzQgbXKH8HCXtUlEREREJDsU5IuR+HhTOQypQf7ECfjtN7MjF0G+TBl47z34iShasZYX2i7E69MPYeJEU14fHu68BF0eBfmdO+EqYqh0aoupJujQIVfXK8q6doV/SV1O7cgR81PM2b/PCA2F4GDSgnzLli5rk4iIiIhIdijIFyP2HuwyZcwPS5ZASgrUqwdXXZWra3frBv37m+2ePTMed1qCrm3bPBknv2sX3EhqRUHLlqlvSjLTvDn4VyjFLqqbHfZ114qxzZvNY4MGmEkAN2wwOxTkRURERKSQU5AvRvKjrD696GhYtQoefjjjMXuP/LZtQGBgnoyT37ULOqOy+uzw8ICbbkpXXq9x8s7j4zdvhoQECArShIkiIiIiUugpyBcj+R3kPT0hIsI8XsyptB5yXV5//jwciElO65FXkL+s8HCNk0/PKcjby+pbtMjRpI8iIiIiIgVJ/2ItRpyC/K5d5qdECbjhhnz/3fbS+oMHIS6OXAf5PXugKRsoy3GswEBNUJYNTZooyNtZFvz+u9lu0oS0uSJUVi8iIiIibkBBvhhxCvL23viICAgIyPffXbo0VKhgtrdvJ9fj5P/+O62s3taxo/lCQi6pYUP4hwYAJG8u3kF+xw44fhx8fKBxYzTRnYiIiIi4FQX5YiTTIJ9HZfXZ4VRef4Xj5FetgscfN82tWBF69dL4+CtVqhQk1zYz13sejjVJtphas8Y8NmsG3oln0ursFeRFRERExA0oyBcTFy7A3r1mu2a1ZFi82DwpwCDvNHM9ZLu8/vRpiIw0S9IvWgSHDkEAp2nNKnOCgny2Xd0sgL1UMU+KcXm9PciHh2Nmq09JMSs3hIa6tF0iIiIiItmhIF9M7NtnwryPD4Qd+B1OnjT17s2bF1gbnGauh2wH+c8/hzNnoGpVmPnWabZO+prjvYbgxQVTXlCjRn41ucjROHlj7Vrz2KoVzhPdiYiIiIi4AQ0sLibsZfU1aoDH4tSy+g4dCnRseYaZ69u2NVPc28fJV66c6etmzICefMFkz6lUHbHCrPltl9mi9ZKlJk1gEw3owo/Fdgm6s2fhzz/NdqtWwDyNjxcRERER96Ie+WLC1ePjIa20fts2M2t4dsbJb9sGf688wWzuouquX0yIr1kTHnoIvv8eJkwomMYXEel75C9sKp498hs2mOqU0NDU7440Y72IiIiIuBkF+WLCHuTrXnUGVq82Two4yNeoYTrg4+Phv/9Sd16mvH7mTOjJl/iQCHXrmmS/Ywe89RbcdFPmi9ZLlkJC4HBZE+RTimmQt5fVh4eD7chhs5ahzZb2pZKIiIiISCGnIF9M2IP89cmpvdrVq6d2zxccb++04ezZGSefnAyzZkEfPjM77rkHatfO72YWeSWb1gPA++gBM1dCMWOf6K5VK9J64+vWhaAgl7VJRERERORKKMgXE/Yg3/DAj2ajSxeXtMNeXv/vv6k72rY14/R37oSNG53O/flnuHDgEB1YYnb06VNg7SzKrm4RRAxXmSfFcMI7pyCv9eNFRERExA0pyBcDlmUP8hZhf/5gdkZFuaQtDRuax2efhc8+AwIC4LbbzM7XX3c6d8YMuI15eJJigpZmp88TxXnm+v/+g/37wcMjdcEGzVgvIiIiIm5IQb4YOHzYjEuvzQ689+8GLy9o394lbRkxwoxNPnkS7rwT+vaFM/c/bg5++ikcOADA8ePwzTfpyurVG59nmjZNC/Ipm4tXkLePj2/YEEr5WeqRFxERERG3pCBfDNjL6vuUTi2rv+468Pd3SVvKl4fly2HsWNMr+skn0KB/c040vM6M3X/7bQBmz4YKiTFcxwozEdkdd7ikvUVRzZqw09sE+bO/F68g71RWv3u3+cbI2xsaNXJpu0REREREroSCfDFgD/I3eaQGeReV1dt5ecHzz8OKFaZaft8+GLhpBAAJb0zjxP54ZsyA3swxL7j+eqhUyXUNLmI8PCDp6gYA2LYUr7XkMx0f36QJ+Pi4qkkiIiIiIldMQb4Y2LkTfEig6alfzA4XTXR3sYgIM7/dsGGwqGQ3tlOLkmdPMK76DDZsgDttKqvPLwEtzcz1pY7vh7g4F7emYCQlwe+/m21NdCciIiIi7kxBvhjYuROu51d8ks+Znu0GDVzdJIeAAFNNv/+gJwdufxSAhy5M4Wq20sxab9aJ79XLtY0sgupGlOEAoebJli2ubUwB2bwZzp0zq8xdfTWwerU5oInuRERERMTNKMgXAzt3QhTpyuptNtc2KBOlS8MNMwZglSlDLXaypPIAc6BTJzOwXvJUkybwN+YLHevv4jFO3l5WHx4OHqdPpa0hf8MNrmuUiIiIiEgOKMgXAxmCfGFVqhS2Bx4AoFJMaupSWX2+aNAAttjMhHdn1hWPcfJO4+OXLYPkZKhdG6pWdWm7RERERESulIJ8EXf6NJQ8vJf6bMHy9DQ93IXZQw+Z2fDAzCZ+662ubU8R5esLJyqaIH+umMxcb196rlUrYOFC86Sw3w8iIiIiIplQkC/idu1K6423RUSYGvbCLCzMLDAPcNNNZkCz5AuPa0yQ995Z9IP88eOwdavZbtkSWLTIPLnxRpe1SUREREQkpwpFkJ86dSrVqlWjZMmShIeHs84+m3Qm2rVrh81my/Bz8803O84ZMGBAhuNRhbmkPB+5TVl9ev/3f/D00/D6665uSZEWfJ0ZI1/65F44cCDTcywLNm2CbdsKsmV568QJGDDAbNeuDWXP7Yd//zXr8LVv79K2iYiIiIjkhMuD/Jw5cxgxYgRjx45lw4YNNG7cmMjISA4fPpzp+V9++SUHDx50/GzevBlPT09uv/12p/OioqKczvv0008L4u0UOru3JtKRxeaJuwT58uXhlVegWjVXt6RIq9s6mNW0AuCPMV+xe7cJ7mC+ABo/HurXhyGN1vBMva/46IPzLmxtzqxfD9deC999Z5aKf+UV0nrjW7Qo/BUqIiIiIiKZcHmQnzx5MkOGDGHgwIHUr1+fadOm4efnx/Tp0zM9Pzg4mIoVKzp+Fi5ciJ+fX4Yg7+Pj43RemTJlCuLtFCpHjsDhb1YTyGnO+JWHpk1d3SQpRJo2hS89bgPgZPQ8atSAsmWhYUOoVQtGj4bj/x5iKe2Yl9KTyCGVWd5mJNau3S5u+eVZFrz7LrRuDXv2QI0asGoV9OyJxseLiIiIiNtzaZBPTExk/fr1dEr3D2oPDw86derEavsaz5cRHR1Nnz59KFWqlNP+pUuXUqFCBerUqcPQoUM5duxYltc4f/48cXFxTj/uKiEB5s2D7t3NcPPgtQsAOHptpCklFkkVHAw3R/cC4Hp+JazEYU6cMOute3iYnDv/jg8piemJr8ARrls1EatmTVIiu5ghED//DLGxaV35hcTo0fDgg5CYCD16pPXMY1kaHy8iIiIibq+EK3/50aNHSU5OJiQkxGl/SEgI//7772Vfv27dOjZv3kx0dLTT/qioKHr27En16tXZuXMnzzzzDF26dGH16tV4enpmuM6ECRMYN25c7t5MIXDkCDRuDAcPmueV2cdDHu9AClR+sJtrGyeFUrsB1WBqczx//509U75mU8R97NoFbdpAaEUL6qXeW+++y8JNFbHeeZfO/Aw//2h+7MqVM98evf8+2GwueS92KSnwzjtme/x4eOaZdE3atAkOHwY/v9Tp60VERERE3I9bd9FGR0fTsGFDWrZs6bS/T58+3HLLLTRs2JAePXowf/58fvvtN5YuXZrpdUaNGsWpU6ccPzExMQXQ+rw3f74J8cHBMPJpi7+vH0qplDPQujWed/RydfOksOpl/mx4fTOPa6+F226D0FBMLfrWrVCqFNx9NzdO7YHt559o6r+dZxnP5nq3Q506pvv+6FGIjjbLJLjY1q1mgjtfX3jqqYu+V7D3xt9wgxk0LyIiIiLihlwa5MuVK4enpyeHDh1y2n/o0CEqVqx4ydfGx8fz2WefMWjQoMv+nho1alCuXDl27NiR6XEfHx8CAwOdftzRkiXm8YEHYEKjTwn4dYFZi/2DDyCTSgQRwBHkWbIE0g9BsVe63HEHBAQAphr9nnG1eJlnGd/oczP7+5kzULOmOXffvgJseOZWrjSPLVuCl9dFBzU+XkRERESKAJcGeW9vb5o1a8bixYsd+1JSUli8eDERERGXfO3cuXM5f/48ffv2vezv2b9/P8eOHSM0NDTXbS5Udu+GLVsAM/TXHuQjrz0CjzxinoweDfXquaiB4hZq1zZjMpKT4dtvzb7Tp+Hzz832RV+W2b9jO3IkdYevb9oKA4WgmsUe5Nu0uejA+fPw669mW+PjRURERMSNuby0fsSIEbz//vvMmjWLLVu2MHToUOLj4xk4cCAA/fr1Y9SoURleFx0dTY8ePShbtqzT/jNnzvDkk0+yZs0a9uzZw+LFi+nevTu1atUiMjKyQN5TgRk1Cq65Bu65h90Ld3DggKkWbv35cNOz2qiRqS0WuRx7r/y8eeZxzhyIjzel861bO51avrx5dFohsnJl81gAQf6L5i+zsWQ4Z555Gfbvz3A8yyC/ejWcPQshIea+ERERERFxUy6d7A6gd+/eHDlyhDFjxhAbG0uTJk348ccfHRPg7du3D4+LZlvfunUrK1as4Oeff85wPU9PT/766y9mzZrFyZMnCQsLo3Pnzrz44ov4FKUxsRcuQFKSmdnr44+pNvtT3mcAcVVbUOLzT8245ehoU1ovcjm33QZjxpjS85Mn08rqBw3KMHldhQrm0dEjDwUW5DcsPkG39c/jTRJMWAevPGd61wcMgA4dOEwFtm837c1Q1GMfH9+pk8sn5BMRERERyQ2bZRWydaMKgbi4OIKCgjh16lThHy+/fr0JYAsWOO9/8kmYNMk1bRL31KAB/PMPjBwJr7wCJUqYHu+LVpU4cAAqVTLTLiQmpq5q+P77cN99cNNN8P33+dbEN5vO4JGN97KXKpwsXZ3GJ5c5HU/yC+KPs1cTG1SHW56uD+HhZrC8v7/ZXrcOZs6E/v3zrY0iIiIiIjlxJTnU5aX1kkvNmpHy3fdEBa5iER3Nvjp14PnnXdoscUP28vqJE81j164ZQjyYlebADKk/cSJ1p71HPh8nu9u0Ceps/AyA9xlC8zNLOf3HDvNFVp06YLPhdfYULfmNW059bNad69gRgoKgSRP4/XdzIU10JyIiIiJuTkG+CNi0CX6Ki6BHqUUk/fmPGQvs5+fqZom7ue0282gv0sliRQhvbyhd2mw7yuurVDGP+Vha//aYw3TETIy5vmZvLlyA7/+tCePGmdnzz56lb5PN9OQL1t/2MvTubdqVkgJ//mkeGzQw5QQiIiIiIm7M5WPkJffss9Vffz14NdIM9ZJDDRuaGey3bzcLyUdFZXlq+fJmKP3hw1C3Lmk98qdOmRnvU5eryyvbt0OJb76gBMmcrdeMJt1r8+MrZpL9Pn3MOQmUZO4/DUikAZMmALVSX3zgAKxZA3/9Bd265Wm7RERERERcQT3yRYA9yHfo4Np2iJuz2dLGjg8dasbIZyHDhHcBAaaEHfKlV/6VV+B2aw4Afvf24ZZbzP4FC8ycj2Cmi0hMNG2zL2sPQFgY9Oxphps0a5bnbRMRERERKWgK8m7uwgVYljrfV8eOrm2LFAEjR5qhGc8+e8nTCnIJur17YdGs/7ie1DXg77iDli1NYD91Km1pePuyc61ba1J6ERERESnaFOTd3O+/m0rmMmWgcWNXt0bcnqcntGqVOhV91gpyCbpXX4Vbk+figWVSepUqeHqmVcl/+615XLXKPGZYP15EREREpIhRkHdz9rL69u0vm71E8oy9Rz6/g3xsLHzwAfTBzFbvGBAPjvL6b74x8/MpyIuIiIhIcaHo5+Y0Pl5coaBK62fPhtDzu2nFWiwPD7j9dsexTp3A19eU3n/xhflSwccHrr02z369iIiIiEihpCDvxhIS0sYFK8hLQbpkaX0eriX/009wB58DYGvXDipWdBzz84MbbzTb9iH9LVqYMC8iIiIiUpQpyLuxNWtMmK9YMXUJMJECkmmPfB6vJX/unJnILrOyejt7ef22beaxdes8+dUiIiIiIoWagrwbS19Wr1m6pSBddoy8ZeX6d/z6K1RN+JembMQqUcIsIXeRrl2d/+xrfLyIiIiIFAcK8m7MHuS17JwUNHtp/dGjkJKSuvOqq8zjuXNw/Hiuf8dPP8GLjAbA1rkzlC2b4ZyQEDPJvp165EVERESkOFCQd2P9+8Ntt2l8vBS8cuXMY0pKusxesmRaV30elNef+2IBtzOPFA9PeOmlLM+zl9fXqZPWLhERERGRokxB3o0NGQJz50K1aq5uiRQ3Xl5QpozZzo8l6P7bfpan9g0D4PwDw6FJkyzPHTIEevSAF1/M1a8UEREREXEbCvIikiP5uQTd0UdfpDp7iPWujO/EcZc8t2xZ+Oorp5XpRERERESKNAV5EcmRy054l1N//02DH/4PgCW3vgX+/jm/loiIiIhIEaQgLyI5kula8vYl6HK6lnxKCtb9D1DCusDXdKfqw91z1UYRERERkaKohKsbICLuKV9K62fMwLZyBWcoxXMBb7IxPFdNFBEREREpktQjLyI5kuel9ZYFL7wAwFjGUefGKpTQV40iIiIiIhkoyItIjthL6zPtkf/vv3QLzGfT3r2wbx9JNi/e4UEiI/OkmSIiIiIiRY6CvIjkSKY98mFh4OEBSUlw6NCVXXDlSgA2WNeSgK+CvIiIiIhIFhTkRSRHMp3srkQJCA0121daXp8a5FfSmjp1oGrV3LdRRERERKQoUpAXkRzJdLI7yPk4eUeQb6PeeBERERGRS1CQF5EcsQf5Y8cuGg6fkyAfF4e1aROgIC8iIiIicjkK8iKSI+XKmceUFDh+PN2BHKwln7xyDTbLYic1CK5XkY4d866dIiIiIiJFjYK8iOSIlxeUKWO2c7uW/MpJpqx+nWdr5s0DH588aqSIiIiISBGkIC8iOZYXa8n/8gskLjVBvtrdbahfPw8bKCIiIiJSBCnIi0iOXXIt+WwE+dhY6NvnAuGsBSDiiTZ53EIRERERkaJHQV5EcuySPfIHD5r15LOQnAx33QUVDm8igDNYgYHQoEH+NVZEREREpIhQkBeRHMt0LfkKFcwAesuCAweyfO3775uy+g7epqzeFhEBHvpfkoiIiIjI5ehfzSKSY5muJe/hAVddZbazKK+/cAFefdVs39fABHnaqKxeRERERCQ7FORFJMcyLa2HtCXosgjyX3wBu3ZB2bJQ+4iCvIiIiIjIlVCQF5Ecy3SyO0gbJ5/JWvKWBZMmme1n+8XgsT8GPD2hZcv8a6iIiIiISBGiIC8iOZZlj/wlZq5fsgQ2bABfXxhUb5XZ2bgx+PvnX0NFRERERIoQBXkRybFMJ7uDSwZ5e2/84MEQuEll9SIiIiIiV0pBXkRyzN4jf/SoWU7OIYsgv3Ej/PyzqaQfMQJYqSAvIiIiInKlFORFJMfKljWPlgXHj6c7kEWQt89Uf8cdUK3cGfjzT7NDQV5EREREJNsU5EUkx7y8IDjYbDtNeGcP8kePwrlzAOzZA3PmmN1PPgmsW2e68StXTluuTkRERERELktBXkRyJdMJ78qUAT8/s71/PwCTJ5vc3rkzNG0K/PGHOd6qVYG1VURERESkKFCQF5FcyTTI22xOa8lfuAAzZ5qnTz6Zes7u3eaxVq0CaKWIiIiISNGhIC8iuZKdteT/+gtOn4agIGjfPvX4nj3msXr1AmiliIiIiEjRUcLVDRAR95adteRXxJnN1q3NjPVAWo98tWr53EIRERERkaJFQV5EcuWyPfIxMazYZDbbtk09ZlnqkRcRERERySEFeRHJlcv1yFsxMaz8y+xyBPnDh+HsWeex9CIiIiIiki0aIy8iuXK5IJ+0K4YDB8xSdS1apB6z98ZXqgTe3gXRTBERERGRIkNBXkRyJTul9QDNmoGvb+ox+/h4ldWLiIiIiFwxBXkRyZXL9ch7n4sjgLi0snpQkBcRERERyQUFeRHJFXuQP3YMkpPTHfD3hzJlAKhMjHOQt5fWa8Z6EREREZErpiAvIrlSrpyZsy79RPR2F8JMr3xlYmjdOt0B9ciLiIiIiOSYgryI5EqJEnDDDWb7f/9zPnbM1wT5liH7HD33gIK8iIiIiEguKMiLSK499ph5fO89OHMmbf/uCybItwiNSduZnAx795ptBXkRERERkSumIC8iuda1K9SuDadOwYwZafv/PG6CfL1S6YL8wYOQlGS68itVKuCWioiIiIi4PwV5Eck1D4+0XvkpU0yne0ICrPnPBPmwlHRB3l5WX6UKeHoWbENFRERERIoABXkRyRP9+0NwMOzaBd98A+vXw+5kE+R9j2YS5FVWLyIiIiKSIwryIpIn/Pxg6FCzPXkyrFgB+6gCgC0mxkxrD1p6TkREREQklxTkRSTPDBsGXl6wcqWZ+O4/UsfAJySYheZBPfIiIiIiIrmkIC8ieSY0FO66y2zv3g2J+JAUHGJ2xMSkHQAFeRERERGRHCoUQX7q1KlUq1aNkiVLEh4ezrp167I8t127dthstgw/N998s+Mcy7IYM2YMoaGh+Pr60qlTJ7Zv314Qb0Wk2BsxIm3bzw88q5tx8uzbZx7tQV6l9SIiIiIiOeLyID9nzhxGjBjB2LFj2bBhA40bNyYyMpLDhw9nev6XX37JwYMHHT+bN2/G09OT22+/3XHOpEmTePPNN5k2bRpr166lVKlSREZGkpCQUFBvS6TYatQIOnUy2+Hh4FElNcjHxJhl5/bvN8/VIy8iIiIikiMuD/KTJ09myJAhDBw4kPr16zNt2jT8/PyYPn16pucHBwdTsWJFx8/ChQvx8/NzBHnLspgyZQrPPfcc3bt3p1GjRnz44YccOHCAr7/+ugDfmUjx9corJtAPHw5UThfkY2IgJQVKloSKFV3aRhERERERd+XSIJ+YmMj69evpZO++Azw8POjUqROrV6/O1jWio6Pp06cPpUqVAmD37t3ExsY6XTMoKIjw8PAsr3n+/Hni4uKcfkQk55o1gz//hO7dcQ7y9rL6qlXBZnNZ+0RERERE3JlLg/zRo0dJTk4mJCTEaX9ISAixsbGXff26devYvHkzgwcPduyzv+5KrjlhwgSCgoIcP5XtwUNEcq+KWYKOmJi0pedUVi8iIiIikmMuL63PjejoaBo2bEjLli1zdZ1Ro0Zx6tQpx0+MfXZtEcm9zHrkFeRFRERERHLMpUG+XLlyeHp6cujQIaf9hw4douJlxs/Gx8fz2WefMWjQIKf99tddyTV9fHwIDAx0+hGRPGIP8v/9Bzt3mm0FeRERERGRHHNpkPf29qZZs2YsXrzYsS8lJYXFixcTERFxydfOnTuX8+fP07dvX6f91atXp2LFik7XjIuLY+3atZe9pojkg9BQ8PSECxdgzRqzT0vPiYiIiIjkWAlXN2DEiBH079+f5s2b07JlS6ZMmUJ8fDwDBw4EoF+/flSqVIkJEyY4vS46OpoePXpQtmxZp/02m41HH32U8ePHU7t2bapXr87o0aMJCwujR48eBfW2RMTO0xPCwjRGXkREREQkj7g8yPfu3ZsjR44wZswYYmNjadKkCT/++KNjsrp9+/bh4eFcOLB161ZWrFjBzz//nOk1n3rqKeLj47nvvvs4efIkbdu25ccff6RkyZL5/n5EJBOVK5sgb6cgLyIiIiKSYzbLsixXN6KwiYuLIygoiFOnTmm8vEhe6NMH5swx2/7+EBen5edERERERNK5khzq1rPWi4ibSL+kY/XqCvEiIiIiIrmgIC8i+c++ljyorF5EREREJJcU5EUk/6XvkdeM9SIiIiIiuaIgLyL57+LSehERERERyTEFeRHJfwryIiIiIiJ5RkFeRPJf+fJgX/5RQV5EREREJFdcvo68iBQDNhtMnAhbt0LDhq5ujYiIiIiIW1OQF5GC8cgjrm6BiIiIiEiRoNJ6ERERERERETeiIC8iIiIiIiLiRhTkRURERERERNyIgryIiIiIiIiIG1GQFxEREREREXEjCvIiIiIiIiIibkRBXkRERERERMSNKMiLiIiIiIiIuBEFeRERERERERE3oiAvIiIiIiIi4kYU5EVERERERETciIK8iIiIiIiIiBtRkBcRERERERFxIwryIiIiIiIiIm5EQV5ERERERETEjSjIi4iIiIiIiLgRBXkRERERERERN6IgLyIiIiIiIuJGSri6AYWRZVkAxMXFubglIiIiIiIiUhzY86c9j16KgnwmTp8+DUDlypVd3BIREREREREpTk6fPk1QUNAlz7FZ2Yn7xUxKSgoHDhwgICAAm83m6uZkKS4ujsqVKxMTE0NgYKCrmyM5oM/Q/ekzdH/6DN2fPkP3p8/Q/ekzLBr0ObqWZVmcPn2asLAwPDwuPQpePfKZ8PDw4KqrrnJ1M7ItMDBQN5qb02fo/vQZuj99hu5Pn6H702fo/vQZFg36HF3ncj3xdprsTkRERERERMSNKMiLiIiIiIiIuBEFeTfm4+PD2LFj8fHxcXVTJIf0Gbo/fYbuT5+h+9Nn6P70Gbo/fYZFgz5H96HJ7kRERERERETciHrkRURERERERNyIgryIiIiIiIiIG1GQFxEREREREXEjCvIiIiIiIiIibkRB3o1NnTqVatWqUbJkScLDw1m3bp2rmyRZmDBhAi1atCAgIIAKFSrQo0cPtm7d6nROu3btsNlsTj8PPPCAi1osF3v++eczfD5169Z1HE9ISGDYsGGULVsWf39/evXqxaFDh1zYYrlYtWrVMnyGNpuNYcOGAboHC6Nff/2Vbt26ERYWhs1m4+uvv3Y6blkWY8aMITQ0FF9fXzp16sT27dudzjl+/Dh33303gYGBlC5dmkGDBnHmzJkCfBfF26U+w6SkJJ5++mkaNmxIqVKlCAsLo1+/fhw4cMDpGpndu6+88koBv5Pi63L34YABAzJ8PlFRUU7n6D50rct9hpn93Wiz2Xj11Vcd5+g+LHwU5N3UnDlzGDFiBGPHjmXDhg00btyYyMhIDh8+7OqmSSaWLVvGsGHDWLNmDQsXLiQpKYnOnTsTHx/vdN6QIUM4ePCg42fSpEkuarFkpkGDBk6fz4oVKxzHHnvsMb777jvmzp3LsmXLOHDgAD179nRha+Viv/32m9Pnt3DhQgBuv/12xzm6BwuX+Ph4GjduzNSpUzM9PmnSJN58802mTZvG2rVrKVWqFJGRkSQkJDjOufvuu/n7779ZuHAh8+fP59dff+W+++4rqLdQ7F3qMzx79iwbNmxg9OjRbNiwgS+//JKtW7dyyy23ZDj3hRdecLo3H3744YJovnD5+xAgKirK6fP59NNPnY7rPnSty32G6T+7gwcPMn36dGw2G7169XI6T/dhIWOJW2rZsqU1bNgwx/Pk5GQrLCzMmjBhggtbJdl1+PBhC7CWLVvm2HfDDTdYw4cPd12j5JLGjh1rNW7cONNjJ0+etLy8vKy5c+c69m3ZssUCrNWrVxdQC+VKDR8+3KpZs6aVkpJiWZbuwcIOsL766ivH85SUFKtixYrWq6++6th38uRJy8fHx/r0008ty7Ksf/75xwKs3377zXHODz/8YNlsNuu///4rsLaLcfFnmJl169ZZgLV3717HvqpVq1qvv/56/jZOsiWzz7B///5W9+7ds3yN7sPCJTv3Yffu3a0OHTo47dN9WPioR94NJSYmsn79ejp16uTY5+HhQadOnVi9erULWybZderUKQCCg4Od9n/yySeUK1eOa665hlGjRnH27FlXNE+ysH37dsLCwqhRowZ33303+/btA2D9+vUkJSU53ZN169alSpUquicLqcTERD7++GPuvfdebDabY7/uQfexe/duYmNjne67oKAgwsPDHffd6tWrKV26NM2bN3ec06lTJzw8PFi7dm2Bt1ku79SpU9hsNkqXLu20/5VXXqFs2bI0bdqUV199lQsXLrimgZKppUuXUqFCBerUqcPQoUM5duyY45juQ/dy6NAhvv/+ewYNGpThmO7DwqWEqxsgV+7o0aMkJycTEhLitD8kJIR///3XRa2S7EpJSeHRRx+lTZs2XHPNNY79d911F1WrViUsLIy//vqLp59+mq1bt/Lll1+6sLViFx4ezsyZM6lTpw4HDx5k3LhxXHfddWzevJnY2Fi8vb0z/MMzJCSE2NhY1zRYLunrr7/m5MmTDBgwwLFP96B7sd9bmf1daD8WGxtLhQoVnI6XKFGC4OBg3ZuFUEJCAk8//TR33nkngYGBjv2PPPII1157LcHBwaxatYpRo0Zx8OBBJk+e7MLWil1UVBQ9e/akevXq7Ny5k2eeeYYuXbqwevVqPD09dR+6mVmzZhEQEJBheKDuw8JHQV6kgA0bNozNmzc7ja8GnMaKNWzYkNDQUDp27MjOnTupWbNmQTdTLtKlSxfHdqNGjQgPD6dq1ap8/vnn+Pr6urBlkhPR0dF06dKFsLAwxz7dgyKuk5SUxB133IFlWbz77rtOx0aMGOHYbtSoEd7e3tx///1MmDABHx+fgm6qXKRPnz6O7YYNG9KoUSNq1qzJ0qVL6dixowtbJjkxffp07r77bkqWLOm0X/dh4aPSejdUrlw5PD09M8yIfejQISpWrOiiVkl2PPTQQ8yfP59ffvmFq6666pLnhoeHA7Bjx46CaJpcodKlS3P11VezY8cOKlasSGJiIidPnnQ6R/dk4bR3714WLVrE4MGDL3me7sHCzX5vXervwooVK2aYBPbChQscP35c92YhYg/xe/fuZeHChU698ZkJDw/nwoUL7Nmzp2AaKFekRo0alCtXzvH/Tt2H7mP58uVs3br1sn8/gu7DwkBB3g15e3vTrFkzFi9e7NiXkpLC4sWLiYiIcGHLJCuWZfHQQw/x1VdfsWTJEqpXr37Z12zcuBGA0NDQfG6d5MSZM2fYuXMnoaGhNGvWDC8vL6d7cuvWrezbt0/3ZCE0Y8YMKlSowM0333zJ83QPFm7Vq1enYsWKTvddXFwca9euddx3ERERnDx5kvXr1zvOWbJkCSkpKY4vasS17CF++/btLFq0iLJly172NRs3bsTDwyNDubYUDvv37+fYsWOO/3fqPnQf0dHRNGvWjMaNG1/2XN2HrqfSejc1YsQI+vfvT/PmzWnZsiVTpkwhPj6egQMHurppkolhw4Yxe/ZsvvnmGwICAhxjwoKCgvD19WXnzp3Mnj2bm266ibJly/LXX3/x2GOPcf3119OoUSMXt14AnnjiCbp160bVqlU5cOAAY8eOxdPTkzvvvJOgoCAGDRrEiBEjCA4OJjAwkIcffpiIiAhatWrl6qZLOikpKcyYMYP+/ftTokTaX4G6BwunM2fOOFVE7N69m40bNxIcHEyVKlV49NFHGT9+PLVr16Z69eqMHj2asLAwevToAUC9evWIiopiyJAhTJs2jaSkJB566CH69OnjNKxC8s+lPsPQ0FBuu+02NmzYwPz580lOTnb8/RgcHIy3tzerV69m7dq1tG/fnoCAAFavXs1jjz1G3759KVOmjKveVrFyqc8wODiYcePG0atXLypWrMjOnTt56qmnqFWrFpGRkYDuw8Lgcv8vBfNF6Ny5c3nttdcyvF73YSHl6mnzJefeeustq0qVKpa3t7fVsmVLa82aNa5ukmQByPRnxowZlmVZ1r59+6zrr7/eCg4Otnx8fKxatWpZTz75pHXq1CnXNlwcevfubYWGhlre3t5WpUqVrN69e1s7duxwHD937pz14IMPWmXKlLH8/PysW2+91Tp48KALWyyZ+emnnyzA2rp1q9N+3YOF0y+//JLp/zv79+9vWZZZgm706NFWSEiI5ePjY3Xs2DHDZ3vs2DHrzjvvtPz9/a3AwEBr4MCB1unTp13wboqnS32Gu3fvzvLvx19++cWyLMtav369FR4ebgUFBVklS5a06tWrZ7388stWQkKCa99YMXKpz/Ds2bNW586drfLly1teXl5W1apVrSFDhlixsbFO19B96FqX+3+pZVnWe++9Z/n6+lonT57M8Hrdh4WTzbIsK9+/LRARERERERGRPKEx8iIiIiIiIiJuREFeRERERERExI0oyIuIiIiIiIi4EQV5ERERERERETeiIC8iIiIiIiLiRhTkRURERERERNyIgryIiIiIiIiIG1GQFxEREREREXEjCvIiIiKSIzabja+//trVzeD555+nSZMmrm6GiIhIgVGQFxERKaSOHDnC0KFDqVKlCj4+PlSsWJHIyEhWrlzp6qbliT179mCz2di4caOrmyIiIuJWSri6ASIiIpK5Xr16kZiYyKxZs6hRowaHDh1i8eLFHDt2zNVNExERERdSj7yIiEghdPLkSZYvX87EiRNp3749VatWpWXLlowaNYpbbrnFcd7kyZNp2LAhpUqVonLlyjz44IOcOXPGcXzmzJmULl2a+fPnU6dOHfz8/Ljttts4e/Yss2bNolq1apQpU4ZHHnmE5ORkx+uqVavGiy++yJ133kmpUqWoVKkSU6dOvWSbY2JiuOOOOyhdujTBwcF0796dPXv2ZPs9L126FJvNxuLFi2nevDl+fn60bt2arVu3Op33yiuvEBISQkBAAIMGDSIhISHDtT744APq1atHyZIlqVu3Lu+8847j2L333kujRo04f/48AImJiTRt2pR+/fplu60iIiKupCAvIiJSCPn7++Pv78/XX3/tCJyZ8fDw4M033+Tvv/9m1qxZLFmyhKeeesrpnLNnz/Lmm2/y2Wef8eOPP7J06VJuvfVWFixYwIIFC/joo4947733mDdvntPrXn31VRo3bswff/zByJEjGT58OAsXLsy0HUlJSURGRhIQEMDy5ctZuXIl/v7+REVFkZiYeEXv/dlnn+W1117j999/p0SJEtx7772OY59//jnPP/88L7/8Mr///juhoaFOIR3gk08+YcyYMbz00kts2bKFl19+mdGjRzNr1iwA3nzzTeLj4xk5cqTj9508eZK33377itopIiLiMpaIiIgUSvPmzbPKlCljlSxZ0mrdurU1atQo688//7zka+bOnWuVLVvW8XzGjBkWYO3YscOx7/7777f8/Pys06dPO/ZFRkZa999/v+N51apVraioKKdr9+7d2+rSpYvjOWB99dVXlmVZ1kcffWTVqVPHSklJcRw/f/685evra/3000+ZtnX37t0WYP3xxx+WZVnWL7/8YgHWokWLHOd8//33FmCdO3fOsizLioiIsB588EGn64SHh1uNGzd2PK9Zs6Y1e/Zsp3NefPFFKyIiwvF81apVlpeXlzV69GirRIkS1vLlyzNto4iISGGkHnkREZFCqlevXhw4cIBvv/2WqKgoli5dyrXXXsvMmTMd5yxatIiOHTtSqVIlAgICuOeeezh27Bhnz551nOPn50fNmjUdz0NCQqhWrRr+/v5O+w4fPuz0+yMiIjI837JlS6Zt/fPPP9mxYwcBAQGOaoLg4GASEhLYuXPnFb3vRo0aObZDQ0MBHG3bsmUL4eHhWbYzPj6enTt3MmjQIEc7/P39GT9+vFM7IiIieOKJJ3jxxRd5/PHHadu27RW1UURExJU02Z2IiEghVrJkSW688UZuvPFGRo8ezeDBgxk7diwDBgxgz549dO3alaFDh/LSSy8RHBzMihUrGDRoEImJifj5+QHg5eXldE2bzZbpvpSUlBy388yZMzRr1oxPPvkkw7Hy5ctf0bXSt81mswFku232+QHef//9DIHf09PTsZ2SksLKlSvx9PRkx44dV9Q+ERERV1OPvIiIiBupX78+8fHxAKxfv56UlBRee+01WrVqxdVXX82BAwfy7HetWbMmw/N69epleu61117L9u3bqVChArVq1XL6CQoKyrM21atXj7Vr12bZzpCQEMLCwti1a1eGdlSvXt1x3quvvsq///7LsmXL+PHHH5kxY0aetVFERCS/KciLiIgUQseOHaNDhw58/PHH/PXXX+zevZu5c+cyadIkunfvDkCtWrVISkrirbfeYteuXXz00UdMmzYtz9qwcuVKJk2axLZt25g6dSpz585l+PDhmZ579913U65cObp3787y5cvZvXs3S5cu5ZFHHmH//v151qbhw4czffp0ZsyYwbZt2xg7dix///230znjxo1jwoQJvPnmm2zbto1NmzYxY8YMJk+eDMAff/zBmDFj+OCDD2jTpg2TJ09m+PDh7Nq1K8/aKSIikp8U5EVERAohf39/wsPDef3117n++uu55pprGD16NEOGDHHMrt64cWMmT57MxIkTueaaa/jkk0+YMGFCnrXh8ccf5/fff6dp06aMHz+eyZMnExkZmem5fn5+/Prrr1SpUoWePXtSr149x9JwgYGBedam3r17M3r0aJ566imaNWvG3r17GTp0qNM5gwcP5oMPPmDGjBk0bNiQG264gZkzZ1K9enUSEhLo27cvAwYMoFu3bgDcd999tG/fnnvuucdpCT4REZHCymZZluXqRoiIiEjhUq1aNR599FEeffRRVzdFRERELqIeeRERERERERE3oiAvIiIiIiIi4kZUWi8iIiIiIiLiRtQjLyIiIiIiIuJGFORFRERERERE3IiCvIiIiIiIiIgbUZAXERERERERcSMK8iIiIiIiIiJuREFeRERERERExI0oyIuIiIiIiIi4EQV5ERERERERETfy/3ajRpgdbrHIAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High - MSE: 0.0038, MAE: 0.0577, R: 0.0956\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low - MSE: 0.0010, MAE: 0.0276, R: 0.7720\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Close - MSE: 0.0017, MAE: 0.0388, R: 0.3221\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA/EAAAIjCAYAAABLQJsFAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAD3XklEQVR4nOzdZ3RUVRcG4HfSCym00HtvUqUIUkMRlaKIWGgK2BAQERWVJoqgKIIoSBEQ+QARpYhSpRfpCNIh9F5CD0nmfj+2J3eSzCTTS/I+a2XdyZQ7J20y++x99jFomqaBiIiIiIiIiLyen6cHQERERERERETWYRBPRERERERE5CMYxBMRERERERH5CAbxRERERERERD6CQTwRERERERGRj2AQT0REREREROQjGMQTERERERER+QgG8UREREREREQ+gkE8ERERERERkY9gEE9EROQCxYsXR7du3VI+X7NmDQwGA9asWeOxMaWVdozkHAaDAUOHDvX0MIiIKItiEE9ERFnO9OnTYTAYUj5CQkJQtmxZ9O7dGxcvXvT08GyydOnSLBsQNm7cONXPydKHt3z9u3fvxosvvogiRYogODgYuXLlQmxsLH744QckJyd7enhERJRNBHh6AERERK4yfPhwlChRAvfv38eGDRvw3XffYenSpdi3bx/CwsLcOpaGDRvi3r17CAoKsulxS5cuxYQJE7wmkHWmDz74AD169Ej5fNu2bRg3bhwGDRqEChUqpFz/0EMPeWJ4qUyZMgWvvvoq8uXLh86dO6NMmTK4desWVq1ahZdffhnnz5/HoEGDPD1MIiLKBhjEExFRlvXYY4+hVq1aAIAePXogd+7c+PLLL7Fw4UI899xzZh9z584dhIeHO30sfn5+CAkJcfp5fVnz5s1TfR4SEoJx48ahefPmaNy4scXHuepnZMmWLVvw6quvol69eli6dCkiIiJSbuvXrx+2b9+Offv2uW08RESUvbGcnoiIso2mTZsCAE6cOAEA6NatG3LkyIFjx46hdevWiIiIwAsvvAAAMBqNGDt2LCpVqoSQkBDky5cPr7zyCq5fv57qnJqmYcSIEShcuDDCwsLQpEkT7N+/P91zW1oTv3XrVrRu3Ro5c+ZEeHg4HnroIXz99dcp45swYQIApCovV5w9xrQSExORK1cudO/ePd1tN2/eREhICAYMGJBy3fjx41GpUiWEhYUhZ86cqFWrFmbPnp3p82Rk6NChMBgM+Pfff/H8888jZ86caNCgAQApxzcX7Hfr1g3FixdPdZ213ytzhg0bBoPBgJ9++ilVAK/UqlUr094Cu3btwmOPPYbIyEjkyJEDzZo1w5YtW1LdJzExEcOGDUOZMmUQEhKC3Llzo0GDBlixYkWq+x08eBAdOnRArly5EBISglq1amHRokWZfh1ERJQ1MBNPRETZxrFjxwAAuXPnTrkuKSkJLVu2RIMGDfDFF1+klNm/8sormD59Orp3744+ffrgxIkT+Oabb7Br1y5s3LgRgYGBAIDBgwdjxIgRaN26NVq3bo2dO3eiRYsWePDgQabjWbFiBZ544gkUKFAAffv2Rf78+XHgwAEsWbIEffv2xSuvvIJz585hxYoV+PHHH9M93tVjDAwMRPv27bFgwQJMmjQp1VKA3377DQkJCejUqRMAYPLkyejTpw86dOiAvn374v79+9i7dy+2bt2K559/PtPvRWaeeeYZlClTBp9++ik0TbP58dZ+r9K6e/cuVq1ahYYNG6Jo0aJ2jX3//v149NFHERkZiYEDByIwMBCTJk1C48aNsXbtWtSpUweATFiMHDkSPXr0QO3atXHz5k1s374dO3fuTKla2L9/P+rXr49ChQrhvffeQ3h4OObNm4d27drhl19+Qfv27e0aIxER+RCNiIgoi/nhhx80ANrKlSu1y5cva6dPn9bmzJmj5c6dWwsNDdXOnDmjaZqmde3aVQOgvffee6kev379eg2A9tNPP6W6/s8//0x1/aVLl7SgoCDt8ccf14xGY8r9Bg0apAHQunbtmnLdX3/9pQHQ/vrrL03TNC0pKUkrUaKEVqxYMe369eupnsf0XG+88YZm7t+1K8ZozrJlyzQA2uLFi1Nd37p1a61kyZIpn7dt21arVKlShufKzM8//5zqe6RpmjZkyBANgPbcc8+lu3+jRo20Ro0apbu+a9euWrFixVI+t/Z7Zc6ePXs0AFrfvn2t/joAaEOGDEn5vF27dlpQUJB27NixlOvOnTunRUREaA0bNky5rmrVqtrjjz+e4bmbNWumValSRbt//37KdUajUXvkkUe0MmXKWD1GIiLyXSynJyKiLCs2NhZ58+ZFkSJF0KlTJ+TIkQO//vorChUqlOp+r732WqrPf/75Z0RFRaF58+a4cuVKykfNmjWRI0cO/PXXXwCAlStX4sGDB3jzzTdTlbn369cv07Ht2rULJ06cQL9+/RAdHZ3qNtNzWeKOMQKyBCFPnjyYO3duynXXr1/HihUr8Oyzz6ZcFx0djTNnzmDbtm1WnddWr776qt2PtfZ7Zc7NmzcBwGwZvTWSk5OxfPlytGvXDiVLlky5vkCBAnj++eexYcOGlOeIjo7G/v37ceTIEbPnunbtGlavXo2OHTvi1q1bKV/H1atX0bJlSxw5cgRnz561a5xEROQ7WE5PRERZ1oQJE1C2bFkEBAQgX758KFeuHPz8Us9fBwQEoHDhwqmuO3LkCOLj4xETE2P2vJcuXQIAnDx5EgBQpkyZVLfnzZsXOXPmzHBsqrS/cuXK1n9Bbh4jIN+fp59+GrNnz0ZCQgKCg4OxYMECJCYmpgri3333XaxcuRK1a9dG6dKl0aJFCzz//POoX7++XV9fWiVKlLD7sdZ+r8yJjIwEANy6dcuu5758+TLu3r2LcuXKpbutQoUKMBqNOH36NCpVqoThw4ejbdu2KFu2LCpXroxWrVqhc+fOKd35jx49Ck3T8NFHH+Gjjz6y+LWknaQiIqKshUE8ERFlWbVr107pTm9JcHBwusDeaDQiJiYGP/30k9nH5M2b12ljtJc7x9ipUydMmjQJf/zxB9q1a4d58+ahfPnyqFq1asp9KlSogEOHDmHJkiX4888/8csvv+Dbb7/F4MGDMWzYMIfHEBoamu46g8Fgdn182j3bHflelS5dGgEBAfjnn39sHLHtGjZsiGPHjmHhwoVYvnw5pkyZgq+++goTJ05Ejx49YDQaAQADBgxAy5YtLY6XiIiyNgbxREREaZQqVQorV65E/fr1zQaPSrFixQBIpte0VPry5cuZdj0vVaoUAGDfvn2IjY21eD9LpfXuGKPSsGFDFChQAHPnzkWDBg2wevVqfPDBB+nuFx4ejmeffRbPPvssHjx4gKeeegqffPIJ3n//fZdsr5czZ04cP3483fWq+kCx9ntlTlhYGJo2bYrVq1fj9OnTKFKkiE2Pz5s3L8LCwnDo0KF0tx08eBB+fn6pzql2A+jevTtu376Nhg0bYujQoejRo0fKzy8wMDDD3xkiIsrauCaeiIgojY4dOyI5ORkff/xxutuSkpJw48YNALLmPjAwEOPHj0+VER47dmymz1GjRg2UKFECY8eOTTmfYnoutR962vu4Y4yKn58fOnTogMWLF+PHH39EUlJSqlJ6ALh69Wqqz4OCglCxYkVomobExESrn8sWpUqVwsGDB3H58uWU6/bs2YONGzemup+13ytLhgwZAk3T0LlzZ9y+fTvd7Tt27MCMGTPMPtbf3x8tWrTAwoULERcXl3L9xYsXMXv2bDRo0CClZD/t9zBHjhwoXbo0EhISAAAxMTFo3LgxJk2ahPPnz6d7LtPvAxERZV3MxBMREaXRqFEjvPLKKxg5ciR2796NFi1aIDAwEEeOHMHPP/+Mr7/+Gh06dEDevHkxYMAAjBw5Ek888QRat26NXbt24Y8//kCePHkyfA4/Pz989913ePLJJ1GtWjV0794dBQoUwMGDB7F//34sW7YMAFCzZk0AQJ8+fdCyZUv4+/ujU6dObhmjqWeffRbjx4/HkCFDUKVKFVSoUCHV7S1atED+/PlRv3595MuXDwcOHMA333yDxx9/3O6mcJl56aWX8OWXX6Jly5Z4+eWXcenSJUycOBGVKlVKaRYHWP/ztOSRRx7BhAkT8Prrr6N8+fLo3LkzypQpg1u3bmHNmjVYtGgRRowYYfHxI0aMwIoVK9CgQQO8/vrrCAgIwKRJk5CQkIDRo0en3K9ixYpo3LgxatasiVy5cmH79u2YP38+evfunXKfCRMmoEGDBqhSpQp69uyJkiVL4uLFi9i8eTPOnDmDPXv2OPhdJSIir+fBzvhEREQuobaY27ZtW4b369q1qxYeHm7x9u+//16rWbOmFhoaqkVERGhVqlTRBg4cqJ07dy7lPsnJydqwYcO0AgUKaKGhoVrjxo21ffv2acWKFctwizllw4YNWvPmzbWIiAgtPDxce+ihh7Tx48en3J6UlKS9+eabWt68eTWDwZBuuzlnjjEjRqNRK1KkiAZAGzFiRLrbJ02apDVs2FDLnTu3FhwcrJUqVUp75513tPj4eKvOr2kZbzF3+fJls4+ZNWuWVrJkSS0oKEirVq2atmzZsnRbzCnWfK8ysmPHDu3555/XChYsqAUGBmo5c+bUmjVrps2YMUNLTk5OuR/SbDGnaZq2c+dOrWXLllqOHDm0sLAwrUmTJtqmTZtS3WfEiBFa7dq1tejoaC00NFQrX7689sknn2gPHjxIdb9jx45pXbp00fLnz68FBgZqhQoV0p544glt/vz5Vn0dRETk2wyaZqYjDBERERERERF5Ha6JJyIiIiIiIvIRDOKJiIiIiIiIfASDeCIiIiIiIiIfwSCeiIiIiIiIyEcwiCciIiIiIiLyEQziiYiIiIiIiHxEgKcH4I2MRiPOnTuHiIgIGAwGTw+HiIiIiIiIsjhN03Dr1i0ULFgQfn6W8+0M4s04d+4cihQp4ulhEBERERERUTZz+vRpFC5c2OLtDOLNiIiIACDfvMjISA+PhoiIiIiIiLK6mzdvokiRIinxqCUM4s1QJfSRkZEM4omIiIiIiMhtMlvSzcZ2RERERERERD6CQTwRERERERGRj2AQT0REREREROQjuCbeTpqmISkpCcnJyZ4eCmVxgYGB8Pf39/QwiIiIiIjICzCIt8ODBw9w/vx53L1719NDoWzAYDCgcOHCyJEjh6eHQkREREREHsYg3kZGoxEnTpyAv78/ChYsiKCgoEy7BxLZS9M0XL58GWfOnEGZMmWYkSciIiIiyuYYxNvowYMHMBqNKFKkCMLCwjw9HMoG8ubNi7i4OCQmJjKIJyIiIiLK5tjYzk5+fvzWkXuw0oOIiIiIiBRGokREREREREQ+gkE8ERERERERkY9gEE9eJy4uDgaDAbt37/b0UIiIiIiIiLwKg/hswGAwZPgxdOhQt47n6NGj6N69OwoXLozg4GCUKFECzz33HLZv3+7WcRAREREREfkadqfPBs6fP59yee7cuRg8eDAOHTqUcp3p/uOapiE5ORkBAa751di+fTuaNWuGypUrY9KkSShfvjxu3bqFhQsX4u2338batWtd8rxERERERERZATPxDtI04M4dz3xomnVjzJ8/f8pHVFQUDAZDyucHDx5EREQE/vjjD9SsWRPBwcHYsGEDunXrhnbt2qU6T79+/dC4ceOUz41GI0aOHIkSJUogNDQUVatWxfz58zP4Xmno1q0bypQpg/Xr1+Pxxx9HqVKlUK1aNQwZMgQLFy60+Ni1a9eidu3aCA4ORoECBfDee+8hKSkp5fb58+ejSpUqCA0NRe7cuREbG4s7d+6k3D5lyhRUqFABISEhKF++PL799lvrvnlERERERERehJl4B929C5gkst3q9m0gPNw553rvvffwxRdfoGTJksiZM6dVjxk5ciRmzZqFiRMnokyZMli3bh1efPFF5M2bF40aNUp3/927d2P//v2YPXu22S36oqOjzT7P2bNn0bp1a3Tr1g0zZ87EwYMH0bNnT4SEhGDo0KE4f/48nnvuOYwePRrt27fHrVu3sH79emj/zXL89NNPGDx4ML755htUr14du3btQs+ePREeHo6uXbta/00iIiIiIiLyMAbxBAAYPnw4mjdvbvX9ExIS8Omnn2LlypWoV68eAKBkyZLYsGEDJk2aZDaIP3LkCACgfPnyNo3t22+/RZEiRfDNN9/AYDCgfPnyOHfuHN59910MHjwY58+fR1JSEp566ikUK1YMAFClSpWUxw8ZMgRjxozBU089BQAoUaIE/v33X0yaNIlBPBERERER+RQG8Q4KC5OMuKee21lq1apl0/2PHj2Ku3fvpgv8Hzx4gOrVq5t9jGZt/X8aBw4cQL169WAwGFKuq1+/Pm7fvo0zZ86gatWqaNasGapUqYKWLVuiRYsW6NChA3LmzIk7d+7g2LFjePnll9GzZ8+UxyclJSEqKsqu8RARERERkRfQNGDrVqBaNSAkxNOjcRsG8Q4yGJxX0u5J4Wm+CD8/v3RBd2JiYsrl2//NXPz+++8oVKhQqvsFBwebfY6yZcsCAA4ePGgx0LeHv78/VqxYgU2bNmH58uUYP348PvjgA2zduhVh/810TJ48GXXq1En3OCIiIiIi8lHffQe88QbQuzcwfrynR+M2bGxHZuXNmzdVV3sAqfZtr1ixIoKDg3Hq1CmULl061UeRIkXMnrNatWqoWLEixowZA6PRmO72GzdumH1chQoVsHnz5lSTChs3bkRERAQKFy4MQLbRq1+/PoYNG4Zdu3YhKCgIv/76K/Lly4eCBQvi+PHj6cZZokQJG78rRERERETkFTQNGDdOLs+aBSQkeHY8bsRMPJnVtGlTfP7555g5cybq1auHWbNmYd++fSkZ9IiICAwYMABvvfUWjEYjGjRogPj4eGzcuBGRkZFm15obDAb88MMPiI2NxaOPPooPPvgA5cuXx+3bt7F48WIsX77c7BZzr7/+OsaOHYs333wTvXv3xqFDhzBkyBD0798ffn5+2Lp1K1atWoUWLVogJiYGW7duxeXLl1GhQgUAwLBhw9CnTx9ERUWhVatWSEhIwPbt23H9+nX079/ftd9IIiIiIiJyvo0bAbVt9o0bwPLlwJNPenRI7sIgnsxq2bIlPvroIwwcOBD379/HSy+9hC5duuCff/5Juc/HH3+MvHnzYuTIkTh+/Diio6NRo0YNDBo0yOJ5a9euje3bt+OTTz5Bz549ceXKFRQoUACPPPIIxo4da/YxhQoVwtKlS/HOO++gatWqyJUrF15++WV8+OGHAIDIyEisW7cOY8eOxc2bN1GsWDGMGTMGjz32GACgR48eCAsLw+eff4533nkH4eHhqFKlCvr16+e07xcREREREbnR5MlyDAwEEhOBOXOyTRBv0OztNpaF3bx5E1FRUYiPj0dkZGSq2+7fv48TJ06gRIkSCMlGzRPIc/g7R0RERERk4sYNoGBB4N494KuvgLfekkZlly45t/u3m2UUh5rimngiIiIiIiLyHT/9JAF85cpA375AsWLAnTvA0qWeHplbMIgnIiIiIiIi36Bpeil9z56yXVinTvL5nDmeG5cbMYgnIiIiIiIi37BjB7BnDxAcDLz4olyngvjffwdu3vTc2NyEQTwRERERERH5BpWFf/ppIFcuuVy1KlCuHHD/PrBokefG5iYM4omIiIiIiMj73b4NzJ4tl3v21K83GIBnn5XL2aCknkE8EREREREReb958ySQL10aaNQo9W0qiF+2DLh2zf1jcyMG8UREREREROT9pkyRY48ekn03VbEi8NBDQFISsGCB+8fmRgziiYiIiIiIyLtduQJs3iyXu3Y1f59s0qWeQTwRERERERF5tw0b5FipEpA/v/n7qJL6v/6SoD+LYhBPTtetWze0a9cu5fPGjRujX79+bh/HmjVrYDAYcOPGDa84DxERERER2Wn9ejk2aGD5PiVLAsWKAUYjcOiQe8blAQzis4lu3brBYDDAYDAgKCgIpUuXxvDhw5GUlOTy516wYAE+/vhjq+7riYB5165deOaZZ5AvXz6EhISgTJky6NmzJw4fPuy2MRARERERUQZUJv7RRzO+X7Ficjx50rXj8SCPB/ETJkxA8eLFERISgjp16uDvv/+2eN/p06enBKLqIyQkJNV9NE3D4MGDUaBAAYSGhiI2NhZHjhxx9ZfhE1q1aoXz58/jyJEjePvttzF06FB8/vnnZu/74MEDpz1vrly5EBER4bTzOdOSJUtQt25dJCQk4KeffsKBAwcwa9YsREVF4aOPPvL08IiIiIiI6M4dYOdOuWxtEH/qlGvH5EEeDeLnzp2L/v37Y8iQIdi5cyeqVq2Kli1b4tKlSxYfExkZifPnz6d8nEwzwzJ69GiMGzcOEydOxNatWxEeHo6WLVvi/v37rvkiNE1+qTzxoWk2DTU4OBj58+dHsWLF8NprryE2NhaLFi0CoJfAf/LJJyhYsCDKlSsHADh9+jQ6duyI6Oho5MqVC23btkVcXFzKOZOTk9G/f39ER0cjd+7cGDhwILQ040pbTp+QkIB3330XRYoUQXBwMEqXLo2pU6ciLi4OTZo0AQDkzJkTBoMB3bp1AwAYjUaMHDkSJUqUQGhoKKpWrYr58+enep6lS5eibNmyCA0NRZMmTVKN05y7d++ie/fuaN26NRYtWoTY2FiUKFECderUwRdffIFJkyZZfOwvv/yCSpUqITg4GMWLF8eYMWNS3f7tt9+iTJkyCAkJQb58+dChQ4eU26z5WoiIiIiI6D9btkjX+SJFgKJFM75vNsjEB3jyyb/88kv07NkT3bt3BwBMnDgRv//+O6ZNm4b33nvP7GMMBgPyW2hkoGkaxo4diw8//BBt27YFAMycORP58uXDb7/9hk6qW6Ez3b0L5Mjh/PNa4/ZtIDzc7oeHhobi6tWrKZ+vWrUKkZGRWLFiBQAgMTERLVu2RL169bB+/XoEBARgxIgRaNWqFfbu3YugoCCMGTMG06dPx7Rp01ChQgWMGTMGv/76K5o2bWrxebt06YLNmzdj3LhxqFq1Kk6cOIErV66gSJEi+OWXX/D000/j0KFDiIyMRGhoKABg5MiRmDVrFiZOnIgyZcpg3bp1ePHFF5E3b140atQIp0+fxlNPPYU33ngDvXr1wvbt2/H2229n+PUvW7YMV65cwcCBA83eHh0dbfb6HTt2oGPHjhg6dCieffZZbNq0Ca+//jpy586Nbt26Yfv27ejTpw9+/PFHPPLII7h27RrWqzU8VnwtRERERERkwtpSekAP8rNwJt5jQfyDBw+wY8cOvP/++ynX+fn5ITY2FpvV1gFm3L59G8WKFYPRaESNGjXw6aefolKlSgCAEydO4MKFC4iNjU25f1RUFOrUqYPNmzdbDOITEhKQkJCQ8vnNmzcd/fK8mqZpWLVqFZYtW4Y333wz5frw8HBMmTIFQUFBAIBZs2bBaDRiypQpMPy3D+MPP/yA6OhorFmzBi1atMDYsWPx/vvv46mnngIgEzHLli2z+NyHDx/GvHnzsGLFipSfU8mSJVNuz5UrFwAgJiYmJYhOSEjAp59+ipUrV6JevXopj9mwYQMmTZqERo0a4bvvvkOpUqVSMuLlypXDP//8g1GjRlkci1pmUb58eeu/eZDJp2bNmqWU25ctWxb//vsvPv/8c3Tr1g2nTp1CeHg4nnjiCURERKBYsWKoXr261V8LERERERGZUAkxa4J4ZuJd58qVK0hOTka+fPlSXZ8vXz4cPHjQ7GPKlSuHadOm4aGHHkJ8fDy++OILPPLII9i/fz8KFy6MCxcupJwj7TnVbeaMHDkSw4YNs+8LCQuTjLgnhIXZdPclS5YgR44cSExMhNFoxPPPP4+hQ4em3F6lSpWUAB4A9uzZg6NHj6Zbz37//n0cO3YM8fHxOH/+POrUqZNyW0BAAGrVqpWupF7ZvXs3/P39bQpWjx49irt376J58+aprn/w4EFKcHzgwIFU4wCQEiRbYmmMmTlw4EBKpYdSv359jB07FsnJyWjevDmKFSuGkiVLolWrVmjVqhXat2+PsLAwq74WIiIiIiL6T2Kivj98Rp3pFZWJP3lSlh//l4zMSjxaTm+revXqpQrMHnnkEVSoUAGTJk2yuvu5Oe+//z769++f8vnNmzdRpEgR6x5sMDhU0u5OTZo0wXfffYegoCAULFgQAQGpf/zhab6O27dvo2bNmvjpp5/SnStv3rx2jUGVx9vi9n+TJL///jsKFSqU6rbg4GC7xgFIBh0ADh48mGnAb4uIiAjs3LkTa9aswfLlyzF48GAMHToU27Ztc9nXQkRERESUJe3eLUuYc+YEKlbM/P4qiL99G7hxQx6XxXissV2ePHng7++Pixcvprr+4sWLFte8pxUYGIjq1avj6NGjAJDyOFvPGRwcjMjIyFQfWVF4eDhKly6NokWLpgvgzalRowaOHDmCmJgYlC5dOtVHVFQUoqKiUKBAAWzdujXlMUlJSdixY4fFc1apUgVGoxFr1641e7uqBEhOTk65rmLFiggODsapU6fSjUNNtlSoUCHdzgZbtmzJ8Otr0aIF8uTJg9GjR5u93dI2dxUqVMDGjRtTXbdx40aULVsW/v7+AKQiITY2FqNHj8bevXsRFxeH1atXW/W1EBERERHRf0z3h/ezInwNCwPy5JHLWbSk3mNBfFBQEGrWrIlVq1alXGc0GrFq1Sqrs6LJycn4559/UKBAAQBAiRIlkD9//lTnvHnzJrZu3erUTGt28cILLyBPnjxo27Yt1q9fjxMnTmDNmjXo06cPzpw5AwDo27cvPvvsM/z22284ePAgXn/99Qz3eC9evDi6du2Kl156Cb/99lvKOefNmwcAKFasGAwGA5YsWYLLly/j9u3biIiIwIABA/DWW29hxowZOHbsGHbu3Inx48djxowZAIBXX30VR44cwTvvvINDhw5h9uzZmD59eoZfn+oB8Pvvv6NNmzZYuXIl4uLisH37dgwcOBCvvvqq2ce9/fbbWLVqFT7++GMcPnwYM2bMwDfffIMBAwYAkGUL48aNw+7du3Hy5EnMnDkTRqMR5cqVs+prISIiIiKi/5gG8dbK6tvMaR40Z84cLTg4WJs+fbr277//ar169dKio6O1CxcuaJqmaZ07d9bee++9lPsPGzZMW7ZsmXbs2DFtx44dWqdOnbSQkBBt//79Kff57LPPtOjoaG3hwoXa3r17tbZt22olSpTQ7t27Z/W44uPjNQBafHx8utvu3bun/fvvvzadzxt07dpVa9u2rc23nz9/XuvSpYuWJ08eLTg4WCtZsqTWs2fPlO9NYmKi1rdvXy0yMlKLjo7W+vfvr3Xp0iXVuRo1aqT17ds35fN79+5pb731llagQAEtKChIK126tDZt2rSU24cPH67lz59fMxgMWteuXTVN0zSj0aiNHTtWK1eunBYYGKjlzZtXa9mypbZ27dqUxy1evFgrXbq0FhwcrD366KPatGnTNADa9evXM/zebNu2TXvqqae0vHnzasHBwVrp0qW1Xr16aUeOHNE0TdP++uuvdOeZP3++VrFiRS0wMFArWrSo9vnnn6fctn79eq1Ro0Zazpw5tdDQUO2hhx7S5s6dm3K7NV+LKV/9nSMiIiIicojRqGl58mgaoGmbNln/uPbt5THjxrlubC6QURxqyqBpdnb3cpJvvvkGn3/+OS5cuIBq1aph3LhxKQ3KGjdujOLFi6dkVN966y0sWLAAFy5cQM6cOVGzZk2MGDEiVUMwTdMwZMgQfP/997hx4wYaNGiAb7/9NmX9szVu3ryJqKgoxMfHpyutv3//Pk6cOIESJUogJCTE8W8AUSb4O0dERERE2dLBg0CFCkBoqKxvN2nCnaG33gLGjgUGDAA+/9yVI3SqjOJQUx4P4r0Rg3jyJvydIyIiIqJsafJkoFcvoHFj4K+/rH/cV18B/fsDzzwD/Lds1xdYG8R7bE08ERERERERkUX2rIcHsvyaeAbxRERERERE5H02bJDjo4/a9jjTveKzIAbxRERERERE5F3OngVOnJBt5WzdaUxl4i9cABISnD82D2MQbye2EiB34e8aEREREWU7qpS+WjUgIsK2x+bJI83wAOD0aacOyxswiLdRYGAgAODu3bseHgllFw8ePAAA+Pv7e3gkRERERERu8scfcmzUyPbHGgx6SX0WXBcf4OkB+Bp/f39ER0fj0qVLAICwsDAYDAYPj4qyKqPRiMuXLyMsLAwBAfxzJSIiIqJsICEB+O03ufzUU/ado2hR4NChLLkunlGBHfLnzw8AKYE8kSv5+fmhaNGinCwiIiIiouxh+XLg5k2gYEHgkUfsO0cW7lDPIN4OBoMBBQoUQExMDBITEz09HMrigoKC4OfHlS9ERERElE38/LMcn3lGGtvZIwt3qGcQ7wB/f3+uUyYiIiIiInKW+/eBhQvlcseO9p8nC2fimd4jIiIiIiIi76BK6QsXBurWtf88KojPgpl4BvFERERERETkHebNk6MjpfSAXk5/+jRgNDo+Li/CIJ6IiIiIiIg87949vZT+mWccO1fhwrLVXEICkMUakjOIJyIiIiIiIs9btgy4fRsoUgSoU8excwUGSnd7IMuti2cQT0RERERERJ7nrFJ6JYuui2cQT0RERERERJ517x6waJFcdqQrvSm1Lp6ZeCIiIiIiIiIn+uMP4M4dCbxr13bOOZmJJyIiIiIiInKBn3+W4zPPSEM6Z2AmnoiIiIiIiMjJEhOBxYvlsrNK6QFm4omIiIiIiIic7t9/pZQ+MhKoVct552UmnoiIiIiIiMjJdu6UY40azulKr6hM/LVrsnVdFsEgnoiIiIiIiDzHNIh3pshIICpKLmehbDyDeCIiIiIiIvIcVwXxQJZcF88gnoiIiIiIiDwjORnYvVsuuyKIz4Lr4hnEExERERERkWccPgzcvQuEhQFlyzr//MzEExERERERETnJrl1yrFoV8Pd3/vmZiSciIiIisuD+feD994GVKz09EiLyFa5cDw8AJUtKNj462jXn94AATw+AiIiIiLKI8eOBzz4DZs4ETp927lZRRJQ1uTqI79BBPrIQvrISERERkeMSEoCxY+XyuXPAhg0eHQ4R+QBNc30QnwUxiCciIiIix82eLcG7MmeO58ZCRL7hxAkgPh4ICgIqVvT0aHwGg3giIiIicozRCHz+uVxu0UKO8+cDSUmeGxMReT+Vha9SRQJ5sgqDeCIiIiJyzO+/AwcOAJGRkpHPnRu4fBn46y9Pj4yIvJnqTM9SepswiCciIiIix4waJcfXXpMA/umn5fO5cz03JiLyfioTX7262Ztq1gSWLnXzmHwAg3giIiIist/GjfIRFAT06SPXdeokx19+AR488NzYiEicOwfMmAHcvevpkeg0DdixQy6bycRPmyaBfPfuwI0b7h2at2MQT0RERET2U2vhO3cGChaUyw0bAvnzyzvvFSs8NjQiAnD/PtCsGdCtG1C3LnD4sKdHJM6dk2U3/v7AQw+lu3nvXjleugQMHereoXk7BvFEREREZJ+DB4GFCwGDAXjnHf16f3/gmWfkMrvUE3nWiBHytwoA//wD1KoF/Pyz885/7559FTeqlL5CBSA0NNVNmiZDVb75JvXn2R2DeCIiIiKyz5gxcmzbFihXLvVtqqR+4ULJBBKR++3Zo/esmDBBqmRu3QI6dgT69bN/uUtcHDB+PNC8ORAVJZn0y5dtO0cG+8OfPSuFPP7+wJNPAsnJwJtvSnBPDOKJiIiIyB6aJgE6oK+FN1W3LlCkiAQMf/zh3rERkWzx+PLLcnzqKeD114FVq4B335Xbv/4aePhh4M8/rYuOjxwBhg+XgL1ECfm7X7kSSEwEDh2S50hIsH58GXSmV6X05cvLXEFoKLB2LQt7FAbxRERERGS7Y8ck8xYUBDzySPrb/fyAZ5+Vy3znTeR+X30ljeOio6UeHQACAoDPPpMJuOhoiZYfewxo2hTYujX14xMTZf38mDFSgl+2LDBkiNS1+/kBjRoBX3whkwBRUcCGDUCvXtanyzPIxKvS+SpVgGLFgEGD5PMBA2ReMLtjEE9EREREttu0SY61agHBwebvo4L4JUuAO3fcMy4iAo4eBQYPlstjxgAFCqS+vU0buc/bb8vf75o1Uj3TrJmU3BctCoSEyDKZAQNkMsDfH2jVCpg+XbrNrVkjj2/ZUtbY+/sDM2fKJEFmLl8GTp+Wy9WqpbtZZeKrVJHjgAFAqVLSC2/ECDu+H1kMg3giIiIist3GjXI0l4VXataUd95377KknshdNA3o2VPvSt+9u/n75c4tmfTDh+U+fn7A6tXA+vUSYBuNEsg/+ijw7bfA+fPyd9y1qzzWVPPmerZ/0CDZXjIjqpS+TBkgIiLdzSoTr5rWh4RI9T8gBQZXrljxfcjCGMQTERERke1UJj6jIN5gkK5UALB8uevHRETA5s2SJQ8NBb7/Xv4OM1K0qGzKvm+fRMr/+5+c4/x5mYBbtw547TUgb96Mz/Pqq0DfvnK5c2d9D3hz/vpLjmZK6R880Jvpq0w8ADz+OFCxolT5r1+f8VCyOgbxRERERGSbGzeA/fvlckZBPAC0aCHH5cvZWprIHf7+W44tWgAlS1r/uAoVpFldp05SWp8/f+YTAGmNGQO0bi3bzj39NHD1avr7bNsmFQCAlPWnceiQBOqRkTK/YKphQzkyiCciIiIissXWrRKQlyoF5MuX8X0bNpTmdydPSndrInItlQGvWdP9z+3vD8yeDZQuLX/zL74o+8Mp8fHSKyMpCejQAXjuuXSnMG1ql3YOoUEDOW7Y4KLx+wgG8URERERkG2vWwyvh4UD9+nJ5xQrXjYmIhCeDeEA61f/yi5Tz//kn8PHHcr2mSff6EyeA4sWByZPNZvrTNrUz9eijcty5E7h92zXD9wUM4omIiIhIuk0/eGDdfa1ZD2/KtKSeiFzn9m19QbmngnhAOtJNmiSXhw+XhnhTpwLz5sk2d3PmyBZ3ZqRtameqaFGgSBFJ7qfdES87YRBPRERElN1t2CBbUBUvLmtVM9qIOSlJf/esMuyZUUH86tWy2JWIXGP3bsl4FyqU+VIXV+vcWRriaRrwwguy3h4APv0UqFPH4sNMy+nNUdn47LwunkE8ERERUXb3v//JdlLnzwPvvCPprg8/lOx8Wvv2SbYvMlJaRVujWjUgTx553JYtTh06EZlQpfRmur57xFdfAbVrA9evS7O7li1lb3kLrl/Xt49nEG8Zg3giIiKi7G7VKjn27AmUKyfd5z/5RC4fPpz6vmo9fN260sTKGn5+so80wJJ6Ilfy9Hr4tIKDgZ9/lonBUqWAmTPl9cCCffvkWLSoLK03RzW327Il+xb2MIgnIiIiys7OnpU9nfz8gFGjgH//BRYskCz7jRvAwIGp72/renhFldSzuR2R63hbEA9IRH74sLy2xMRkeNeMmtopFSsCOXPKFva7djlxnD6EQTwRERFRdqay8DVqyDtjPz+gfXtg/nzJtC9cCKxZo9/f3iBeZeK3bQOuXXN42ESUxp073tHUzpzgYNlqMhMZNbVT/Py41RyDeCIiIqLsTAXxzZqlvr5CBdkOCpA1rEYjcO4cEBcn76IzaExlVqFCkkIzGqXBHRE515498vdVoIB8+KDMmtopKojPruviGcQTERERZVeaZjmIB4ChQ6WB3c6dwKxZeha+ShW53lbcao7IdbyxlN4GmmZdJh7Qm9tt2CCPy24YxBMRERFlV4cPy5r4oCDz28XFxACDBsnlQYOAlSvlsq2l9IppEK/eecfHS8a/TBngyBH7zktEPh/Enzwpu1sGBgJly2Z835o1gZAQ4MoVaemR3TCIJyIiIsquVBb+kUeAsDDz9+nbFyhWTIL977/X72+Phg1lwuDkSQnYV6+WlNvkycDRo8CcOfad1xWuXcueKT5fcPeudDxfulQC19OngYQE4MED4OJFaaC2YYNMOmWn9uU+HsSrpnYVKkggn5GgIH1FT3YsqWcQT0RERJRdZVRKr4SEAJ99JpdVUGtvEB8eri9mfe45ed5Tp6TpFQCsW2ffeZ1t6lQgd27vmlQgoWnA888DHTsCjz8O1Kol3c9DQuT3KH9+oFIlqbdu3lzv65DV3b0rkxeAzwbx1pbSK6Yl9dkNg3giIiKi7Cg5GfjrL7mcURAPAM8+q6e98ucHSpSw/3lVl/qdO+X46qt69/tNm7wjczp9uhy5HZ73+e472TEhKEh2VChUKHXa1mCQXRZKl5YGjNOnA7/84rHhuo1qapc/P1CwoKdHYxdrm9op2bm5HYN4IiIiouxo927g+nUgIgJ4+OGM72swAOPHA1FRQJcu8rm92rWTresKFQL+/FOCstq1gVy5JJuogntPuX0b2LJFLsfFeXQolMY//wD9+8vl0aOlfPzMGSmlv3ZNFkgnJsrlI0eAd9+V+/bqJTsrZGU+XkoP6OX01mbi69WTeZoTJ2S1T3bCIJ6IiIgoO1Kl9I0aAQEBmd//4YclOBo1yrHnLV9e1sQfPgy0bCnX+fnptbGeLqlftw5ISpLLJ054diyku3sX6NRJAvbWrYE+ffTbVPY9d26ZIFKGDpVs/bVrQPfukqnOqlQQX6OGZ8dhpwsX5CUBsD4THxkJVKsml7NbST2DeCIiIqLsSO3VnlkpvSk/J711LFQofSO9hg3l6OkgXk1uANIwTQX05Flvvy1rvvPnB374wbpqkKAg2RoxJER2RJgwwfXj9BRVweKjmfhx42SFT7168vJgLVVSv3Gja8blrRjEExEREWU3Dx7oC0ltCeJdSQXx69fLu3lPUdvoATKOM2c8NxYSCxYAEyfK5ZkzZetDa1WoAHz+uVweOFBv/matv/6S7Q8drUBxpXv3gP375bIPBvG3bgHffiuXBw607bGVK8sxuxXNMIgnIiIiym62bJHy5JgY/V2wp1WrBuTIIfvG79vnmTFcuqQvzM2bV45cF+9Zd+8CPXvK5YED9caItnjjDVm6cf8+8MILcrTG9evAiy/K9ofvvafv0uBt9u6VCaeYGNvS2F5iyhT5sy9bFmjTxrbHFiggx6ze8iAtBvFERERE2Y0qGW/a1LEmdc4UEADUry+XPVVSr5YYVK0KVK8ul7Nbis/bbNgga9oLFQI+/ti+cxgMUoKfO7c0dHzzTese16ePRIfR0fL5++8DX39t3xhcybSpnbf8PVspMRH46iu5PGCA7St2VCP+8+edOy5vxyCeiIiIKLuxZn94T/D0unhVSh8bq2+jx0y8Z6ltEGNjZY27vQoUAGbPliB3yhT5yMiCBbKe3s8P+OMPYMgQub5fP+D77+0fhyv4cGf6uXOl9US+fEDnzrY/XmXiL1707Cocd2MQT0RERJSdGI3Arl1yWXWF8hamQbymufe5NU0P4ps1A4oXl8vMxHuWCuKbNHH8XC1a6Nn83r2B7dvN3+/SJeDVV+Xyu+8CdetKEP/OO3Ldq68CP/7o+HicZdMmOdaq5dlx2EjTZKdAQIoeQkJsP0dMjMyzGI3yY8suGMQTERERZSfnzsk6Y39/oFQpT48mtYcfBoKD5d34oUPufe7jx2Xru8BA2e5OZeIZxHvOrVt6oO2MIB6Qkvg2bWSruqeflr3lTWka8NprwOXLsteZysAbDNLcrndvuU/37nozOU86cwY4eFAi2UaNPD0amyxbBvzzDxAeLt9ye/j7SxYfyF7r4hnEExEREWUnR47IsUQJCVi9SXCwZD0B95fUqyUG9epJgz2ViWc5veeonQpKlgSKFnXOOf38pMN96dLAqVPAc89J47pdu4C1ayVQX7BAejTMnCm/k4rBIGvin3hCxjVsmHPG5IgVK+RYu7a+dt9HqE0DevUCcua0/zzZcV08g3giIiKi7OTwYTmWLevZcVjiqXXxpqX0gJ6JP3tWsrbkfs4spTcVFQX8+isQFiY/9zJlgBo1gMaNJVMPSAa+WrX0j/XzA0aOlID+55/13Qw8RQXx9nTt96AdO6SPpL+/tBlwhArimYl3owkTJqB48eIICQlBnTp18Pfff1v1uDlz5sBgMKBdu3apru/WrRsMBkOqj1atWrlg5EREREQ+SGXiy5Tx7DgsUUH82rXuWxdvNOqd6WNj5Zg3rwR5miadt8j9XBXEA7K14owZ8jPOkUM6pJUrJ0s63n5btpTL6LEdO8plT2bjjcbUzRh9yP/+J8eOHR0vssiO28x5NIifO3cu+vfvjyFDhmDnzp2oWrUqWrZsiUuZdCWIi4vDgAED8Oijj5q9vVWrVjh//nzKx//UbwkRERFRduftmfh69aSU+cwZWaPuDnv2AFevAhEREsQBkmllczvPuXFDb8DoiiAeADp0AO7ckbX3587J2vK//wa++EJ+BzMyeLD8jixYoI/T3fbulbX74eH6MhQfcfCgHNWcnSNYTu9mX375JXr27Inu3bujYsWKmDhxIsLCwjBt2jSLj0lOTsYLL7yAYcOGoWTJkmbvExwcjPz586d85HRkkQURERF5ty1bgA8+YMmztbw9Ex8erm+V5a6SepXNbNQodZ8Arov3nHXrJNNctqwepXmTihVlPT0ADB2a+jZNA6ZPl9L8a9dcNwb1e9u4sWPb73mAehlyxlwiM/Fu9ODBA+zYsQOxJqUffn5+iI2NxebNmy0+bvjw4YiJicHLL79s8T5r1qxBTEwMypUrh9deew1Xr17NcCwJCQm4efNmqg8iIiLyAYmJUo/56afeteWTt0pOBo4dk8veGsQD7l8Xr5rapS1JZod6z3FlKb2zDB4sa+QXLdK76F+9CrRvL93rP/tMSu+XLHHN8/voevikJNkMAnDOyxAz8W505coVJCcnI5/aE+A/+fLlw4ULF8w+ZsOGDZg6dSomT55s8bytWrXCzJkzsWrVKowaNQpr167FY489huTkZIuPGTlyJKKiolI+ihQpYt8XRURERO41b56+Xlllpciykydl4iM4GPDm9zsqiF+zxvXP9eCBPlmQNohnJt5zfCGIL1cOePFFuTxkiPRxqFoVWLhQMuMlSkhk+eSTEtTfuOG8575/3/LvrZeLi5NAPjQUKFTI8fMxE+/Fbt26hc6dO2Py5MnIkyePxft16tQJbdq0QZUqVdCuXTssWbIE27Ztw5oM/gm8//77iI+PT/k4zeYlRERE3k/T9D2KAMmmGo2eG48vUDWspUpJW2hv1bChrEk+dkwfs6scOwbcuyfr4StWTH0bM/GecfWq9CkApFTcm330kfwtLV0qEw5nz0qN+JYtso/822/L2vnp0yUrb2UT70xt3CiBfMGC6X9vvZxqy1G6tBQyOEpl4i9elGKj7MBjQXyePHng7++Pixcvprr+4sWLyJ8/f7r7Hzt2DHFxcXjyyScREBCAgIAAzJw5E4sWLUJAQACOqdKwNEqWLIk8efLg6NGjFscSHByMyMjIVB9ERETk5Vatkjf6YWGyjvrKFc9v9+TtvL2pnRIZCagGxkuXuva5THsEGAypb2Mm3jPWrpVjxYpAmqpdr1O6NNCli1zWNOCll2T/tOrVJdX8xRey333p0hLgv/KKc55XldLHxqb/vfVyzlwPDwAxMTIZYDQCmfRHzzI8FsQHBQWhZs2aWKXWIAEwGo1YtWoV6tWrl+7+5cuXxz///IPdu3enfLRp0wZNmjTB7t27LZbAnzlzBlevXkUBVWdBREREWYPKwvfooZfcsqQ+Y97e1M7U44/L0Z1BfFoqE3/hgmTryT18oZTe1KhREpz//DMwdapsWWeqfn1g82Zpmrh7N3DggOPP6aPr4QF9LtFZL0P+/vpcT3ZZF+/Rcvr+/ftj8uTJmDFjBg4cOIDXXnsNd+7cQffu3QEAXbp0wfvvvw8ACAkJQeXKlVN9REdHIyIiApUrV0ZQUBBu376Nd955B1u2bEFcXBxWrVqFtm3bonTp0mjZsqUnv1QiIiJypj17gOXLJf3Sr5++JpRBfMZ8JRMPAK1by3HNGtkGzFUyCuJz5pQye8B9292RHsR7eym9kjcvMHGibFlnSZ48gIpHHN3++soVfVs7H1sPDzg/Ew9kv3XxHg3in332WXzxxRcYPHgwqlWrht27d+PPP/9MaXZ36tQpnLdhOsXf3x979+5FmzZtULZsWbz88suoWbMm1q9fj+DgYFd9GURERORuY8bI8ZlnJFuq3siuW5d1tppbs0a6PlWpIh34Bw8GZs92LJj0pUx8+fJSzv7ggd493hUy+p4YDFwX726XLslacsB3gnhrqS3pZs+W0nt7rV4tj69cGTCzDNnbOTsTD2S/DvUBnh5A79690bt3b7O3ZdSMDgCmT5+e6vPQ0FAsW7bMSSMjIiIir3T6tJ7JGjBAjhUrypvZCxekbDUrvPkfMULSSufOAfv26deHh0szNlvXCj94oK/t9oVMvMEgJfUTJkhJfZs2rnmezCY2iheXXgsM4t1Dvf+vUkWy11lJmzbSw+PYMdmS7uGH7TuPD5fS378PnDoll5mJt5/PdKcnIiIiAgCMGyf7EzVuDNSqJdcZDFmrpP7ECck+GwyStRszBujZU96p3rkD/Pij7ec8flw6P+XI4TvZO1VSv3SpY5lLS+7d07cotBTEq0w8m9u5h9o2zVfWw9siRw59Mmr2bPvOoWk+HcQfOyZfQmSkrEJwluyWiWcQT0RERL4jPh6YNEkuqyy8kpWC+B9+kGNsrJTg9u8PfP89MHSoXD91qu1BbUZd2K0UHy/zJ27TuDEQEiKBtmk1grOo3Y2io4Hcuc3fR3WoZybePdQOE7Vre3YcrvL883KcO9e+/dCOHJElNUFBshWjjzFdD+/MpvoqiGcmnoiIyFscPAhMm6YvpKPsa+VK4NYtCUQfeyz1bc2ayXHbNuDGDbcPzWmSk/Ug/uWXU9/WqZOU4x48KMsGbOHgQtR//pE497XX7Hq4fcLCgKZN5bIrutRbM7HBTLz7aJo+WVO5smfH4iotW0rDxPPn9a30bKFeGxo1kqU1PsYV6+EBvZyemXgiIiJ3+P13oG5dKQscNAhYsECybkeOAJ98AlStClSoIMHMQw8BX34pJcGUPamOzA0bSmd6U4ULSzM0o1FfV+uLVqwAzpwBcuUC2rVLfVtkpDS5AyQbbwsHm9qtXCnzC9OnAxcv2nUK+5iW1DubNd8TNrZzn/PngevXZc+wcuU8PRrXCArSu9jb2qX+3j1g8mS57NbZNOdxRWd6gJl4IiIi99A0YPRo4Mknga1bJUIYORJ4+mmgaFH5D//hh1JaGRAgwVlCAvD225KZY1Yse9q9W47Vq5u/PSuU1Kvg/MUXAXO766js/Ny5UpVgLQe3lzt0SI5JSfYtybebCuI3bpQAz5msCeJVOf3Vq7Z9v8l2Kgtfpowso8iqVJf6+fNt201jzhz5PSxaVP53+iBXZ+IvXLBvlYKvYRBPRETud/8+0LUr8O67Esz37CnrnHv0kMy7v798tGwpAc3Fi8C//8o+vOHhUoJYpYpkJLLDf2vSqUx8tWrmb/f1IP7yZWDhQrmctpReqV9fspR37gDz5ll/bgcz8SqIB+xbkm+3EiWkGic5WW/o5SzWfE8iI6UqAuDkoatl9VJ6pWFDiTpv3ACs3VlL04Dx4+XyG2/I5LYPclUmPiZGirOMRnkZzeoYxBMRkXudPy/Nqn78UQL1b76RAL5XLwnKd++WbNeNG8CffwIvvSRvoA0G4JVXgD17gAYNgNu35THlywNTpmSdvcHJskuXpFbSYJClFeY0aiTv5A4d0ruO+5JZs4DEROm6b+lrNBjk7wKwvqT+7l0p0QcczsQDsiR/yxa7TmMfV5XUq4iidOmM76ey8QziXSu7BPH+/tLfArC+S/2mTTKJGRJieYLPy926pa9Zd3YmPiBAAnkge5TUM4gnIiL3uHABeO89Cbq3bpXGPsuWSUYhbUOp0FDZisecUqVkvfOYMRLcHz0qmfxSpYCxY2XNIGVNqpS+dGkgIsL8faKj9b2XV61yx6icR9P0oDyzN+ldukggsHkzcOBA5uc+elSOOXNa7sKegZs39TffbdvK0dYl+Q5RQfwffzivJ8adO/q7/cwiCq6Ld4/sEsQDekn9okUyKZ2ZcePk+MILdv0NewP1MpQ3r7xUO1t22maOQTwREbnW0aPAq69KJmvUKIkGqlUD/v5b7yZuK39/2XLr5ElpdFewIHD2LPDWW8ATT7DEPqvKbD284qsl9X//DezfL5k29Qbfkvz55XcdsC6adrCGVa1jjYmRthSALMm3JvZwigYNZOLm0iVgxw7nnFNFFLly6eXyljAT73pGo/z+A9kjiK9VSyYk790DlizJ+L5nzwK//CKX33zT9WNzEVeth1fUunhm4omIiByxYYOsZZ00ScrdH3kEWLxY3oRnVr5qjRw5JHA/flyeIzwcWL1aGuZReteuubmtuJNlth5eadFCjgsXAleuuHRITqWC8Q4dgKiozO+vsvUzZwIPHmR8XwffPatS+nLlJJ4uU0YC+J9/tut0tgsK0if9/vrLOee0pUcAM/GuFxcnyz6Cg6WyKqszGPQu9QsWZHzfiRNlcvrRR6VvjI9y1Xp4hZl4IiIiZ/jhB2llXacOsG6ddJd+4on0W4M5KjhY1sd/8418/tFHUrJPuu3bJRApVUr6CvgiazPxDRpIoH/7NvD5564elXPcvy+dpwHr17s+9pikni5fzjyT5+C7Z9Mg3p4l+U5Rt64ct21zzvlUJt6aIF5l4hnEu44qpa9QwWebttns6afluHSp5aVgCQnA99/L5T593DMuF2Em3nkYxBMRkets3izHDz6QDIKrde0KPPusZCyef15K90neHLdsKd+PO3eAp55y/lZdrnbnjh5JZpaJ9/MDPv5YLo8fL/0YvN2KFdL1qXBh6VxtjYAAoFs3uaz2jrbEiZl4QJbk+/nJvJxpwzuXqlVLjtu3O+d89mTiWU7vOtlpPbxSs6ZsF3fnjuUu9fPmyTKSwoWBdu3cOjxnYybeeRjEExGRa1y/rjfcUhk0VzMYpOywWDEpsX/jDfc8rzc7cgRo3lxK6WvXlozi8eNA587OaxDmDv/8I43f8ueXj8w8/rhUgNy7B3z2WfrbjUbJfI8aBcyYASxfLs8RH+/8sVvj11/l2K6dbZUqKmu/bJn0iLDESdvLqSC+YEG919y0aXad0nY1a8oxLs45e0jZ8j0pVkyO8fGcHHSV7BjEGwwyqQroa95NaRrw9ddy+bXXfL5CgZl452EQT0RErqH2nypdWlrRukt0NPDTTxIIzZolH9nVyZOyjvjCBdmu7M8/Ze1lSAjw++/AiBGeHqH1VCl9Zll4xWDQs/ETJ+rbqwHyxrhfP2ke9957ks1u2VK+R7lzAwMGuHeXg6Qk6VANAO3b2/bYUqXkZ2za2T6t+HjJ5AF2vXs2GvU33yqIB/SS+hkzZFc8l4uO1lN4zmhuZ0sQHx4OhIXJZV/qs+BLsmMQD+gl9YsXp+9tsWqV/K6HhsqSMSe6dAmoXx/o2FF/eUjr4EGZD+3fX15iHHH1qswlA85piWMOM/FERESOUqX09eq5/7nr1weGDJHLr7+uZzmzk4sXpUv76dMSeS1fLtuLVa8uQS0ADB0qW3b5AtXULrP18KZiY2UZR0IC8Mkncp2mSeA+frzeWKp5cwkccueWpRhjxkjzqA0bnP91mLNxo7zDzZXL+lJ6U+rN/dSpMiGQlgpW8+UDIiNtPv2ZMzKnERCgV5UD0t4iJkZ+1ZzVay5TqqTe0XXxt27pyyysndjIk0eODOKdLzFRIkYg+wXxjzwi1UXx8dKY1dTIkXLs2VP//XMCTZN/jZs2SXPKatVk51bT26dOleKXpUuBr76S3SgcoV6GCheWOTFXUJn4Cxey/iY1DOKJiMg1VBD/yCOeef5Bg4AmTeTN+lNPAZ06OacE11d89ZU07ipeXLZay5dPv61rV9n2T9Nkz2FfaNZlayYekCBdVRtMnSpl2B9/rO9eMHGivINVpfRXrkg2rGBBecfZsKFk7O/ccd7XYY6aZHrySfvKZdu1k2qXc+fMT8r8848cHWxqV6oUEBioXx8YKL31AGDtWrtObbuHH5ajo+viTTestmYnAIBBvCsdOSKBfI4cskY8O/Hz0ytwTEvq//5bgvqAAH1fRysZjTInYmkLyLlz5akCAuRl4fx5KegZOlSy5c89B/ToIZsFFCkij+nTR+Ya7eXgih6r5MsnL/vJyVn/z5RBPBERpXf/vjTaMpfVs0Zyst4d3hOZeEDenSxdKsG8v7+8a6lYUdZBO1oX6As2bZLjkCGS+khr7FhZM379up7t8VZJScDevXLZlkw8IIF4bKwECC1b6hUaX31lvjz1iSdkr+qXXtLXo8bGuu53RtP0IN7WUnolKEgmZgC9i7Vy86b+NTdqZNfp066HN6UKB9ats+vUtnNWJt6eiIJBvOuYltIbDJ4diyeokvrfftP/744aJccXXsh0YsNolBVso0fLXGCePNLkv0qV9I0nL1zQ28V88AGwc6e83BmNwLBhMoc5d678C/3sM1lKU7myzIHbOJeQilqS46qmdoCMWc1XZ/V18QziiYgovTFjZK/tfv3se/y//0oGPEcOz5ZGhoRIGfXWrbLe+coVSTG0bZu1u0wnJelrhuvUMX+f4GDg/fflsqqa8FaHD8vEUo4c9u0frdbGq3eRn3yS8e92dLRk7pctk+fcskUmtez14IEsHF+8OP1tu3YBp07Jmtfmze1/jp495bh0qSyhUN55Rz4vWVKWEdjBmiD+77/d1EagenXJXJ4/79i7dAbx3iW7rodXGjaU5TRXrsgynoMH9cm9gQMzffgbb8h8+bvvym6TavORuDhZXaZe4jVNirCuXZOipkGDpLR96lRpH5Mjh6w+KlFChvHuu/JvdMoUmVuZMSP9S+HWrfJv5plnMu6V6o5MPKCX1Gf1dfEM4omIKD2VRf/uO70U1xYqC1y7tmTBPa1mTcncDR0qNcCLF0tWfuTI9I2EAN/P1P/7r9RBRkSYj7yU2rXluH+/TLp4K7UevmpV2zq3K3Xr6lnuQYPkwxotWujd38eOtf15jUbgf/+TlFi3bkCbNqkXngKSeQOAVq30xmn2KFsWaNxYnlO1i1+5Us/MT51q90LUjIL4UqUkc/fggf6y4VLh4fK3CziWjVcRhS0dtnLnliODeOfL7kF8YKBMLgNS5z56tPwfattW/33PwJ9/yrFFC5mD//tvmeOqXVtK4Js2BRYuBGbPlmNgIDB9uhTxKC+8IKuWvvtOXnJN53/r1AHefFMuv/KKrDBKTJR/qfXry/PNny+Fbpa4IxMP6M3tmIknIqLsR/23NRqBt96yPaj19Hp4c4KCpKx4924Jdu7dk2CualXgxx+Bzz+Xza+rVZNgSgVvvujvv+X48MMZB70FCsiCR01zTrdvV7FnPXxas2ZJSb5qcGetN9+UFNQff+iNt6yxcqV8/59/Xrb0U4vJu3VLvUWZo6X0pkwb3MXHy6JWQNJ0jRvbfdqMgniDwQMl9c5YF89MvHfJ7kE8oJfUz5mj76piRfXMrVt6Ydns2dJJ/uGH5eV99WrpLn//vrSGefVVud/gwfKvL61SpeQ+5tpEjBghVf0nTkhTvAYNpPw+OVl/bfjwQ8vz4szEOxeDeCIiSi0xETh2TC77+ckWN+bKgDPiyc70malYUd7Z/PijtNY+eFCC94ED5bo9e+Qdz7Rp0vDMF6kgXmXaM6LSLW5Jo9rJns70aYWFyQJRW5UqJRl0ABg3zrrHfPGFlMbv3CnVECNGSMl8iRKy7V///nK/o0cleAkIkLX4jmrfXkpyT5+WLlUnT0pjw88+s/uUd+/K0AHLRR0qiHd7cztnZOLtCeId6e5F6d27pzcazM5BfGys7B5x5Yr8H27USKqIMvHvv3IsUEAvFlHCw6XYp2dPmZO/fVsK09591/bhRUToG5vMnCn/ZqKjZeJgxw5psH/iBDBpUvrHjh8vzx0eLit7XImZeCIiyp7i4mRNdWio7JcNSDcbc9Pr5ly9qmfyrXgD4hEGA/Dii5Ji7NNHgsNnnpG10+odDwD07eumDbCdTAXkltbDm1KBvgr8vY2mOScT74i+feU4Y4a+0bElp09LmguQlNaxY9I9Kn9+ebzBIJnyxYv1LHzjxrL9n6NCQvQGd6qyYupUWehqJxXr5sxpeYcr1S9v82brXyYcoprbbd9u39KX+Hh9pwpm4j3vwAH5OebJIxOr2VVwcOrJPNWzJBOZFTEEBEhgPXq0/K3OmpV6lwlbPPaYzHkDUqK/d6+0mQkP1/tnfvxx6tVZO3dKaw5A5hNNS/hdQWXiGcQTEVH2ompny5aV2rh8+SRLMn68dY/fskWO5cpJVtCbRUdL9/GdO4F58+TrbdtW3u3kzStZemu/bm9x547+ri4rZOLPnJGJoYAAoFIlz4yhcWNpjHj3rnR4ysh770lmsUED4Ntv5fdIefRRPQvfs6deMuuMUnpFTUABMonQtKlDpzMtpbfUNLxCBYm/7t1z06qMhx6SKOTqVfsaVKqZiXz5JL1oLQbxrpHdO9Obev55OdasKQvcraDa1mRUxGAwSCC9Zg1QvrxjQ/zhB3nOFSv07ecAWYFWpozMj335pVx365bs7vrggfxrVV3xXUll4llOT0RE2Ytp95mICODTT+Xz4cOt22ddNbXzxlJ6a0VH69uuDR0qe/L4ip07pW6yUCH93UxGatSQZRNnz8qHt1FZ+AoVJNPsCQaD3s3+m28sb724ebPUlhoM0gjPXEAyYoQs6bh4Ud82TzW0coYKFWSsrVrJZJSDMloPrxgMMj8BuKmkPjhYAnnAvpJ6exfnMoh3Da6H1z3+uETHv/9u9YSGu799fn7yXGnbrQQGyssbICuKLl2SoP3IEQn2p01zzxwNM/FERJQ9pX3X3q2blJvfvKmXCWfEG5va2aN7dynbvXXL+m7m3sCW9fBA6m0AvbGkXq2H91QpvfLcc5JVP31aL4M3ZTTqZffdu0smzZyQEFlQGhAgn9euLRMuzvTVV9KIz5YsswXWBPGAjzW3czSIv3o14720fM0ffwC9e7tpj0AzGMSnFhurb3ZuBW/69nXoIP82b98GWraUNjN+fjK36a7CPDV3feFC1vozTYtBPBERpZZ2Hxg/Pyk5B2S7qhMnLD82KUkPBH05Ew/I161K6X/4wTsDXHNsDeJN7+uNX6PKxDvS1M4ZQkL01s7mtpubNUuywhERmXfAr1lTr3BRHeW9lLVBvFoXv2GDdKt2ObUu3p5MvKqAsLWuWHUNS06WdfVZQWKiTDpNmCB7jnmCN0WhPubyZSnqATy32siUn5/eR1O9dA8bJquL3CVfPsn4JydbVzzoqxjEExFRaubetT/6qGQHjEZ5s2fJvn2yJjsy0qq9bb1e3bp6o7DevX1jWt+eIN5b18UnJempXRW0edJrr0nN6KZNEqirtu23b+tbQakmdpl55x0JBF96yXXjzYCmWV4VYHofa4P4hx6SP/tbt/Q37y6lMvE7dtj2d6lpwPr1crl+fdueMzhYbxKYVUrqly7Vo8CFC9373BcvShe006flc2+IQn3M/v1yLFlSmst5g2bN9OX8TZpY3Z/PaQIC9A74q1e797ndiUE8ERHpbt3Su8GoTLyiSoWnTJGgxRxVSl+nTsb7k/uSzz6T7Oq2bXpFgre6dEkafRkMlsu5zVEB//btbkqjWmnDBildzp3bOyo7ChTQJ3U+/BAoVkzG9dxz8ndTsqS+dt4akZEea+TVpo2sUz1zxvJ9LlyQlwQ/P6B06YzP5++vZ9vcUlJfsaLsoHHrll49ZI3Dh+XvJDjYvomhrLYufto0/fLq1bJsytX+/hvo3Fk2HVdLtOrXl14kZBNvLWKYMQMYNQqYP19eG9xN9Qf84Qf3P7e7ZJF3WERE5BTqzXBMTPo3VK1byzv5+HhZ02uOamrn6+vhTeXPL+9GANlc15G9qV1Nja18eSAqyvrHVaokaZxbt6QjvyucOCEB8LJl1j9GrT1/8kl9DbmnTZggyywefVQC8C1bgCVL5LYxYyQ49HJHj8qQL1yQ5fOWqCx88eLWfVmqpN4tQXxAgL7EwpZ18Wpwdeva97Ny1V7xt27Zt12eIy5ckAZqgHxdiYmyPt6VJkyQSd5Zs6Rled26wE8/Ze2UqQtZ05neE/LnBwYO9NwGNd26yXHlSr1gKqthEE9ERLq06+FN+fnJnuqAZKTTlrAajcDGjXLZG7KmzvTqq9KxJzER6NgRuHHD+c+haZIFO3XKvm2zANv2hzfl769n7l2xLn7bNnmzPnOmfC+tKX/WNOC33+Ryu3bOH5O9goJkacW6dZLGHj9elpr07evcLvMu9PPP+uXvvweuXzd/P2tL6RXV3G79ejetPFEl9bZMrKkgXg3WVq7IxK9cKVUZAwY475zW+PFHqbypW1df1uHqknq1HKt9e/m5bd4saVNXbx6eRXlrJt7TSpaUSUVNk1/zrIhBPBER6TJ7196tm7zZPHw4fUb1448l2xoaKm8KsxKDQZYRlCghAfbLLzsna3blimwplCePrLWOipIS7RIl7Nuf3p718Ip6jLPXxS9aJO+mLl2Sz+PirMu67d4tExphYVbvl+x2BQtKQL9iheUt5bzQvHlyDAiQlTHffWf+frYG8TVqyI/r6lXg338dH2emVDm8LZl4tR7em4L42bPl+OWXsoTEHTRNL6V/6SV9ouz33yVD7gpnzwIHDuivp97Q58KHaZoexFep4tmxeKPu3eU4fbr7i1zcgUE8ERHpVCbe0rv2iAg9Y2O6PnzpUmlBC0hEYEspt6+IipLoJzAQWLAgdYO/pCRpsLV4sTT2s8a1a0Dz5vK9u3pVX4seGCjHd96RN7zW0jTHgniVvXdmJv6bbyTjdu+e7FvepYtcP2VK5o9VpfQtW8rEEDnFkSMyP+LvL9X/gPwp37+f/r62BvFBQXoRjltK6tVk4bZt1lXHnDwpHwEB9lcLuSKI/+sv/XKvXkBCgvPObcmWLbJ0JjQUePZZ+fvPl0+qgdaudc1zrlwpx1q1PFdnnYWcOSM/roAA88Vz2d3TT8sqsaNH3Tc35k4M4omISKfetWf0juDNNyWTsmyZvAk8fhx44QUJIl97TW/8lRXVqgV8/rlcfvtt4KOPpFdArlxyW5s2sud3v37699Kc+HgJTnfvlv4DqjT7zh15A9+ypRy7ds28hbhy7JjURQcH25eWUYH/3r3A3bu2Pz6tTz6R3xWjEejRQzLyqunbr79mHgSpUvr27R0fC6VQpfSxsfLnWrSoFEmkbXOxfTuwZo1ctmWjCbUuXj3WpUqXln4OiYn670tGVBa+Rg37W3k7O4iPi5OPgAB5LThwQO/B4UoqC//MM1Jd5ecnr1+Add9Le6xYIcfmzV1z/mxGZeHLleNqBHNy5JDVb0DWbHDHIJ6IiISmZZ6JB2Sx2ZNPyuVRo4CnnpIsWN265vfPzmr69JHS0wcPgBEjpBHUrVvyRrhoUQnQv/5amss1by4NnFQpOSD3fewxiZLy5AFWrZImaYUKSS2ywQBMnSqNBbdt0zfdzYzKoFevbt87uiJFpBtRcjKwa5ftjzf14IG+D/rw4bLwOjBQxlajhtw+a5blxx87Jh2b/P1luQE5jSql79hRfiT9+8vnn3+uF4PExQFPPCFzOc2b27bHs1r58OuvtjWNt1unTnKcOzfz+zq6Hh7Q94p3VhCvsvAPPwyMGyeXP/nEtiocW925A8yZI5dNtzhUPR0WLnR+/bGm6Zn42Fjnnjub8tamdt5EldTPm2d5Ux1fxSCeiIjE+fPyX87fX99k1RKVUZ0+HdizRzJIP/+cPdIBBoNksdq1k4+vvgJ27pTy+BMngD//lIyWn5+8ae3cWcpUa9aUDXMff1yaOeXMKbebewdWqJC+Jn7YMOs23naklF59Xc5aF79tm0SAuXPLvumma8V79JDjlCmWAwWVCWzcmGW3TnTokPy5BgToS6B79JBv8dGj8m2/fl2KSy5elL3f58+3bbfIOnXk8UlJspmDyz37rBxXrMg8sHZGEO/sTLwK4ps0kZmV1q1lkuuVV1zXHXD+fHmtL1Uq9feiWTOpUDh7VpYHOdO+ffJLFRbmst1LLl+WOdTsgk3tMteggRTs3Lkjv/ZZCYN4IiISKm1WokTmwXjjxvIOH5Cgf+5coHBhlw7Pq+TMKanGX3+VCY3q1eX74OcnpfALF8oygw8/BKpVk8fs3ClZ9fXrZX39ihVA1aqWn+OFF6TKISlJ1pJntk7W0SAecN66eBWYNG6cPgJ87jlZh7t/v+XJAm/sSp8FmJbSq7mR8HDgjTfk8mefyeqFAwdkHun336XAxFaffy5/Dr/95oay+jJlpLojOVl6VVhy8aLMYhgMtpUWpOXMLeY0Tf9badpUxvbtt/JDWb9eKnJcQZXSd++eeoItJER6VwDO71KvSukbNnTqNox378oOdc2by1zpQw+Z7++QFTGIz5zBoG83l9VK6hnEExGRsGY9vGIwSDf6HDmkBLRxY5cOzScVKybfo127pMph5kwJzOvWlX4Caks3SwwGaRKYN6/UTQ4ZYvm+J07IJAFg+/ZyppyViTfNLqYVHS3rcAHzDe4uXtS3KvSRLdt8hWkpvanevSV+275deppFREi/RXvn5SpWlEQyIOX6Lt9uTmXjVYm4OaqzVZUqMglnL2dm4o8elV4YQUF6drpYMVmmA0hzy82bHX8eU8eOSUWCn5/5/iVq4szZ6+KdvB5+1y6pIsmfH3jxRSlq0jTZ0MIt/Rg8LDlZ3wGCnekz1qWL/Dtdt05+/bMKBvFERCSsWQ9vqk0baY37+uuuG1NWkT+/lNXPmiVvyq0NtGNigIkT5fKoUZKlS0vVPyckyLraUqXsH6fadzsuTjLl9khIADZtksvmgnhAL6mfM0d6BJhavFjejdeqJev0ySkOHJC5oMDA9AUOMTH62lF/fyk7VYU29ho6VLL4u3a5YZ9mNSuxZo1MmJmjSukffdSx51JB/LVrehMBe6nJrrp1U+/A8OabQP36UhveuDEwY4Zjz2Nq1So5NmxofpamdWv5Jdi3T6qJnCEhQe9478B6eE2T4bdsKcUXU6fKy0eJEvL79vTTcr8lS2w7rzP6eLrbsWPybQ0Nla+fLCtSRJ87cuafkqcxiCciImFLJl7xkX2xfdpTTwHvvSeX33hDmsQpCQkSkR08KG/If/vNsZ9JVJSe/e7Z074gZcsWqWfNlw+oUMH8fRo0kMmiO3fSNyRTW8uxK71TqVL65s3NJ6IHD5Zv+dy5enM6R+TNK6tJAGDQIOt3XrRL8eISCGua5YWvzlgPD+iN7YxG67a1y4ilihV/f+mt0b69rI/v1g0YMMDxSQNAyi0Ay1vs5cqlbzHgrJL6zZtlm8l8+exOG69YIXOMsbHA8uVSSNCpk8wNHD0qhUqqsGDJEuv78n32mUw2desm32pfoZraVapkW8+K7EpNUqreilkBf+xERCRszcST+3z6qWxpB0id8tSpEkR07y7BSWSk1D8XLOj4c40fL/XUmzcDEybY/njT9fCWJhQMBj0bP3GivBNftEjSJOpdFtfDO5WlUnolf35ZUq6ymc7w5puSJTx3Tt+Z0WVUl3pzJfU3bkhHP8DxTHxgoEx2AY6V1JuuhzdXsZIjh0xIfPSRfD5mjGwZ4OgWkNu2ybFWLcv3URN5ixbZ/zymVCl9bKxdk4xnzsiXvmOHZJ5795bA/X//kzkZFcQ2aybLQk6etK6QaNw46TWanCwvPY8/LsVlvoDr4W3Trp1sJKN2mcwKGMQTEZGkIFTppC2ZeHIPg0GioL595fOePaXs9X//k1bjCxY4b2FkkSLA6NFy+f33pbTeFhkFJqa6dJGx79ghAX/btno6rGxZy1l8stn+/fIRGOjeNgMhIfqW5yNHSsBRsaLME5Yr5+QdKZ95Rv5ONm2ShdGmNm6UoLlMGaBAAcefyxnbzB08KP0fQkKkisAcPz/ZonHuXIle//xTmmGGh0vlTZMmsm7+8mXrnvPePT36yyiIVz1Odu92zlZzDq6HX7RIXhaqVZMAffx48yXkYWHSHxDIvKR++nT95fTFF+VbunKlTAqcO2fXMN2KQbxtVM9Gf39Pj8R5GMQTEZE0RktOlncyzsjmkvMZDLKdXe/e8sZ62TK5fsoUSUE5U69e8m727l3J/Fv7Rv7ePSmnBzIP4mNiZJKgYEGJ6OrUkTruZ5+VJQNcquE0ahl0s2bSV9CdOnSQX6UHD2Qi4cABKfo5fFi2Q3faduQFC+ql8qrsQHFWKb3ijOZ2arLrkUcy79besaM05mvcWF8Lcfas9AD44gugfHl5Hcisg+DevbLbRUxMxv0mypaVv78bN4BLl6z8giy4dk0v4bdzPbwqCHjuOVmmkZEnnpBjRkH8/PnAyy/L5bfekp6ja9fKt2XPHllpcOCAXUN1GxXEs6ld9sUgnoiIUq+HZ/DkvQwGqQF9801JKYwYYb7DtKP8/IDJkyV9sXy5vMu1xqZNEq0VLChZz8wMHy7ByMGDEvwvWybl0GpNLjnFkSNydLRZnT0MBgmoVq2SjzVrJH4NCJAY+MwZJz6Z6lKfts+CqqF1tJRecUYQv3q1HFXqODM1asg37to1ed4tW6QGvGpVua5nT+k1sXev5XOYltJn9DofEqKnug8etG58lvz1l8zUVKgg+xba6OZN/VvVpk3m93/8cTlu3mz+x/Pnn8Dzz8t8R48eskrBYJDNQjZvlpetU6fkV8VbM/L37+t/08zEZ18M4omIiOvhfYkK5OPjgQ8+cN3zlC0LDBsml996C7hwIfPHmJbSczLIa6htlRzZuMARERESqzZtKvMzjRtLWT0g3eud5umnZXJr+3ZZmtGqlTyRqg5xdibe3r3ijUZ9H7TMKlbMyZ1bKle6dJGv9csvZQ395s0S7FvaEFtlxDMqpVfKl5ejo0G8g6X0y5YBiYnycqSGlJGiRWWyymiUgN3UmTOy6iIxUeZ7Jk5M/TJVsqTMQz70kPxo+/Sxa8gud/CgFM7lzOmc1SHkmxjEExGRfZ3pybPCw13/HP37S1Bw/TowcGDm97d2PTy5laeDeHOqV5fjzp1OPGlMjL60ZMYMiQAPHJBMcN260sXeGRzNxO/bJ1FieLi+raO9AgJkku3AAenelZwsrdrNrVPwwSBeldJbk4VXLJXU9+sH3L4t5fIzZ5pfH50nj37bL784r0G/M6n5nxo1OFeanTGIJyIiZuLJvIAAfZ/6n37SJ3vMuX0b+Ptvucwg3mskJ0vLC8C7gvgaNeTo1Ew8AHz9tXQsGzYMmDZNupUdOiQl9c6KeBwN4tVkV4MG0m3QGQoXBmbPlgZ4p0/re5Apt2/rC73dFcTHxUnD1IAAu5bIJCYCv/8ul+0J4v/8U84ByHl++UWC80mTgKAgy4+vWlV29ANkV09v61ivvietW3t2HORZDOKJiIiZeLLs4YflHbTRKGvYLdm4UZpmFS1qvnU0ecSZMxLIBAZm3MvM3VySiQck+Bw7Vja+795dMvNly0og6SzOCuKdPdkVGqo3j1u8OPVtu3bJ33ChQtbVYKsgPqOJu8yoZQzVq8uaChtt3ChFQLlzS/8/a9WuLT+i+Hg5x9270g8UkKIFa5rBDRkik15nz7p21ZKtbt2SJnyAPllB2RODeCKi7C4+XrY6AhjEk3lDh8rxf/8D/v3X/H24Ht4rqVL6EiW8a3ulatXkeOaMY/3hPMLRIH7DBjm6omJFRXZpg3hbSukBPYiPi5NdJ+yxdasc69Sx6+GqlP2JJ2z73fX317PUS5bILghxcTKJNWSIdecIDdWLkCZM0OcjPG3FCpmUK12a/66zOwbxRETZnSqlz58fiIz07FjIO1WvDrRvL+tsLWXjuR7eK3njenhAErNqAwOnl9S7miP7xMfH6w3xKlVy3pgUFcT//bc+OQvYHsTnyQPkyiV/86oVuq3U8pratW1+qKbpQXzbtrY/9ZNPyvGnn4DPP5fL48dL/z9rxcbK5h+aJs3/HzywfN/z54EPP0y/isHZVCm96sJP2ReDeCKi7I7r4ckaKhs/b56+SbFy8yawY4dcZhDvVbw1iAf0dfFOL6l3NUcy8SdPyjF3btc0pyxYUPZL0zRg6VL9erW9nLWN9AwGx9bFJybqP1g7MvH790svh+Bg+3ritWghKyguXJChPPmkfZMBX3whP+59+4CXXgJu3Eh/nw0b5Hf5k09kDb2rGI36j5RBPDGIJyLK7rgenqzx0ENAhw4SHKit5wDZtHjECOmgVrKkrIknr+HNQbxaF+/uTPy9e7JnvdFo5wlUEH/jhvSBsIUK4osVs/PJrZC2PfuNG3o2vWZN68/jSBC/b5+8NkRHS+23jVRX+thY27LnSmSk3ksvLEyy8PbIk0fK6QHJ6lesCPz2m3yuabLbZ5Mm+g6cGzZYtxunPXbtknOHhztvt0TyXQziiYiyO2biyVpDhkiGbv58YO9eeTdbsaJer9qli0eHR+l5cxDvqUz8kCESHI4aZecJcuWSo6ZJ5zVbqCDelZNdqpZ8+XIgIUH/Bhcvrk9AWMORIF6th3/4YcDP9nBDldLb0pU+rVdekaf+/HPH5kw6dpRt3cqUkbL59u1lv/kXXpCNEJKSgE6dpM+D6TIAZ1NzMi1aSIUCZW8M4omIsjtm4slalSvLO1pA0lzt20vNa8GCwKxZ0hGcvIameXcQrzLxR464dxsvlUn99lvbE+kApE47Z065bGtJ/alTcnRlJr5GDfmbvH1bok9VSm/tenhFTezaE8Sr9fB2lNKfP68/XM1H2OOZZ6QY4PXX7T+H0qgRsGcP8N570jhv/nzp8+nvL5shzJ4NPPus3PeXXxx/PnO4Hp5MMYgnIsrONI2ZeLLN4MGSjb9xQzZbHjRIJoJeeIFd6b3M1at6cOyNu/7lyaNve7dnj3ue8/RpvbL8zBngjz/sPJG96+LdUU5vMOiR3pIlelM7a9fDK6bbzNm69sDKpnaaBrz7LtCyJfDYY7ISoH17/aHW7IaXkcBAxx5vKjQUGDlS5kTq1JEf4V9/STbeYACeekru99dfwLVrznteQHoUqrkY7g9PAODETTOJiMjnnD0rm+gGBHjnu3zyPhUrSgpzzx7gnXdkHTx5JZWFL1RIAhBvVL26BNa7dgGPPur651u1KvXnkybZme3Nk0dmA7wxiAfki5o8OfVWc7Zm4kuUkCj47l2Z8bB2CcCtW/pWlJlMHOzfD4webf62Dh1sGKsbVa8uW85pWup5y7JlpVhp3z6ZO3Hm6iI12VSjhuMTG5Q1MIgnIsrOVBa+ZEnnpiwoa3v1VU+PgKzgzaX0SvXq0sTMXeviVRDfoYOURP/xh1S427xE3d5MvCqnd3UDyGbNgJAQfdIA0JsQWCswUJrSHTggJfXWjnn7dolwixaVrUszsH+/HMuXl1L15GRJ+oeH65ltb2Wu8OjppyWI/+UX5wbxaj286llIxHJ6IqLsjOvhibIsXwjiVVzpjg71mqYH8a++Kl3FjUZgyhQ7TmbPXvEJCbLgG3B9Jj4sTAJ5pUwZ6RRvK3ua29mwHl4l7OvXlz3ZX3oJ6NEDeO4532zepiYeli2TlgTO8OCB9CgEuB6edAziiYiyM66HJy93/740wje3PzNlzBeCeNXcbv9++Vm70sGDEkMHBwOPPKIXlEydakeDO3sy8adPyzE01LYu8fYyXSdg63p4xZEgPpP18IAk+QGgQgUbx+WlqlSR4oWEBH1Pd0dt2CArFGJibF8RQVkXg3giouyMmXjyci++CFStKs3AY2KABg0kW6d2sCLLfCGIL1xY4tnkZClDdiWVha9fX+Lodu3kd+rcOb1c2WoqCL961frHmK6Hd0cTSNPaa3ujPxcH8SoTX7GijePyUqYN7hYscPx8t28DP/wglx97zK7d+iiL4q8CEVF2xkw8eTGjEVixQv/88mVg40Z5U/vKK54bl6/whSDeYNCz8a5eF6+CeFVlHhQEdO8ulydOtPFk9mTi3bUeXilUSPZG8/MDmja17xy2BvHnzkkTPD+/TNfgJyXp/4KySiYe0IP433+3r7okOVnK5zt3lpYCs2bJ9Y5st0dZD4N4IqLsKiFB9vgGmIknr3TihGyRFhwsWzbt2CHdxAHJ4CUmenZ8jrhxQ5qHZ7RMYPVq6XZtzzZod+7oy6+9OYgH3LMuPjlZtkwHUi8V79lTjsuX6y+HVrEniHdXZ3pTCxbIepSqVe17vJrgPX8eiI/P/P4qC1+5MpAjR4Z3PXZM/obDwtw3r+EODz8sFSa3b6eehLTGmTPy99qypQTvd+7I559+CrRt65rxkm9iEE9ElF0dPy6pzoiITDsIE3mCCuoqV5Zy+ho1pOlVeLi8+VeZZl80ejTQq5fsiZ2cnP72q1eB55+XteJff237+Y8fl2N0NJArl0NDdTl3ZOJ37pQJk6gooGZN/fpSpYDmzaXp3eTJNpzQV4L4XLmASpXsf3xUlL6nmVp+lRE7SukrVMhaZeJ+fvpe97aW1C9fLr8mkZHAa68BmzbJTobvvy87wRIpWehPhoiIbGK6Ht4d6zOtlJQEXLwoWxNT9rZ7txxVkAfIG2RVequCAF+0ZYsc16wBRo5Mf/vrr8vfAQCsWyeFM7bwhVJ6RWXi9+61o8GclVQpfePG6YMhtTTjhx8kmLeKI+X07gzincGWkno7g/is5umn5bhwoW0VQ0eOyLFzZ+Dbb4F69bzq3zN5EQbxRETZlQfXwycnS4Zx5kygb195Y12unCSNAgOlMCBfPmD2bLcPjbyIysSbBvGA3gRL7THtazQtden40KGy1l+ZOxeYNw/w95eM3L17etBvLV8K4kuVkoKg+/eBPXtc8xxp18ObUtt2Xbhgwy4Iaou5+HjrozSVife12nFrg3ijEdi2TS5bsb2c6kyfVZramWrQQP6HXb8uE3LWTg6pf8tc4UaZYRBPRJRdeaAzfXKyvKGJjJQS6a5dgXHjgLVr5c3L9ev6fW/fBl54AejTR/bJpexHBbrVqqW+XlUH+2omPi5OgsWgIKBjR/m7eP55+f0/f17+RgDgww/1AHPlStuew5eCeD8/faKmQQPJjKsAzxnu35dtugDzQXxIiL6Fuqp+yFTOnHqK1JoO9UajvsVcVs3EHzokTSzCwqyKzLNaZ3pT/v7SLNHPD5gyRcrhraEy8WXKuG5slDUwiCciyq48kIkfNAj47jsplQ8Plzfs/fpJRn7NGsmsXrwopcMffiiPGT9eMvVnz7ptmOQFLl6UgNZgAB56KPVt6k2/rwbxpmv9J0+WQPvUKWmy1rOnNPGrUQP44AMgNlbuqzLJ1vKlIB4ARo2Ster37wPffy8/48cfl2aGjtq8Wc5boIDl0u18+eR44YKVJ/X315sNWBPEX7ggs5H+/tI13pdYG8SrUvqaNTNdwG006qfLiuX0gGxh+P33cnnUKOmDkRGjETh6VC4ziKfMMIgnIsqu3JyJnz5dfxPzww9Shbp+PfDVV7L+r1EjeeMeEyMZyo8/BhYvlr5KmzdLUDN9umToKetT6+HLlk3f5FoF8YcOWV5Dff2669ZXO0o1cKteXapS/vc/iXl++UW2pQoOlomtwEA9c/z335LktJavBfF160ol9rp1EvwYDMDSpUCnTo6fW02ANG1qeX2x6u1pdRAP2LYuXq2HL1TI9zqUqSD+6NGMlw7YsB7+5ElZJhIUBJQs6YQxeqmXX9b/7737rmTlLTl7Vr4nAQFA8eJuGR75MLuC+Bs3bmDKlCl4//33ce3aNQDAzp07cdaONMmECRNQvHhxhISEoE6dOvhbvQBkYs6cOTAYDGjXrl2q6zVNw+DBg1GgQAGEhoYiNjYWR1RtChERievXZdNtwC1B/Pr10okbAD76COjWTRJSmXniCcnEVa0KXLokezrnzy+P/+svyVxQ1mRpPTwgb3BDQ6ViQ3VhNxUXBxQsKL/apmvNvYX62lRDt4cfTt3cbsQIfclAsWJA6dJScr92rXXnT0rSl1/7ShAPSID96KPAr7/qkzjHjtm317apjNbDKyoTb3U5PWBbEO+r6+EB2S8tLEwC+Iz24du+XY4PP5zpKVUVTblyvjenYat33pEAHpClIosXm7+fCldKlsz63xNynM1B/N69e1G2bFmMGjUKX3zxBW781wFkwYIFeN/aBR//mTt3Lvr3748hQ4Zg586dqFq1Klq2bIlLly5l+Li4uDgMGDAAjz76aLrbRo8ejXHjxmHixInYunUrwsPD0bJlS9x39D8AEVFWokrpCxbMdC9fW2zfLhnEf//VA+zjx2W7ncREoEMHaeJli1KlZJudTz+VEsM7d4AZMySrVrNm6nX0lHVYWg8PZN6h/o8/JPA7cQJo2FDWo3pTXwVzExT9+wNvvQW88YYcTang09qS+lOnJJAPDva9ym2lShWpUtA0G/dvT+PWLb3XWkZBvMsz8Z7YXs5Z/Pz0ZVeW1rAkJupdCU338LNA9TzIqqX0aY0cKVl5o9H8bhQAm9qRbWwO4vv3749u3brhyJEjCAkJSbm+devWWLdunU3n+vLLL9GzZ090794dFStWxMSJExEWFoZp06ZZfExycjJeeOEFDBs2DCXT1N9omoaxY8fiww8/RNu2bfHQQw9h5syZOHfuHH777TebxkZElKW5YD38gwfyJrlrV8ki5soFtGgBtGwpS0Zr1pTg2579gMPCJBA7dEgyq716STfr3buBDP5lkA/LKBMPZLwufv16ORYvLm+aP/tMmmXv2+f0YdrswgXza/39/IAvvwS++SZ9lYpaF29tcztVSl+ihO/uv20w6FUE6uuxx/HjUsWQJ0/GSXC3ZeJ9MYgHpBwK0NeCpHXwoJTGREZaVR+flZvamWMwAG+/LZf37JHfybTY1I5sYfNL+7Zt2/CK2lDTRKFChXDBhunLBw8eYMeOHYhV/5kA+Pn5ITY2Fps3b7b4uOHDhyMmJgYvv/xyuttOnDiBCxcupDpnVFQU6tSpk+E5ExIScPPmzVQfRERZmgvWw+/bJ2t2AwMl6I6PB1askGWUBQvKfrlhYY49h8EAPPIIMGkS8MUXct3kyTbs7Uw+4dYtvcGTrUG8pulB/NSpss48d26Z8KlVSy/T9hQ1OVG+vDR3tEaTJvK7v3+/dZliX1sPb0np0nJUvwv2OHdOjplVJDiUic+kghSA7+4Rr6h17paWvZo2erBi5ii7BfGA/LsNC5PGruZW+qq5dQbxZA2bg/jg4GCzQe7hw4eRN29eq89z5coVJCcnI5+a+vxPvnz5LE4GbNiwAVOnTsXkyZPN3q4eZ8s5AWDkyJGIiopK+ShSpIjVXwcRkU9yQSZevYdr2FAC+J07gW+/Bd58E1i+3Pllvc89J0HQoUP69lGUNezdK8F4oUKApbcWlvaKP3kSOHNG1pTWqQM89ZRMMNWvL4nCjBpLuUNmFQbm5M6tLyuwpqSeQbzO1iDepky8yjirSdGM+PKaeCB1EG9u1lT9A1CNHjKgadmvnB6QChtVfaNeB0ypwJ7l9GQNm4P4Nm3aYPjw4Uj8rzulwWDAqVOn8O677+Lpp592+gCVW7duoXPnzpg8eTLyqJlPJ3n//fcRHx+f8nFa7eNJRJRVuSATr7aCUrsLVa8OvPaa7AOvmnQ5U0SE3rnawtxutnTnjgS2S5YAP/7om938rQl0VRB/8GDq0lSVha9RQ890588v27UB0jTNkw0RTROWtrBlqzkG8ToVxBcsmPH9bN5iDpA9AoH0M0nm+Ho5fZUq0mTh+nXz6xtsCOLPnZOqLX//7Jd1Vn/3aYP4pCT925rdvidkH5uD+DFjxuD27duIiYnBvXv30KhRI5QuXRoRERH45JNPrD5Pnjx54O/vj4tppjwvXryI/Go61MSxY8cQFxeHJ598EgEBAQgICMDMmTOxaNEiBAQE4NixYymPs/acSnBwMCIjI1N9EBFlWUajPuXvgky8Fe/hnEZ1vP/5Z/MN7o4fB377Let3sdc0+V7kyyd9CitXBp58EujSBXjvPU+PznYZNbVTSpaUmOL+felGr6iqjLS9b5s2lYmfc+csVwS7Q9rO9NZSTdlWrsx4+Yim6YU2DOJl2y4g8yBevU28dMmG1ws1k3TmDPBfo2ez4uP1/QF9NRMfFKRHoGn/gIxGm36xVSl96dLyN5ydqNe0tEH8yZMSyIeEyGYARJmxOYiPiorCihUrsHjxYowbNw69e/fG0qVLsXbtWoRbu7gLQFBQEGrWrIlVJlPKRqMRq1atQr169dLdv3z58vjnn3+we/fulI82bdqgSZMm2L17N4oUKYISJUogf/78qc558+ZNbN261ew5iYiypTNnZDPawECnbUZrY2Nip3n4YSlPvH8f+Omn1LedPi0VoO3b61nYrOrwYalGUEtzo6L0+GL6dIkhfIk1mXh/f337atN18SoTnzaIDw6WLQsBYMEC54zTVjdu6FviZTRBYU6DBhJHnT5tOai9cAFo00YSwwaD7683VkF8XFzG25NnxNpMfEyMHJOSgP92T85cdLRep2+pazugZ+Hz5LG+EYI3srQu/sgRKQEKDbVqYjg7ltIr6jVt9+7Uk3FqXr10ad9tRknuZfevSYMGDfD6669j4MCBqRrJ2aJ///6YPHkyZsyYgQMHDuC1117DnTt30L17dwBAly5dUratCwkJQeXKlVN9REdHIyIiApUrV0ZQUBAMBgP69euHESNGYNGiRfjnn3/QpUsXFCxYMN1+8kRE2ZZpms5Jm9EeOGBTY2KnMRiAnj3l8vff62+KEhOBZ5+VrviAdCf//nv3jcvd1OYw9epJAHLjhqwDr1hR3lv/+KNHh2eTBw/06uTMSs7Trou/ckUPEBo0SH//p56S44IFnmmGqJrqFSsmuzfYIjxcfr6A+ZL6efNk2cqSJRLsjx0r3el9WYECEhcmJ+txsK2sDeIDA6X3AGBjSb1aK5RRSb2vr4dXLAXxqgyrWrX0WyuYkR2b2ilVqsi36MoVvUoEYFM7sp3N796GDx+e4e2DBw+2+lzPPvssLl++jMGDB+PChQuoVq0a/vzzz5TGdKdOnYKfjdNRAwcOxJ07d9CrVy/cuHEDDRo0wJ9//plqOzwiomzNhevha9RwfxbhhReAd94B/vlH3lvWqQMMGgRs3iwZ6RdekAZ7r78OFCkCPPaYe8fnDiqIj40FcuaUywaDfM29e8vX/8Ybcp23O3BAAvmoqMwLRdJ2qFel9BUr6gGZqVatpFz12DH5fTHd4s0d7C2lV2JjgbVrpaT+pZckI3/ggATw8+bJfapVA2bOlGDB1xkMkpn85x/5WlVm3hbWBvGALEe5elWa26nl7pmqXFk6d1oTxPvqenhFBfE7d8pMaWCg/jlg9S92dg7iQ0KkAmHfPnk9UKXzbGpHtrI5iP/1119TfZ6YmIgTJ04gICAApUqVsimIB4DevXujd+/eZm9bs2ZNho+dPn16uusMBgOGDx+e6WQDEVG2pVKVTny34In18ErOnMAzz0i2efJkyaKp7ed++AFo10629Jk+HejYUcqtbS1l9nYqiG/YMPX1nTvLmvgDB4A1a2SrMm9nuh4+s0kHlQRVQYGlUnolRw6gZUvZ7nDBAvcH8fY2tVOaNQM++kj6PISHS+m34u8PvP++3B4U5PBQvYZpEG+rpCS927w1QXz+/PK7ZFcmft8+y/fx9e3llNKlZQnBjRvyQ1Ev+OoX28q1VNm5nB6Qv38VxD/5pFzHTDzZyuZ8ya5du1J97Nu3D+fPn0ezZs3w1ltvuWKMRETkTCrSqVXLaac07UzvCaqkfs4coFs3udyvn6yHNxhkX/lmzaRT++OPy7rirOLkSYkRAgL0cmslMlICeQCYMMH9Y7OHLVuwqUzegQPSWyuzIB7QS+rT5CTcwp7t5Uw9/LCUmCcnS4CaI4dc162bVJ58/HHWCuABx5rbqSZ1/v76mveMqA71Nm0zZ0s5va8H8QZD+pJ6TbNpFvfyZSklNxj0nhbZjbkO9czEk62cUvQYGRmJYcOG4aOPPnLG6YiIyFUuXZJNuAGnpWWTk/W1vp7IxAOy/rl8eVn/feOGlNSPGqXfHhQE/PKLvN8+dw5o0QI4f94zY3U2lYWvVct8z6zXX5fjb79JT0Nvp36XrAl0S5WSit67dyWDqmKJjIL4J56QoG7v3vSB4fz5kqk/ccKuoWfo7l09A2nv30lAgATry5fLxM3NmxJL/fCDBPNZkSNBvCqlz5/fqqXaKR3qbcrEq5mkCxcsd8TLKmvigfRBfFycvOgGBVlVH6+qZooVA8LCXDJCr2fa3A6QfjLqV4SZeLKW01Yuqj3WiYjIi6llSlWqWJeassLBg9LsPkcOz2URTBvc5cwJzJ2bPiMZFQUsXSrr4g8elDmMrBDIr10rx7Sl9ErlynJbcrIsN/BmRqNtQXxAgN4Me9o0+RqLFMk4VsqVS5+/Ms3GL14MdOokAfKQIdaPOTlZfv8PHAD+9z/g3Xdl7X2dOqm74P/zj3x9MTGSTbdXsWJA8+bydfpCjwNHqW3yHAnirSmlB+zMxEdE6L9wlrLxWaWcHkgfxKuZs4ce0tfIZ0BNZGXH9fBK1apyjIuTrVGPH5fXhogI/XeQKDM2r4kfN25cqs81TcP58+fx448/4rGs2C2IiCgrUW2t1abTTmDamNiTW+O88QZw65aUy1t6r1y0qL42/NAhoHFj4K+/rH+T740srYc39cYbcr/vv5ft9ry15PrECckuBwdbX2pbqZKsL50xQz7PKAuvPPWUNIdbsECaIq5bJ/0SkpPl9jlzpJLDXLA9Y4Ys1bh7V3p7ZdTl/umngZEjJbA3LaXPDsG3s6hM/PHj8vOxJqOuWLtHvGJXJh6QmbJTpySIT/sLmJCgzxZmhSBelXz8+6+84NrYEEXNc2TnID5nTmnaGRcnk5Y3b8r1ZcrwtYGsZ3MQ/9VXX6X63M/PD3nz5kXXrl1TtoMjIiIv5YIg3tPr4ZXgYOsyqCVL6oH84cN6IK+2e/Yl58/LWkqDAahf3/L92rWTAOXCBSmr79jRXSO0jfr1rFLFqqQeAD0YUJXM1gTx7drJxMaWLVKd8dxzwP370mTqyhUpWZ8wARgxIvXjrl8H3npLqofTCg+XZGS1avKxd6+c4/335fdMBfueWnLiqwoXlkmnBw+kl0VmOxaYsjcTb3MQX6mS/CKZa26nGnCEhZnfMsHX5M8vs6GnTsmLvw1BfHKyNJUEsu7yD2tVry5B/K5dkoUHWEpPtrE5iD/hioViRETkeidPyt5a/v4Zp21t5MnO9PYqUUIC+caNJQhu3BjYtAnIm9fDA7ORauRWtao0jbYkKAjo1QsYPlwCS28M4jUNGD9eLnfqZP3j0mb0rAniCxSQJoCbNskaeU2Tx82dK7FYhw7AxImyVaHput1RoySQVzFbUJBMNgQGynKStJUoFSoAffrImnXF3qZ22ZW/v0y8HTwoJfWuDOJVJt6mcnog4+Z2puvhs0qatXZtCeK3bk29v2gmli2TOY1cuYA2bVw8Ri9Xvbos59m1S3+NYVM7soUHCx+JiMitVq+W48MPS9tyJzAa9TJhT2fibVW8uATyxYtLcNCvn2fHYw9rSumVXr0khli3TvobepvVqyWRGR4OvPyy9Y8zDeJz5bJ+2yrVpV7TZBJk0SIgNFSy9CVKyH7hP/6o3//0aeDrr+XyZ59JTJY/vyRXIyPNLyV54w3g999lravCIN52qqT+2DHbHmdvEH/5sr60wioZBfFZaT28otbF//qrfLP8/aV8JhOTJsmxa1fZLz07M+1Qz+3lyB5WZeKfUv/prLDAtIsLERF5DxeU0h85Itu2hYbqDcZ8SfHikn2tVw+YPRt48UXAl9q7qCC+UaPM71uokDRDO3VKgiEn9TV0GhUgd+uWcVVBWmXKSIO7pCTZpcDavgzPPAMMHiwB3p9/6s/p7y/Z87feAsaOlYaJfn7A0KFSct+wofRdsFarVpLxf/ppmWQoWdL6x5Kwt0O9rUF8njwy0WU0yrIKq5uMVaggD7x8WT5MS3o2bpRjVorQVBC/dascK1XKNCo/exZYskQu9+rlwrH5CBXEHzyoV34wE0+2sOpfXVRUlNUfRETkhTRNz8S7YD18tWoSSPmi2rWBvn3l8quvyqSEL7h2TTqeA9aVkAN6AHn8uGvGZK+jR/U3+H362PbYwED9za+13wdAMuknTsjadZWBVV56SbLrBw9KCfD+/cD06XLbqFG2V0VXrixduTdt8mzzR1/laBBvbb+LgAA9/rZpXXx4uJRvAKmz8XfvAvPmyeUOHWw4oZerWTP1L7IVpfRTp8rkSMOG2Xd/eFMFC8qkUXKyTBgBWWueh1zPqrdcP5gu5iIiIt9z8KB0QQsJkbSzk/jienhzPv5YKkPj4oAPP5QMrLfbsEGOFSpYv5ZfNfXztiB+/HiZZ2rd2r5sVN++ssXcCy/Y9jhL1QiRkUCPHsCXXwJffSV/NkajlODXrWv7+AAG746wJ4hPSNCDI1t2n8iXT5ab2LUu/vhxWRPSuLFc99tv0sG9eHHbZpi8XY4cso5FNfLL5B9AcjIwZYpcfuUVF4/NRxgMko1fsUI+z5VLPoisxX8pRETZgSqlr1/fbNnjuXP6G15zbtyQEuLY2NTrUr2lM72jwsOlkRkAjBunV4l6s8z2hzdHZeJtXVvsSvHxEoAD9vcl6NVLOs07sv96Wm++KYH3ihWyh7y/P/Dpp847P1nPdE286uSdGZVJDwqyLTiye5s5c+vi1b6HXbpkvVkcVVIPZBrEmza0s2GFbpZn2h+DpfRkK7teUebPn4+OHTuibt26qFGjRqoPIiLyQhmU0p89K0mVYsWkuVda8fFAixbSjXvVKnm/9ssv8mY6q2TiAaBlS6BzZ8kI9+ghW1p5M1ua2ineWE7/ww+yhKFiRZkk8hbFi8s6dqVHD9/s+5AVFCsmpe737ulbrmfGdD28Lcsf1Dp4hzvUnz0LrFwpl7t0sfFkPkAF8QaDdIbMABvamWcaxLOUnmxlcxA/btw4dO/eHfny5cOuXbtQu3Zt5M6dG8ePH8djvtQNiIgou0hOlo3QAaBp03Q3f/KJBOp370pn7nHj9Nvi4yW43bZNunDXrQvcvCnLO59/Xi4HB6ff5stXffmlrFPct0++Vc89B3TvDrz+OvDFF/I9csSWLZLhVQGGvW7d0idQbAniS5WSo7cE8cnJ+u9b377etwNX//5yDAsDhgzx7Fiys4AAfWs5a0vqz56Voy2l9IADmfjKleW4f7/MBM6aJTOdjz6q/+FlJU2bSplD/fpSXm8BG9pZxkw8OcLmIP7bb7/F999/j/HjxyMoKAgDBw7EihUr0KdPH8THx7tijERE5Ijdu6UePjIyXd37yZP6WsVWreS9Z9++UtZ8/bpct3WrlEGuWiXZ33fflfvPnSvHhx6S5mJZQZ48epf0jRuBOXOkodl33wHvvCPd660t503r8GH5fn7zjUwO2HseQBqkGY2SWS9c2PrHqUz82bPSad1Z4uKkM/xPP9n2uCVLpLlcrlzyvfU2detKdcrq1c4t1Sfb2bou3tbO9IrKxNscxJcvLyXz167Jg1UnxK5dbTyRjyhTRmY7f/stw7uxoZ1lpUvLUi6AmXiync1B/KlTp/DII48AAEJDQ3Hr1i0AQOfOnfG///3PuaMjIiLHqfXwjRunayH/8cdAYqJU2S9dKp23AQlkS5SQzHHOnHKKqlUlWP/sM9n7Wq0zrVPHfV+KOzz/vGw5NnGiNLgbORJ4/31JOv36qz6JYYubN6XKQc11r1vnWPO82bPlaEsWHpBqCrVneVyc/c+f1vTpMunx0kuy77G1VBa+Vy/JdnujJ5/Mer/jvkgls10dxKtMvM3l9CEh+iCnT5dmoqGhspdhGtOmAcOHy6SpTytTRl5ULGBDu4z5+0uPgKgomQQlsoXNQXz+/Plx7do1AEDRokWxZcsWAMCJEyeg+fyrERGRhzx4ABw65Jp3dSqIT1NKf/Soniz6+GMpZR44UDLswcEScObMKcs6q1VLfcrWrSXBP3o08MEHzh+yp7VsKW86+/YF3ntPGpqp79UXX+hN8KxhNEoy7sABCSiGD5frBw0C/v3X9rH98AMwc6b8vLp1s+2xBoNrmttt2iTHBw+ATp2s26bv5EnJcBsMwGuvOW8slDV5fSYe0NfFf/aZHNu3lwooE1evyqTVkCHAnj12PIeDjh8HmjQBvv/e9c+1a5c0tIuKYkM7S2bMkN0QrN0GkUixOYhv2rQpFv3X+ah79+5466230Lx5czz77LNo37690wdIRJTlaZp00CpfHmjUSFKalu5naw32yZP6XmRpmtoNGyaZksceS73rXMeOsg3ZSy/JUnpLTeuKFJES87R7bGdVzz0nkx0A0Lu3dFy2xiefSMVpUBCwYIFsYffYY7IFVpcuUglhrZ079YB3+HD5dbGVs5vbJSdLxQYgWf7Dh63b633WLDk2aSJ7thNlxN4g3tbgyO5MPKCvi795U45mSul//13+ZgB9dw93uX0baNtWXt+/+ML1z/fPP3KsVYsN7SwxGOR/A5GtrA7ilyxZAqPRiO+//x4f/Jd2eeONNzBt2jRUqFABw4cPx3fffeeygRIRZVk//6x3/lm/Xurq2rSRd0DXrsntvXpJ9BURIbXd9+5lft4VK2QN/N270jVHZYkgWWG1flllhk3VrStrGTNpOpztfPCBvC9PTpYq2T/+AM6cAZKS0t9X04CFC/WGaN99J2XZBoOUmObMKW/iP/nEuue+elXmehISgCeekEy+PZzd3G7/fmm0lyOHTFYYDFItkNEKO02TagIgazbuJuczDeKtKVhyNBN/5YptE2wAUr3GolAhs7uBLFyoX1bNKd1B06RyR23tfuyYc/timKOeS81tEJETaVby9/fXChYsqA0aNEg7evSotQ/zSfHx8RoALT4+3tNDIaKsLj5e0woU0DRA0/r00bSePTXN318+NxjkQ95/pf4oU0bT1q41f06jUdM+/VTT/PzkvjVralpcXKq7dOwoN7Vt6/ovMatJSNC0Ro1S/zj8/TWtcGFNq1pV00qW1LRcufRvP6Bpr7+e/jyzZ+uP3bYt4+dMStK0li3l/iVLatq1a/aP/9tv5Txt2th/DlPffSfna9ZMPv/oI/k8MlLTjh0z/5gtW+Q+YWGadvOmc8ZBWdv9+/rL4YULmd8/Kkrue+CAbc+TlKS/BJ89a+Mg9+7V/+jfey/dzXfvalp4uH6XunUtn+rcOU3bvl1ezp3hk0/kOQMDNS0kRC7v3eucc1vSvLk8z5Qprn0eoqzE2jjU6kz8iRMn8Morr2DOnDkoW7YsGjVqhB9//BH3rMkGERGReUOGyMbHpUtLV7nvv5f0xdNP6zFixYrSLv733yUrX6AAcOSI1FK/9posrPz7b6mRXLpUFh8OGiSl9y+/LOX0xYqlPOXevcC8eXLZXBaeMqbK4jt0kDLwgADJzJ85Iz+K48elgEKtfGjfHvjqq/Tn6dRJli4kJ0upfkbbzg0bJuX7ISHy3Dlz2j/+zMrpX3tNMv0PHlh3PrUe/r+etxg8WC7fvClNAs1lM3/8UY7t2+uN9ogyEhysL7vIrKT+zh29iaStmXh/fyAmRi7bvC6+bFlpN24wmC2lX7VKxqaaOO7ZY7mKp3lzKUN/6CFphOdI1vz332UZDwBMmKAvkbKnJ4ctmIknciF7ZghWr16tdenSRQsPD9eioqK0V155Rfv777/tmm3wRszEE5Fb7Nqlp2uXLUt/+8mT5lNB169Lxt5chl59BAVp2vffm33aN9+Uu3To4NSvJttKSpKs2d9/a9qff2rapk2a9u+/ct2dOxk/9soVTStWTH4epUtr2qlT6c89YID+Y50xw/HxHj6sZ8HTZvkuXtSfa/16685XurTc/48/9Ovi4jQtOlqu/+ST1PdPSJBKBUu/9kSWNGsmvzeTJ2d8P/U7Hh5uXya7WjV5/O+/2zHINWssPrBHDznvq69qWo4ccnnfvvT3O348/Ut6TIymDR2qabdu2TacgwelKkY9r6Zp2ssvy+eDB9v4tdngyhV97Ky2IbKe0zPxppo0aYIZM2bg/Pnz+Pzzz/HPP/+gbt26qMrFk0RE1jEaJeVpNEo6tkWL9PcpWtR8Gik6WjL2q1cD1asDefNKpr1CBVkD/9hjsra+Z0+zT33qlBzTNKsnO/n7S3HEww9LV/t69eRHUaBA5tum5c4tBRQlSkh2sWFD2TcdkEx2mzZ6A6qhQ52zfrxYMdnO+u7d9M27VIM6QM+wZ+TSJT0rWrdu6udQ28cNG5Y647d0qVQqFChgdskwkUWq2mPp0ozvZ7oe3mCw/Xkcam7XqJFs35FGcjLwX19oPPWUvHQD5tfFr1snxxo1gM8/BwoXlr+1oUOBN96wbTg9eshrSYMGsnUoIMVdgGsz8SoLX6IEq22IXMGuIF6JiIhAs2bN0KRJE0RHR+NfV9flEBFlFVOmSMSUIwfw5Zf2naNJE3kHeOmSbPr977/A9u3yDrd2bYsPUyWi2aWrvLcrXhxYu1a2XI6Lkxhg+XIJipculRL6//1Pb5DnqKAg2VkASF9Sv3mz+cuWqPtUqiRzS6ZefFFimQcPZKcD1ZFbNbR78UWZACGyltoE6c8/ZRLKEnub2ikObTNnwdat8lIdGSl/46qk3VwQv369HJs3BwYMkL9T1Tv611+tX+py+rSspjIYgNmz9S7o7gziWUpP5Bp2BfH37t3DzJkz0bhxY5QpUwZz5sxB//79ERcX5+ThERFlQdeuyebjgOxZ5uYNYlV2Sb1RJc8rUkQy8uXLyxvvli31feXXr5f1885kaV28afZ98+bMu4Cr+5tuUagYDMCkSRK0bN0KjB0rHfbVRgzsSk+2qlZNJr3u3ct4i0d7t5dTHMrEW6C60j/+uATTGQXxKhPfsKEcAwNlg5K8eWUnCEu7kKb1229yrF9fn7gD9CD+8GE7OvBbSW0vxyCeyDVsCuK3bNmCXr16oUCBAnj11VdRuHBhrFy5EkePHsUHH3yAQm5+I0pE5JMWLwauX5eIrXdvtz61pulvTJmJ9y4FC0ogr970PvwwsG2bNLdyNnNBfGKiPJ9y8aJe2m9J2qZ2aRUuDIwZI5c//BAYMUKep1o1vrkn2xkMejZ+wQLL9/PGTLwKqNu2laMK4nft0ptgAtLn9MgR+Vrr19ev9/OTlVJA5ssJFPU9Ut8zpUgRKQJLSsq8SaC9VCa+ShXXnJ8ou7M6iK9YsSLq16+PnTt3YuTIkTh//jxmzZqFJk2auHJ8RERZj0ohPf20tDZ3o1u39C3mmYn3PvnySZZt8WLJxtkbhGRGBfHHjunX7d0rvxvR0fpqjIxK6h88kNUbgOUgHpANEmJjpbv22LFyHbPwZK+nnpLjkiWWy8odDeKdnYk/eFCy3oGBeiBevrwslbl1K3UgrUrpq1YFoqJSn0c99o8/Mn/OK1f0jH7aIN5gkL4dgGtK6jWN5fRErmZ1EB8bG4udO3di+/bteO211xCV9pWFKKvYtk228cpowR2RvYxGYMUKudyypdufXr0pzZFDdkIi7xMZKVu8hYS47jnMZeJVwF63rp4BzKi53e7dEpjnyiU7a1liMACTJ+u/b/7+sqUekT3q1ZPJrhs3pHLFHG/LxKtS+qZN5e8bkPlb1Q/atKQ+bSm9qRYtJCO/f7/eoNSSRYvk30316tJcLi1VUn/ggPVfh7XOnJEt/gICgHLlnH9+IrIhiB83bhy7z1PWd/Ei0LixdAsvVEj25j540NOjoqxk1y5JkUREpG7n7SbqTSmz8NlbqVJyNA3iTde3q8x6Rpl401L6zDqAFy+ud9lv25ZLOch+/v56Sfqvv5q/j7dl4tOW0ivm1sVnFMTnyqX/28gsG2+plF5xZXM7lYUvV05vpkdEzuVQd3qiLOfLLyUD7+cn0/xffy01Z02bAocOeXp0lBUsXy7Hpk2lttLN2NSOAD0Tf+6cvrxCBez16umN6vbsAW7fNn+OzNbDp/Xqq8COHcD06XYNmSiFKqn/7bfU68kBKeU+e1YuO5qJv34dSEiw7xzK+fPS2BGQLSNN1awpRxXEX7umN4R79FHz51O712W0Lv7WLb3gS32v0nJlOT1L6Ylcj0E8kXL1KjBhglz+9Vf5D9mmjQT0f/0FdO2aeatmosyo9fDm9oV3A24vR4Bk9FRZ74kTEmjExUlGvU4dKUQqUkQCJNNmd6Yy6kxvSY0a3DOaHNekiawXv3BBduo0FR+vT0wVKGDf+XPm1OdYHc3GL1okbx0efjh9t3zTTLymyXZwgKyXj4kxfz4VxK9aZXmCYelS6RdQtqyecU9LXX/woL79o7OwMz2R6zGIJ1LGjgXu3JEFZE8+KR1kFi6U/3DBwTKVrurciOxx65Ye+XhgPTzATDwJgyH1uniVha9cWQ/uVYbd3Lr406cl2+nvL8EJkTsFBUnfCCB9l3pVSh8dDYSF2Xd+Pz89iHY0iJ87V47mMuKVKslkwfXrwMmTGZfSK9WqySTsnTt6E7y0TEvpLS11KV5c+m4kJGS+C4Wt2JmeyPUcCuLv37/vrHEQedaNG8C4cXL5ww9T/9crUwbo3l0ujxrl9qFRFrJmjeyvVbKkvijZzbi9HCnmgnjTrLq6bG5dvArsq1Vjg0TyDBUUL1iQukjO0T3iFfUa6Uhzu7Nn9eZ7nTqlvz0oSA90d+ywLog3GDLuUn//vl5qb6mUHpAJuPLl5bIzS+qTk/XzMRNP5Do2B/FGoxEff/wxChUqhBw5cuD4f11xPvroI0ydOtXpAyRyi2++AW7elGnxdu3S3z5ggEzN//GH7MNEZA+1Ht5DWXiAje1IZ9rcTgXqpuvbTYP4tCuJbF0PT+RsLVtKJvnEidT/lh1taqc4o7ndvHnyt/PII5L5NkeV1K9bp6+NzyiIBzJeF79ypfSxKFwYqFUr4/O4ornd0aOS3Q8LM98Vn4icw+YgfsSIEZg+fTpGjx6NIJOWk5UrV8aUKVOcOjgit7h1C/jqK7n8wQcSrKdVqhTQoYNcHj3afWOjrMXD6+EBZuJJpzLxBw/q+72bZuKrVZMg6do12eNa0TQ9Y8ggnjwlPBxo1Uouz5+vTzQ5K4hXE53nz9t/jtmz5fj885bvo4L4GTMki12smPSjyEhsrGTSDx5MXwqvSunbtTP/dsaUI0H8nTvSn/X991Nfr0rpK1XK/PmJyH42/3nNnDkT33//PV544QX4+/unXF+1alUc5FZc5Iu++07epZYtK1vLWfLuu3KcM0c6QBHZ4sQJ4MgReefVtKnHhsFMPCkqiF+9WjJnuXPL6iElKEjP5JmW1E+cKHvEBwRknjEkciVVLj5ihPy+5soFfPKJXOdoEF+6tBz37LHv8UeOyOSYvz/wzDOW76eC+Ph4OVrzNxUdDdSvL5dNS+qTkqSRHpBxKb3iSBD/11/y8dlnepEZwKZ2RO5icxB/9uxZlFavbCaMRiMSExOdMigit7l7FxgzRi4PGiT/bS2pUUOmv5OTZSs6Iluodzn16umdw9xM09jYjnQqiFf/uuvWTd8EK21zu927gbfeksuffeZ4oETkiLZt9WA7KUkaxKktEVVwbC8VTK9bZ9/GNP/7nxxjYy13mgeAhx5K/dbD2okxtS5+6VKpPvjmG6BxY9loJ3duy1vUmTLtUJ92q77MqGAdAN54Q9biA9xejshdbA7iK1asiPVm2mHOnz8f1atXd8qgiNxmyhTg0iVZuJVRvZuisvFTpgBXrrh2bJS1eEEpfXy8viURg3gqWjR1uau50njTdfG3bkmxUkKCdAbv39894ySyJDISOHRIXtvOnJGM8tatMtn09NOOnbtWLVlOcvmyBLm20DS9lP655zK+b2ho6m3grA3i1br4P/6Q9e9vvgls3CjX9ekjlTKZKVVKuuPfuSM7TthCBeuArIP/7LPU17MzPZFr2RzEDx48GL1798aoUaNgNBqxYMEC9OzZE5988gkGDx7sijESuUZiop6Ff/ddfVPYjDRrJtP79+7JtDeRNZKSZFNfwKNN7VQWPiLC/q2XKOsICpJAXjG337u6bv9+4MUXpUS4SBFg+nTLW1cRuZOfnwTzhQoBFSoAtWsDVas6/vsZHKz//q9da9tjd++WyYXgYNnmLTOqaiBfvtRLWjJSpYo0yzMa9eZ5X34pW9VZ+3Y8IEBWEgK2l9SrTHyXLnIcOVKuO3JEPmcmnsi1bA7i27Zti8WLF2PlypUIDw/H4MGDceDAASxevBjNmzd3xRiJXGPOHODUKfmv2bWrdY8xGID33pPL48dLOT5RZrZuld0PcuUCatb02DDY1I7SUiX1fn7m93vPl0/uo2my1tbfX146c+d27ziJPKFRIznaGsSrUvonnrBu9VSDBnJs0cL6yQeDAViyBJg2TbLoGzfKUhfTiTlr2LMuPjFRr04YNkzmph88kAkLo1H+1fH/DJFrWVFsk96jjz6KFStWOHssRO6jaXqX+X79pGbOWk89Jf8lT52S7OqTT7pkiJSFqPXwqqWwh7CpHaVVsqQ0tqtaFciRw/x96tWTbegAybaxIz1lF6q0fe1aedtgTYBtNOpBvDWr9ACge3cJ9ps1s218lSrJhyPsCeIPH5ZAPiJCuul/841k3o8dk9urVGGlDpGr2ZyJP336NM6cOZPy+d9//41+/frh+++/d+rAiFxq6VJZuBURAbz6qm2P9fcHHn9cLqt1zkQZ+esvOXq4WomZeEqrTh05qiZZ5qi1t088Abz9tuvHROQt6taVZSfnz+sBamY2bpT1+ZGR+t9OZvz9pd+EJypc7AniTTvQGwzSXHDQIP12ltITuZ7NQfzzzz+Pv/57Q3rhwgXExsbi77//xgcffIDhw4c7fYBELjFqlBxffVX2arGVWtfMIJ4yo2n6HkUqYvIQZuIprZdekixjRmton3sO2LED+PVX7vtM2UtoqKyxB6wvqVcN7dq3t63Iz1NMg3hru/Cba1737rv6en5HdwYgoszZ/O943759qP3fK9q8efNQpUoVbNq0CT/99BOmT5/u7PEROd+mTcD69dLIrl8/+87RpIl0hDl61PrpeWtoGvDbbzKNT1nDqVOyHj4wEChXzqND4fZylJafn5QMBwdbvo/BIG/Krel2TZTV2LIuXtOAX36Ry9aW0ntamTJSCXDzpmxVZw1ze8EHBwN//gl89ZU0wSQi17I5iE9MTETwf//tV65ciTZt2gAAypcvj/Pnzzt3dESuoLLwnTvbv8lxZKS+MNSZ2fiFC2X6vnZtaTFLvk+92ylfXuoyPUhl4llOT0RkHdP94jNz86ZsSQfozeq8XXCwlMMDsguFNSxtI1eypORGPPyvjihbsDmIr1SpEiZOnIj169djxYoVaNWqFQDg3LlzyM12teTt/v1XWiwbDMA77zh2rv9+950axKs6vPPnZZHqtWvOOzd5xt69cnzoIc+OA8zEExHZ6pFHJFN98mTmc+tqojQy0re28axaVY47d2Z+39u39UaXXPtO5Dk2B/GjRo3CpEmT0LhxYzz33HOo+t9f/qJFi1LK7Im81uefy7FtW8mMOkKti1+9WvZWcdTdu8Dvv8vlnDmBAweANm1kT3ryXSoTnzZl4QFsbEdEZJscOYBateRyZiX1vlrtpL6+bdsyv6/K1ufPD+TJ47oxEVHGbA7iGzdujCtXruDKlSuYNm1ayvW9evXCxIkTnTo4Iqc6cgSYNUsuv/uu4+erVg3Im1empTdtcvx8S5dKIF+8uNTtRUVJm9sXXwSSkx0/P3mGl2TiNY2ZeCIie1i7Lt5XJ0offliO1gTxqpSeWXgiz7Krz6y/vz+SkpKwYcMGbNiwAZcvX0bx4sURExPj7PEROc/77wNJSVIGX7eu4+fz83Nul/qff5bjM8/If8eFC2Vh2YIFQN++1reNJe+RkAAcOiSXPZyJv3FDLxhhEE9EZD1r18X7aia+Zk1ZZXj6tD4RYYkXFZcRZWs2B/F37tzBSy+9hAIFCqBhw4Zo2LAhChYsiJdffhl37951xRiJHLd5s7SM9fMDRo923nmdFcTfvQssWSKXn3lGjo0aAT/+KP9ZJ0wA1qxx7DnI/Q4ckCqKnDmBQoU8OhT15jIqyje2PSIi8hYNGsjbh6NHM+7g7qtBfESEvsIws2w8M/FE3sHmIL5///5Yu3YtFi9ejBs3buDGjRtYuHAh1q5di7ffftsVYyRyjKYBAwbI5W7dnDt93KKFHHftynz6OiN//KGX0qvFaQDQsSPQvbtcXrDA/vOTZ6hS+ipVZDLGg1hKT0Rkn6goWUEHZFxSr4J4X3ydtbaknpl4Iu9gcxD/yy+/YOrUqXjssccQGRmJyMhItG7dGpMnT8b8+fNdMUYix/z6q6xZDw0Fhg937rljYoDq1eXy8uX2n0eV0nfokD7Ya9dOjosWsaTe16h3O17Qmd5XM0RERN7AmpJ6X36dtSaIv3RJPgwGoGJF94yLiMyzOYi/e/cu8pmZYoyJiWE5PXmfxETgvffk8ttvu6ak2dGt5u7dS19Kb6pZM5mAOHVKDwrJN5hm4j2MmXgiIvtZ09wuqwTxlvIFqpS+ZEkgPNw94yIi82wO4uvVq4chQ4bg/v37Kdfdu3cPw4YNQ7169Zw6OCKHTZokXeljYoCBA13zHGpd/PLlgNFo++P/+AO4cwcoVkz/L2oqLAyIjZXLixfbP05yPy/KxPtq12QiIm/w6KNyPHAAuHnT/H18OYivWhUICACuXAFOnjR/H5bSE3kPm4P4r7/+Ghs3bkThwoXRrFkzNGvWDEWKFMGmTZvw9ddfu2KMRPaJjweGDZPLQ4dK5xZXqFdPNpK9fFnWxtsqo1J65ckn5bhokX1jJPe7cgU4f14uV6rk2bHAt9dqEhF5Wu7cQK5ccvnUqfS3JydLqTngm0F8SIg+32yppJ5N7Yi8h81BfOXKlXHkyBGMHDkS1apVQ7Vq1fDZZ5/hyJEjqOQFb1SJUkyYIIFUuXJAjx6ue56gIKBpU7lsa0n9vXt6dt1cKb3yxBNy/PtvPRoj76ZSFiVLum4CyQbMxBMROaZoUTmaC+KvXpVA3mAA8uZ177icJbN18czEE3mPAHseFBYWhp49ezp7LETONWeOHN99FwgMdO1zPfaYZMnnzZP96K3tRP7nn1JKX7QoULu25fsVKCD/XbdtA37/HXj5ZeeMm1zHi9bDA8zEExE5qmhRYPdu80G8eo3Nk8f1bzlc5eGHZRWiuSDeaGQmnsibWBXEL7KhhLdNmzZ2D4bIaQ4dkinjgAC9u7srdewI9OsH7Nkj//0yCshNWVNKrzz5pJx70SIG8b7Ai9bDA2xsR0TkqIwy8b68Hl5RmfgdOyRo9zOp1z15UnIOQUFAmTKeGR8R6awK4ttZGQQZDAYkJyc7Mh4i5/jlFznGxgI5c7r++XLlkkD+xx9lGtuaID45WZraAcDTT2d+/yefBAYPBlaskDL80FDHxkyu5UWZeKOR5fRERI4qUkSOWTWIr1hR3lrcuiW5kAoV9NvUvHT58r5baUCUlVi1Jt5oNFr1wQCevMb8+XLs0MF9z9mrlxznzJGmepnZsQO4cQOIigLq1Mn8/lWryjuIe/eA1asdGiq5WHIysH+/XPaCTPz160BSklyOifHsWIiIfFVWz8QHBAA1asjl7dtT36ZK6b1gXpqIYEdjOyKvd+yYdIn39wfatnXf89avL9PYd+8CP/2U+f1XrJBj06Yy1swYDOxS727nzwNbttj+uOPH5fcgJAQoXdr547KRysLnzAkEB3t2LEREvkoF8adPp78tKwTxgOXmdlu3ypFBPJF3sDqIX716NSpWrIibZjbHjI+PR6VKlbBu3TqnDo48JCFB1nePGiWBiK9RpfRNmkiHGXcxGIBXXpHLkyYBmpbx/VUQ37y59c+hgvglSzI/PzmuTRvZQvDdd6Um3Vqq7rBSJesmaFyMTe2IiByngvgzZ6TgylRWWbJkLoifOVPPHTRu7PYhEZEZVgfxY8eORc+ePREZGZnutqioKLzyyiv46quvnDo48pDZs4Gvvwbee0+6l0ydmv6/lTfzRCm90rmzZF/37pXt4Cy5fRvYtEku2xLEN2kie9KfOwfs3OnYWCljN27o9YSjRwNdugAPHlj3WC9aDw9knTeXRESeVKCAzMsmJaXf7TWrZeJ37wYSE+WtjFot+NFH1q3+IyLXszqI37NnD1q1amXx9hYtWmDHjh1OGRR5mAqCg4MlWOzRA6hWTbZD83YnT8r0sZ+fe7rSp5UzpzS4AyQbb8n69fLfsXhxoFQp688fHAy0aCGXWVLvWmoSJjJSFgr+9BPw+OOAmWqkdLysMz0z8UREjvP3BwoXlstp18VnldfZ0qWB6Gjg/n1g5UqgfXsp0GzTBhg61NOjIyLF6iD+4sWLCMygHWVAQAAuX77slEGRB8XH62XeW7YAX34pgem+fbIXurc3VFOl9A0beu4/qWmDuxs3zN9HfY9jY63fU15RJfUTJjAb70pqLfyTTwKLFwPh4fKOplEjWSufES/NxPv6m0siIk+z1Nwuq2TiDQagVi25/MwzksupWFE23/FjJy0ir2H1n2OhQoWwT7WmNGPv3r0oUKCAUwZFHrRkiWSIK1SQ7Ptbb0mjOJXVnjzZk6PLnCdL6ZVHHpG10PfuWW5wZ896eKVjR/kPe/WqlNdv3Gj/WMkyFcTXrQu0agWsXSut3Xfvlr8NtT1gWnfuyN8M4HWZeF9/c0lE5GnmgviEBODaNbmcFV5nVUn9nTuSlV+4UIrSiMh7WB3Et27dGh999BHu37+f7rZ79+5hyJAheOKJJ5w6OPIAFQSb7lueMyfwwQdyedEiWc/tjc6cATZvlmnk9u09N47MGtxduCCVDQYD0KyZ7ecPCwNWrZJqg5s3pbx++XLHx006TdODeLUAsGZN+f2qXBm4dAlo3Rro00dqDpUjR4D+/eXxMTFes58bM/FERM5hLoi/dEmOgYHylsnX1a4tRz8/KSr0gk1WiCgNq4P4Dz/8ENeuXUPZsmUxevRoLFy4EAsXLsSoUaNQrlw5XLt2DR+oQI980+3b+rr3tJnsmjXlVfzuXe9di71ggRzr1wcKFvTsWF58URrc/fNP+oztypVyrFEDyJ3bvvNHRsp5W/2/vfsOj6Ls2gB+b3olJARSaKFIDQEBiYiiQDDhRTqKSJGAoBQFQUVUiooGUBBBFD8QRERUFPQVEcRgkN6LIiIdKaGaBBJIQjLfH+edbJa07bO7uX/XtddMZmdnn7gscuac5zwJ8pl06QJ8/bVk/8lyR4/K4ure3kDTpvrjtWvLXPnnnpOf586VlMWnn8pnUa8e8H//J8+p0x4cABvbERFZR/Xqsi28zFzh+fCuUHLeuTMwahSwfDkQH6/1aIioOEb/VRMWFoatW7ciOjoaEyZMQI8ePdCjRw+88soriI6OxubNmxHGNI9z++knySrWqVO0DFinA554Qva/+ML+YzOGI5TSq4KD5f+AAPDss4bBdeH58Jbw85Mat969pWt6nz5yzMdHorVGjYDXXzdtaTQSaha+RQvAy8vwOV9fWb1hzRrJtP/xB5CYCKxbJ9+TRx6R75IazDsAV2m4RESkteIy8a42ZcnTU+5Rq316icjxmHS/sGbNmlizZg2uXLmCHTt2YPv27bhy5QrWrFmDWrVqmTWAefPmISoqCj4+PoiNjcXOUpblWrlyJVq2bImKFSvC398fzZo1w9KlSw3OGTRoEHQ6ncGjtK76VEjhUvrimq317SvbdeuAK1fsNy5jpKUBmzfLfs+emg6lwKRJQNWqwIkTwPTpckxRLJsPfycvL7lVPnKkdFAHZHLexYvA4cPSSrZ/f+OXRiOxY4ds77235HM6dZJKi+7dJTp+4QXg2DFpgpeQ4DDpmPx8famnq/wDk4hIK+UhiCcix2fWvzKDg4Nxzz33oFWrVgi2YPLPV199hbFjx2Ly5MnYu3cvmjZtivj4eFxS/8V5h5CQELz66qvYtm0bDh48iMTERCQmJmLdunUG5yUkJODChQsFj+XLl5s9xnLj5k3gxx9lv6RMdoMGUgJ++7Y+4HcUu3dLgFy7tr7WTWuBgcB778n+tGkS4B0+LJ3NfXyk7N8aPDyADz6QQD09HTh1SpqvffihPLd8udTGXb9unfcrDwo3tStNlSrAqlXyL7h33pE/fw7m4kUgL0/2HWSKPhGR01KD+KtXpfEbwCCeiOxP01TRrFmzMHToUCQmJqJRo0aYP38+/Pz8sGjRomLPf+ihh9CjRw80bNgQderUwejRoxETE4PNagb2f7y9vREeHl7wsORGQ7nx88/yf6MaNfRrixRHzcY7Wkn9rl2yVVuqOorevaXxXHa2lNerWfi2bSWQtyadTubK16wp87iHD5cbM+rSaA89pJ8cTSXLygIOHJD9soJ4J6BW9cfESIkkERGZLyhI36ldnRfPIJ6I7E2zID4nJwd79uxBXKF5wW5uboiLi8O2bdvKfL2iKEhOTsaRI0fQtm1bg+dSUlJQpUoV1K9fH8OHD8fVq1dLvVZ2djYyMjIMHuVOWaX0qscfl+c3bTLs6qK13btl62hBvE4nWXIvL5mGMG2aHLd0PryxHn4YSEkBKleWNeXvu8/xpkI4mj17JHUdGQlUq6b1aCzy77/6YhD2HSUiso47S+oZxBORvWkWxF+5cgV5eXlFmuGFhYUhVf3bsBjp6ekICAiAl5cXOnfujLlz56JjobnFCQkJ+Oyzz5CcnIzp06dj48aN6NSpE/LUetJiJCUlISgoqOBR3VHKse0lO1vm8QKGS8sVp1o1ySIDsu6Io1Az8aVVEWjlrruA8eNlX/2zbY358MZq2RLYulX+1XHihHSxp5IVLqUv7YaWE3jvPZlhER3tGP0eiYhcAYN4ItKaY3ReMkFgYCD279+PXbt24a233sLYsWORkpJS8Pzjjz+Orl27okmTJujevTtWr16NXbt2GZxzpwkTJiA9Pb3g8Y8jZZjtITlZ/qUfEQG0bl32+Y7Wpf7iRakK0Olkzr4jmjABUJs/Vq5ctPu/rdWtq58K8eef9n1vZ2PsfHgHd+0aMHu27E+Z4jB99oiInB6DeCLSmmb/rAsNDYW7uzsu3jFH9+LFiwgv5W9BNzc31K1bF82aNcO4cePQu3dvJCUllXh+7dq1ERoaimPHjpV4jre3NypUqGDwKFe+/Va2PXsa9y/9Xr2kYdr+/aUHhPn50uxr+nTDNq7WppbSN2ggzeQcka8v8PHHUlY/YIA2EVWjRrJlEF8yRQHU6TxOHsTPnCm9DGNigB49tB4NEZHrUIN4zoknIq1oFsR7eXmhRYsWSE5OLjiWn5+P5ORktDYmG1zoNdnZ2SU+f/bsWVy9ehUREREWjddlZWYCK1bI/qOPGveaSpVkCS1AOp+XZPVq4KWXgJdflmZr7doBixYB1u454KhN7e7UsaOkR999V5v3b9hQtocPa/P+zuDsWVk9wN1d1ojX2NWrwK1bpr/uyhVgzhzZf/11ZuGJiKxJnXV55gxw44b0QwVktVEiInvQ9J92Y8eOxYIFC7BkyRIcPnwYw4cPR2ZmJhITEwEAAwcOxIQJEwrOT0pKwvr163HixAkcPnwYM2fOxNKlS9G/f38AwI0bN/Diiy9i+/btOHXqFJKTk9GtWzfUrVsX8fHxmvyODu/LLyVdV7eufq67MdSS+s8+A3Jziz9n3jzZVqsmpe4pKcCQIfLzzp0WDduAswTxgHSK12qedYMGsk1NlY5nVJRaSt+0KeDnp+lQDh+WbE/r1rICpCnefVf+YXn33UC3brYZHxFReVW4nF7Nwvv7AwEB2o2JiMoXTYP4Pn364N1338WkSZPQrFkz7N+/H2vXri1odnfmzBlcuHCh4PzMzEyMGDECjRs3Rps2bfDtt9/i888/x1NPPQUAcHd3x8GDB9G1a1fUq1cPQ4YMQYsWLbBp0yZ4e3tr8js6PHX9qWHDTAsuu3eXud1nzgBffVX0+aNHZdk6nQ747TdZu/ztt+VmwfXrwKRJ1hi9lD+r5fSO2NTOkQQG6tMHzMYXb8cO2TpAKf2UKZLd2b9filmMdfmyLIgASBbeyXvzERE5nMLl9OfPyz5L6YnInnSKoihaD8LRZGRkICgoCOnp6a49P37/fknVeXpKGXGVKqa9/u23Zd2qxo2BgwcNa3bHjpXW2J07S1m96sQJCeQVReZmqyXe5jpzRkr1PTykTN/X17Lrubr4eLm5smAB8L+bX1TI/fcDW7YAS5YAAwdqNoyDB6UYoLC1a+XjK8vzz0tDu5YtpeCFQTwRkXXl5gLe3vJPmQ8+AEaNAtq0ATZv1npkROTsjI1DOVOyPFuwQLY9epgewAPAiBGS3T10yDBQz8oCFi+W/ZEjDV9TuzbQtavsq5N2LaFm4aOjGcAbQ21ux0x8UTk5skY8YPNMfFoa8MgjwLRpxT8/ZYpsH31U/nEIAIMGyVz30uzbB8ydK/tTpzKAJyKyBU9PIDJS9tXZgczEE5E9MYgvrzIzgc8/l/1hw8y7RsWKEsgDQFKS3JIGZOm5tDQJ2ItLHY4eLdvPPrN8brYzzYd3BGrlAzvUF3XwoHSRCw4G7rrLpm/17rvAjz/KyoOLFhk+t3cvsGqVBOBTpgAzZsjHlpoqX9WSaqfy8oCnn5btY48Zl7UnIiLzqCX1DOKJSAsM4surr76S8vO6daVrvLnGjJGasu3bgY0bJcJQG9oNH158W+yHHpJ1r7KygIULzX9vgEG8qZiJL5n6Zyk21qYp7GvXDItQnnkG2LpV//PkybLt21c+Ll9fYNkyyfysWqUvcrnTRx/Jr1Chgn59eCIisg01iD9yRLYM4onInhjEl1dqQ7uhQy1bfyo8HBg8WPaTkiSY378f8PHRH7+TTgc895zsf/ABcPu2ee/NpnamUzPxp09L+3LSO3hQts2a2fRt3n9fejs2aQL07ClzK3v2lAZJO3fKzBQ3N30wD0jriqlTZf+556RXZGHnzgGvvCL706YBXFGTiMi21CBerY5iEE9E9sQgvjw6cEC6cHt6ykRbS734oqyr/fPP+uC8b18gJKTk1zzxhKw3f+YM8P335r3vsWNAerpUAkRHm3eN8qZSJX3/AzV9QEIN4mNibPYWaWkSxAOyQMOSJfJ2Fy/Kgg9qID5gAFCvnuFrx42TIpbMTODBB6XQJSNDnhs9Wm4MxMZKST0REdmWutiLikE8EdkTg/jySG1o1727eQ3t7lSrFvD447KvZsbVufIl8fWVOmJAH9WYSi1/btZMbkiQcTgvvqj8fOD332XfhkH8++/LfafoaMm+BwTIPazQUJkLn5ws98OKW4HR3R347jspngGA+fOl3P7VV4Fvv5Xn/+//LCusISIi46iZeBWDeCKyJ/5zr7zJzASWLpV9cxvaFafwQtatWhlX3j5ihCwNt2mTRDCmUm8YcD68aTgvvqjTpyWV7eUF1K9vk7dIT9fPVZ84UR9sR0UB33wjXwUASEyUnpDFCQqSQH3DBmlnce6crPQIyKqONrz/QEREhTCIJyItMYgvb37+WWpwa9UC2re33nWjo2U9LEDqfo0RGal/jTnZeDa1Mw8z8UWppfSNG+ujaSubO1fK6Rs2BHr1MnzuwQdlUYcePYA33yz7Wu3ayZBfekky8PXqGc6hJyIi27oziLdGYSMRkbEYxJc3O3bItmNH69fdLlkiC1U/9pjxr1Hn0H/9tSzvZazbt/XZeza1Mw0z8UXZeD58RgYwa5bsT5wogfedHn0UWLnS+GyOry8wfbpk43fvBvz9rTdeIiIqXUgI4Ocn+5UqSSEXEZG9MIgvb2yZvfb1Nb2zd2ysRC23bklne2P99ZcsURcQYLPyZ5elZuKPHQOys7Udi6OwcRD/wQfAv/8CDRqYdo/LGGFhQGCgda9JRESl0+n02XiW0hORvTGIL0/y8/XzyFu10nYsKp1OWm4DwK+/Gv869WZE8+bFpzWpZBERMrk6Px84elTr0TiGAwdka4Mg/uRJ/bz1117jH1ciIlehBvFhYdqOg4jKHwbx5cnff0tdr6+vvqTaEbRrJ1tjg/h//9XXJsfGWvz26hqv5YZOx3nxhWVmSlUCYPUgXlGkm7y6LFzfvla9PBERaYiZeCLSCoP48qRw9tpGzbvMogbx27dLiXxpsrKALl2AP/6QjPKzz1r01qtWSTOa0aPLWWU558XrHTok0XZYmNU7Ey1cKMvG+frKPpd/IyJyHS1ayJYrgxCRvfGflOXJzp2ydZRSelXdukC1akBuLrB1a8nn5ebKhOItW4CKFYF164Dq1c1+23PngMGDgStXgDlzgDZtgOPHzb6cc2EmXs9G8+H/+Ue/UMNbb8kfcyIich1Dh0o/X2MX5SEishYG8eWJoy7JptPps/EbNhR/Tn4+MGQI8OOPgI8PsHo10KSJ2W+pKHK5tDRJSleqBOzZI0UK33xj9mWdhxrEMxNvkyBeUYBhw2Tp+dat9YswEBGR63B3l36+jlTcSETlA4P48iInB9i/X/YdLYgHyp4X/9JLwNKl8n/Mb76RtLkFPv5YEvk+PsC338p/mjZtpGXAo48Czz/v4nPl1XL6I0dkub7yzAZB/GefAWvXAt7ewKJFbGZHRERERNbDIL68+P13mfQdHAzUqaP1aIpSg/hduyR9Wdju3cDMmbK/aBHQubNFb3XsmL70bdo0WfarWjUgJQV4+WU5Pnu2zJd3WTVrykTtnBxpn15eKYrVg/jLl4ExY2T/9dflzxcRERERkbUwiC8vCpfS63TajqU4UVHyyMsDNm82fG76dNn27w8MHGjR2+TlAYMGSX+8du0M++J5eABJSbIMGACMHy8xrktyc9NHl+V5Xvy5c7LagYeHfoqBhX75RaZpNGjAeZJEREREZH0M4ssLR50PX1hxJfVHj0q9OyBRtYVmzpS+eIGBwOLFxXcLHz9elos5dgz48EOL39JxcV68PgvfoIHUvlvB33/L9r77OE+SiIiIiKyPQXx5oXamd+Qgvn172RYO4t99V0qeO3cGoqMtuvzNm8CUKbI/e7ZUlBcnIAB4803Zf+MN4No1i97Wcanz4stzJt4G8+GPHJFt/fpWuyQRERERUQEG8eVBZqY+UHO05eUKUzPxe/dKPXJqKrBkiRyzQhZ++3YJ5KtWBRITSz83MVHuGfz7LzB1qsVv7ZislYnPy5NpDi+8YPmY7M0GQbyaia9Xz2qXJCIiIiIqwCC+PNi7V5Zoq1oViIjQejQlq1oVuOsuGetvvwHvvy/N+Fq3Bu6/3+LLb9wo2wcfLLstgLu7FAEAwAcfuOj68WoQ/9dflrXi37FDVg6YORM4fdo6Y7MXKwfxisJMPBERERHZFoP48sAZSulVajb++++Bjz6S/fHjrdKMTw3i27Y17vz4eHnk5uq71ruU2rWlKcCNG8DFi+ZfZ8MG/f7q1ZaPy16ys+UGBmC1ID41Vf5zurnJf14iIiIiImtjEF8eqE3tHLmUXqUG8YsWAenpki3u0sXiy2ZnSzk9IJl4Y737rgRk33wjDfFcire3vjHA0aPmX6dwDwNnCuL//FOmAoSEAJGRVrmkmoWvVctqffKIiIiIiAwwiC8PnKEzvUoN4lUvvlh8C3kT7doF3LoFVKliWplzdDQwZIjsz55t8TAcz113ydbcID47G9i6Vf/zhg2SinYGhUvprbTsIufDExEREZGtMYh3dVevAidOyH7LltqOxRhhYfqu6VWrAv36WeWyhUvpTY3X1CZ4GzbIdH2XogbxavRpqu3b5e5IWJjUj+fkAMnJ1hufLdmwqR3nwxMRERGRrTCId3VqFr5ePaBiRU2HYrTevWX76quAl5dVLlm4qZ2pWraUZeeuXQMOHLDKcByHpZl4tZS+XTvgkUdk31lK6m24vBwz8URERERkKwziXZ0zldKrXntNGo4NH26Vy+Xm6iu+zQniPT31ryvcw80cigJcvizjWbIEmDgRmDFDwwy/Gm2aG8Sr/0EKB/E//uj4JQs3bkhXfQBo2tRql2UmnoiIiIhszUPrAZANKYo+Be1MQbynp1WjoL17gcxM6V/WuLF51+jQQWLT5GRg3DjzrnH0KNC+PXD2bNHnmjaVTvh2p2bijx2TwNuU/gNZWfpuge3aATVqSMnChQvAvn1AixbWH6+1LFsGXL8O1K0LNG9ulUvm5upnrjATT0RERES2wky8q8rPB0aO1M9Pbt9e2/FoSL2P8cAD5vfIU//z/fabBGvm+P57CeB1Ool327cHGjSQ5zZvNu+aFouKAjw8gJs3gXPnTHvt1q3yH6NqVQmGvb2Bhx+W5xy5pF5RgHnzZH/ECKs0TgSAkyeB27cBPz/5T0JEREREZAsM4l3R7dvAoEGyzrpOB3z8MdCkidaj0owl8+FVTZoAoaGS0d+507xrqFOwX38dOH1a7q88/7wc02z5Og8PWQ8NML2kvvB8eLVboDPMi9+0Cfj9d4m2Bw2y2mULz4e3UrN7IiIiIqIiGMS7muxs4LHHgKVLAXd3KRseNkzrUWkmL0+f5bYkiHdz069+Z+68eDWILzwFu00b2e7YIfdeNGHuvHg1iC9c5fGf/8h2927g/HnLx1YcRZGO+MVJTZU/84MHA6NGSbf8O33wgWz79weCg602LM6HJyIiIiJ7YBDvSm7dArp2BVatktLmlSuBvn21HpWmDhwAMjKAChUs71/WoYNszVlBLTcX+PNP2S/cDL1hQ1k0ICtLw8735nSov35dX5Kg3t0AZKm5Vq1kf80a64yvsNxcub6vL1C5siwd0LMn8NRTUi4RESHB+eLFUjL/4ouGrz93Tr4XgEw3sSJ2piciIiIie2AQ70o+/BD4+WfA31+6sHXtqvWINKeW0t9/vxQmWEJNOG/bJkG3KY4ckfgzMBCoWVN/3M0NaN1a9tUO+nZnzlrxmzdLmUNUlDwKs2VJ/ZIlkuUHgCtXgD175KbVJ58Af/whdezNmwNDhsg5c+YA33yjf/3//Z+Mu21bqy4tB+j/8zGIJyIiIiJbYhDvKm7fBt5/X/ZnzdKnjcuRHTuAX36RamuVNebDq+rWBapXlwptU+ewF16S/M750vfdJ1vN5sWbk4kvPB/+TmoQv359yWXv5sjOBt54Q/bfegvYvx/473+lPH7iRGDFClm/b88eYOFCYPx4OXfwYPndcnKkPwQgpfZWxnJ6IiIiIrIHLjHnKr79FjhzRkqMBw7UejR2d/asdJ/PzZWg+J13gHvvlR5mgCReLaXTSTZ+yRKZF9+xo/GvLRzE30mdF69ZJl5NHZ84IVlqY0oWSgvimzWT9uznzgEpKUBCgnXG+fHHwD//yLXHjgV8fEqfIzF1qvxH3bQJ6N0bGD0auHgRiIwEune3zpj+JyNDVtYDmIknIiIiIttiJt4VKAowc6bsjxwpwU05s3Chfum3rVslMO7YEbh2TWYXWGvJcnPnxZcWxLdqJXHzP//Iw+6qV5ceCjk5ciOoLGlpwN69sl9cEK/T6Re9t1Z5QWamZN8Bybob82fcwwP48kugShX5AJ5+Wo4//TTg6Wmdcf2PWsQQFgYEBVn10kREREREBhjEu4ItW4BduyQQGz5c69HY3e3bwIIFsv/ee9LjzM1N30X+vvusF7Op8+L37JFY1lilBfH+/pK8BjTKxru5AXXqyL4xJfW//Qbk50sZfrVqxZ9To4ZsL12yzhjnzJFr1a4t5fHGiowEvvhCbizcvi1/EGywWgOb2hERERGRvTCIdwWzZsl24EDJOpYzP/4oq5mFhso9jAULpNO7utpZYqL13qtqVZnznJ+vn29flqtXpbIckAbqxXGYefHGNLdTyxCKy8Kr1D+Hly9bNi5A7pbMmCH7U6aYfkemQwf9XPp+/YDwcMvHdAfOhyciIiIie2EQ7+yOHwe++072x4wx6xLvvw/Exsp0YWc0f75sBw+WYgQAiI6W4P72beuvsqdm441dL/7332Vbu7Z0py+OGsRrPi++rEx8Xp40kAOATp1KPq9yZdlaIxM/c6YE8o0aAU88Yd41Xn0V2LdPVnCwAWbiiYiIiMheGMQ7u9mzZU58p04S5Jjh3Xdlye8vvrDu0Ozh5Elg3TrZHzq06POWLitXHFPnxZdWSq9Sm9vt3y/Tv+3O2A71ycnSwS0kRF/qUBxrZeIvX5Y/44Bk0839QHU6mbPg62vZeErATDwRERER2QuDeGf277/AokWyP26cWZdITZXO7oAsz+ZsFiyQexgdO8oScPbw0EMSEx46JGX8ZTlwQLalBfHVq8v08rw8uaFid8YG8Z99JtvHHwe8vEo+z1qZ+MWLgRs3ZO33nj0tu5aNKArXiCciIiIi+2EQ78w+/hjIypLoUK3xNtGePfr9jRv1Hd6dQU6O/h6G2njcHipVkukHAPD112Wfb0wmHtB4qTk1iD95suQ/BNevA6tWyX5Zyxiqmfi0NPmgzHXokGx79ZI7Jw7owgW5z+DuLlMmiIiIiIhsiUG8s8rJAebOlf1x48wOcHbv1u9nZgI7dlhhbHby/fcyjz88HOja1b7v3b+/bJcuLf28vDzgjz9kv6wgXtPmdpGRgJ+fDPjkyeLPWblSbhrddZesi1ea4GB96fuVK+aPS60MsFeZhRFu3pTsu0rNwteqVXpxAhERERGRNTCId1Z5ecDzz0sw9fjjZl9m1y7Zqg2/namk/uOPZTtkiNWX/S5Tnz6yDPnevcCff5Z83rFjwK1bEh+XlaVVM/Hbtkn3e7vS6couqVfvWAwcWPZNIzc3KVkALJsXf+yYbNWx2VB+PrB8uWTWS7J1q/xa0dFSlKAo+qZ2nA9PRERERPbAIN5Z+foCL7wgqXMz03+Kos/Eq9XRzhLEHz0qPdZ0uuIb2tlaaKi+Ofvnn5d8nlpKHx1ddk+2mBgJ9tPSgMOHrTJM05QWxJ89q2/Hr5YhlEUtqTd3Xnx6uv4GgLqOvQ0tXCjN7//zH7lHdidFkaKXmzflxk3PnnIPbeVKeZ7z4YmIiIjIHhjEl2Pnzkk5uru7vi/e9u1ARoa24zKGmhTu1AmoWVObMaix7LJlJWfOjZ0PD0g1gTrXXtN58cUF8cuWSRTbti0QFWXc9dTmduZm4tUsfJUqQIUK5l3DBAsXynb/fuDTT4s+/8MP8v3w9QVefBHw95ebYD//LM8zE09ERERE9sAgvhxTs/CNGwMNG8q047w84LfftB2XMdR55vHx2o2hSxeJLc+cATZtKv4cU4J4QD8v/ssvLV+dzWRqKlmd5K1SFH1X+gEDjL+epZl4O5bSHzqkn1oCAK+8YngzKy9PlpoHgOeeA2bMAI4fB0aP1hfCtGhh82ESERERETGIL8/UIL5lS9mq6587Q0m92nvNDlXWJfL1BXr3lv2SSupNDeIfeUSmCGzYII3SXn659L5weXnA5s0ys2LIEOmSbraSMvH79kn9uLc38Oijxl/PWpl4OzS1UzPv//mP3Mu4dAl4+23988uXy42joCBg/Hg5FhYmS9ifOCF9DNTvERERERGRLXloPQDSzp1BfFycNItz9CBeUSRwAiTQ1dKAAbLM3YoVsliAj4/+ufR04NQp2W/SxLjr3XsvsGYN8Nprsvzf9OnAvHnAk09KA3l/f3l4ekrFxA8/GMbI99wDPPOMmb+MGsSfOSPd+NRfRp270K2bRLHGsjQTb6fO9Lm5+l9x2DCZXtKlC/Dee9JvoXp1YNIkef6ll6TxfmFVq8qDiIiIiMgeGMSXU4Wb2t1zj2zbtZMs8KFD0qE7IkK78ZXm2jV9qbOx07NtpW1bCfL++QdYvVqfmQf0Jf/VqgEhIcZfMyFBpgmsXg1MmSId8OfNK/n8ihUliDx0CNi40YIgvnJlmR+QkSElBFlZ0jhRLaUva2344q4HWJ6Jt3E5/dq10huiShXJxHt4AB07AuvXS9DeoYNUfoSFSfk8EREREZGWGMSXU6dPA1evSkZXzRJXqgQ0by4Z4ORk45uQ25uahY+IkG7uWnJzA/r1A6ZNk2xu4SDe1FL6wnQ6yQY/8ogE88nJUiqfmSmPrCygUSNJjrdtK2vLt2snQbyilL0CXIlvWq+e3N1RO+ypIiOBhx827XrWmhNv40y8Wkrfv79+qcL33pPPbeVKfWXKa69JFQQRERERkZYYxJdTahOvmBiZ6qyKi5Mg/pdfHD+IL2vddXsZMECC+DVrZP56aKgctySIV6nBfJcupZ8XGysN1i5ckIZrZse9rVvrSzSqV5cLt2oF9Oihj3CNZUkmPiND0uOATYP4K1dkSgIADBqkP964sVQ0fPihDCUqSkrtiYiIiIi0xsZ25dSd8+FVcXGy/eUXyeia4sIFYMmS4tfYtia1qZ2jBPGNGgF33w3cvg107y4l2W3bSjM0wLIg3li+vhJrA5KNN1tSklzg/HmZG79ihaynZk4gbUkm/vhx2VaubNo8fBN98YXMiW/Romjfgtdfl6kK6r7ahZ6IiIiISEsM4supkoL4Nm0kM3/uHHDkiGnX7N9fspnLlllliCVylKZ2hT35pGy3bAF++kmWnEtPl/nVbdrYZwwPPihbi5YI9PeXOxDWaIigZuIzMoDsbNNea6dS+sWLZVs4C68KDQXWrZP1401ZWY+IiIiIyJZYTl8O5edLyTxQNIj39QXuv1/mYP/yC9CggXHXPHlSlkUDZMkzU3ugmcLRyukBYMQImZ9/6xYQEAAEBsq2Xj2gRg37jKFtW+CttyzMxFtTxYpyF+P2bSmpr1bN+NfaoTP9/v3y8PIC+vYt/pxWrfQVDkREREREjoBBfDl0/LhkiX18ZO7vnTp0kCD+22+B4cNlya2yqEt0Afosv604YhDv6SnLkWnpvvvkszp9Wh41a2o7Hri5STo7NdX0IN4OnenVhnZdu0pTRyIiIiIiZ8By+nJIDbKbNSu+V1n37hJ/paRI5/Xc3NKvpyj6VcgA4PffJSNtC7dvy1RtwLGCeEcQECBzuwELS+qtydx58TYup1cU4OuvZb+4UnoiIiIiIkfFIL4cKmk+vKphQ+CrryTA/+oroGdP4ObNkq+3datk9/39ZT3027eBAwesP25A1mPPy5N5+466jr2W1HnxDlNSb26HehuX058+LY0YPT2B9u1t8hZERERERDbBIL4cKiuIB2S98++/l5L71auBzp2B69eLP3fJEv1r1OXFbVVSr5bSR0VJtQAZskpzO2syJxN/44aU4AM2C+K3bZNts2bSB4KIiIiIyFkwDCpn8vJKbmp3p06dgLVrpUnbr78CHTvKXPrCbt7UlyU/+aT+mrYO4llKX7w2bWRt+aNHJdOsOXMy8erycpUqAcHB1h8TgO3bZdu6tU0uT0RERERkMwziy5kDB4DMTCl9N6bz/IMPSpO74GBgxw6gTx8pl1f9978S2NeoIecyiNdWxYqSXQYcJBtvTibeDp3p1Uw8g3giIiIicjYM4suR3bulLB6QZeSM6ToPAPfcA6xfL0uorVsHPP+8/jm1lH7AAClvV4P4P/+UmwXWxiC+bG3bytYh5sWbk4m3cWf6mzeBfftk/957bfIWREREREQ2wyC+nFi5UoK71FQgOhr4v/8z7fUtWgCffy77H3wgjwsXJKgH9OvCR0bKIz9f1uC2tpMnZcsgvmQO1dzOnEy8jTvT790r1STh4Q6wDB8RERERkYkYxLs4RQFmzAB69ZIMZEICsGWLlL+bqkcPYNo02R89WtaQz8+XbGa9evrz1Gz8rl2Wj/9OzMSX7YEHZPvnn6Y3hbc6czLxNi6nL1xKr9PZ5C2IiIiIiGyGQbyLmzwZGD9e9keNAn74AahQwfzrvfSSrKudny/d6wFpaFeYrebFp6cDV6/Kfq1a1r22KwkNBRo3lv1Nm7Qdi0WZeBuV06tN7VhKT0RERETOiEG8C1MUYN482X/nHWDuXMDDw7Jr6nTAxx/r5117eUmzu8JsFcSrpfShodIxn0rmMEvNqZn4GzekFKQsmZnA+fOyb4NMvKKwqR0REREROTfNg/h58+YhKioKPj4+iI2Nxc6dO0s8d+XKlWjZsiUqVqwIf39/NGvWDEuXLjU4R1EUTJo0CREREfD19UVcXByOquW55cyxY8C1a4C3N/Dcc9a7rpeXzLHv2ROYPr3oKmBqEH/kCJCRYb33ZSm98dSbLJpn4oOCAE9P2TempF5dXi44GAgJsfpw/vlH7hF4eEifByIiIiIiZ6NpEP/VV19h7NixmDx5Mvbu3YumTZsiPj4el0oovQ0JCcGrr76Kbdu24eDBg0hMTERiYiLWqd3VAMyYMQNz5szB/PnzsWPHDvj7+yM+Ph63bt2y16/lMHbskG3z5hJ4W1OlSsC33wJjxhR9rnJlfcOwvXtNv/ZPP0mAdWdjPDa1M16TJrJVY2LN6HSmzYu3Uyl906ay2gIRERERkbPRNIifNWsWhg4disTERDRq1Ajz58+Hn58fFi1aVOz5Dz30EHr06IGGDRuiTp06GD16NGJiYrB582YAkoWfPXs2XnvtNXTr1g0xMTH47LPPcP78eXz33Xd2/M0cgxrEx8ba/70taW43e7YE/6+/bnicmXjjVa8u2/R04Pp1bcdi0rx4G3emZyk9ERERETk7zYL4nJwc7NmzB3FxcfrBuLkhLi4O29R/aZdCURQkJyfjyJEjaPu/2uGTJ08iNTXV4JpBQUGIjY0t9ZrZ2dnIyMgweLgCRwjiTZ0Xn5+vH/fq1YZxnxrEs6ld2QIDpZIdAM6e1XYsJmXi7dSZnk3tiIiIiMhZaRbEX7lyBXl5eQgLCzM4HhYWhtTU1BJfl56ejoCAAHh5eaFz586YO3cuOnbsCAAFrzP1mklJSQgKCip4VFfTmE7s1i19OboWAYu5Qfxff0n2GJC1vNW16QFm4k1VrZps//lH23GYlYm3QTl9djawb5/sMxNPRERERM5K88Z2pgoMDMT+/fuxa9cuvPXWWxg7dixSUlIsuuaECROQnp5e8PhH86jHcvv2Abm5Ej+p89PtSW0aduKENNczlpopdXeX7eLF0lE8Px84dUqOMYg3jnovSvM/zubMibdBJn7vXiAnR74TrOYgIiIiImelWRAfGhoKd3d3XLx40eD4xYsXER4eXuLr3NzcULduXTRr1gzjxo1D7969kZSUBAAFrzP1mt7e3qhQoYLBw9kVLqXX6ez//sHB+jhszx7jX6cG8cOGAT4+wB9/yOvPn5cAzMNDn2Gm0qlBvObl9MZm4m/e1A+2Th2rD6NwKb0W3wkiIiIiImvQLIj38vJCixYtkJycXHAsPz8fycnJaG1CrWt+fj6ys7MBALVq1UJ4eLjBNTMyMrBjxw6TrukK1C7cWs79La65XUZG6ZlhNdDq1EmWsAOARYv0pfQ1a1q+1n154XSZ+NOnZRsQAISGWn0Y6neinP1VQEREREQuRtNwaOzYsXjyySfRsmVLtGrVCrNnz0ZmZiYSExMBAAMHDkTVqlULMu1JSUlo2bIl6tSpg+zsbKxZswZLly7FRx99BADQ6XQYM2YMpk6dirvuugu1atXCxIkTERkZie7du2v1a2pCy6Z2qpYtgS+/BL74Qubn79unr5b+7jugWzfD89PSgD//lP3YWMDXV167fLl+yTSWQRvPYYJ4YzPx6nyJWrVskipnZ3oiIiIicgWaBvF9+vTB5cuXMWnSJKSmpqJZs2ZYu3ZtQWO6M2fOwM1NXyyQmZmJESNG4OzZs/D19UWDBg3w+eefo0+fPgXnvPTSS8jMzMSwYcOQlpaG+++/H2vXroWPj4/dfz+tXLok8ZBOB9xzj3bjUDPxhw7Jo7CFC4sG8eqNhzp1JO5r3x6oUQM4cwZ4/315jvPhjecwje2MzcSrQXxUlNWHcPasPNzd9X8uiYiIiIickeaFyaNGjcKoUaOKfe7OhnVTp07F1KlTS72eTqfDG2+8gTfeeMNaQ3Q6ajDcsCGg5fT+++8Hnn1WYre775aHry/wwAPAzz9LF3p1GTSgaKbUzQ0YNAh44w3gyBE5xiDeeE43J/7kSdnaIIhXS+ljYgB/f6tfnoiIiIjIbjQP4sn6HGE+PCBZzzlzih5v1EjK5v/7X2DAAP3x4sqd1SBexSDeeGoQf/160RsmdqVm4rOygMzMkqPowuX0Vqb2ZdByegkRERERkTU43RJzVDZHmA9fmkcfle2KFfpj+fn6cRcO4mvVAtq10//MIN54fn5ASIjsa1pSHxgIeHvLfmkl9TYsp1eDeC2nlxARERERWQODeBeTlwfs3Cn7jhrE9+4t23XrJEMMAH/9Jfv+/vomdqr/9TkEwMZ2pnKIefE6nXHz4tVyeit/yPn5+mUOOR+eiIiIiJwdg3gX89dfUj7t5wc0bqz1aIrXuDHQoIGs+/7DD3JMLaW/556iS8j16gU0awbEx8v682Q8p5kXn5mpD/CtnIk/dkyWNvTxkakcRERERETOjEG8i1FL0osLhh2FTle0pF4N4oubx+/nJ8vTrV1rk5XHXJrDLDNXViZeXSM+KAioWNGqb717t2zvvttxvxNERERERMZiEO9iHH0+vEoN4tetkywp1/C2DYcJ4svKxNuolB7QB/GcD09EREREroBBvItxliA+OhqoXx/IzgaWLpVu9YD2HfVdjUPMiQfKzsTboakd58MTERERkStgEO9CbtwAfv9d9h09GC5cUq8uIVenjj5hS9Zhi0z8f/8LNG8OHD5swouMzcRbOYjPywP27pV9BvFERERE5AoYxLuQ3bulE3e1akBkpNajKZsaxKtxHUvpra9wYztFsfx62dnAiBHSo+Cbb0x4obGZeCuX0//1lyxPHxAA1Ktn1UsTEREREWmCQbwLWbVKtm3bajsOYzVpAtx1l/5nBvHWp5bTZ2UB//5r+fWWLgXOnZP9K1dMeKGaibdzOb06H755c8Dd3aqXJiIiIiLSBIN4F5GbCyxfLvv9+2s7FmMVLqkHGMTbgo8PEBoq+5aW1OflAdOn6382KYhXM/F2LqdnUzsiIiIicjUM4l3EunWS5AwLAzp21Ho0xuvTR7YVK0pmnqzPWvPiv/lG1lxXmZ2Jv7OuPyMDuHZN9q0cxLOpHRERERG5GgbxLuKzz2T7xBPOtRZ2TAzw/ffAmjXONW5nUnhevLkUBUhKkn21aaJJQXx4uNSz37xZdCDqGvEhIUCFCkVeatL7FJKbC+zfL/sM4omIiIjIVTCIdwFpadIxHAAGDNB0KGbp2pWl9LZkjUz8Tz8BBw5Ig7gpU+SYScG1ry/QrJnsb9pk+FwppfTLl0slfuEyfmMdOiSN+IKCZOUDIiIiIiJXwCDeBaxYIcFKdLQ+TiJSWWOt+Lfflu0zz+i7vJucIX/gAdlu3mx4vJTO9IsWyfatt0xvzKfOh2/ZUvovEBERERG5AgbxLmDpUtkOGMBghYqyNBO/aROwZQvg5QU8/7y+UV5WljyMpgbxd2biS+hMf/06sHGjfv/990u+dH5+0WOFg3giIiIiIlfBIN7JnTghMZFOB/Trp/VoyBFZOidezcInJgKRkVJS7+Ulx0zKxt9/v2z/+EPfyA7Ql9PfkYlfv17mtavvNXs2kJ5e9LLffCM3FiZPNjzOzvRERERE5IoYxDu5zz+XbYcOQNWq2o6FHFPhIP7OxvCluXEDGDQIWLsWcHMDXnxRjut0+my8yR3q1Vr8LVv0x0vIxP/4o2yfeQZo1EgC+LlzDS/5998yxn//Bd54A/jwQzmenQ0cPCj7zMQTERERkSthEO/EFEVfSj9woLZjIcel3ty5dcv4oHvPHqB5c2DJEgngZ8wwbA5nVhAPFD8vvpggPj9fViwAgC5dgNdek/333pPSekB+n8ceAzIzpfk9ADz7rAT/Bw9KFj80FKhRw8QxEhERERE5MAbxTmz7dlm3288P6NFD69GQo/LyAsLCZL+sefH5+cCsWbJawNGj0hTv11+BceMMz7M4iFfnxaelyQMwCOL37gVSU6V0v21bCdbr1ZMqfDXbPm6cdMyvXFluOgweLOPv0wdYsEDOYVM7IiIiInI1DOKdmJqF79VLgh2ikhgzLz4/X0rTx42TLHaPHhIkt21b9FyLg/jdu2XNeDULX7ky4O9fcJpaSv/ww3ITwt0dePVVOfbuu1IhoAbzS5fKXP3584G4OMnMFw7iiYiIiIhcCYN4J5WdDXz5pew749rwZF9ldahXFGDUKAmIPTyAjz4Cvv0WCAkp/nyzg/hatYCICLlLsHNniWvEq0F85876Y088AdSuLe85aJAce/llID5e9j09pcldkyb617CpHRERERG5GgbxTsrNDfjkEwlm2rfXejTk6MoK4l99VQJ3nU4C+WeeKb0MvXJl2ZocxOt0hiX1xawRn5oK7Nol+//5j/6lHh76bDwA3HefNLMrLChIbgBUqybVKffdZ+L4iIiIiIgcHIN4J+XpKeXOixdLqTFRaapVk21xQfz06UBSkuzPnw88/njZ1zM7Ew8UH8QXysT/9JNsW7bUN6xTDRgAxMRI+fzy5fI9uFP16sChQzKnXx0nEREREZGr8NB6AERkeyXNiZ8/X0rSAelAP2yYcdezShC/dauk1wGDIL64UnqVp6c0sbt9G/DxKfktKlSQBxERERGRq2EQT1QOFFdOP38+MGKE7L/6qn4deGNYFMRHR0vde3o6sGGDHPtfOX1ODvDzz3LokUeKf7mHhz72JyIiIiIqb1hOT1QOFM7E5+dLh/fhw6Wh3ejRwJtvmnY9NYi/fNmMwbi76yer37ol2/9l4jdtknXgw8JknXoiIiIiIjLEIJ6oHIiIkJ5yubnAs8/qs+4TJgDvvWf6WuqFM/GKYsaA1JJ6Vc2aAAxL6d34txMRERERURH8ZzJROeDpKYE8oF9f/e235WFqAA8AlSrJ9vZtICPDjAEVDuLDwwFfXwDA6tVyqLj58ERERERExCCeqNxQS+oBYM4cycKby9cX8PeXfbPmxd9zD+DtLfv/K6U/eVI6ynt4AHFx5o+NiIiIiMiVMYgnKicSEqSj+yefSEm9pSxqbuftLYE8UBDEJyfLj7Gx7CxPRERERFQSBvFE5cSUKdIQfvBg61zPoiAeALp2lW1sLAB9EN+hg2XjIiIiIiJyZVyoiagc8fKy3rUqV5at2UH82LHA/fcDLVtCUfSrzTGIJyIiIiIqGYN4IjKLxZl4d3egdWsAwB+/A5cuAX5+wL33Wmd8RERERESuiOX0RGQWi4P4QtRS+gcesG61ABERERGRq2EQT0RmsUUQz670RERERESlYxBPRGZRg/jLly27zu3bwMaNss/58EREREREpWMQT0RmsVYmftcu4Pp1oFIloGlTy8dFREREROTKGMQTkVmsFcSrpfTt2gFu/BuJiIiIiKhU/CczEZnF2kE8S+mJiIiIiMrGIJ6IzKIG8deuAXl55l0jKwvYulX2GcQTEREREZWNQTwRmaVSJdkqCvDvv+ZdY/NmICcHqF4dqFvXemMjIiIiInJVDOKJyCweHkBwsOybW1JfuJRep7POuIiIiIiIXBmDeCIym6Xz4jkfnoiIiIjINAziichslgTx164Be/fKfvv21hsTEREREZErYxBPRGZTg/jLl01/bUqKzKdv2BCIjLTqsIiIiIiIXBaDeCIymyWZ+M8/l21cnPXGQ0RERETk6hjEE5HZzA3i9+wBVq0C3NyA4cOtPy4iIiIiIlfFIJ6IzGZuED9pkmyfeELK6YmIiIiIyDgM4onIbOYE8du2AWvWAO7uwOTJthkXEREREZGrYhBPRGYzJ4ifOFG2iYlA3brWHxMRERERkStjEE9EZqtcWbbGBvEpKbI2vKcn8NprNhsWEREREZHLYhBPRGYzJROvKPos/LBhQM2athsXEREREZGrYhBPRGZTg/iMDCAnp/Rzf/4Z2LwZ8PEBXnnF9mMjIiIiInJFDOKJyGxBQdKgDgCuXi35vMJZ+BEjgMhI24+NiIiIiMgVMYgnIrO5uQGVKsn+5csln3fmDLBrF+DhAYwfb5+xERERERG5IgbxRGQRY+bFHzwo24YNgSpVbD8mIiIiIiJXxSCeiCxiTBD/+++yjYmx/XiIiIiIiFwZg3gisogpmfgmTWw/HiIiIiIiV8Ygnogswkw8EREREZH9MIgnIotUrizbkoL47GzgyBHZZyaeiIiIiMgyDOKJyCJlZeIPHwby8oDgYKBqVfuNi4iIiIjIFTGIJyKLlBXEF54Pr9PZZ0xERERERK6KQTwRWaSsIJ7z4YmIiIiIrIdBPBFZRA3iL18u/nl2piciIiIish4G8URkEbWx3aVLQE5O0eeZiSciIiIish4G8URkkerVgbAwCeB/+83wuStXgAsXZL9xY/uPjYiIiIjI1WgexM+bNw9RUVHw8fFBbGwsdu7cWeK5CxYswAMPPIDg4GAEBwcjLi6uyPmDBg2CTqczeCQkJNj61yAqt9zcgM6dZf+HHwyfU7PwtWsDgYH2HRcRERERkSvSNIj/6quvMHbsWEyePBl79+5F06ZNER8fj0uXLhV7fkpKCvr27Ytff/0V27ZtQ/Xq1fHwww/j3LlzBuclJCTgwoULBY/ly5fb49chKre6dJHtDz8AiqI/zvnwRERERETWpWkQP2vWLAwdOhSJiYlo1KgR5s+fDz8/PyxatKjY85ctW4YRI0agWbNmaNCgARYuXIj8/HwkJycbnOft7Y3w8PCCR3BwsD1+HaJyq2NHwNsbOHkS+PNP/XHOhyciIiIisi7NgvicnBzs2bMHcXFx+sG4uSEuLg7btm0z6hpZWVnIzc1FSEiIwfGUlBRUqVIF9evXx/Dhw3H16tVSr5OdnY2MjAyDBxEZz98f6NBB9guX1DMTT0RERERkXZoF8VeuXEFeXh7CwsIMjoeFhSE1NdWoa4wfPx6RkZEGNwISEhLw2WefITk5GdOnT8fGjRvRqVMn5OXllXidpKQkBAUFFTyqV69u3i9FVI4VLqkHgLw84NAh2WcQT0RERERkHR5aD8Bc06ZNw5dffomUlBT4+PgUHH/88ccL9ps0aYKYmBjUqVMHKSkp6KCmCu8wYcIEjB07tuDnjIwMBvJEJnrkEWD4cGDbNlkzPi0NyMoCfHyAunW1Hh0RERERkWvQLBMfGhoKd3d3XLx40eD4xYsXER4eXupr3333XUybNg0///wzYsqYbFu7dm2Ehobi2LFjJZ7j7e2NChUqGDyIyDTVqgF33y2N7das0c+Hb9QI8HDa24VERERERI5FsyDey8sLLVq0MGhKpzapa926dYmvmzFjBt58802sXbsWLVu2LPN9zp49i6tXryIiIsIq4yaikhUuqVfnw7OpHRERERGR9WjanX7s2LFYsGABlixZgsOHD2P48OHIzMxEYmIiAGDgwIGYMGFCwfnTp0/HxIkTsWjRIkRFRSE1NRWpqam4ceMGAODGjRt48cUXsX37dpw6dQrJycno1q0b6tati/j4eE1+R6LyRA3i160Ddu+Wfc6HJyIiIiKyHk2LXPv06YPLly9j0qRJSE1NRbNmzbB27dqCZndnzpyBm5v+PsNHH32EnJwc9O7d2+A6kydPxpQpU+Du7o6DBw9iyZIlSEtLQ2RkJB5++GG8+eab8Pb2tuvvRlQeNW8OREQAFy4AP/0kx5iJJyIiIiKyHp2iKIrWg3A0GRkZCAoKQnp6OufHE5lo2DBgwQL9z6mpwB2LUBARERER0R2MjUM1LacnItejltQDQJUqDOCJiIiIiKyJQTwRWVWHDrKsHMD58ERERERE1sYgnoisys8PiIuTfc6HJyIiIiKyLq7eTERW9/bbEsyPHq31SIiIiIiIXAuDeCKyuiZNgK++0noURERERESuh+X0RERERERERE6CQTwRERERERGRk2AQT0REREREROQkGMQTEREREREROQkG8UREREREREROgkE8ERERERERkZNgEE9ERERERETkJBjEExERERERETkJBvFEREREREREToJBPBEREREREZGTYBBPRERERERE5CQYxBMRERERERE5CQbxRERERERERE6CQTwRERERERGRk2AQT0REREREROQkGMQTEREREREROQkG8UREREREREROgkE8ERERERERkZPw0HoAjkhRFABARkaGxiMhIiIiIiKi8kCNP9V4tCQM4otx/fp1AED16tU1HgkRERERERGVJ9evX0dQUFCJz+uUssL8cig/Px/nz59HYGAgdDqd1sMpUUZGBqpXr45//vkHFSpU0Ho4ZAZ+hs6Pn6Hz42fo/PgZOj9+hs6Pn6Hz42eoPUVRcP36dURGRsLNreSZ78zEF8PNzQ3VqlXTehhGq1ChAr9oTo6fofPjZ+j8+Bk6P36Gzo+fofPjZ+j8+Blqq7QMvIqN7YiIiIiIiIicBIN4IiIiIiIiIifBIN6JeXt7Y/LkyfD29tZ6KGQmfobOj5+h8+Nn6Pz4GTo/fobOj5+h8+Nn6DzY2I6IiIiIiIjISTATT0REREREROQkGMQTEREREREROQkG8UREREREREROgkE8ERERERERkZNgEO/E5s2bh6ioKPj4+CA2NhY7d+7UekhUjKSkJNxzzz0IDAxElSpV0L17dxw5csTgnIceegg6nc7g8cwzz2g0YirOlClTinxGDRo0KHj+1q1bGDlyJCpVqoSAgAD06tULFy9e1HDEdKeoqKgin6FOp8PIkSMB8HvoiH777Td06dIFkZGR0Ol0+O677wyeVxQFkyZNQkREBHx9fREXF4ejR48anHPt2jX069cPFSpUQMWKFTFkyBDcuHHDjr9F+VbaZ5ibm4vx48ejSZMm8Pf3R2RkJAYOHIjz588bXKO47+60adPs/JuUX2V9DwcNGlTk80lISDA4h99DbZX1GRb3/0adTod33nmn4Bx+Dx0Lg3gn9dVXX2Hs2LGYPHky9u7di6ZNmyI+Ph6XLl3Semh0h40bN2LkyJHYvn071q9fj9zcXDz88MPIzMw0OG/o0KG4cOFCwWPGjBkajZhK0rhxY4PPaPPmzQXPPf/88/jhhx+wYsUKbNy4EefPn0fPnj01HC3dadeuXQaf3/r16wEAjz76aME5/B46lszMTDRt2hTz5s0r9vkZM2Zgzpw5mD9/Pnbs2AF/f3/Ex8fj1q1bBef069cPhw4dwvr167F69Wr89ttvGDZsmL1+hXKvtM8wKysLe/fuxcSJE7F3716sXLkSR44cQdeuXYuc+8Ybbxh8N5999ll7DJ9Q9vcQABISEgw+n+XLlxs8z++htsr6DAt/dhcuXMCiRYug0+nQq1cvg/P4PXQgCjmlVq1aKSNHjiz4OS8vT4mMjFSSkpI0HBUZ49KlSwoAZePGjQXHHnzwQWX06NHaDYrKNHnyZKVp06bFPpeWlqZ4enoqK1asKDh2+PBhBYCybds2O42QTDV69GilTp06Sn5+vqIo/B46OgDKqlWrCn7Oz89XwsPDlXfeeafgWFpamuLt7a0sX75cURRF+fPPPxUAyq5duwrO+emnnxSdTqecO3fObmMncednWJydO3cqAJTTp08XHKtZs6by3nvv2XZwZJTiPsMnn3xS6datW4mv4ffQsRjzPezWrZvSvn17g2P8HjoWZuKdUE5ODvbs2YO4uLiCY25uboiLi8O2bds0HBkZIz09HQAQEhJicHzZsmUIDQ1FdHQ0JkyYgKysLC2GR6U4evQoIiMjUbt2bfTr1w9nzpwBAOzZswe5ubkG38kGDRqgRo0a/E46qJycHHz++ecYPHgwdDpdwXF+D53HyZMnkZqaavC9CwoKQmxsbMH3btu2bahYsSJatmxZcE5cXBzc3NywY8cOu4+Zypaeng6dToeKFSsaHJ82bRoqVaqEu+++G++88w5u376tzQCpWCkpKahSpQrq16+P4cOH4+rVqwXP8XvoXC5evIgff/wRQ4YMKfIcv4eOw0PrAZDprly5gry8PISFhRkcDwsLw19//aXRqMgY+fn5GDNmDNq0aYPo6OiC40888QRq1qyJyMhIHDx4EOPHj8eRI0ewcuVKDUdLhcXGxuLTTz9F/fr1ceHCBbz++ut44IEH8McffyA1NRVeXl5F/tEZFhaG1NRUbQZMpfruu++QlpaGQYMGFRzj99C5qN+t4v5fqD6XmpqKKlWqGDzv4eGBkJAQfjcd0K1btzB+/Hj07dsXFSpUKDj+3HPPoXnz5ggJCcHWrVsxYcIEXLhwAbNmzdJwtKRKSEhAz549UatWLRw/fhyvvPIKOnXqhG3btsHd3Z3fQyezZMkSBAYGFpkSyO+hY2EQT2RHI0eOxB9//GEwlxqAwbywJk2aICIiAh06dMDx48dRp04dew+TitGpU6eC/ZiYGMTGxqJmzZr4+uuv4evrq+HIyByffPIJOnXqhMjIyIJj/B4SaSc3NxePPfYYFEXBRx99ZPDc2LFjC/ZjYmLg5eWFp59+GklJSfD29rb3UOkOjz/+eMF+kyZNEBMTgzp16iAlJQUdOnTQcGRkjkWLFqFfv37w8fExOM7voWNhOb0TCg0Nhbu7e5HO1xcvXkR4eLhGo6KyjBo1CqtXr8avv/6KatWqlXpubGwsAODYsWP2GBqZoWLFiqhXrx6OHTuG8PBw5OTkIC0tzeAcficd0+nTp/HLL7/gqaeeKvU8fg8dm/rdKu3/heHh4UUavt6+fRvXrl3jd9OBqAH86dOnsX79eoMsfHFiY2Nx+/ZtnDp1yj4DJJPUrl0boaGhBX938nvoPDZt2oQjR46U+f9HgN9DrTGId0JeXl5o0aIFkpOTC47l5+cjOTkZrVu31nBkVBxFUTBq1CisWrUKGzZsQK1atcp8zf79+wEAERERNh4dmevGjRs4fvw4IiIi0KJFC3h6ehp8J48cOYIzZ87wO+mAFi9ejCpVqqBz586lnsfvoWOrVasWwsPDDb53GRkZ2LFjR8H3rnXr1khLS8OePXsKztmwYQPy8/MLbtKQttQA/ujRo/jll19QqVKlMl+zf/9+uLm5FSnRJsdw9uxZXL16teDvTn4Pnccnn3yCFi1aoGnTpmWey++htlhO76TGjh2LJ598Ei1btkSrVq0we/ZsZGZmIjExUeuh0R1GjhyJL774At9//z0CAwML5n8FBQXB19cXx48fxxdffIH//Oc/qFSpEg4ePIjnn38ebdu2RUxMjMajJ9ULL7yALl26oGbNmjh//jwmT54Md3d39O3bF0FBQRgyZAjGjh2LkJAQVKhQAc8++yxat26Ne++9V+uhUyH5+flYvHgxnnzySXh46P8XyO+hY7px44ZBJcTJkyexf/9+hISEoEaNGhgzZgymTp2Ku+66C7Vq1cLEiRMRGRmJ7t27AwAaNmyIhIQEDB06FPPnz0dubi5GjRqFxx9/3GAqBdlOaZ9hREQEevfujb1792L16tXIy8sr+H9kSEgIvLy8sG3bNuzYsQPt2rVDYGAgtm3bhueffx79+/dHcHCwVr9WuVLaZxgSEoLXX38dvXr1Qnh4OI4fP46XXnoJdevWRXx8PAB+Dx1BWX+XAnITdMWKFZg5c2aR1/N76IC0bo9P5ps7d65So0YNxcvLS2nVqpWyfft2rYdExQBQ7GPx4sWKoijKmTNnlLZt2yohISGKt7e3UrduXeXFF19U0tPTtR04GejTp48SERGheHl5KVWrVlX69OmjHDt2rOD5mzdvKiNGjFCCg4MVPz8/pUePHsqFCxc0HDEVZ926dQoA5ciRIwbH+T10TL/++muxf38++eSTiqLIMnMTJ05UwsLCFG9vb6VDhw5FPturV68qffv2VQICApQKFSooiYmJyvXr1zX4bcqn0j7DkydPlvj/yF9//VVRFEXZs2ePEhsbqwQFBSk+Pj5Kw4YNlbffflu5deuWtr9YOVLaZ5iVlaU8/PDDSuXKlRVPT0+lZs2aytChQ5XU1FSDa/B7qK2y/i5VFEX5+OOPFV9fXyUtLa3I6/k9dDw6RVEUm98pICIiIiIiIiKLcU48ERERERERkZNgEE9ERERERETkJBjEExERERERETkJBvFEREREREREToJBPBEREREREZGTYBBPRERERERE5CQYxBMRERERERE5CQbxRERERERERE6CQTwRERGZRafT4bvvvtN6GJgyZQqaNWum9TCIiIjsgkE8ERGRg7p8+TKGDx+OGjVqwNvbG+Hh4YiPj8eWLVu0HppVnDp1CjqdDvv379d6KERERE7DQ+sBEBERUfF69eqFnJwcLFmyBLVr18bFixeRnJyMq1evaj00IiIi0ggz8URERA4oLS0NmzZtwvTp09GuXTvUrFkTrVq1woQJE9C1a9eC82bNmoUmTZrA398f1atXx4gRI3Djxo2C5z/99FNUrFgRq1evRv369eHn54fevXsjKysLS5YsQVRUFIKDg/Hcc88hLy+v4HVRUVF488030bdvX/j7+6Nq1aqYN29eqWP+559/8Nhjj6FixYoICQlBt27dcOrUKaN/55SUFOh0OiQnJ6Nly5bw8/PDfffdhyNHjhicN23aNISFhSEwMBBDhgzBrVu3ilxr4cKFaNiwIXx8fNCgQQN8+OGHBc8NHjwYMTExyM7OBgDk5OTg7rvvxsCBA40eKxERkVYYxBMRETmggIAABAQE4LvvvisINovj5uaGOXPm4NChQ1iyZAk2bNiAl156yeCcrKwszJkzB19++SXWrl2LlJQU9OjRA2vWrMGaNWuwdOlSfPzxx/jmm28MXvfOO++gadOm2LdvH15++WWMHj0a69evL3Ycubm5iI+PR2BgIDZt2oQtW7YgICAACQkJyMnJMel3f/XVVzFz5kzs3r0bHh4eGDx4cMFzX3/9NaZMmYK3334bu3fvRkREhEGADgDLli3DpEmT8NZbb+Hw4cN4++23MXHiRCxZsgQAMGfOHGRmZuLll18ueL+0tDR88MEHJo2TiIhIEwoRERE5pG+++UYJDg5WfHx8lPvuu0+ZMGGCcuDAgVJfs2LFCqVSpUoFPy9evFgBoBw7dqzg2NNPP634+fkp169fLzgWHx+vPP300wU/16xZU0lISDC4dp8+fZROnToV/AxAWbVqlaIoirJ06VKlfv36Sn5+fsHz2dnZiq+vr7Ju3bpix3ry5EkFgLJv3z5FURTl119/VQAov/zyS8E5P/74owJAuXnzpqIoitK6dWtlxIgRBteJjY1VmjZtWvBznTp1lC+++MLgnDfffFNp3bp1wc9bt25VPD09lYkTJyoeHh7Kpk2bih0jERGRo2EmnoiIyEH16tUL58+fx3//+18kJCQgJSUFzZs3x6efflpwzi+//IIOHTqgatWqCAwMxIABA3D16lVkZWUVnOPn54c6deoU/BwWFoaoqCgEBAQYHLt06ZLB+7du3brIz4cPHy52rAcOHMCxY8cQGBhYUEUQEhKCW7du4fjx4yb93jExMQX7ERERAFAwtsOHDyM2NrbEcWZmZuL48eMYMmRIwTgCAgIwdepUg3G0bt0aL7zwAt58802MGzcO999/v0ljJCIi0gob2xERETkwHx8fdOzYER07dsTEiRPx1FNPYfLkyRg0aBBOnTqFRx55BMOHD8dbb72FkJAQbN68GUOGDEFOTg78/PwAAJ6engbX1Ol0xR7Lz883e5w3btxAixYtsGzZsiLPVa5c2aRrFR6bTqcDAKPHpvYDWLBgQZFg393dvWA/Pz8fW7Zsgbu7O44dO2bS+IiIiLTETDwREZETadSoETIzMwEAe/bsQX5+PmbOnIl7770X9erVw/nz5632Xtu3by/yc8OGDYs9t3nz5jh69CiqVKmCunXrGjyCgoKsNqaGDRtix44dJY4zLCwMkZGROHHiRJFx1KpVq+C8d955B3/99Rc2btyItWvXYvHixVYbIxERkS0xiCciInJAV69eRfv27fH555/j4MGDOHnyJFasWIEZM2agW7duAIC6desiNzcXc+fOxYkTJ7B06VLMnz/famPYsmULZsyYgb///hvz5s3DihUrMHr06GLP7devH0JDQ9GtWzds2rQJJ0+eREpKCp577jmcPXvWamMaPXo0Fi1ahMWLF+Pvv//G5MmTcejQIYNzXn/9dSQlJWHOnDn4+++/8fvvv2Px4sWYNWsWAGDfvn2YNGkSFi5ciDZt2mDWrFkYPXo0Tpw4YbVxEhER2QqDeCIiIgcUEBCA2NhYvPfee2jbti2io6MxceJEDB06tKCLetOmTTFr1ixMnz4d0dHRWLZsGZKSkqw2hnHjxmH37t24++67MXXqVMyaNQvx8fHFnuvn54fffvsNNWrUQM+ePdGwYcOC5d8qVKhgtTH16dMHEydOxEsvvYQWLVrg9OnTGD58uME5Tz31FBYuXIjFixejSZMmePDBB/Hpp5+iVq1auHXrFvr3749BgwahS5cuAIBhw4ahXbt2GDBggMEye0RERI5IpyiKovUgiIiIyLFERUVhzJgxGDNmjNZDISIiokKYiSciIiIiIiJyEgziiYiIiIiIiJwEy+mJiIiIiIiInAQz8UREREREREROgkE8ERERERERkZNgEE9ERERERETkJBjEExERERERETkJBvFEREREREREToJBPBEREREREZGTYBBPRERERERE5CQYxBMRERERERE5if8HFK1/6LkFpk4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Volume - MSE: 0.0023, MAE: 0.0422, R: -2.1018\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Metrics (All Features):\n",
      "Mean Squared Error (MSE): 0.0019\n",
      "Mean Absolute Error (MAE): 0.0375\n",
      "R Score: -0.0113\n"
     ]
    }
   ],
   "source": [
    "# Load Best Model\n",
    "best_lr, best_optimizer_name, best_hidden_size, best_num_layers, best_dropout = best_params_RNN\n",
    "model = MyRNN(\n",
    "    input_size=5,\n",
    "    hidden_size=best_hidden_size,\n",
    "    num_layers=best_num_layers,\n",
    "    output_size=5,\n",
    "    dropout=best_dropout\n",
    ").to(device)\n",
    "\n",
    "# Load the saved state_dict\n",
    "model.load_state_dict(torch.load(\"best_model_RNN.pth\"))\n",
    "model.eval()\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions_tensor = model(X_test_tensor)\n",
    "predictions = predictions_tensor.cpu().numpy()\n",
    "y_test = y_test_tensor.cpu().numpy()\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    mse = mean_squared_error(y_test[:, i], predictions[:, i])\n",
    "    mae = mean_absolute_error(y_test[:, i], predictions[:, i])\n",
    "    r2 = r2_score(y_test[:, i], predictions[:, i])\n",
    "    print(f\"{feature} - MSE: {mse:.4f}, MAE: {mae:.4f}, R: {r2:.4f}\")\n",
    "\n",
    "    # Plot RNN Charts\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(y_test[:, i], label=f\"True {feature}\", color=\"blue\")\n",
    "    plt.plot(predictions[:, i], label=f\"Predicted {feature}\", color=\"red\")\n",
    "    plt.title(f\"Predicted vs True {feature}\")\n",
    "    plt.xlabel(\"Sample Index\")\n",
    "    plt.ylabel(f\"{feature} Value\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "mse_total = mean_squared_error(y_test, predictions)\n",
    "mae_total = mean_absolute_error(y_test, predictions)\n",
    "r2_total = r2_score(y_test, predictions)\n",
    "\n",
    "print(\"\\nOverall Metrics (All Features):\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_total:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_total:.4f}\")\n",
    "print(f\"R Score: {r2_total:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=16, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0585, Val Loss: 0.0221\n",
      "Epoch [2/50], Train Loss: 0.0485, Val Loss: 0.0274\n",
      "Epoch [3/50], Train Loss: 0.0252, Val Loss: 0.0145\n",
      "Epoch [4/50], Train Loss: 0.0132, Val Loss: 0.0051\n",
      "Epoch [5/50], Train Loss: 0.0072, Val Loss: 0.0155\n",
      "Epoch [6/50], Train Loss: 0.0036, Val Loss: 0.0078\n",
      "Epoch [7/50], Train Loss: 0.0039, Val Loss: 0.0051\n",
      "Epoch [8/50], Train Loss: 0.0031, Val Loss: 0.0033\n",
      "Epoch [9/50], Train Loss: 0.0032, Val Loss: 0.0128\n",
      "Epoch [10/50], Train Loss: 0.0024, Val Loss: 0.0058\n",
      "Epoch [11/50], Train Loss: 0.0031, Val Loss: 0.0025\n",
      "Epoch [12/50], Train Loss: 0.0029, Val Loss: 0.0031\n",
      "Epoch [13/50], Train Loss: 0.0039, Val Loss: 0.0171\n",
      "Epoch [14/50], Train Loss: 0.0025, Val Loss: 0.0058\n",
      "Epoch [15/50], Train Loss: 0.0034, Val Loss: 0.0029\n",
      "Epoch [16/50], Train Loss: 0.0028, Val Loss: 0.0017\n",
      "Epoch [17/50], Train Loss: 0.0039, Val Loss: 0.0164\n",
      "Epoch [18/50], Train Loss: 0.0028, Val Loss: 0.0065\n",
      "Epoch [19/50], Train Loss: 0.0038, Val Loss: 0.0032\n",
      "Epoch [20/50], Train Loss: 0.0028, Val Loss: 0.0020\n",
      "Epoch [21/50], Train Loss: 0.0036, Val Loss: 0.0134\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=16, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0614, Val Loss: 0.0568\n",
      "Epoch [2/50], Train Loss: 0.0580, Val Loss: 0.0206\n",
      "Epoch [3/50], Train Loss: 0.0338, Val Loss: 0.0162\n",
      "Epoch [4/50], Train Loss: 0.0197, Val Loss: 0.0071\n",
      "Epoch [5/50], Train Loss: 0.0111, Val Loss: 0.0037\n",
      "Epoch [6/50], Train Loss: 0.0101, Val Loss: 0.0162\n",
      "Epoch [7/50], Train Loss: 0.0080, Val Loss: 0.0037\n",
      "Epoch [8/50], Train Loss: 0.0097, Val Loss: 0.0060\n",
      "Epoch [9/50], Train Loss: 0.0087, Val Loss: 0.0054\n",
      "Epoch [10/50], Train Loss: 0.0079, Val Loss: 0.0153\n",
      "Epoch [11/50], Train Loss: 0.0079, Val Loss: 0.0147\n",
      "Epoch [12/50], Train Loss: 0.0076, Val Loss: 0.0021\n",
      "Epoch [13/50], Train Loss: 0.0112, Val Loss: 0.0089\n",
      "Epoch [14/50], Train Loss: 0.0065, Val Loss: 0.0097\n",
      "Epoch [15/50], Train Loss: 0.0056, Val Loss: 0.0068\n",
      "Epoch [16/50], Train Loss: 0.0069, Val Loss: 0.0051\n",
      "Epoch [17/50], Train Loss: 0.0059, Val Loss: 0.0013\n",
      "Epoch [18/50], Train Loss: 0.0053, Val Loss: 0.0045\n",
      "Epoch [19/50], Train Loss: 0.0050, Val Loss: 0.0056\n",
      "Epoch [20/50], Train Loss: 0.0057, Val Loss: 0.0021\n",
      "Epoch [21/50], Train Loss: 0.0044, Val Loss: 0.0014\n",
      "Epoch [22/50], Train Loss: 0.0045, Val Loss: 0.0028\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=16, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0662, Val Loss: 0.0508\n",
      "Epoch [2/50], Train Loss: 0.0576, Val Loss: 0.0334\n",
      "Epoch [3/50], Train Loss: 0.0366, Val Loss: 0.0210\n",
      "Epoch [4/50], Train Loss: 0.0228, Val Loss: 0.0093\n",
      "Epoch [5/50], Train Loss: 0.0155, Val Loss: 0.0089\n",
      "Epoch [6/50], Train Loss: 0.0132, Val Loss: 0.0098\n",
      "Epoch [7/50], Train Loss: 0.0127, Val Loss: 0.0055\n",
      "Epoch [8/50], Train Loss: 0.0109, Val Loss: 0.0015\n",
      "Epoch [9/50], Train Loss: 0.0098, Val Loss: 0.0047\n",
      "Epoch [10/50], Train Loss: 0.0087, Val Loss: 0.0104\n",
      "Epoch [11/50], Train Loss: 0.0079, Val Loss: 0.0032\n",
      "Epoch [12/50], Train Loss: 0.0093, Val Loss: 0.0014\n",
      "Epoch [13/50], Train Loss: 0.0075, Val Loss: 0.0026\n",
      "Epoch [14/50], Train Loss: 0.0077, Val Loss: 0.0085\n",
      "Epoch [15/50], Train Loss: 0.0071, Val Loss: 0.0066\n",
      "Epoch [16/50], Train Loss: 0.0068, Val Loss: 0.0040\n",
      "Epoch [17/50], Train Loss: 0.0068, Val Loss: 0.0060\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=16, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0337, Val Loss: 0.0843\n",
      "Epoch [2/50], Train Loss: 0.0605, Val Loss: 0.0163\n",
      "Epoch [3/50], Train Loss: 0.0515, Val Loss: 0.0093\n",
      "Epoch [4/50], Train Loss: 0.0323, Val Loss: 0.0099\n",
      "Epoch [5/50], Train Loss: 0.0086, Val Loss: 0.0042\n",
      "Epoch [6/50], Train Loss: 0.0081, Val Loss: 0.0146\n",
      "Epoch [7/50], Train Loss: 0.0048, Val Loss: 0.0132\n",
      "Epoch [8/50], Train Loss: 0.0031, Val Loss: 0.0050\n",
      "Epoch [9/50], Train Loss: 0.0029, Val Loss: 0.0028\n",
      "Epoch [10/50], Train Loss: 0.0044, Val Loss: 0.0158\n",
      "Epoch [11/50], Train Loss: 0.0024, Val Loss: 0.0038\n",
      "Epoch [12/50], Train Loss: 0.0035, Val Loss: 0.0027\n",
      "Epoch [13/50], Train Loss: 0.0039, Val Loss: 0.0070\n",
      "Epoch [14/50], Train Loss: 0.0035, Val Loss: 0.0143\n",
      "Epoch [15/50], Train Loss: 0.0035, Val Loss: 0.0044\n",
      "Epoch [16/50], Train Loss: 0.0036, Val Loss: 0.0019\n",
      "Epoch [17/50], Train Loss: 0.0029, Val Loss: 0.0090\n",
      "Epoch [18/50], Train Loss: 0.0027, Val Loss: 0.0072\n",
      "Epoch [19/50], Train Loss: 0.0021, Val Loss: 0.0021\n",
      "Epoch [20/50], Train Loss: 0.0022, Val Loss: 0.0016\n",
      "Epoch [21/50], Train Loss: 0.0033, Val Loss: 0.0099\n",
      "Epoch [22/50], Train Loss: 0.0021, Val Loss: 0.0028\n",
      "Epoch [23/50], Train Loss: 0.0025, Val Loss: 0.0013\n",
      "Epoch [24/50], Train Loss: 0.0031, Val Loss: 0.0117\n",
      "Epoch [25/50], Train Loss: 0.0028, Val Loss: 0.0059\n",
      "Epoch [26/50], Train Loss: 0.0036, Val Loss: 0.0089\n",
      "Epoch [27/50], Train Loss: 0.0040, Val Loss: 0.0180\n",
      "Epoch [28/50], Train Loss: 0.0034, Val Loss: 0.0126\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=16, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0299, Val Loss: 0.1095\n",
      "Epoch [2/50], Train Loss: 0.0645, Val Loss: 0.0265\n",
      "Epoch [3/50], Train Loss: 0.0373, Val Loss: 0.0076\n",
      "Epoch [4/50], Train Loss: 0.0223, Val Loss: 0.0106\n",
      "Epoch [5/50], Train Loss: 0.0144, Val Loss: 0.0113\n",
      "Epoch [6/50], Train Loss: 0.0092, Val Loss: 0.0171\n",
      "Epoch [7/50], Train Loss: 0.0075, Val Loss: 0.0043\n",
      "Epoch [8/50], Train Loss: 0.0074, Val Loss: 0.0039\n",
      "Epoch [9/50], Train Loss: 0.0087, Val Loss: 0.0157\n",
      "Epoch [10/50], Train Loss: 0.0086, Val Loss: 0.0049\n",
      "Epoch [11/50], Train Loss: 0.0074, Val Loss: 0.0042\n",
      "Epoch [12/50], Train Loss: 0.0071, Val Loss: 0.0187\n",
      "Epoch [13/50], Train Loss: 0.0061, Val Loss: 0.0032\n",
      "Epoch [14/50], Train Loss: 0.0065, Val Loss: 0.0031\n",
      "Epoch [15/50], Train Loss: 0.0058, Val Loss: 0.0174\n",
      "Epoch [16/50], Train Loss: 0.0065, Val Loss: 0.0033\n",
      "Epoch [17/50], Train Loss: 0.0051, Val Loss: 0.0031\n",
      "Epoch [18/50], Train Loss: 0.0062, Val Loss: 0.0255\n",
      "Epoch [19/50], Train Loss: 0.0061, Val Loss: 0.0064\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=16, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0501, Val Loss: 0.0783\n",
      "Epoch [2/50], Train Loss: 0.0630, Val Loss: 0.0133\n",
      "Epoch [3/50], Train Loss: 0.0499, Val Loss: 0.0088\n",
      "Epoch [4/50], Train Loss: 0.0367, Val Loss: 0.0068\n",
      "Epoch [5/50], Train Loss: 0.0233, Val Loss: 0.0026\n",
      "Epoch [6/50], Train Loss: 0.0204, Val Loss: 0.0521\n",
      "Epoch [7/50], Train Loss: 0.0207, Val Loss: 0.0148\n",
      "Epoch [8/50], Train Loss: 0.0196, Val Loss: 0.0146\n",
      "Epoch [9/50], Train Loss: 0.0140, Val Loss: 0.0352\n",
      "Epoch [10/50], Train Loss: 0.0142, Val Loss: 0.0050\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=16, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0200, Val Loss: 0.0801\n",
      "Epoch [2/50], Train Loss: 0.0670, Val Loss: 0.0372\n",
      "Epoch [3/50], Train Loss: 0.0562, Val Loss: 0.0414\n",
      "Epoch [4/50], Train Loss: 0.0444, Val Loss: 0.0242\n",
      "Epoch [5/50], Train Loss: 0.0356, Val Loss: 0.0099\n",
      "Epoch [6/50], Train Loss: 0.0229, Val Loss: 0.0083\n",
      "Epoch [7/50], Train Loss: 0.0203, Val Loss: 0.0039\n",
      "Epoch [8/50], Train Loss: 0.0176, Val Loss: 0.0318\n",
      "Epoch [9/50], Train Loss: 0.0121, Val Loss: 0.0065\n",
      "Epoch [10/50], Train Loss: 0.0096, Val Loss: 0.0130\n",
      "Epoch [11/50], Train Loss: 0.0104, Val Loss: 0.0146\n",
      "Epoch [12/50], Train Loss: 0.0083, Val Loss: 0.0126\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=16, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0334, Val Loss: 0.1000\n",
      "Epoch [2/50], Train Loss: 0.0620, Val Loss: 0.0257\n",
      "Epoch [3/50], Train Loss: 0.0385, Val Loss: 0.0120\n",
      "Epoch [4/50], Train Loss: 0.0240, Val Loss: 0.0061\n",
      "Epoch [5/50], Train Loss: 0.0157, Val Loss: 0.0133\n",
      "Epoch [6/50], Train Loss: 0.0127, Val Loss: 0.0047\n",
      "Epoch [7/50], Train Loss: 0.0129, Val Loss: 0.0040\n",
      "Epoch [8/50], Train Loss: 0.0290, Val Loss: 0.0108\n",
      "Epoch [9/50], Train Loss: 0.0113, Val Loss: 0.0440\n",
      "Epoch [10/50], Train Loss: 0.0073, Val Loss: 0.0112\n",
      "Epoch [11/50], Train Loss: 0.0116, Val Loss: 0.0541\n",
      "Epoch [12/50], Train Loss: 0.0119, Val Loss: 0.0290\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=16, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0539, Val Loss: 0.1225\n",
      "Epoch [2/50], Train Loss: 0.0639, Val Loss: 0.0961\n",
      "Epoch [3/50], Train Loss: 0.0585, Val Loss: 0.0506\n",
      "Epoch [4/50], Train Loss: 0.0467, Val Loss: 0.0307\n",
      "Epoch [5/50], Train Loss: 0.0411, Val Loss: 0.0090\n",
      "Epoch [6/50], Train Loss: 0.0239, Val Loss: 0.0318\n",
      "Epoch [7/50], Train Loss: 0.0206, Val Loss: 0.0180\n",
      "Epoch [8/50], Train Loss: 0.0211, Val Loss: 0.0414\n",
      "Epoch [9/50], Train Loss: 0.0216, Val Loss: 0.0201\n",
      "Epoch [10/50], Train Loss: 0.0165, Val Loss: 0.0155\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=32, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0189, Val Loss: 0.0765\n",
      "Epoch [2/50], Train Loss: 0.0454, Val Loss: 0.0077\n",
      "Epoch [3/50], Train Loss: 0.0258, Val Loss: 0.0069\n",
      "Epoch [4/50], Train Loss: 0.0114, Val Loss: 0.0105\n",
      "Epoch [5/50], Train Loss: 0.0046, Val Loss: 0.0027\n",
      "Epoch [6/50], Train Loss: 0.0031, Val Loss: 0.0048\n",
      "Epoch [7/50], Train Loss: 0.0022, Val Loss: 0.0026\n",
      "Epoch [8/50], Train Loss: 0.0019, Val Loss: 0.0016\n",
      "Epoch [9/50], Train Loss: 0.0020, Val Loss: 0.0024\n",
      "Epoch [10/50], Train Loss: 0.0021, Val Loss: 0.0047\n",
      "Epoch [11/50], Train Loss: 0.0018, Val Loss: 0.0036\n",
      "Epoch [12/50], Train Loss: 0.0017, Val Loss: 0.0014\n",
      "Epoch [13/50], Train Loss: 0.0020, Val Loss: 0.0031\n",
      "Epoch [14/50], Train Loss: 0.0022, Val Loss: 0.0066\n",
      "Epoch [15/50], Train Loss: 0.0018, Val Loss: 0.0041\n",
      "Epoch [16/50], Train Loss: 0.0021, Val Loss: 0.0030\n",
      "Epoch [17/50], Train Loss: 0.0024, Val Loss: 0.0043\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=32, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0249, Val Loss: 0.0519\n",
      "Epoch [2/50], Train Loss: 0.0508, Val Loss: 0.0077\n",
      "Epoch [3/50], Train Loss: 0.0261, Val Loss: 0.0134\n",
      "Epoch [4/50], Train Loss: 0.0130, Val Loss: 0.0166\n",
      "Epoch [5/50], Train Loss: 0.0108, Val Loss: 0.0043\n",
      "Epoch [6/50], Train Loss: 0.0077, Val Loss: 0.0249\n",
      "Epoch [7/50], Train Loss: 0.0055, Val Loss: 0.0084\n",
      "Epoch [8/50], Train Loss: 0.0066, Val Loss: 0.0065\n",
      "Epoch [9/50], Train Loss: 0.0051, Val Loss: 0.0042\n",
      "Epoch [10/50], Train Loss: 0.0059, Val Loss: 0.0171\n",
      "Epoch [11/50], Train Loss: 0.0056, Val Loss: 0.0034\n",
      "Epoch [12/50], Train Loss: 0.0065, Val Loss: 0.0129\n",
      "Epoch [13/50], Train Loss: 0.0056, Val Loss: 0.0071\n",
      "Epoch [14/50], Train Loss: 0.0054, Val Loss: 0.0186\n",
      "Epoch [15/50], Train Loss: 0.0050, Val Loss: 0.0047\n",
      "Epoch [16/50], Train Loss: 0.0058, Val Loss: 0.0103\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=32, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0315, Val Loss: 0.0711\n",
      "Epoch [2/50], Train Loss: 0.0510, Val Loss: 0.0037\n",
      "Epoch [3/50], Train Loss: 0.0330, Val Loss: 0.0045\n",
      "Epoch [4/50], Train Loss: 0.0214, Val Loss: 0.0180\n",
      "Epoch [5/50], Train Loss: 0.0145, Val Loss: 0.0071\n",
      "Epoch [6/50], Train Loss: 0.0091, Val Loss: 0.0144\n",
      "Epoch [7/50], Train Loss: 0.0065, Val Loss: 0.0021\n",
      "Epoch [8/50], Train Loss: 0.0073, Val Loss: 0.0024\n",
      "Epoch [9/50], Train Loss: 0.0067, Val Loss: 0.0116\n",
      "Epoch [10/50], Train Loss: 0.0059, Val Loss: 0.0023\n",
      "Epoch [11/50], Train Loss: 0.0067, Val Loss: 0.0063\n",
      "Epoch [12/50], Train Loss: 0.0056, Val Loss: 0.0023\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=32, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0242, Val Loss: 0.0581\n",
      "Epoch [2/50], Train Loss: 0.0444, Val Loss: 0.0357\n",
      "Epoch [3/50], Train Loss: 0.0353, Val Loss: 0.0181\n",
      "Epoch [4/50], Train Loss: 0.0152, Val Loss: 0.0266\n",
      "Epoch [5/50], Train Loss: 0.0136, Val Loss: 0.0235\n",
      "Epoch [6/50], Train Loss: 0.0103, Val Loss: 0.0214\n",
      "Epoch [7/50], Train Loss: 0.0074, Val Loss: 0.0085\n",
      "Epoch [8/50], Train Loss: 0.0129, Val Loss: 0.0131\n",
      "Epoch [9/50], Train Loss: 0.0081, Val Loss: 0.0153\n",
      "Epoch [10/50], Train Loss: 0.0079, Val Loss: 0.0064\n",
      "Epoch [11/50], Train Loss: 0.0157, Val Loss: 0.0183\n",
      "Epoch [12/50], Train Loss: 0.0068, Val Loss: 0.0192\n",
      "Epoch [13/50], Train Loss: 0.0043, Val Loss: 0.0117\n",
      "Epoch [14/50], Train Loss: 0.0067, Val Loss: 0.0200\n",
      "Epoch [15/50], Train Loss: 0.0077, Val Loss: 0.0134\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=32, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0230, Val Loss: 0.0662\n",
      "Epoch [2/50], Train Loss: 0.0526, Val Loss: 0.0609\n",
      "Epoch [3/50], Train Loss: 0.0456, Val Loss: 0.0038\n",
      "Epoch [4/50], Train Loss: 0.0282, Val Loss: 0.0085\n",
      "Epoch [5/50], Train Loss: 0.0191, Val Loss: 0.0142\n",
      "Epoch [6/50], Train Loss: 0.0156, Val Loss: 0.0149\n",
      "Epoch [7/50], Train Loss: 0.0145, Val Loss: 0.0044\n",
      "Epoch [8/50], Train Loss: 0.0117, Val Loss: 0.0058\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=32, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0341, Val Loss: 0.0458\n",
      "Epoch [2/50], Train Loss: 0.0534, Val Loss: 0.0596\n",
      "Epoch [3/50], Train Loss: 0.0454, Val Loss: 0.0112\n",
      "Epoch [4/50], Train Loss: 0.0300, Val Loss: 0.0040\n",
      "Epoch [5/50], Train Loss: 0.0183, Val Loss: 0.0071\n",
      "Epoch [6/50], Train Loss: 0.0146, Val Loss: 0.0133\n",
      "Epoch [7/50], Train Loss: 0.0281, Val Loss: 0.0150\n",
      "Epoch [8/50], Train Loss: 0.0112, Val Loss: 0.0052\n",
      "Epoch [9/50], Train Loss: 0.0105, Val Loss: 0.0149\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=32, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0241, Val Loss: 0.0747\n",
      "Epoch [2/50], Train Loss: 0.0592, Val Loss: 0.0907\n",
      "Epoch [3/50], Train Loss: 0.0465, Val Loss: 0.0571\n",
      "Epoch [4/50], Train Loss: 0.0365, Val Loss: 0.0054\n",
      "Epoch [5/50], Train Loss: 0.0233, Val Loss: 0.0191\n",
      "Epoch [6/50], Train Loss: 0.0172, Val Loss: 0.0251\n",
      "Epoch [7/50], Train Loss: 0.0129, Val Loss: 0.0198\n",
      "Epoch [8/50], Train Loss: 0.0136, Val Loss: 0.0249\n",
      "Epoch [9/50], Train Loss: 0.0137, Val Loss: 0.0084\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=32, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0285, Val Loss: 0.0650\n",
      "Epoch [2/50], Train Loss: 0.0513, Val Loss: 0.0978\n",
      "Epoch [3/50], Train Loss: 0.0564, Val Loss: 0.0532\n",
      "Epoch [4/50], Train Loss: 0.0429, Val Loss: 0.0095\n",
      "Epoch [5/50], Train Loss: 0.0322, Val Loss: 0.0303\n",
      "Epoch [6/50], Train Loss: 0.0289, Val Loss: 0.0876\n",
      "Epoch [7/50], Train Loss: 0.0417, Val Loss: 0.0202\n",
      "Epoch [8/50], Train Loss: 0.0178, Val Loss: 0.0046\n",
      "Epoch [9/50], Train Loss: 0.0107, Val Loss: 0.0034\n",
      "Epoch [10/50], Train Loss: 0.0415, Val Loss: 0.0414\n",
      "Epoch [11/50], Train Loss: 0.0194, Val Loss: 0.0693\n",
      "Epoch [12/50], Train Loss: 0.0080, Val Loss: 0.0142\n",
      "Epoch [13/50], Train Loss: 0.0076, Val Loss: 0.0040\n",
      "Epoch [14/50], Train Loss: 0.0139, Val Loss: 0.0095\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=32, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0363, Val Loss: 0.0455\n",
      "Epoch [2/50], Train Loss: 0.0615, Val Loss: 0.0569\n",
      "Epoch [3/50], Train Loss: 0.0549, Val Loss: 0.0535\n",
      "Epoch [4/50], Train Loss: 0.0406, Val Loss: 0.0168\n",
      "Epoch [5/50], Train Loss: 0.0285, Val Loss: 0.0198\n",
      "Epoch [6/50], Train Loss: 0.0218, Val Loss: 0.0115\n",
      "Epoch [7/50], Train Loss: 0.0185, Val Loss: 0.0065\n",
      "Epoch [8/50], Train Loss: 0.0127, Val Loss: 0.0132\n",
      "Epoch [9/50], Train Loss: 0.0132, Val Loss: 0.0140\n",
      "Epoch [10/50], Train Loss: 0.0132, Val Loss: 0.0056\n",
      "Epoch [11/50], Train Loss: 0.0198, Val Loss: 0.0087\n",
      "Epoch [12/50], Train Loss: 0.0425, Val Loss: 0.0716\n",
      "Epoch [13/50], Train Loss: 0.0287, Val Loss: 0.0194\n",
      "Epoch [14/50], Train Loss: 0.0127, Val Loss: 0.0147\n",
      "Epoch [15/50], Train Loss: 0.0139, Val Loss: 0.0222\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=64, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0168, Val Loss: 0.0493\n",
      "Epoch [2/50], Train Loss: 0.0389, Val Loss: 0.0387\n",
      "Epoch [3/50], Train Loss: 0.0341, Val Loss: 0.0014\n",
      "Epoch [4/50], Train Loss: 0.0236, Val Loss: 0.0217\n",
      "Epoch [5/50], Train Loss: 0.0098, Val Loss: 0.0181\n",
      "Epoch [6/50], Train Loss: 0.0054, Val Loss: 0.0091\n",
      "Epoch [7/50], Train Loss: 0.0036, Val Loss: 0.0049\n",
      "Epoch [8/50], Train Loss: 0.0042, Val Loss: 0.0048\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=64, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0183, Val Loss: 0.0392\n",
      "Epoch [2/50], Train Loss: 0.0340, Val Loss: 0.0261\n",
      "Epoch [3/50], Train Loss: 0.0315, Val Loss: 0.0080\n",
      "Epoch [4/50], Train Loss: 0.0232, Val Loss: 0.0110\n",
      "Epoch [5/50], Train Loss: 0.0127, Val Loss: 0.0055\n",
      "Epoch [6/50], Train Loss: 0.0099, Val Loss: 0.0139\n",
      "Epoch [7/50], Train Loss: 0.0075, Val Loss: 0.0115\n",
      "Epoch [8/50], Train Loss: 0.0085, Val Loss: 0.0119\n",
      "Epoch [9/50], Train Loss: 0.0053, Val Loss: 0.0033\n",
      "Epoch [10/50], Train Loss: 0.0055, Val Loss: 0.0074\n",
      "Epoch [11/50], Train Loss: 0.0038, Val Loss: 0.0024\n",
      "Epoch [12/50], Train Loss: 0.0040, Val Loss: 0.0065\n",
      "Epoch [13/50], Train Loss: 0.0041, Val Loss: 0.0024\n",
      "Epoch [14/50], Train Loss: 0.0032, Val Loss: 0.0041\n",
      "Epoch [15/50], Train Loss: 0.0050, Val Loss: 0.0138\n",
      "Epoch [16/50], Train Loss: 0.0038, Val Loss: 0.0046\n",
      "Epoch [17/50], Train Loss: 0.0036, Val Loss: 0.0019\n",
      "Epoch [18/50], Train Loss: 0.0040, Val Loss: 0.0065\n",
      "Epoch [19/50], Train Loss: 0.0037, Val Loss: 0.0012\n",
      "Epoch [20/50], Train Loss: 0.0041, Val Loss: 0.0042\n",
      "Epoch [21/50], Train Loss: 0.0029, Val Loss: 0.0038\n",
      "Epoch [22/50], Train Loss: 0.0043, Val Loss: 0.0040\n",
      "Epoch [23/50], Train Loss: 0.0032, Val Loss: 0.0023\n",
      "Epoch [24/50], Train Loss: 0.0037, Val Loss: 0.0018\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=64, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0212, Val Loss: 0.0590\n",
      "Epoch [2/50], Train Loss: 0.0353, Val Loss: 0.0372\n",
      "Epoch [3/50], Train Loss: 0.0311, Val Loss: 0.0114\n",
      "Epoch [4/50], Train Loss: 0.0228, Val Loss: 0.0104\n",
      "Epoch [5/50], Train Loss: 0.0110, Val Loss: 0.0115\n",
      "Epoch [6/50], Train Loss: 0.0069, Val Loss: 0.0020\n",
      "Epoch [7/50], Train Loss: 0.0080, Val Loss: 0.0020\n",
      "Epoch [8/50], Train Loss: 0.0055, Val Loss: 0.0016\n",
      "Epoch [9/50], Train Loss: 0.0062, Val Loss: 0.0053\n",
      "Epoch [10/50], Train Loss: 0.0054, Val Loss: 0.0061\n",
      "Epoch [11/50], Train Loss: 0.0070, Val Loss: 0.0129\n",
      "Epoch [12/50], Train Loss: 0.0061, Val Loss: 0.0040\n",
      "Epoch [13/50], Train Loss: 0.0048, Val Loss: 0.0125\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=64, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0185, Val Loss: 0.0473\n",
      "Epoch [2/50], Train Loss: 0.0361, Val Loss: 0.0404\n",
      "Epoch [3/50], Train Loss: 0.0396, Val Loss: 0.0276\n",
      "Epoch [4/50], Train Loss: 0.0407, Val Loss: 0.0086\n",
      "Epoch [5/50], Train Loss: 0.0267, Val Loss: 0.0131\n",
      "Epoch [6/50], Train Loss: 0.0256, Val Loss: 0.0195\n",
      "Epoch [7/50], Train Loss: 0.0200, Val Loss: 0.0133\n",
      "Epoch [8/50], Train Loss: 0.0150, Val Loss: 0.0166\n",
      "Epoch [9/50], Train Loss: 0.0162, Val Loss: 0.0233\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=64, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0211, Val Loss: 0.0469\n",
      "Epoch [2/50], Train Loss: 0.0350, Val Loss: 0.0355\n",
      "Epoch [3/50], Train Loss: 0.0309, Val Loss: 0.0312\n",
      "Epoch [4/50], Train Loss: 0.0301, Val Loss: 0.0146\n",
      "Epoch [5/50], Train Loss: 0.0294, Val Loss: 0.0138\n",
      "Epoch [6/50], Train Loss: 0.0191, Val Loss: 0.0327\n",
      "Epoch [7/50], Train Loss: 0.0105, Val Loss: 0.0073\n",
      "Epoch [8/50], Train Loss: 0.0071, Val Loss: 0.0035\n",
      "Epoch [9/50], Train Loss: 0.0099, Val Loss: 0.0057\n",
      "Epoch [10/50], Train Loss: 0.0095, Val Loss: 0.0050\n",
      "Epoch [11/50], Train Loss: 0.0118, Val Loss: 0.0055\n",
      "Epoch [12/50], Train Loss: 0.0191, Val Loss: 0.0172\n",
      "Epoch [13/50], Train Loss: 0.0108, Val Loss: 0.0100\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=64, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0262, Val Loss: 0.0446\n",
      "Epoch [2/50], Train Loss: 0.0386, Val Loss: 0.0228\n",
      "Epoch [3/50], Train Loss: 0.0449, Val Loss: 0.0306\n",
      "Epoch [4/50], Train Loss: 0.0371, Val Loss: 0.0030\n",
      "Epoch [5/50], Train Loss: 0.0255, Val Loss: 0.0174\n",
      "Epoch [6/50], Train Loss: 0.0133, Val Loss: 0.0050\n",
      "Epoch [7/50], Train Loss: 0.0087, Val Loss: 0.0062\n",
      "Epoch [8/50], Train Loss: 0.0313, Val Loss: 0.0032\n",
      "Epoch [9/50], Train Loss: 0.0152, Val Loss: 0.0081\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=64, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0221, Val Loss: 0.0259\n",
      "Epoch [2/50], Train Loss: 0.0374, Val Loss: 0.0450\n",
      "Epoch [3/50], Train Loss: 0.0360, Val Loss: 0.0336\n",
      "Epoch [4/50], Train Loss: 0.0387, Val Loss: 0.0276\n",
      "Epoch [5/50], Train Loss: 0.0243, Val Loss: 0.0200\n",
      "Epoch [6/50], Train Loss: 0.0242, Val Loss: 0.0126\n",
      "Epoch [7/50], Train Loss: 0.0262, Val Loss: 0.0161\n",
      "Epoch [8/50], Train Loss: 0.0278, Val Loss: 0.0072\n",
      "Epoch [9/50], Train Loss: 0.0315, Val Loss: 0.0437\n",
      "Epoch [10/50], Train Loss: 0.0299, Val Loss: 0.0107\n",
      "Epoch [11/50], Train Loss: 0.0298, Val Loss: 0.0078\n",
      "Epoch [12/50], Train Loss: 0.0195, Val Loss: 0.0274\n",
      "Epoch [13/50], Train Loss: 0.0324, Val Loss: 0.0061\n",
      "Epoch [14/50], Train Loss: 0.0331, Val Loss: 0.0629\n",
      "Epoch [15/50], Train Loss: 0.0359, Val Loss: 0.0081\n",
      "Epoch [16/50], Train Loss: 0.0227, Val Loss: 0.0081\n",
      "Epoch [17/50], Train Loss: 0.0270, Val Loss: 0.0097\n",
      "Epoch [18/50], Train Loss: 0.0122, Val Loss: 0.0277\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=64, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0245, Val Loss: 0.0539\n",
      "Epoch [2/50], Train Loss: 0.0391, Val Loss: 0.0441\n",
      "Epoch [3/50], Train Loss: 0.0446, Val Loss: 0.0359\n",
      "Epoch [4/50], Train Loss: 0.0374, Val Loss: 0.0310\n",
      "Epoch [5/50], Train Loss: 0.0316, Val Loss: 0.0090\n",
      "Epoch [6/50], Train Loss: 0.0242, Val Loss: 0.0102\n",
      "Epoch [7/50], Train Loss: 0.0254, Val Loss: 0.0275\n",
      "Epoch [8/50], Train Loss: 0.0279, Val Loss: 0.0086\n",
      "Epoch [9/50], Train Loss: 0.0259, Val Loss: 0.0093\n",
      "Epoch [10/50], Train Loss: 0.0236, Val Loss: 0.0281\n",
      "Epoch [11/50], Train Loss: 0.0247, Val Loss: 0.0026\n",
      "Epoch [12/50], Train Loss: 0.0159, Val Loss: 0.0163\n",
      "Epoch [13/50], Train Loss: 0.0246, Val Loss: 0.0745\n",
      "Epoch [14/50], Train Loss: 0.0350, Val Loss: 0.0442\n",
      "Epoch [15/50], Train Loss: 0.0336, Val Loss: 0.0717\n",
      "Epoch [16/50], Train Loss: 0.0223, Val Loss: 0.0221\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=64, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0330, Val Loss: 0.0263\n",
      "Epoch [2/50], Train Loss: 0.0483, Val Loss: 0.0479\n",
      "Epoch [3/50], Train Loss: 0.0457, Val Loss: 0.0453\n",
      "Epoch [4/50], Train Loss: 0.0386, Val Loss: 0.0030\n",
      "Epoch [5/50], Train Loss: 0.0270, Val Loss: 0.0417\n",
      "Epoch [6/50], Train Loss: 0.0298, Val Loss: 0.0251\n",
      "Epoch [7/50], Train Loss: 0.0266, Val Loss: 0.0036\n",
      "Epoch [8/50], Train Loss: 0.0268, Val Loss: 0.0177\n",
      "Epoch [9/50], Train Loss: 0.0301, Val Loss: 0.0065\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=128, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0190, Val Loss: 0.0459\n",
      "Epoch [2/50], Train Loss: 0.0250, Val Loss: 0.0397\n",
      "Epoch [3/50], Train Loss: 0.0280, Val Loss: 0.0364\n",
      "Epoch [4/50], Train Loss: 0.0239, Val Loss: 0.0241\n",
      "Epoch [5/50], Train Loss: 0.0214, Val Loss: 0.0247\n",
      "Epoch [6/50], Train Loss: 0.0118, Val Loss: 0.0155\n",
      "Epoch [7/50], Train Loss: 0.0071, Val Loss: 0.0129\n",
      "Epoch [8/50], Train Loss: 0.0074, Val Loss: 0.0057\n",
      "Epoch [9/50], Train Loss: 0.0069, Val Loss: 0.0048\n",
      "Epoch [10/50], Train Loss: 0.0158, Val Loss: 0.0054\n",
      "Epoch [11/50], Train Loss: 0.0111, Val Loss: 0.0127\n",
      "Epoch [12/50], Train Loss: 0.0042, Val Loss: 0.0069\n",
      "Epoch [13/50], Train Loss: 0.0040, Val Loss: 0.0084\n",
      "Epoch [14/50], Train Loss: 0.0047, Val Loss: 0.0087\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=128, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0162, Val Loss: 0.0204\n",
      "Epoch [2/50], Train Loss: 0.0267, Val Loss: 0.0324\n",
      "Epoch [3/50], Train Loss: 0.0272, Val Loss: 0.0296\n",
      "Epoch [4/50], Train Loss: 0.0209, Val Loss: 0.0228\n",
      "Epoch [5/50], Train Loss: 0.0178, Val Loss: 0.0279\n",
      "Epoch [6/50], Train Loss: 0.0106, Val Loss: 0.0118\n",
      "Epoch [7/50], Train Loss: 0.0058, Val Loss: 0.0070\n",
      "Epoch [8/50], Train Loss: 0.0118, Val Loss: 0.0019\n",
      "Epoch [9/50], Train Loss: 0.0070, Val Loss: 0.0062\n",
      "Epoch [10/50], Train Loss: 0.0099, Val Loss: 0.0051\n",
      "Epoch [11/50], Train Loss: 0.0065, Val Loss: 0.0089\n",
      "Epoch [12/50], Train Loss: 0.0045, Val Loss: 0.0097\n",
      "Epoch [13/50], Train Loss: 0.0051, Val Loss: 0.0107\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=128, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0189, Val Loss: 0.0405\n",
      "Epoch [2/50], Train Loss: 0.0256, Val Loss: 0.0255\n",
      "Epoch [3/50], Train Loss: 0.0294, Val Loss: 0.0280\n",
      "Epoch [4/50], Train Loss: 0.0254, Val Loss: 0.0261\n",
      "Epoch [5/50], Train Loss: 0.0194, Val Loss: 0.0327\n",
      "Epoch [6/50], Train Loss: 0.0096, Val Loss: 0.0088\n",
      "Epoch [7/50], Train Loss: 0.0067, Val Loss: 0.0019\n",
      "Epoch [8/50], Train Loss: 0.0109, Val Loss: 0.0043\n",
      "Epoch [9/50], Train Loss: 0.0085, Val Loss: 0.0089\n",
      "Epoch [10/50], Train Loss: 0.0135, Val Loss: 0.0018\n",
      "Epoch [11/50], Train Loss: 0.0078, Val Loss: 0.0076\n",
      "Epoch [12/50], Train Loss: 0.0112, Val Loss: 0.0054\n",
      "Epoch [13/50], Train Loss: 0.0070, Val Loss: 0.0101\n",
      "Epoch [14/50], Train Loss: 0.0089, Val Loss: 0.0031\n",
      "Epoch [15/50], Train Loss: 0.0061, Val Loss: 0.0048\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=128, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0143, Val Loss: 0.0381\n",
      "Epoch [2/50], Train Loss: 0.0294, Val Loss: 0.0372\n",
      "Epoch [3/50], Train Loss: 0.0244, Val Loss: 0.0318\n",
      "Epoch [4/50], Train Loss: 0.0201, Val Loss: 0.0255\n",
      "Epoch [5/50], Train Loss: 0.0241, Val Loss: 0.0076\n",
      "Epoch [6/50], Train Loss: 0.0291, Val Loss: 0.0216\n",
      "Epoch [7/50], Train Loss: 0.0255, Val Loss: 0.0162\n",
      "Epoch [8/50], Train Loss: 0.0186, Val Loss: 0.0135\n",
      "Epoch [9/50], Train Loss: 0.0156, Val Loss: 0.0211\n",
      "Epoch [10/50], Train Loss: 0.0105, Val Loss: 0.0237\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=128, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0153, Val Loss: 0.0277\n",
      "Epoch [2/50], Train Loss: 0.0276, Val Loss: 0.0451\n",
      "Epoch [3/50], Train Loss: 0.0217, Val Loss: 0.0257\n",
      "Epoch [4/50], Train Loss: 0.0178, Val Loss: 0.0191\n",
      "Epoch [5/50], Train Loss: 0.0115, Val Loss: 0.0242\n",
      "Epoch [6/50], Train Loss: 0.0178, Val Loss: 0.0270\n",
      "Epoch [7/50], Train Loss: 0.0222, Val Loss: 0.0111\n",
      "Epoch [8/50], Train Loss: 0.0296, Val Loss: 0.0218\n",
      "Epoch [9/50], Train Loss: 0.0245, Val Loss: 0.0201\n",
      "Epoch [10/50], Train Loss: 0.0178, Val Loss: 0.0162\n",
      "Epoch [11/50], Train Loss: 0.0155, Val Loss: 0.0482\n",
      "Epoch [12/50], Train Loss: 0.0140, Val Loss: 0.0247\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=128, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0219, Val Loss: 0.0326\n",
      "Epoch [2/50], Train Loss: 0.0282, Val Loss: 0.0378\n",
      "Epoch [3/50], Train Loss: 0.0307, Val Loss: 0.0209\n",
      "Epoch [4/50], Train Loss: 0.0279, Val Loss: 0.0129\n",
      "Epoch [5/50], Train Loss: 0.0345, Val Loss: 0.0046\n",
      "Epoch [6/50], Train Loss: 0.0232, Val Loss: 0.0121\n",
      "Epoch [7/50], Train Loss: 0.0166, Val Loss: 0.0174\n",
      "Epoch [8/50], Train Loss: 0.0164, Val Loss: 0.0169\n",
      "Epoch [9/50], Train Loss: 0.0135, Val Loss: 0.0124\n",
      "Epoch [10/50], Train Loss: 0.0092, Val Loss: 0.0069\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=128, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0173, Val Loss: 0.0140\n",
      "Epoch [2/50], Train Loss: 0.0356, Val Loss: 0.0351\n",
      "Epoch [3/50], Train Loss: 0.0254, Val Loss: 0.0381\n",
      "Epoch [4/50], Train Loss: 0.0225, Val Loss: 0.0261\n",
      "Epoch [5/50], Train Loss: 0.0195, Val Loss: 0.0112\n",
      "Epoch [6/50], Train Loss: 0.0231, Val Loss: 0.0214\n",
      "Epoch [7/50], Train Loss: 0.0336, Val Loss: 0.0144\n",
      "Epoch [8/50], Train Loss: 0.0193, Val Loss: 0.0070\n",
      "Epoch [9/50], Train Loss: 0.0260, Val Loss: 0.0091\n",
      "Epoch [10/50], Train Loss: 0.0286, Val Loss: 0.0133\n",
      "Epoch [11/50], Train Loss: 0.0233, Val Loss: 0.0095\n",
      "Epoch [12/50], Train Loss: 0.0331, Val Loss: 0.0250\n",
      "Epoch [13/50], Train Loss: 0.0243, Val Loss: 0.0232\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=128, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0183, Val Loss: 0.0343\n",
      "Epoch [2/50], Train Loss: 0.0296, Val Loss: 0.0529\n",
      "Epoch [3/50], Train Loss: 0.0242, Val Loss: 0.0299\n",
      "Epoch [4/50], Train Loss: 0.0203, Val Loss: 0.0234\n",
      "Epoch [5/50], Train Loss: 0.0214, Val Loss: 0.0170\n",
      "Epoch [6/50], Train Loss: 0.0230, Val Loss: 0.0148\n",
      "Epoch [7/50], Train Loss: 0.0293, Val Loss: 0.0214\n",
      "Epoch [8/50], Train Loss: 0.0327, Val Loss: 0.0067\n",
      "Epoch [9/50], Train Loss: 0.0187, Val Loss: 0.0250\n",
      "Epoch [10/50], Train Loss: 0.0287, Val Loss: 0.0099\n",
      "Epoch [11/50], Train Loss: 0.0256, Val Loss: 0.0119\n",
      "Epoch [12/50], Train Loss: 0.0217, Val Loss: 0.0127\n",
      "Epoch [13/50], Train Loss: 0.0169, Val Loss: 0.0172\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adam, hidden_size=128, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0255, Val Loss: 0.0637\n",
      "Epoch [2/50], Train Loss: 0.0274, Val Loss: 0.0619\n",
      "Epoch [3/50], Train Loss: 0.0247, Val Loss: 0.0525\n",
      "Epoch [4/50], Train Loss: 0.0257, Val Loss: 0.0277\n",
      "Epoch [5/50], Train Loss: 0.0222, Val Loss: 0.0316\n",
      "Epoch [6/50], Train Loss: 0.0256, Val Loss: 0.0141\n",
      "Epoch [7/50], Train Loss: 0.0305, Val Loss: 0.0086\n",
      "Epoch [8/50], Train Loss: 0.0281, Val Loss: 0.0215\n",
      "Epoch [9/50], Train Loss: 0.0274, Val Loss: 0.0101\n",
      "Epoch [10/50], Train Loss: 0.0253, Val Loss: 0.0191\n",
      "Epoch [11/50], Train Loss: 0.0219, Val Loss: 0.0140\n",
      "Epoch [12/50], Train Loss: 0.0226, Val Loss: 0.0049\n",
      "Epoch [13/50], Train Loss: 0.0201, Val Loss: 0.0179\n",
      "Epoch [14/50], Train Loss: 0.0189, Val Loss: 0.0229\n",
      "Epoch [15/50], Train Loss: 0.0168, Val Loss: 0.0043\n",
      "Epoch [16/50], Train Loss: 0.0156, Val Loss: 0.0404\n",
      "Epoch [17/50], Train Loss: 0.0126, Val Loss: 0.0191\n",
      "Epoch [18/50], Train Loss: 0.0179, Val Loss: 0.0130\n",
      "Epoch [19/50], Train Loss: 0.0115, Val Loss: 0.0276\n",
      "Epoch [20/50], Train Loss: 0.0165, Val Loss: 0.0264\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=16, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0954, Val Loss: 0.2656\n",
      "Epoch [2/50], Train Loss: 0.0827, Val Loss: 0.2384\n",
      "Epoch [3/50], Train Loss: 0.0724, Val Loss: 0.2150\n",
      "Epoch [4/50], Train Loss: 0.0638, Val Loss: 0.1946\n",
      "Epoch [5/50], Train Loss: 0.0568, Val Loss: 0.1770\n",
      "Epoch [6/50], Train Loss: 0.0511, Val Loss: 0.1616\n",
      "Epoch [7/50], Train Loss: 0.0465, Val Loss: 0.1484\n",
      "Epoch [8/50], Train Loss: 0.0427, Val Loss: 0.1368\n",
      "Epoch [9/50], Train Loss: 0.0396, Val Loss: 0.1268\n",
      "Epoch [10/50], Train Loss: 0.0371, Val Loss: 0.1181\n",
      "Epoch [11/50], Train Loss: 0.0351, Val Loss: 0.1106\n",
      "Epoch [12/50], Train Loss: 0.0335, Val Loss: 0.1040\n",
      "Epoch [13/50], Train Loss: 0.0322, Val Loss: 0.0982\n",
      "Epoch [14/50], Train Loss: 0.0311, Val Loss: 0.0932\n",
      "Epoch [15/50], Train Loss: 0.0302, Val Loss: 0.0888\n",
      "Epoch [16/50], Train Loss: 0.0295, Val Loss: 0.0850\n",
      "Epoch [17/50], Train Loss: 0.0289, Val Loss: 0.0816\n",
      "Epoch [18/50], Train Loss: 0.0284, Val Loss: 0.0786\n",
      "Epoch [19/50], Train Loss: 0.0280, Val Loss: 0.0760\n",
      "Epoch [20/50], Train Loss: 0.0277, Val Loss: 0.0736\n",
      "Epoch [21/50], Train Loss: 0.0274, Val Loss: 0.0715\n",
      "Epoch [22/50], Train Loss: 0.0271, Val Loss: 0.0696\n",
      "Epoch [23/50], Train Loss: 0.0268, Val Loss: 0.0680\n",
      "Epoch [24/50], Train Loss: 0.0266, Val Loss: 0.0665\n",
      "Epoch [25/50], Train Loss: 0.0264, Val Loss: 0.0651\n",
      "Epoch [26/50], Train Loss: 0.0263, Val Loss: 0.0638\n",
      "Epoch [27/50], Train Loss: 0.0261, Val Loss: 0.0627\n",
      "Epoch [28/50], Train Loss: 0.0259, Val Loss: 0.0617\n",
      "Epoch [29/50], Train Loss: 0.0258, Val Loss: 0.0607\n",
      "Epoch [30/50], Train Loss: 0.0256, Val Loss: 0.0598\n",
      "Epoch [31/50], Train Loss: 0.0254, Val Loss: 0.0590\n",
      "Epoch [32/50], Train Loss: 0.0253, Val Loss: 0.0582\n",
      "Epoch [33/50], Train Loss: 0.0252, Val Loss: 0.0575\n",
      "Epoch [34/50], Train Loss: 0.0250, Val Loss: 0.0568\n",
      "Epoch [35/50], Train Loss: 0.0249, Val Loss: 0.0561\n",
      "Epoch [36/50], Train Loss: 0.0247, Val Loss: 0.0555\n",
      "Epoch [37/50], Train Loss: 0.0246, Val Loss: 0.0549\n",
      "Epoch [38/50], Train Loss: 0.0245, Val Loss: 0.0543\n",
      "Epoch [39/50], Train Loss: 0.0243, Val Loss: 0.0538\n",
      "Epoch [40/50], Train Loss: 0.0242, Val Loss: 0.0533\n",
      "Epoch [41/50], Train Loss: 0.0241, Val Loss: 0.0527\n",
      "Epoch [42/50], Train Loss: 0.0239, Val Loss: 0.0523\n",
      "Epoch [43/50], Train Loss: 0.0238, Val Loss: 0.0518\n",
      "Epoch [44/50], Train Loss: 0.0237, Val Loss: 0.0513\n",
      "Epoch [45/50], Train Loss: 0.0235, Val Loss: 0.0508\n",
      "Epoch [46/50], Train Loss: 0.0234, Val Loss: 0.0504\n",
      "Epoch [47/50], Train Loss: 0.0233, Val Loss: 0.0499\n",
      "Epoch [48/50], Train Loss: 0.0231, Val Loss: 0.0495\n",
      "Epoch [49/50], Train Loss: 0.0230, Val Loss: 0.0490\n",
      "Epoch [50/50], Train Loss: 0.0229, Val Loss: 0.0486\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=16, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1259, Val Loss: 0.3470\n",
      "Epoch [2/50], Train Loss: 0.1053, Val Loss: 0.3080\n",
      "Epoch [3/50], Train Loss: 0.0907, Val Loss: 0.2762\n",
      "Epoch [4/50], Train Loss: 0.0782, Val Loss: 0.2498\n",
      "Epoch [5/50], Train Loss: 0.0696, Val Loss: 0.2278\n",
      "Epoch [6/50], Train Loss: 0.0625, Val Loss: 0.2090\n",
      "Epoch [7/50], Train Loss: 0.0563, Val Loss: 0.1930\n",
      "Epoch [8/50], Train Loss: 0.0517, Val Loss: 0.1793\n",
      "Epoch [9/50], Train Loss: 0.0483, Val Loss: 0.1676\n",
      "Epoch [10/50], Train Loss: 0.0451, Val Loss: 0.1576\n",
      "Epoch [11/50], Train Loss: 0.0431, Val Loss: 0.1486\n",
      "Epoch [12/50], Train Loss: 0.0419, Val Loss: 0.1409\n",
      "Epoch [13/50], Train Loss: 0.0406, Val Loss: 0.1342\n",
      "Epoch [14/50], Train Loss: 0.0392, Val Loss: 0.1285\n",
      "Epoch [15/50], Train Loss: 0.0380, Val Loss: 0.1235\n",
      "Epoch [16/50], Train Loss: 0.0367, Val Loss: 0.1190\n",
      "Epoch [17/50], Train Loss: 0.0367, Val Loss: 0.1150\n",
      "Epoch [18/50], Train Loss: 0.0356, Val Loss: 0.1115\n",
      "Epoch [19/50], Train Loss: 0.0351, Val Loss: 0.1083\n",
      "Epoch [20/50], Train Loss: 0.0348, Val Loss: 0.1054\n",
      "Epoch [21/50], Train Loss: 0.0344, Val Loss: 0.1029\n",
      "Epoch [22/50], Train Loss: 0.0332, Val Loss: 0.1007\n",
      "Epoch [23/50], Train Loss: 0.0341, Val Loss: 0.0986\n",
      "Epoch [24/50], Train Loss: 0.0335, Val Loss: 0.0967\n",
      "Epoch [25/50], Train Loss: 0.0332, Val Loss: 0.0951\n",
      "Epoch [26/50], Train Loss: 0.0327, Val Loss: 0.0935\n",
      "Epoch [27/50], Train Loss: 0.0333, Val Loss: 0.0921\n",
      "Epoch [28/50], Train Loss: 0.0326, Val Loss: 0.0908\n",
      "Epoch [29/50], Train Loss: 0.0320, Val Loss: 0.0897\n",
      "Epoch [30/50], Train Loss: 0.0323, Val Loss: 0.0886\n",
      "Epoch [31/50], Train Loss: 0.0321, Val Loss: 0.0876\n",
      "Epoch [32/50], Train Loss: 0.0316, Val Loss: 0.0866\n",
      "Epoch [33/50], Train Loss: 0.0321, Val Loss: 0.0859\n",
      "Epoch [34/50], Train Loss: 0.0316, Val Loss: 0.0850\n",
      "Epoch [35/50], Train Loss: 0.0317, Val Loss: 0.0843\n",
      "Epoch [36/50], Train Loss: 0.0315, Val Loss: 0.0835\n",
      "Epoch [37/50], Train Loss: 0.0318, Val Loss: 0.0827\n",
      "Epoch [38/50], Train Loss: 0.0309, Val Loss: 0.0820\n",
      "Epoch [39/50], Train Loss: 0.0312, Val Loss: 0.0813\n",
      "Epoch [40/50], Train Loss: 0.0305, Val Loss: 0.0807\n",
      "Epoch [41/50], Train Loss: 0.0307, Val Loss: 0.0802\n",
      "Epoch [42/50], Train Loss: 0.0310, Val Loss: 0.0796\n",
      "Epoch [43/50], Train Loss: 0.0299, Val Loss: 0.0790\n",
      "Epoch [44/50], Train Loss: 0.0297, Val Loss: 0.0785\n",
      "Epoch [45/50], Train Loss: 0.0301, Val Loss: 0.0779\n",
      "Epoch [46/50], Train Loss: 0.0299, Val Loss: 0.0774\n",
      "Epoch [47/50], Train Loss: 0.0301, Val Loss: 0.0769\n",
      "Epoch [48/50], Train Loss: 0.0294, Val Loss: 0.0763\n",
      "Epoch [49/50], Train Loss: 0.0294, Val Loss: 0.0758\n",
      "Epoch [50/50], Train Loss: 0.0292, Val Loss: 0.0752\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=16, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.2007, Val Loss: 0.2997\n",
      "Epoch [2/50], Train Loss: 0.1557, Val Loss: 0.2449\n",
      "Epoch [3/50], Train Loss: 0.1256, Val Loss: 0.2063\n",
      "Epoch [4/50], Train Loss: 0.1030, Val Loss: 0.1770\n",
      "Epoch [5/50], Train Loss: 0.0903, Val Loss: 0.1543\n",
      "Epoch [6/50], Train Loss: 0.0769, Val Loss: 0.1375\n",
      "Epoch [7/50], Train Loss: 0.0702, Val Loss: 0.1243\n",
      "Epoch [8/50], Train Loss: 0.0646, Val Loss: 0.1143\n",
      "Epoch [9/50], Train Loss: 0.0586, Val Loss: 0.1068\n",
      "Epoch [10/50], Train Loss: 0.0539, Val Loss: 0.1008\n",
      "Epoch [11/50], Train Loss: 0.0513, Val Loss: 0.0957\n",
      "Epoch [12/50], Train Loss: 0.0485, Val Loss: 0.0921\n",
      "Epoch [13/50], Train Loss: 0.0453, Val Loss: 0.0885\n",
      "Epoch [14/50], Train Loss: 0.0446, Val Loss: 0.0862\n",
      "Epoch [15/50], Train Loss: 0.0431, Val Loss: 0.0840\n",
      "Epoch [16/50], Train Loss: 0.0423, Val Loss: 0.0826\n",
      "Epoch [17/50], Train Loss: 0.0402, Val Loss: 0.0812\n",
      "Epoch [18/50], Train Loss: 0.0418, Val Loss: 0.0797\n",
      "Epoch [19/50], Train Loss: 0.0414, Val Loss: 0.0788\n",
      "Epoch [20/50], Train Loss: 0.0389, Val Loss: 0.0777\n",
      "Epoch [21/50], Train Loss: 0.0375, Val Loss: 0.0770\n",
      "Epoch [22/50], Train Loss: 0.0382, Val Loss: 0.0761\n",
      "Epoch [23/50], Train Loss: 0.0362, Val Loss: 0.0750\n",
      "Epoch [24/50], Train Loss: 0.0363, Val Loss: 0.0746\n",
      "Epoch [25/50], Train Loss: 0.0370, Val Loss: 0.0739\n",
      "Epoch [26/50], Train Loss: 0.0368, Val Loss: 0.0731\n",
      "Epoch [27/50], Train Loss: 0.0356, Val Loss: 0.0723\n",
      "Epoch [28/50], Train Loss: 0.0354, Val Loss: 0.0721\n",
      "Epoch [29/50], Train Loss: 0.0359, Val Loss: 0.0715\n",
      "Epoch [30/50], Train Loss: 0.0352, Val Loss: 0.0710\n",
      "Epoch [31/50], Train Loss: 0.0354, Val Loss: 0.0702\n",
      "Epoch [32/50], Train Loss: 0.0335, Val Loss: 0.0698\n",
      "Epoch [33/50], Train Loss: 0.0348, Val Loss: 0.0690\n",
      "Epoch [34/50], Train Loss: 0.0321, Val Loss: 0.0687\n",
      "Epoch [35/50], Train Loss: 0.0339, Val Loss: 0.0682\n",
      "Epoch [36/50], Train Loss: 0.0343, Val Loss: 0.0674\n",
      "Epoch [37/50], Train Loss: 0.0334, Val Loss: 0.0670\n",
      "Epoch [38/50], Train Loss: 0.0334, Val Loss: 0.0667\n",
      "Epoch [39/50], Train Loss: 0.0314, Val Loss: 0.0660\n",
      "Epoch [40/50], Train Loss: 0.0312, Val Loss: 0.0651\n",
      "Epoch [41/50], Train Loss: 0.0311, Val Loss: 0.0645\n",
      "Epoch [42/50], Train Loss: 0.0312, Val Loss: 0.0642\n",
      "Epoch [43/50], Train Loss: 0.0313, Val Loss: 0.0636\n",
      "Epoch [44/50], Train Loss: 0.0309, Val Loss: 0.0630\n",
      "Epoch [45/50], Train Loss: 0.0305, Val Loss: 0.0625\n",
      "Epoch [46/50], Train Loss: 0.0292, Val Loss: 0.0619\n",
      "Epoch [47/50], Train Loss: 0.0308, Val Loss: 0.0611\n",
      "Epoch [48/50], Train Loss: 0.0301, Val Loss: 0.0606\n",
      "Epoch [49/50], Train Loss: 0.0298, Val Loss: 0.0598\n",
      "Epoch [50/50], Train Loss: 0.0285, Val Loss: 0.0593\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=16, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1407, Val Loss: 0.2561\n",
      "Epoch [2/50], Train Loss: 0.1168, Val Loss: 0.2230\n",
      "Epoch [3/50], Train Loss: 0.0981, Val Loss: 0.1960\n",
      "Epoch [4/50], Train Loss: 0.0833, Val Loss: 0.1739\n",
      "Epoch [5/50], Train Loss: 0.0717, Val Loss: 0.1558\n",
      "Epoch [6/50], Train Loss: 0.0625, Val Loss: 0.1411\n",
      "Epoch [7/50], Train Loss: 0.0554, Val Loss: 0.1290\n",
      "Epoch [8/50], Train Loss: 0.0498, Val Loss: 0.1192\n",
      "Epoch [9/50], Train Loss: 0.0455, Val Loss: 0.1111\n",
      "Epoch [10/50], Train Loss: 0.0421, Val Loss: 0.1046\n",
      "Epoch [11/50], Train Loss: 0.0396, Val Loss: 0.0992\n",
      "Epoch [12/50], Train Loss: 0.0376, Val Loss: 0.0948\n",
      "Epoch [13/50], Train Loss: 0.0361, Val Loss: 0.0913\n",
      "Epoch [14/50], Train Loss: 0.0349, Val Loss: 0.0883\n",
      "Epoch [15/50], Train Loss: 0.0340, Val Loss: 0.0859\n",
      "Epoch [16/50], Train Loss: 0.0333, Val Loss: 0.0839\n",
      "Epoch [17/50], Train Loss: 0.0327, Val Loss: 0.0822\n",
      "Epoch [18/50], Train Loss: 0.0323, Val Loss: 0.0807\n",
      "Epoch [19/50], Train Loss: 0.0320, Val Loss: 0.0795\n",
      "Epoch [20/50], Train Loss: 0.0317, Val Loss: 0.0785\n",
      "Epoch [21/50], Train Loss: 0.0314, Val Loss: 0.0776\n",
      "Epoch [22/50], Train Loss: 0.0312, Val Loss: 0.0768\n",
      "Epoch [23/50], Train Loss: 0.0310, Val Loss: 0.0761\n",
      "Epoch [24/50], Train Loss: 0.0309, Val Loss: 0.0754\n",
      "Epoch [25/50], Train Loss: 0.0307, Val Loss: 0.0749\n",
      "Epoch [26/50], Train Loss: 0.0306, Val Loss: 0.0743\n",
      "Epoch [27/50], Train Loss: 0.0305, Val Loss: 0.0738\n",
      "Epoch [28/50], Train Loss: 0.0303, Val Loss: 0.0734\n",
      "Epoch [29/50], Train Loss: 0.0302, Val Loss: 0.0730\n",
      "Epoch [30/50], Train Loss: 0.0301, Val Loss: 0.0726\n",
      "Epoch [31/50], Train Loss: 0.0300, Val Loss: 0.0722\n",
      "Epoch [32/50], Train Loss: 0.0299, Val Loss: 0.0718\n",
      "Epoch [33/50], Train Loss: 0.0297, Val Loss: 0.0714\n",
      "Epoch [34/50], Train Loss: 0.0296, Val Loss: 0.0711\n",
      "Epoch [35/50], Train Loss: 0.0295, Val Loss: 0.0707\n",
      "Epoch [36/50], Train Loss: 0.0294, Val Loss: 0.0704\n",
      "Epoch [37/50], Train Loss: 0.0293, Val Loss: 0.0701\n",
      "Epoch [38/50], Train Loss: 0.0292, Val Loss: 0.0697\n",
      "Epoch [39/50], Train Loss: 0.0291, Val Loss: 0.0694\n",
      "Epoch [40/50], Train Loss: 0.0290, Val Loss: 0.0691\n",
      "Epoch [41/50], Train Loss: 0.0288, Val Loss: 0.0688\n",
      "Epoch [42/50], Train Loss: 0.0287, Val Loss: 0.0684\n",
      "Epoch [43/50], Train Loss: 0.0286, Val Loss: 0.0681\n",
      "Epoch [44/50], Train Loss: 0.0285, Val Loss: 0.0678\n",
      "Epoch [45/50], Train Loss: 0.0284, Val Loss: 0.0675\n",
      "Epoch [46/50], Train Loss: 0.0283, Val Loss: 0.0671\n",
      "Epoch [47/50], Train Loss: 0.0282, Val Loss: 0.0668\n",
      "Epoch [48/50], Train Loss: 0.0281, Val Loss: 0.0665\n",
      "Epoch [49/50], Train Loss: 0.0279, Val Loss: 0.0662\n",
      "Epoch [50/50], Train Loss: 0.0278, Val Loss: 0.0659\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=16, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1057, Val Loss: 0.2701\n",
      "Epoch [2/50], Train Loss: 0.0906, Val Loss: 0.2411\n",
      "Epoch [3/50], Train Loss: 0.0767, Val Loss: 0.2171\n",
      "Epoch [4/50], Train Loss: 0.0687, Val Loss: 0.1969\n",
      "Epoch [5/50], Train Loss: 0.0597, Val Loss: 0.1802\n",
      "Epoch [6/50], Train Loss: 0.0556, Val Loss: 0.1664\n",
      "Epoch [7/50], Train Loss: 0.0495, Val Loss: 0.1548\n",
      "Epoch [8/50], Train Loss: 0.0489, Val Loss: 0.1451\n",
      "Epoch [9/50], Train Loss: 0.0452, Val Loss: 0.1367\n",
      "Epoch [10/50], Train Loss: 0.0439, Val Loss: 0.1297\n",
      "Epoch [11/50], Train Loss: 0.0418, Val Loss: 0.1238\n",
      "Epoch [12/50], Train Loss: 0.0405, Val Loss: 0.1188\n",
      "Epoch [13/50], Train Loss: 0.0406, Val Loss: 0.1143\n",
      "Epoch [14/50], Train Loss: 0.0396, Val Loss: 0.1105\n",
      "Epoch [15/50], Train Loss: 0.0403, Val Loss: 0.1075\n",
      "Epoch [16/50], Train Loss: 0.0394, Val Loss: 0.1049\n",
      "Epoch [17/50], Train Loss: 0.0376, Val Loss: 0.1025\n",
      "Epoch [18/50], Train Loss: 0.0383, Val Loss: 0.1006\n",
      "Epoch [19/50], Train Loss: 0.0373, Val Loss: 0.0989\n",
      "Epoch [20/50], Train Loss: 0.0366, Val Loss: 0.0974\n",
      "Epoch [21/50], Train Loss: 0.0377, Val Loss: 0.0963\n",
      "Epoch [22/50], Train Loss: 0.0369, Val Loss: 0.0950\n",
      "Epoch [23/50], Train Loss: 0.0371, Val Loss: 0.0940\n",
      "Epoch [24/50], Train Loss: 0.0361, Val Loss: 0.0931\n",
      "Epoch [25/50], Train Loss: 0.0360, Val Loss: 0.0926\n",
      "Epoch [26/50], Train Loss: 0.0373, Val Loss: 0.0914\n",
      "Epoch [27/50], Train Loss: 0.0366, Val Loss: 0.0905\n",
      "Epoch [28/50], Train Loss: 0.0365, Val Loss: 0.0897\n",
      "Epoch [29/50], Train Loss: 0.0351, Val Loss: 0.0890\n",
      "Epoch [30/50], Train Loss: 0.0366, Val Loss: 0.0883\n",
      "Epoch [31/50], Train Loss: 0.0356, Val Loss: 0.0879\n",
      "Epoch [32/50], Train Loss: 0.0357, Val Loss: 0.0874\n",
      "Epoch [33/50], Train Loss: 0.0349, Val Loss: 0.0870\n",
      "Epoch [34/50], Train Loss: 0.0360, Val Loss: 0.0865\n",
      "Epoch [35/50], Train Loss: 0.0348, Val Loss: 0.0861\n",
      "Epoch [36/50], Train Loss: 0.0355, Val Loss: 0.0856\n",
      "Epoch [37/50], Train Loss: 0.0341, Val Loss: 0.0851\n",
      "Epoch [38/50], Train Loss: 0.0353, Val Loss: 0.0845\n",
      "Epoch [39/50], Train Loss: 0.0337, Val Loss: 0.0838\n",
      "Epoch [40/50], Train Loss: 0.0336, Val Loss: 0.0834\n",
      "Epoch [41/50], Train Loss: 0.0342, Val Loss: 0.0830\n",
      "Epoch [42/50], Train Loss: 0.0340, Val Loss: 0.0826\n",
      "Epoch [43/50], Train Loss: 0.0344, Val Loss: 0.0822\n",
      "Epoch [44/50], Train Loss: 0.0345, Val Loss: 0.0816\n",
      "Epoch [45/50], Train Loss: 0.0329, Val Loss: 0.0812\n",
      "Epoch [46/50], Train Loss: 0.0329, Val Loss: 0.0810\n",
      "Epoch [47/50], Train Loss: 0.0342, Val Loss: 0.0805\n",
      "Epoch [48/50], Train Loss: 0.0327, Val Loss: 0.0800\n",
      "Epoch [49/50], Train Loss: 0.0332, Val Loss: 0.0795\n",
      "Epoch [50/50], Train Loss: 0.0334, Val Loss: 0.0793\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=16, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1555, Val Loss: 0.3357\n",
      "Epoch [2/50], Train Loss: 0.1315, Val Loss: 0.2977\n",
      "Epoch [3/50], Train Loss: 0.1163, Val Loss: 0.2661\n",
      "Epoch [4/50], Train Loss: 0.0991, Val Loss: 0.2403\n",
      "Epoch [5/50], Train Loss: 0.0894, Val Loss: 0.2189\n",
      "Epoch [6/50], Train Loss: 0.0797, Val Loss: 0.2005\n",
      "Epoch [7/50], Train Loss: 0.0733, Val Loss: 0.1854\n",
      "Epoch [8/50], Train Loss: 0.0683, Val Loss: 0.1731\n",
      "Epoch [9/50], Train Loss: 0.0644, Val Loss: 0.1629\n",
      "Epoch [10/50], Train Loss: 0.0611, Val Loss: 0.1539\n",
      "Epoch [11/50], Train Loss: 0.0584, Val Loss: 0.1471\n",
      "Epoch [12/50], Train Loss: 0.0574, Val Loss: 0.1411\n",
      "Epoch [13/50], Train Loss: 0.0557, Val Loss: 0.1360\n",
      "Epoch [14/50], Train Loss: 0.0552, Val Loss: 0.1314\n",
      "Epoch [15/50], Train Loss: 0.0525, Val Loss: 0.1274\n",
      "Epoch [16/50], Train Loss: 0.0520, Val Loss: 0.1240\n",
      "Epoch [17/50], Train Loss: 0.0521, Val Loss: 0.1209\n",
      "Epoch [18/50], Train Loss: 0.0510, Val Loss: 0.1185\n",
      "Epoch [19/50], Train Loss: 0.0507, Val Loss: 0.1163\n",
      "Epoch [20/50], Train Loss: 0.0496, Val Loss: 0.1146\n",
      "Epoch [21/50], Train Loss: 0.0497, Val Loss: 0.1130\n",
      "Epoch [22/50], Train Loss: 0.0493, Val Loss: 0.1116\n",
      "Epoch [23/50], Train Loss: 0.0512, Val Loss: 0.1105\n",
      "Epoch [24/50], Train Loss: 0.0490, Val Loss: 0.1089\n",
      "Epoch [25/50], Train Loss: 0.0480, Val Loss: 0.1082\n",
      "Epoch [26/50], Train Loss: 0.0484, Val Loss: 0.1070\n",
      "Epoch [27/50], Train Loss: 0.0489, Val Loss: 0.1064\n",
      "Epoch [28/50], Train Loss: 0.0488, Val Loss: 0.1058\n",
      "Epoch [29/50], Train Loss: 0.0465, Val Loss: 0.1049\n",
      "Epoch [30/50], Train Loss: 0.0457, Val Loss: 0.1043\n",
      "Epoch [31/50], Train Loss: 0.0475, Val Loss: 0.1039\n",
      "Epoch [32/50], Train Loss: 0.0472, Val Loss: 0.1031\n",
      "Epoch [33/50], Train Loss: 0.0457, Val Loss: 0.1026\n",
      "Epoch [34/50], Train Loss: 0.0464, Val Loss: 0.1021\n",
      "Epoch [35/50], Train Loss: 0.0459, Val Loss: 0.1014\n",
      "Epoch [36/50], Train Loss: 0.0464, Val Loss: 0.1009\n",
      "Epoch [37/50], Train Loss: 0.0438, Val Loss: 0.1002\n",
      "Epoch [38/50], Train Loss: 0.0450, Val Loss: 0.0998\n",
      "Epoch [39/50], Train Loss: 0.0444, Val Loss: 0.0992\n",
      "Epoch [40/50], Train Loss: 0.0437, Val Loss: 0.0987\n",
      "Epoch [41/50], Train Loss: 0.0441, Val Loss: 0.0985\n",
      "Epoch [42/50], Train Loss: 0.0438, Val Loss: 0.0982\n",
      "Epoch [43/50], Train Loss: 0.0425, Val Loss: 0.0978\n",
      "Epoch [44/50], Train Loss: 0.0420, Val Loss: 0.0975\n",
      "Epoch [45/50], Train Loss: 0.0421, Val Loss: 0.0968\n",
      "Epoch [46/50], Train Loss: 0.0422, Val Loss: 0.0964\n",
      "Epoch [47/50], Train Loss: 0.0425, Val Loss: 0.0963\n",
      "Epoch [48/50], Train Loss: 0.0421, Val Loss: 0.0961\n",
      "Epoch [49/50], Train Loss: 0.0427, Val Loss: 0.0958\n",
      "Epoch [50/50], Train Loss: 0.0417, Val Loss: 0.0953\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=16, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1097, Val Loss: 0.3061\n",
      "Epoch [2/50], Train Loss: 0.0864, Val Loss: 0.2643\n",
      "Epoch [3/50], Train Loss: 0.0709, Val Loss: 0.2325\n",
      "Epoch [4/50], Train Loss: 0.0605, Val Loss: 0.2080\n",
      "Epoch [5/50], Train Loss: 0.0533, Val Loss: 0.1886\n",
      "Epoch [6/50], Train Loss: 0.0483, Val Loss: 0.1732\n",
      "Epoch [7/50], Train Loss: 0.0448, Val Loss: 0.1608\n",
      "Epoch [8/50], Train Loss: 0.0423, Val Loss: 0.1507\n",
      "Epoch [9/50], Train Loss: 0.0406, Val Loss: 0.1425\n",
      "Epoch [10/50], Train Loss: 0.0393, Val Loss: 0.1357\n",
      "Epoch [11/50], Train Loss: 0.0384, Val Loss: 0.1301\n",
      "Epoch [12/50], Train Loss: 0.0378, Val Loss: 0.1254\n",
      "Epoch [13/50], Train Loss: 0.0373, Val Loss: 0.1215\n",
      "Epoch [14/50], Train Loss: 0.0369, Val Loss: 0.1182\n",
      "Epoch [15/50], Train Loss: 0.0366, Val Loss: 0.1154\n",
      "Epoch [16/50], Train Loss: 0.0363, Val Loss: 0.1130\n",
      "Epoch [17/50], Train Loss: 0.0362, Val Loss: 0.1110\n",
      "Epoch [18/50], Train Loss: 0.0360, Val Loss: 0.1092\n",
      "Epoch [19/50], Train Loss: 0.0359, Val Loss: 0.1077\n",
      "Epoch [20/50], Train Loss: 0.0358, Val Loss: 0.1064\n",
      "Epoch [21/50], Train Loss: 0.0357, Val Loss: 0.1053\n",
      "Epoch [22/50], Train Loss: 0.0356, Val Loss: 0.1042\n",
      "Epoch [23/50], Train Loss: 0.0355, Val Loss: 0.1033\n",
      "Epoch [24/50], Train Loss: 0.0354, Val Loss: 0.1025\n",
      "Epoch [25/50], Train Loss: 0.0354, Val Loss: 0.1018\n",
      "Epoch [26/50], Train Loss: 0.0353, Val Loss: 0.1012\n",
      "Epoch [27/50], Train Loss: 0.0352, Val Loss: 0.1006\n",
      "Epoch [28/50], Train Loss: 0.0352, Val Loss: 0.1000\n",
      "Epoch [29/50], Train Loss: 0.0351, Val Loss: 0.0995\n",
      "Epoch [30/50], Train Loss: 0.0351, Val Loss: 0.0990\n",
      "Epoch [31/50], Train Loss: 0.0350, Val Loss: 0.0986\n",
      "Epoch [32/50], Train Loss: 0.0349, Val Loss: 0.0982\n",
      "Epoch [33/50], Train Loss: 0.0349, Val Loss: 0.0978\n",
      "Epoch [34/50], Train Loss: 0.0348, Val Loss: 0.0975\n",
      "Epoch [35/50], Train Loss: 0.0348, Val Loss: 0.0971\n",
      "Epoch [36/50], Train Loss: 0.0347, Val Loss: 0.0968\n",
      "Epoch [37/50], Train Loss: 0.0346, Val Loss: 0.0965\n",
      "Epoch [38/50], Train Loss: 0.0346, Val Loss: 0.0961\n",
      "Epoch [39/50], Train Loss: 0.0345, Val Loss: 0.0958\n",
      "Epoch [40/50], Train Loss: 0.0345, Val Loss: 0.0955\n",
      "Epoch [41/50], Train Loss: 0.0344, Val Loss: 0.0952\n",
      "Epoch [42/50], Train Loss: 0.0343, Val Loss: 0.0950\n",
      "Epoch [43/50], Train Loss: 0.0343, Val Loss: 0.0947\n",
      "Epoch [44/50], Train Loss: 0.0342, Val Loss: 0.0944\n",
      "Epoch [45/50], Train Loss: 0.0342, Val Loss: 0.0941\n",
      "Epoch [46/50], Train Loss: 0.0341, Val Loss: 0.0938\n",
      "Epoch [47/50], Train Loss: 0.0340, Val Loss: 0.0936\n",
      "Epoch [48/50], Train Loss: 0.0340, Val Loss: 0.0933\n",
      "Epoch [49/50], Train Loss: 0.0339, Val Loss: 0.0930\n",
      "Epoch [50/50], Train Loss: 0.0338, Val Loss: 0.0928\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=16, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1944, Val Loss: 0.4352\n",
      "Epoch [2/50], Train Loss: 0.1617, Val Loss: 0.3859\n",
      "Epoch [3/50], Train Loss: 0.1357, Val Loss: 0.3449\n",
      "Epoch [4/50], Train Loss: 0.1161, Val Loss: 0.3103\n",
      "Epoch [5/50], Train Loss: 0.1002, Val Loss: 0.2808\n",
      "Epoch [6/50], Train Loss: 0.0872, Val Loss: 0.2554\n",
      "Epoch [7/50], Train Loss: 0.0769, Val Loss: 0.2336\n",
      "Epoch [8/50], Train Loss: 0.0694, Val Loss: 0.2150\n",
      "Epoch [9/50], Train Loss: 0.0622, Val Loss: 0.1990\n",
      "Epoch [10/50], Train Loss: 0.0583, Val Loss: 0.1851\n",
      "Epoch [11/50], Train Loss: 0.0532, Val Loss: 0.1730\n",
      "Epoch [12/50], Train Loss: 0.0490, Val Loss: 0.1627\n",
      "Epoch [13/50], Train Loss: 0.0471, Val Loss: 0.1538\n",
      "Epoch [14/50], Train Loss: 0.0450, Val Loss: 0.1459\n",
      "Epoch [15/50], Train Loss: 0.0436, Val Loss: 0.1393\n",
      "Epoch [16/50], Train Loss: 0.0423, Val Loss: 0.1336\n",
      "Epoch [17/50], Train Loss: 0.0420, Val Loss: 0.1288\n",
      "Epoch [18/50], Train Loss: 0.0413, Val Loss: 0.1244\n",
      "Epoch [19/50], Train Loss: 0.0402, Val Loss: 0.1208\n",
      "Epoch [20/50], Train Loss: 0.0403, Val Loss: 0.1176\n",
      "Epoch [21/50], Train Loss: 0.0409, Val Loss: 0.1148\n",
      "Epoch [22/50], Train Loss: 0.0389, Val Loss: 0.1124\n",
      "Epoch [23/50], Train Loss: 0.0390, Val Loss: 0.1102\n",
      "Epoch [24/50], Train Loss: 0.0397, Val Loss: 0.1081\n",
      "Epoch [25/50], Train Loss: 0.0382, Val Loss: 0.1068\n",
      "Epoch [26/50], Train Loss: 0.0381, Val Loss: 0.1055\n",
      "Epoch [27/50], Train Loss: 0.0385, Val Loss: 0.1043\n",
      "Epoch [28/50], Train Loss: 0.0380, Val Loss: 0.1033\n",
      "Epoch [29/50], Train Loss: 0.0377, Val Loss: 0.1026\n",
      "Epoch [30/50], Train Loss: 0.0391, Val Loss: 0.1017\n",
      "Epoch [31/50], Train Loss: 0.0386, Val Loss: 0.1010\n",
      "Epoch [32/50], Train Loss: 0.0373, Val Loss: 0.1002\n",
      "Epoch [33/50], Train Loss: 0.0365, Val Loss: 0.0993\n",
      "Epoch [34/50], Train Loss: 0.0371, Val Loss: 0.0988\n",
      "Epoch [35/50], Train Loss: 0.0376, Val Loss: 0.0983\n",
      "Epoch [36/50], Train Loss: 0.0375, Val Loss: 0.0981\n",
      "Epoch [37/50], Train Loss: 0.0367, Val Loss: 0.0974\n",
      "Epoch [38/50], Train Loss: 0.0373, Val Loss: 0.0970\n",
      "Epoch [39/50], Train Loss: 0.0365, Val Loss: 0.0968\n",
      "Epoch [40/50], Train Loss: 0.0364, Val Loss: 0.0965\n",
      "Epoch [41/50], Train Loss: 0.0380, Val Loss: 0.0961\n",
      "Epoch [42/50], Train Loss: 0.0365, Val Loss: 0.0957\n",
      "Epoch [43/50], Train Loss: 0.0370, Val Loss: 0.0953\n",
      "Epoch [44/50], Train Loss: 0.0371, Val Loss: 0.0951\n",
      "Epoch [45/50], Train Loss: 0.0379, Val Loss: 0.0949\n",
      "Epoch [46/50], Train Loss: 0.0371, Val Loss: 0.0947\n",
      "Epoch [47/50], Train Loss: 0.0367, Val Loss: 0.0944\n",
      "Epoch [48/50], Train Loss: 0.0362, Val Loss: 0.0940\n",
      "Epoch [49/50], Train Loss: 0.0368, Val Loss: 0.0936\n",
      "Epoch [50/50], Train Loss: 0.0358, Val Loss: 0.0934\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=16, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.2122, Val Loss: 0.4403\n",
      "Epoch [2/50], Train Loss: 0.1723, Val Loss: 0.3876\n",
      "Epoch [3/50], Train Loss: 0.1450, Val Loss: 0.3447\n",
      "Epoch [4/50], Train Loss: 0.1232, Val Loss: 0.3092\n",
      "Epoch [5/50], Train Loss: 0.1095, Val Loss: 0.2798\n",
      "Epoch [6/50], Train Loss: 0.0980, Val Loss: 0.2551\n",
      "Epoch [7/50], Train Loss: 0.0855, Val Loss: 0.2343\n",
      "Epoch [8/50], Train Loss: 0.0804, Val Loss: 0.2172\n",
      "Epoch [9/50], Train Loss: 0.0739, Val Loss: 0.2024\n",
      "Epoch [10/50], Train Loss: 0.0711, Val Loss: 0.1897\n",
      "Epoch [11/50], Train Loss: 0.0658, Val Loss: 0.1793\n",
      "Epoch [12/50], Train Loss: 0.0642, Val Loss: 0.1704\n",
      "Epoch [13/50], Train Loss: 0.0628, Val Loss: 0.1630\n",
      "Epoch [14/50], Train Loss: 0.0601, Val Loss: 0.1566\n",
      "Epoch [15/50], Train Loss: 0.0590, Val Loss: 0.1512\n",
      "Epoch [16/50], Train Loss: 0.0585, Val Loss: 0.1462\n",
      "Epoch [17/50], Train Loss: 0.0558, Val Loss: 0.1421\n",
      "Epoch [18/50], Train Loss: 0.0557, Val Loss: 0.1386\n",
      "Epoch [19/50], Train Loss: 0.0548, Val Loss: 0.1359\n",
      "Epoch [20/50], Train Loss: 0.0547, Val Loss: 0.1333\n",
      "Epoch [21/50], Train Loss: 0.0535, Val Loss: 0.1311\n",
      "Epoch [22/50], Train Loss: 0.0518, Val Loss: 0.1292\n",
      "Epoch [23/50], Train Loss: 0.0523, Val Loss: 0.1277\n",
      "Epoch [24/50], Train Loss: 0.0531, Val Loss: 0.1263\n",
      "Epoch [25/50], Train Loss: 0.0508, Val Loss: 0.1248\n",
      "Epoch [26/50], Train Loss: 0.0521, Val Loss: 0.1236\n",
      "Epoch [27/50], Train Loss: 0.0499, Val Loss: 0.1222\n",
      "Epoch [28/50], Train Loss: 0.0508, Val Loss: 0.1213\n",
      "Epoch [29/50], Train Loss: 0.0494, Val Loss: 0.1204\n",
      "Epoch [30/50], Train Loss: 0.0504, Val Loss: 0.1195\n",
      "Epoch [31/50], Train Loss: 0.0496, Val Loss: 0.1189\n",
      "Epoch [32/50], Train Loss: 0.0491, Val Loss: 0.1185\n",
      "Epoch [33/50], Train Loss: 0.0491, Val Loss: 0.1179\n",
      "Epoch [34/50], Train Loss: 0.0487, Val Loss: 0.1176\n",
      "Epoch [35/50], Train Loss: 0.0484, Val Loss: 0.1170\n",
      "Epoch [36/50], Train Loss: 0.0478, Val Loss: 0.1161\n",
      "Epoch [37/50], Train Loss: 0.0463, Val Loss: 0.1157\n",
      "Epoch [38/50], Train Loss: 0.0476, Val Loss: 0.1152\n",
      "Epoch [39/50], Train Loss: 0.0478, Val Loss: 0.1148\n",
      "Epoch [40/50], Train Loss: 0.0461, Val Loss: 0.1144\n",
      "Epoch [41/50], Train Loss: 0.0476, Val Loss: 0.1139\n",
      "Epoch [42/50], Train Loss: 0.0462, Val Loss: 0.1133\n",
      "Epoch [43/50], Train Loss: 0.0462, Val Loss: 0.1131\n",
      "Epoch [44/50], Train Loss: 0.0472, Val Loss: 0.1130\n",
      "Epoch [45/50], Train Loss: 0.0451, Val Loss: 0.1126\n",
      "Epoch [46/50], Train Loss: 0.0451, Val Loss: 0.1125\n",
      "Epoch [47/50], Train Loss: 0.0460, Val Loss: 0.1122\n",
      "Epoch [48/50], Train Loss: 0.0455, Val Loss: 0.1120\n",
      "Epoch [49/50], Train Loss: 0.0438, Val Loss: 0.1116\n",
      "Epoch [50/50], Train Loss: 0.0448, Val Loss: 0.1111\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=32, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1709, Val Loss: 0.4379\n",
      "Epoch [2/50], Train Loss: 0.1351, Val Loss: 0.3711\n",
      "Epoch [3/50], Train Loss: 0.1091, Val Loss: 0.3186\n",
      "Epoch [4/50], Train Loss: 0.0898, Val Loss: 0.2766\n",
      "Epoch [5/50], Train Loss: 0.0754, Val Loss: 0.2426\n",
      "Epoch [6/50], Train Loss: 0.0646, Val Loss: 0.2149\n",
      "Epoch [7/50], Train Loss: 0.0565, Val Loss: 0.1921\n",
      "Epoch [8/50], Train Loss: 0.0503, Val Loss: 0.1733\n",
      "Epoch [9/50], Train Loss: 0.0457, Val Loss: 0.1577\n",
      "Epoch [10/50], Train Loss: 0.0422, Val Loss: 0.1446\n",
      "Epoch [11/50], Train Loss: 0.0396, Val Loss: 0.1337\n",
      "Epoch [12/50], Train Loss: 0.0376, Val Loss: 0.1244\n",
      "Epoch [13/50], Train Loss: 0.0361, Val Loss: 0.1166\n",
      "Epoch [14/50], Train Loss: 0.0349, Val Loss: 0.1100\n",
      "Epoch [15/50], Train Loss: 0.0340, Val Loss: 0.1043\n",
      "Epoch [16/50], Train Loss: 0.0333, Val Loss: 0.0995\n",
      "Epoch [17/50], Train Loss: 0.0327, Val Loss: 0.0953\n",
      "Epoch [18/50], Train Loss: 0.0322, Val Loss: 0.0917\n",
      "Epoch [19/50], Train Loss: 0.0318, Val Loss: 0.0885\n",
      "Epoch [20/50], Train Loss: 0.0315, Val Loss: 0.0858\n",
      "Epoch [21/50], Train Loss: 0.0312, Val Loss: 0.0833\n",
      "Epoch [22/50], Train Loss: 0.0310, Val Loss: 0.0812\n",
      "Epoch [23/50], Train Loss: 0.0307, Val Loss: 0.0793\n",
      "Epoch [24/50], Train Loss: 0.0305, Val Loss: 0.0776\n",
      "Epoch [25/50], Train Loss: 0.0303, Val Loss: 0.0760\n",
      "Epoch [26/50], Train Loss: 0.0301, Val Loss: 0.0746\n",
      "Epoch [27/50], Train Loss: 0.0299, Val Loss: 0.0734\n",
      "Epoch [28/50], Train Loss: 0.0297, Val Loss: 0.0722\n",
      "Epoch [29/50], Train Loss: 0.0295, Val Loss: 0.0711\n",
      "Epoch [30/50], Train Loss: 0.0293, Val Loss: 0.0701\n",
      "Epoch [31/50], Train Loss: 0.0291, Val Loss: 0.0692\n",
      "Epoch [32/50], Train Loss: 0.0290, Val Loss: 0.0683\n",
      "Epoch [33/50], Train Loss: 0.0288, Val Loss: 0.0675\n",
      "Epoch [34/50], Train Loss: 0.0286, Val Loss: 0.0668\n",
      "Epoch [35/50], Train Loss: 0.0284, Val Loss: 0.0660\n",
      "Epoch [36/50], Train Loss: 0.0283, Val Loss: 0.0653\n",
      "Epoch [37/50], Train Loss: 0.0281, Val Loss: 0.0646\n",
      "Epoch [38/50], Train Loss: 0.0279, Val Loss: 0.0639\n",
      "Epoch [39/50], Train Loss: 0.0277, Val Loss: 0.0633\n",
      "Epoch [40/50], Train Loss: 0.0276, Val Loss: 0.0627\n",
      "Epoch [41/50], Train Loss: 0.0274, Val Loss: 0.0621\n",
      "Epoch [42/50], Train Loss: 0.0272, Val Loss: 0.0615\n",
      "Epoch [43/50], Train Loss: 0.0271, Val Loss: 0.0609\n",
      "Epoch [44/50], Train Loss: 0.0269, Val Loss: 0.0604\n",
      "Epoch [45/50], Train Loss: 0.0267, Val Loss: 0.0598\n",
      "Epoch [46/50], Train Loss: 0.0266, Val Loss: 0.0593\n",
      "Epoch [47/50], Train Loss: 0.0264, Val Loss: 0.0587\n",
      "Epoch [48/50], Train Loss: 0.0262, Val Loss: 0.0582\n",
      "Epoch [49/50], Train Loss: 0.0261, Val Loss: 0.0577\n",
      "Epoch [50/50], Train Loss: 0.0259, Val Loss: 0.0572\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=32, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1727, Val Loss: 0.4246\n",
      "Epoch [2/50], Train Loss: 0.1439, Val Loss: 0.3689\n",
      "Epoch [3/50], Train Loss: 0.1208, Val Loss: 0.3225\n",
      "Epoch [4/50], Train Loss: 0.1010, Val Loss: 0.2837\n",
      "Epoch [5/50], Train Loss: 0.0872, Val Loss: 0.2507\n",
      "Epoch [6/50], Train Loss: 0.0754, Val Loss: 0.2230\n",
      "Epoch [7/50], Train Loss: 0.0661, Val Loss: 0.1994\n",
      "Epoch [8/50], Train Loss: 0.0597, Val Loss: 0.1797\n",
      "Epoch [9/50], Train Loss: 0.0530, Val Loss: 0.1629\n",
      "Epoch [10/50], Train Loss: 0.0486, Val Loss: 0.1485\n",
      "Epoch [11/50], Train Loss: 0.0449, Val Loss: 0.1365\n",
      "Epoch [12/50], Train Loss: 0.0426, Val Loss: 0.1263\n",
      "Epoch [13/50], Train Loss: 0.0403, Val Loss: 0.1179\n",
      "Epoch [14/50], Train Loss: 0.0390, Val Loss: 0.1106\n",
      "Epoch [15/50], Train Loss: 0.0371, Val Loss: 0.1041\n",
      "Epoch [16/50], Train Loss: 0.0361, Val Loss: 0.0987\n",
      "Epoch [17/50], Train Loss: 0.0354, Val Loss: 0.0941\n",
      "Epoch [18/50], Train Loss: 0.0348, Val Loss: 0.0900\n",
      "Epoch [19/50], Train Loss: 0.0336, Val Loss: 0.0867\n",
      "Epoch [20/50], Train Loss: 0.0341, Val Loss: 0.0837\n",
      "Epoch [21/50], Train Loss: 0.0336, Val Loss: 0.0810\n",
      "Epoch [22/50], Train Loss: 0.0329, Val Loss: 0.0787\n",
      "Epoch [23/50], Train Loss: 0.0330, Val Loss: 0.0767\n",
      "Epoch [24/50], Train Loss: 0.0329, Val Loss: 0.0748\n",
      "Epoch [25/50], Train Loss: 0.0321, Val Loss: 0.0731\n",
      "Epoch [26/50], Train Loss: 0.0317, Val Loss: 0.0716\n",
      "Epoch [27/50], Train Loss: 0.0326, Val Loss: 0.0704\n",
      "Epoch [28/50], Train Loss: 0.0314, Val Loss: 0.0693\n",
      "Epoch [29/50], Train Loss: 0.0316, Val Loss: 0.0681\n",
      "Epoch [30/50], Train Loss: 0.0305, Val Loss: 0.0672\n",
      "Epoch [31/50], Train Loss: 0.0309, Val Loss: 0.0663\n",
      "Epoch [32/50], Train Loss: 0.0308, Val Loss: 0.0654\n",
      "Epoch [33/50], Train Loss: 0.0306, Val Loss: 0.0644\n",
      "Epoch [34/50], Train Loss: 0.0297, Val Loss: 0.0637\n",
      "Epoch [35/50], Train Loss: 0.0302, Val Loss: 0.0629\n",
      "Epoch [36/50], Train Loss: 0.0298, Val Loss: 0.0621\n",
      "Epoch [37/50], Train Loss: 0.0289, Val Loss: 0.0614\n",
      "Epoch [38/50], Train Loss: 0.0299, Val Loss: 0.0606\n",
      "Epoch [39/50], Train Loss: 0.0294, Val Loss: 0.0599\n",
      "Epoch [40/50], Train Loss: 0.0289, Val Loss: 0.0593\n",
      "Epoch [41/50], Train Loss: 0.0289, Val Loss: 0.0587\n",
      "Epoch [42/50], Train Loss: 0.0288, Val Loss: 0.0580\n",
      "Epoch [43/50], Train Loss: 0.0287, Val Loss: 0.0575\n",
      "Epoch [44/50], Train Loss: 0.0286, Val Loss: 0.0568\n",
      "Epoch [45/50], Train Loss: 0.0281, Val Loss: 0.0562\n",
      "Epoch [46/50], Train Loss: 0.0276, Val Loss: 0.0558\n",
      "Epoch [47/50], Train Loss: 0.0278, Val Loss: 0.0552\n",
      "Epoch [48/50], Train Loss: 0.0282, Val Loss: 0.0546\n",
      "Epoch [49/50], Train Loss: 0.0276, Val Loss: 0.0542\n",
      "Epoch [50/50], Train Loss: 0.0271, Val Loss: 0.0537\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=32, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1528, Val Loss: 0.3277\n",
      "Epoch [2/50], Train Loss: 0.1219, Val Loss: 0.2730\n",
      "Epoch [3/50], Train Loss: 0.1000, Val Loss: 0.2313\n",
      "Epoch [4/50], Train Loss: 0.0852, Val Loss: 0.1986\n",
      "Epoch [5/50], Train Loss: 0.0700, Val Loss: 0.1728\n",
      "Epoch [6/50], Train Loss: 0.0625, Val Loss: 0.1519\n",
      "Epoch [7/50], Train Loss: 0.0565, Val Loss: 0.1353\n",
      "Epoch [8/50], Train Loss: 0.0515, Val Loss: 0.1225\n",
      "Epoch [9/50], Train Loss: 0.0473, Val Loss: 0.1121\n",
      "Epoch [10/50], Train Loss: 0.0460, Val Loss: 0.1035\n",
      "Epoch [11/50], Train Loss: 0.0440, Val Loss: 0.0964\n",
      "Epoch [12/50], Train Loss: 0.0421, Val Loss: 0.0906\n",
      "Epoch [13/50], Train Loss: 0.0408, Val Loss: 0.0858\n",
      "Epoch [14/50], Train Loss: 0.0388, Val Loss: 0.0822\n",
      "Epoch [15/50], Train Loss: 0.0404, Val Loss: 0.0790\n",
      "Epoch [16/50], Train Loss: 0.0373, Val Loss: 0.0765\n",
      "Epoch [17/50], Train Loss: 0.0368, Val Loss: 0.0742\n",
      "Epoch [18/50], Train Loss: 0.0368, Val Loss: 0.0721\n",
      "Epoch [19/50], Train Loss: 0.0364, Val Loss: 0.0706\n",
      "Epoch [20/50], Train Loss: 0.0361, Val Loss: 0.0689\n",
      "Epoch [21/50], Train Loss: 0.0370, Val Loss: 0.0677\n",
      "Epoch [22/50], Train Loss: 0.0362, Val Loss: 0.0665\n",
      "Epoch [23/50], Train Loss: 0.0355, Val Loss: 0.0654\n",
      "Epoch [24/50], Train Loss: 0.0357, Val Loss: 0.0646\n",
      "Epoch [25/50], Train Loss: 0.0362, Val Loss: 0.0637\n",
      "Epoch [26/50], Train Loss: 0.0351, Val Loss: 0.0630\n",
      "Epoch [27/50], Train Loss: 0.0344, Val Loss: 0.0622\n",
      "Epoch [28/50], Train Loss: 0.0338, Val Loss: 0.0615\n",
      "Epoch [29/50], Train Loss: 0.0331, Val Loss: 0.0606\n",
      "Epoch [30/50], Train Loss: 0.0329, Val Loss: 0.0602\n",
      "Epoch [31/50], Train Loss: 0.0325, Val Loss: 0.0594\n",
      "Epoch [32/50], Train Loss: 0.0330, Val Loss: 0.0585\n",
      "Epoch [33/50], Train Loss: 0.0323, Val Loss: 0.0581\n",
      "Epoch [34/50], Train Loss: 0.0314, Val Loss: 0.0574\n",
      "Epoch [35/50], Train Loss: 0.0316, Val Loss: 0.0570\n",
      "Epoch [36/50], Train Loss: 0.0316, Val Loss: 0.0563\n",
      "Epoch [37/50], Train Loss: 0.0317, Val Loss: 0.0556\n",
      "Epoch [38/50], Train Loss: 0.0306, Val Loss: 0.0551\n",
      "Epoch [39/50], Train Loss: 0.0301, Val Loss: 0.0546\n",
      "Epoch [40/50], Train Loss: 0.0313, Val Loss: 0.0541\n",
      "Epoch [41/50], Train Loss: 0.0298, Val Loss: 0.0535\n",
      "Epoch [42/50], Train Loss: 0.0296, Val Loss: 0.0529\n",
      "Epoch [43/50], Train Loss: 0.0309, Val Loss: 0.0522\n",
      "Epoch [44/50], Train Loss: 0.0290, Val Loss: 0.0516\n",
      "Epoch [45/50], Train Loss: 0.0300, Val Loss: 0.0514\n",
      "Epoch [46/50], Train Loss: 0.0297, Val Loss: 0.0510\n",
      "Epoch [47/50], Train Loss: 0.0289, Val Loss: 0.0503\n",
      "Epoch [48/50], Train Loss: 0.0287, Val Loss: 0.0498\n",
      "Epoch [49/50], Train Loss: 0.0295, Val Loss: 0.0492\n",
      "Epoch [50/50], Train Loss: 0.0287, Val Loss: 0.0487\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=32, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1271, Val Loss: 0.3236\n",
      "Epoch [2/50], Train Loss: 0.1015, Val Loss: 0.2799\n",
      "Epoch [3/50], Train Loss: 0.0831, Val Loss: 0.2454\n",
      "Epoch [4/50], Train Loss: 0.0698, Val Loss: 0.2179\n",
      "Epoch [5/50], Train Loss: 0.0601, Val Loss: 0.1958\n",
      "Epoch [6/50], Train Loss: 0.0531, Val Loss: 0.1779\n",
      "Epoch [7/50], Train Loss: 0.0480, Val Loss: 0.1633\n",
      "Epoch [8/50], Train Loss: 0.0444, Val Loss: 0.1515\n",
      "Epoch [9/50], Train Loss: 0.0417, Val Loss: 0.1417\n",
      "Epoch [10/50], Train Loss: 0.0398, Val Loss: 0.1338\n",
      "Epoch [11/50], Train Loss: 0.0385, Val Loss: 0.1272\n",
      "Epoch [12/50], Train Loss: 0.0375, Val Loss: 0.1217\n",
      "Epoch [13/50], Train Loss: 0.0368, Val Loss: 0.1172\n",
      "Epoch [14/50], Train Loss: 0.0362, Val Loss: 0.1135\n",
      "Epoch [15/50], Train Loss: 0.0358, Val Loss: 0.1103\n",
      "Epoch [16/50], Train Loss: 0.0355, Val Loss: 0.1076\n",
      "Epoch [17/50], Train Loss: 0.0352, Val Loss: 0.1054\n",
      "Epoch [18/50], Train Loss: 0.0350, Val Loss: 0.1035\n",
      "Epoch [19/50], Train Loss: 0.0348, Val Loss: 0.1018\n",
      "Epoch [20/50], Train Loss: 0.0347, Val Loss: 0.1004\n",
      "Epoch [21/50], Train Loss: 0.0345, Val Loss: 0.0992\n",
      "Epoch [22/50], Train Loss: 0.0344, Val Loss: 0.0981\n",
      "Epoch [23/50], Train Loss: 0.0343, Val Loss: 0.0971\n",
      "Epoch [24/50], Train Loss: 0.0341, Val Loss: 0.0963\n",
      "Epoch [25/50], Train Loss: 0.0340, Val Loss: 0.0955\n",
      "Epoch [26/50], Train Loss: 0.0339, Val Loss: 0.0948\n",
      "Epoch [27/50], Train Loss: 0.0338, Val Loss: 0.0941\n",
      "Epoch [28/50], Train Loss: 0.0336, Val Loss: 0.0935\n",
      "Epoch [29/50], Train Loss: 0.0335, Val Loss: 0.0930\n",
      "Epoch [30/50], Train Loss: 0.0334, Val Loss: 0.0925\n",
      "Epoch [31/50], Train Loss: 0.0333, Val Loss: 0.0920\n",
      "Epoch [32/50], Train Loss: 0.0332, Val Loss: 0.0915\n",
      "Epoch [33/50], Train Loss: 0.0330, Val Loss: 0.0910\n",
      "Epoch [34/50], Train Loss: 0.0329, Val Loss: 0.0906\n",
      "Epoch [35/50], Train Loss: 0.0328, Val Loss: 0.0901\n",
      "Epoch [36/50], Train Loss: 0.0327, Val Loss: 0.0897\n",
      "Epoch [37/50], Train Loss: 0.0325, Val Loss: 0.0893\n",
      "Epoch [38/50], Train Loss: 0.0324, Val Loss: 0.0889\n",
      "Epoch [39/50], Train Loss: 0.0323, Val Loss: 0.0885\n",
      "Epoch [40/50], Train Loss: 0.0322, Val Loss: 0.0881\n",
      "Epoch [41/50], Train Loss: 0.0321, Val Loss: 0.0877\n",
      "Epoch [42/50], Train Loss: 0.0319, Val Loss: 0.0873\n",
      "Epoch [43/50], Train Loss: 0.0318, Val Loss: 0.0869\n",
      "Epoch [44/50], Train Loss: 0.0317, Val Loss: 0.0865\n",
      "Epoch [45/50], Train Loss: 0.0316, Val Loss: 0.0862\n",
      "Epoch [46/50], Train Loss: 0.0314, Val Loss: 0.0858\n",
      "Epoch [47/50], Train Loss: 0.0313, Val Loss: 0.0854\n",
      "Epoch [48/50], Train Loss: 0.0312, Val Loss: 0.0850\n",
      "Epoch [49/50], Train Loss: 0.0311, Val Loss: 0.0846\n",
      "Epoch [50/50], Train Loss: 0.0309, Val Loss: 0.0842\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=32, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1478, Val Loss: 0.3503\n",
      "Epoch [2/50], Train Loss: 0.1194, Val Loss: 0.3007\n",
      "Epoch [3/50], Train Loss: 0.0970, Val Loss: 0.2614\n",
      "Epoch [4/50], Train Loss: 0.0816, Val Loss: 0.2294\n",
      "Epoch [5/50], Train Loss: 0.0700, Val Loss: 0.2035\n",
      "Epoch [6/50], Train Loss: 0.0607, Val Loss: 0.1824\n",
      "Epoch [7/50], Train Loss: 0.0545, Val Loss: 0.1653\n",
      "Epoch [8/50], Train Loss: 0.0491, Val Loss: 0.1514\n",
      "Epoch [9/50], Train Loss: 0.0460, Val Loss: 0.1401\n",
      "Epoch [10/50], Train Loss: 0.0436, Val Loss: 0.1310\n",
      "Epoch [11/50], Train Loss: 0.0409, Val Loss: 0.1236\n",
      "Epoch [12/50], Train Loss: 0.0400, Val Loss: 0.1172\n",
      "Epoch [13/50], Train Loss: 0.0393, Val Loss: 0.1120\n",
      "Epoch [14/50], Train Loss: 0.0389, Val Loss: 0.1078\n",
      "Epoch [15/50], Train Loss: 0.0383, Val Loss: 0.1043\n",
      "Epoch [16/50], Train Loss: 0.0377, Val Loss: 0.1014\n",
      "Epoch [17/50], Train Loss: 0.0374, Val Loss: 0.0988\n",
      "Epoch [18/50], Train Loss: 0.0372, Val Loss: 0.0967\n",
      "Epoch [19/50], Train Loss: 0.0372, Val Loss: 0.0948\n",
      "Epoch [20/50], Train Loss: 0.0370, Val Loss: 0.0934\n",
      "Epoch [21/50], Train Loss: 0.0372, Val Loss: 0.0920\n",
      "Epoch [22/50], Train Loss: 0.0365, Val Loss: 0.0909\n",
      "Epoch [23/50], Train Loss: 0.0374, Val Loss: 0.0898\n",
      "Epoch [24/50], Train Loss: 0.0366, Val Loss: 0.0889\n",
      "Epoch [25/50], Train Loss: 0.0360, Val Loss: 0.0879\n",
      "Epoch [26/50], Train Loss: 0.0368, Val Loss: 0.0872\n",
      "Epoch [27/50], Train Loss: 0.0356, Val Loss: 0.0866\n",
      "Epoch [28/50], Train Loss: 0.0360, Val Loss: 0.0860\n",
      "Epoch [29/50], Train Loss: 0.0358, Val Loss: 0.0853\n",
      "Epoch [30/50], Train Loss: 0.0356, Val Loss: 0.0848\n",
      "Epoch [31/50], Train Loss: 0.0349, Val Loss: 0.0844\n",
      "Epoch [32/50], Train Loss: 0.0360, Val Loss: 0.0838\n",
      "Epoch [33/50], Train Loss: 0.0352, Val Loss: 0.0833\n",
      "Epoch [34/50], Train Loss: 0.0349, Val Loss: 0.0830\n",
      "Epoch [35/50], Train Loss: 0.0351, Val Loss: 0.0827\n",
      "Epoch [36/50], Train Loss: 0.0348, Val Loss: 0.0824\n",
      "Epoch [37/50], Train Loss: 0.0348, Val Loss: 0.0821\n",
      "Epoch [38/50], Train Loss: 0.0353, Val Loss: 0.0817\n",
      "Epoch [39/50], Train Loss: 0.0342, Val Loss: 0.0814\n",
      "Epoch [40/50], Train Loss: 0.0340, Val Loss: 0.0810\n",
      "Epoch [41/50], Train Loss: 0.0345, Val Loss: 0.0804\n",
      "Epoch [42/50], Train Loss: 0.0349, Val Loss: 0.0800\n",
      "Epoch [43/50], Train Loss: 0.0337, Val Loss: 0.0796\n",
      "Epoch [44/50], Train Loss: 0.0340, Val Loss: 0.0792\n",
      "Epoch [45/50], Train Loss: 0.0337, Val Loss: 0.0786\n",
      "Epoch [46/50], Train Loss: 0.0330, Val Loss: 0.0782\n",
      "Epoch [47/50], Train Loss: 0.0341, Val Loss: 0.0779\n",
      "Epoch [48/50], Train Loss: 0.0335, Val Loss: 0.0774\n",
      "Epoch [49/50], Train Loss: 0.0333, Val Loss: 0.0770\n",
      "Epoch [50/50], Train Loss: 0.0338, Val Loss: 0.0765\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=32, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1713, Val Loss: 0.3922\n",
      "Epoch [2/50], Train Loss: 0.1420, Val Loss: 0.3458\n",
      "Epoch [3/50], Train Loss: 0.1213, Val Loss: 0.3071\n",
      "Epoch [4/50], Train Loss: 0.1026, Val Loss: 0.2755\n",
      "Epoch [5/50], Train Loss: 0.0882, Val Loss: 0.2491\n",
      "Epoch [6/50], Train Loss: 0.0800, Val Loss: 0.2268\n",
      "Epoch [7/50], Train Loss: 0.0714, Val Loss: 0.2081\n",
      "Epoch [8/50], Train Loss: 0.0645, Val Loss: 0.1922\n",
      "Epoch [9/50], Train Loss: 0.0605, Val Loss: 0.1788\n",
      "Epoch [10/50], Train Loss: 0.0565, Val Loss: 0.1674\n",
      "Epoch [11/50], Train Loss: 0.0531, Val Loss: 0.1577\n",
      "Epoch [12/50], Train Loss: 0.0519, Val Loss: 0.1495\n",
      "Epoch [13/50], Train Loss: 0.0511, Val Loss: 0.1422\n",
      "Epoch [14/50], Train Loss: 0.0488, Val Loss: 0.1364\n",
      "Epoch [15/50], Train Loss: 0.0483, Val Loss: 0.1313\n",
      "Epoch [16/50], Train Loss: 0.0454, Val Loss: 0.1268\n",
      "Epoch [17/50], Train Loss: 0.0461, Val Loss: 0.1230\n",
      "Epoch [18/50], Train Loss: 0.0446, Val Loss: 0.1198\n",
      "Epoch [19/50], Train Loss: 0.0460, Val Loss: 0.1170\n",
      "Epoch [20/50], Train Loss: 0.0450, Val Loss: 0.1147\n",
      "Epoch [21/50], Train Loss: 0.0452, Val Loss: 0.1122\n",
      "Epoch [22/50], Train Loss: 0.0437, Val Loss: 0.1103\n",
      "Epoch [23/50], Train Loss: 0.0440, Val Loss: 0.1088\n",
      "Epoch [24/50], Train Loss: 0.0427, Val Loss: 0.1071\n",
      "Epoch [25/50], Train Loss: 0.0433, Val Loss: 0.1058\n",
      "Epoch [26/50], Train Loss: 0.0445, Val Loss: 0.1045\n",
      "Epoch [27/50], Train Loss: 0.0430, Val Loss: 0.1034\n",
      "Epoch [28/50], Train Loss: 0.0430, Val Loss: 0.1025\n",
      "Epoch [29/50], Train Loss: 0.0426, Val Loss: 0.1014\n",
      "Epoch [30/50], Train Loss: 0.0428, Val Loss: 0.1006\n",
      "Epoch [31/50], Train Loss: 0.0427, Val Loss: 0.0998\n",
      "Epoch [32/50], Train Loss: 0.0425, Val Loss: 0.0990\n",
      "Epoch [33/50], Train Loss: 0.0412, Val Loss: 0.0986\n",
      "Epoch [34/50], Train Loss: 0.0417, Val Loss: 0.0980\n",
      "Epoch [35/50], Train Loss: 0.0409, Val Loss: 0.0974\n",
      "Epoch [36/50], Train Loss: 0.0403, Val Loss: 0.0969\n",
      "Epoch [37/50], Train Loss: 0.0402, Val Loss: 0.0965\n",
      "Epoch [38/50], Train Loss: 0.0409, Val Loss: 0.0960\n",
      "Epoch [39/50], Train Loss: 0.0398, Val Loss: 0.0956\n",
      "Epoch [40/50], Train Loss: 0.0413, Val Loss: 0.0950\n",
      "Epoch [41/50], Train Loss: 0.0403, Val Loss: 0.0942\n",
      "Epoch [42/50], Train Loss: 0.0398, Val Loss: 0.0937\n",
      "Epoch [43/50], Train Loss: 0.0395, Val Loss: 0.0934\n",
      "Epoch [44/50], Train Loss: 0.0398, Val Loss: 0.0928\n",
      "Epoch [45/50], Train Loss: 0.0397, Val Loss: 0.0923\n",
      "Epoch [46/50], Train Loss: 0.0387, Val Loss: 0.0919\n",
      "Epoch [47/50], Train Loss: 0.0395, Val Loss: 0.0914\n",
      "Epoch [48/50], Train Loss: 0.0390, Val Loss: 0.0910\n",
      "Epoch [49/50], Train Loss: 0.0393, Val Loss: 0.0905\n",
      "Epoch [50/50], Train Loss: 0.0383, Val Loss: 0.0902\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=32, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1441, Val Loss: 0.3553\n",
      "Epoch [2/50], Train Loss: 0.1137, Val Loss: 0.3060\n",
      "Epoch [3/50], Train Loss: 0.0921, Val Loss: 0.2675\n",
      "Epoch [4/50], Train Loss: 0.0765, Val Loss: 0.2369\n",
      "Epoch [5/50], Train Loss: 0.0651, Val Loss: 0.2121\n",
      "Epoch [6/50], Train Loss: 0.0568, Val Loss: 0.1919\n",
      "Epoch [7/50], Train Loss: 0.0508, Val Loss: 0.1754\n",
      "Epoch [8/50], Train Loss: 0.0464, Val Loss: 0.1619\n",
      "Epoch [9/50], Train Loss: 0.0432, Val Loss: 0.1508\n",
      "Epoch [10/50], Train Loss: 0.0410, Val Loss: 0.1416\n",
      "Epoch [11/50], Train Loss: 0.0394, Val Loss: 0.1340\n",
      "Epoch [12/50], Train Loss: 0.0383, Val Loss: 0.1277\n",
      "Epoch [13/50], Train Loss: 0.0375, Val Loss: 0.1225\n",
      "Epoch [14/50], Train Loss: 0.0369, Val Loss: 0.1182\n",
      "Epoch [15/50], Train Loss: 0.0365, Val Loss: 0.1146\n",
      "Epoch [16/50], Train Loss: 0.0362, Val Loss: 0.1115\n",
      "Epoch [17/50], Train Loss: 0.0360, Val Loss: 0.1089\n",
      "Epoch [18/50], Train Loss: 0.0358, Val Loss: 0.1068\n",
      "Epoch [19/50], Train Loss: 0.0357, Val Loss: 0.1049\n",
      "Epoch [20/50], Train Loss: 0.0356, Val Loss: 0.1033\n",
      "Epoch [21/50], Train Loss: 0.0355, Val Loss: 0.1020\n",
      "Epoch [22/50], Train Loss: 0.0354, Val Loss: 0.1008\n",
      "Epoch [23/50], Train Loss: 0.0353, Val Loss: 0.0998\n",
      "Epoch [24/50], Train Loss: 0.0352, Val Loss: 0.0989\n",
      "Epoch [25/50], Train Loss: 0.0352, Val Loss: 0.0981\n",
      "Epoch [26/50], Train Loss: 0.0351, Val Loss: 0.0974\n",
      "Epoch [27/50], Train Loss: 0.0350, Val Loss: 0.0967\n",
      "Epoch [28/50], Train Loss: 0.0350, Val Loss: 0.0962\n",
      "Epoch [29/50], Train Loss: 0.0349, Val Loss: 0.0956\n",
      "Epoch [30/50], Train Loss: 0.0348, Val Loss: 0.0951\n",
      "Epoch [31/50], Train Loss: 0.0348, Val Loss: 0.0947\n",
      "Epoch [32/50], Train Loss: 0.0347, Val Loss: 0.0943\n",
      "Epoch [33/50], Train Loss: 0.0346, Val Loss: 0.0939\n",
      "Epoch [34/50], Train Loss: 0.0345, Val Loss: 0.0935\n",
      "Epoch [35/50], Train Loss: 0.0345, Val Loss: 0.0931\n",
      "Epoch [36/50], Train Loss: 0.0344, Val Loss: 0.0928\n",
      "Epoch [37/50], Train Loss: 0.0343, Val Loss: 0.0924\n",
      "Epoch [38/50], Train Loss: 0.0343, Val Loss: 0.0921\n",
      "Epoch [39/50], Train Loss: 0.0342, Val Loss: 0.0918\n",
      "Epoch [40/50], Train Loss: 0.0341, Val Loss: 0.0914\n",
      "Epoch [41/50], Train Loss: 0.0340, Val Loss: 0.0911\n",
      "Epoch [42/50], Train Loss: 0.0340, Val Loss: 0.0908\n",
      "Epoch [43/50], Train Loss: 0.0339, Val Loss: 0.0905\n",
      "Epoch [44/50], Train Loss: 0.0338, Val Loss: 0.0902\n",
      "Epoch [45/50], Train Loss: 0.0337, Val Loss: 0.0899\n",
      "Epoch [46/50], Train Loss: 0.0337, Val Loss: 0.0896\n",
      "Epoch [47/50], Train Loss: 0.0336, Val Loss: 0.0893\n",
      "Epoch [48/50], Train Loss: 0.0335, Val Loss: 0.0890\n",
      "Epoch [49/50], Train Loss: 0.0334, Val Loss: 0.0887\n",
      "Epoch [50/50], Train Loss: 0.0334, Val Loss: 0.0884\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=32, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1641, Val Loss: 0.3859\n",
      "Epoch [2/50], Train Loss: 0.1244, Val Loss: 0.3221\n",
      "Epoch [3/50], Train Loss: 0.0976, Val Loss: 0.2738\n",
      "Epoch [4/50], Train Loss: 0.0807, Val Loss: 0.2355\n",
      "Epoch [5/50], Train Loss: 0.0680, Val Loss: 0.2057\n",
      "Epoch [6/50], Train Loss: 0.0581, Val Loss: 0.1826\n",
      "Epoch [7/50], Train Loss: 0.0514, Val Loss: 0.1645\n",
      "Epoch [8/50], Train Loss: 0.0471, Val Loss: 0.1503\n",
      "Epoch [9/50], Train Loss: 0.0456, Val Loss: 0.1388\n",
      "Epoch [10/50], Train Loss: 0.0432, Val Loss: 0.1298\n",
      "Epoch [11/50], Train Loss: 0.0414, Val Loss: 0.1232\n",
      "Epoch [12/50], Train Loss: 0.0414, Val Loss: 0.1177\n",
      "Epoch [13/50], Train Loss: 0.0404, Val Loss: 0.1133\n",
      "Epoch [14/50], Train Loss: 0.0406, Val Loss: 0.1095\n",
      "Epoch [15/50], Train Loss: 0.0407, Val Loss: 0.1067\n",
      "Epoch [16/50], Train Loss: 0.0389, Val Loss: 0.1042\n",
      "Epoch [17/50], Train Loss: 0.0394, Val Loss: 0.1025\n",
      "Epoch [18/50], Train Loss: 0.0390, Val Loss: 0.1009\n",
      "Epoch [19/50], Train Loss: 0.0393, Val Loss: 0.1000\n",
      "Epoch [20/50], Train Loss: 0.0390, Val Loss: 0.0991\n",
      "Epoch [21/50], Train Loss: 0.0391, Val Loss: 0.0980\n",
      "Epoch [22/50], Train Loss: 0.0390, Val Loss: 0.0972\n",
      "Epoch [23/50], Train Loss: 0.0392, Val Loss: 0.0968\n",
      "Epoch [24/50], Train Loss: 0.0393, Val Loss: 0.0962\n",
      "Epoch [25/50], Train Loss: 0.0388, Val Loss: 0.0958\n",
      "Epoch [26/50], Train Loss: 0.0386, Val Loss: 0.0955\n",
      "Epoch [27/50], Train Loss: 0.0392, Val Loss: 0.0951\n",
      "Epoch [28/50], Train Loss: 0.0395, Val Loss: 0.0945\n",
      "Epoch [29/50], Train Loss: 0.0393, Val Loss: 0.0940\n",
      "Epoch [30/50], Train Loss: 0.0393, Val Loss: 0.0939\n",
      "Epoch [31/50], Train Loss: 0.0384, Val Loss: 0.0936\n",
      "Epoch [32/50], Train Loss: 0.0386, Val Loss: 0.0935\n",
      "Epoch [33/50], Train Loss: 0.0388, Val Loss: 0.0930\n",
      "Epoch [34/50], Train Loss: 0.0388, Val Loss: 0.0925\n",
      "Epoch [35/50], Train Loss: 0.0377, Val Loss: 0.0925\n",
      "Epoch [36/50], Train Loss: 0.0375, Val Loss: 0.0922\n",
      "Epoch [37/50], Train Loss: 0.0374, Val Loss: 0.0923\n",
      "Epoch [38/50], Train Loss: 0.0383, Val Loss: 0.0920\n",
      "Epoch [39/50], Train Loss: 0.0385, Val Loss: 0.0918\n",
      "Epoch [40/50], Train Loss: 0.0379, Val Loss: 0.0917\n",
      "Epoch [41/50], Train Loss: 0.0372, Val Loss: 0.0912\n",
      "Epoch [42/50], Train Loss: 0.0375, Val Loss: 0.0908\n",
      "Epoch [43/50], Train Loss: 0.0381, Val Loss: 0.0907\n",
      "Epoch [44/50], Train Loss: 0.0382, Val Loss: 0.0903\n",
      "Epoch [45/50], Train Loss: 0.0380, Val Loss: 0.0903\n",
      "Epoch [46/50], Train Loss: 0.0373, Val Loss: 0.0904\n",
      "Epoch [47/50], Train Loss: 0.0382, Val Loss: 0.0901\n",
      "Epoch [48/50], Train Loss: 0.0370, Val Loss: 0.0898\n",
      "Epoch [49/50], Train Loss: 0.0365, Val Loss: 0.0896\n",
      "Epoch [50/50], Train Loss: 0.0375, Val Loss: 0.0894\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=32, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1501, Val Loss: 0.3674\n",
      "Epoch [2/50], Train Loss: 0.1259, Val Loss: 0.3217\n",
      "Epoch [3/50], Train Loss: 0.1073, Val Loss: 0.2848\n",
      "Epoch [4/50], Train Loss: 0.0910, Val Loss: 0.2545\n",
      "Epoch [5/50], Train Loss: 0.0805, Val Loss: 0.2295\n",
      "Epoch [6/50], Train Loss: 0.0717, Val Loss: 0.2093\n",
      "Epoch [7/50], Train Loss: 0.0652, Val Loss: 0.1921\n",
      "Epoch [8/50], Train Loss: 0.0611, Val Loss: 0.1780\n",
      "Epoch [9/50], Train Loss: 0.0570, Val Loss: 0.1665\n",
      "Epoch [10/50], Train Loss: 0.0536, Val Loss: 0.1570\n",
      "Epoch [11/50], Train Loss: 0.0519, Val Loss: 0.1487\n",
      "Epoch [12/50], Train Loss: 0.0498, Val Loss: 0.1417\n",
      "Epoch [13/50], Train Loss: 0.0500, Val Loss: 0.1358\n",
      "Epoch [14/50], Train Loss: 0.0479, Val Loss: 0.1312\n",
      "Epoch [15/50], Train Loss: 0.0484, Val Loss: 0.1269\n",
      "Epoch [16/50], Train Loss: 0.0468, Val Loss: 0.1236\n",
      "Epoch [17/50], Train Loss: 0.0457, Val Loss: 0.1210\n",
      "Epoch [18/50], Train Loss: 0.0465, Val Loss: 0.1186\n",
      "Epoch [19/50], Train Loss: 0.0450, Val Loss: 0.1163\n",
      "Epoch [20/50], Train Loss: 0.0460, Val Loss: 0.1144\n",
      "Epoch [21/50], Train Loss: 0.0446, Val Loss: 0.1130\n",
      "Epoch [22/50], Train Loss: 0.0449, Val Loss: 0.1116\n",
      "Epoch [23/50], Train Loss: 0.0444, Val Loss: 0.1102\n",
      "Epoch [24/50], Train Loss: 0.0444, Val Loss: 0.1089\n",
      "Epoch [25/50], Train Loss: 0.0431, Val Loss: 0.1074\n",
      "Epoch [26/50], Train Loss: 0.0438, Val Loss: 0.1066\n",
      "Epoch [27/50], Train Loss: 0.0434, Val Loss: 0.1057\n",
      "Epoch [28/50], Train Loss: 0.0438, Val Loss: 0.1054\n",
      "Epoch [29/50], Train Loss: 0.0417, Val Loss: 0.1046\n",
      "Epoch [30/50], Train Loss: 0.0441, Val Loss: 0.1042\n",
      "Epoch [31/50], Train Loss: 0.0435, Val Loss: 0.1035\n",
      "Epoch [32/50], Train Loss: 0.0433, Val Loss: 0.1031\n",
      "Epoch [33/50], Train Loss: 0.0419, Val Loss: 0.1027\n",
      "Epoch [34/50], Train Loss: 0.0426, Val Loss: 0.1021\n",
      "Epoch [35/50], Train Loss: 0.0418, Val Loss: 0.1014\n",
      "Epoch [36/50], Train Loss: 0.0426, Val Loss: 0.1011\n",
      "Epoch [37/50], Train Loss: 0.0413, Val Loss: 0.1009\n",
      "Epoch [38/50], Train Loss: 0.0414, Val Loss: 0.1005\n",
      "Epoch [39/50], Train Loss: 0.0434, Val Loss: 0.1004\n",
      "Epoch [40/50], Train Loss: 0.0417, Val Loss: 0.1001\n",
      "Epoch [41/50], Train Loss: 0.0407, Val Loss: 0.0994\n",
      "Epoch [42/50], Train Loss: 0.0414, Val Loss: 0.0990\n",
      "Epoch [43/50], Train Loss: 0.0402, Val Loss: 0.0987\n",
      "Epoch [44/50], Train Loss: 0.0403, Val Loss: 0.0982\n",
      "Epoch [45/50], Train Loss: 0.0394, Val Loss: 0.0977\n",
      "Epoch [46/50], Train Loss: 0.0408, Val Loss: 0.0974\n",
      "Epoch [47/50], Train Loss: 0.0414, Val Loss: 0.0973\n",
      "Epoch [48/50], Train Loss: 0.0400, Val Loss: 0.0970\n",
      "Epoch [49/50], Train Loss: 0.0393, Val Loss: 0.0968\n",
      "Epoch [50/50], Train Loss: 0.0406, Val Loss: 0.0968\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=64, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0854, Val Loss: 0.2248\n",
      "Epoch [2/50], Train Loss: 0.0729, Val Loss: 0.1982\n",
      "Epoch [3/50], Train Loss: 0.0630, Val Loss: 0.1761\n",
      "Epoch [4/50], Train Loss: 0.0552, Val Loss: 0.1576\n",
      "Epoch [5/50], Train Loss: 0.0492, Val Loss: 0.1421\n",
      "Epoch [6/50], Train Loss: 0.0445, Val Loss: 0.1292\n",
      "Epoch [7/50], Train Loss: 0.0409, Val Loss: 0.1183\n",
      "Epoch [8/50], Train Loss: 0.0380, Val Loss: 0.1092\n",
      "Epoch [9/50], Train Loss: 0.0358, Val Loss: 0.1015\n",
      "Epoch [10/50], Train Loss: 0.0341, Val Loss: 0.0950\n",
      "Epoch [11/50], Train Loss: 0.0328, Val Loss: 0.0895\n",
      "Epoch [12/50], Train Loss: 0.0318, Val Loss: 0.0848\n",
      "Epoch [13/50], Train Loss: 0.0310, Val Loss: 0.0808\n",
      "Epoch [14/50], Train Loss: 0.0303, Val Loss: 0.0773\n",
      "Epoch [15/50], Train Loss: 0.0298, Val Loss: 0.0744\n",
      "Epoch [16/50], Train Loss: 0.0293, Val Loss: 0.0718\n",
      "Epoch [17/50], Train Loss: 0.0289, Val Loss: 0.0696\n",
      "Epoch [18/50], Train Loss: 0.0286, Val Loss: 0.0676\n",
      "Epoch [19/50], Train Loss: 0.0283, Val Loss: 0.0659\n",
      "Epoch [20/50], Train Loss: 0.0281, Val Loss: 0.0643\n",
      "Epoch [21/50], Train Loss: 0.0278, Val Loss: 0.0629\n",
      "Epoch [22/50], Train Loss: 0.0276, Val Loss: 0.0617\n",
      "Epoch [23/50], Train Loss: 0.0274, Val Loss: 0.0606\n",
      "Epoch [24/50], Train Loss: 0.0272, Val Loss: 0.0595\n",
      "Epoch [25/50], Train Loss: 0.0270, Val Loss: 0.0586\n",
      "Epoch [26/50], Train Loss: 0.0268, Val Loss: 0.0577\n",
      "Epoch [27/50], Train Loss: 0.0266, Val Loss: 0.0569\n",
      "Epoch [28/50], Train Loss: 0.0264, Val Loss: 0.0562\n",
      "Epoch [29/50], Train Loss: 0.0262, Val Loss: 0.0554\n",
      "Epoch [30/50], Train Loss: 0.0260, Val Loss: 0.0548\n",
      "Epoch [31/50], Train Loss: 0.0258, Val Loss: 0.0541\n",
      "Epoch [32/50], Train Loss: 0.0257, Val Loss: 0.0535\n",
      "Epoch [33/50], Train Loss: 0.0255, Val Loss: 0.0529\n",
      "Epoch [34/50], Train Loss: 0.0253, Val Loss: 0.0523\n",
      "Epoch [35/50], Train Loss: 0.0251, Val Loss: 0.0518\n",
      "Epoch [36/50], Train Loss: 0.0250, Val Loss: 0.0512\n",
      "Epoch [37/50], Train Loss: 0.0248, Val Loss: 0.0507\n",
      "Epoch [38/50], Train Loss: 0.0246, Val Loss: 0.0502\n",
      "Epoch [39/50], Train Loss: 0.0244, Val Loss: 0.0497\n",
      "Epoch [40/50], Train Loss: 0.0243, Val Loss: 0.0492\n",
      "Epoch [41/50], Train Loss: 0.0241, Val Loss: 0.0487\n",
      "Epoch [42/50], Train Loss: 0.0239, Val Loss: 0.0482\n",
      "Epoch [43/50], Train Loss: 0.0238, Val Loss: 0.0477\n",
      "Epoch [44/50], Train Loss: 0.0236, Val Loss: 0.0473\n",
      "Epoch [45/50], Train Loss: 0.0234, Val Loss: 0.0468\n",
      "Epoch [46/50], Train Loss: 0.0233, Val Loss: 0.0463\n",
      "Epoch [47/50], Train Loss: 0.0231, Val Loss: 0.0459\n",
      "Epoch [48/50], Train Loss: 0.0229, Val Loss: 0.0454\n",
      "Epoch [49/50], Train Loss: 0.0228, Val Loss: 0.0450\n",
      "Epoch [50/50], Train Loss: 0.0226, Val Loss: 0.0445\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=64, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1685, Val Loss: 0.4044\n",
      "Epoch [2/50], Train Loss: 0.1377, Val Loss: 0.3500\n",
      "Epoch [3/50], Train Loss: 0.1139, Val Loss: 0.3055\n",
      "Epoch [4/50], Train Loss: 0.0958, Val Loss: 0.2687\n",
      "Epoch [5/50], Train Loss: 0.0818, Val Loss: 0.2380\n",
      "Epoch [6/50], Train Loss: 0.0701, Val Loss: 0.2123\n",
      "Epoch [7/50], Train Loss: 0.0608, Val Loss: 0.1907\n",
      "Epoch [8/50], Train Loss: 0.0543, Val Loss: 0.1725\n",
      "Epoch [9/50], Train Loss: 0.0486, Val Loss: 0.1573\n",
      "Epoch [10/50], Train Loss: 0.0450, Val Loss: 0.1443\n",
      "Epoch [11/50], Train Loss: 0.0417, Val Loss: 0.1331\n",
      "Epoch [12/50], Train Loss: 0.0388, Val Loss: 0.1238\n",
      "Epoch [13/50], Train Loss: 0.0374, Val Loss: 0.1158\n",
      "Epoch [14/50], Train Loss: 0.0354, Val Loss: 0.1089\n",
      "Epoch [15/50], Train Loss: 0.0345, Val Loss: 0.1031\n",
      "Epoch [16/50], Train Loss: 0.0335, Val Loss: 0.0981\n",
      "Epoch [17/50], Train Loss: 0.0331, Val Loss: 0.0938\n",
      "Epoch [18/50], Train Loss: 0.0323, Val Loss: 0.0901\n",
      "Epoch [19/50], Train Loss: 0.0315, Val Loss: 0.0868\n",
      "Epoch [20/50], Train Loss: 0.0317, Val Loss: 0.0840\n",
      "Epoch [21/50], Train Loss: 0.0314, Val Loss: 0.0815\n",
      "Epoch [22/50], Train Loss: 0.0305, Val Loss: 0.0794\n",
      "Epoch [23/50], Train Loss: 0.0302, Val Loss: 0.0774\n",
      "Epoch [24/50], Train Loss: 0.0300, Val Loss: 0.0756\n",
      "Epoch [25/50], Train Loss: 0.0298, Val Loss: 0.0740\n",
      "Epoch [26/50], Train Loss: 0.0295, Val Loss: 0.0726\n",
      "Epoch [27/50], Train Loss: 0.0297, Val Loss: 0.0714\n",
      "Epoch [28/50], Train Loss: 0.0295, Val Loss: 0.0701\n",
      "Epoch [29/50], Train Loss: 0.0295, Val Loss: 0.0691\n",
      "Epoch [30/50], Train Loss: 0.0290, Val Loss: 0.0681\n",
      "Epoch [31/50], Train Loss: 0.0288, Val Loss: 0.0671\n",
      "Epoch [32/50], Train Loss: 0.0288, Val Loss: 0.0662\n",
      "Epoch [33/50], Train Loss: 0.0284, Val Loss: 0.0654\n",
      "Epoch [34/50], Train Loss: 0.0279, Val Loss: 0.0647\n",
      "Epoch [35/50], Train Loss: 0.0281, Val Loss: 0.0639\n",
      "Epoch [36/50], Train Loss: 0.0279, Val Loss: 0.0633\n",
      "Epoch [37/50], Train Loss: 0.0276, Val Loss: 0.0626\n",
      "Epoch [38/50], Train Loss: 0.0275, Val Loss: 0.0619\n",
      "Epoch [39/50], Train Loss: 0.0273, Val Loss: 0.0613\n",
      "Epoch [40/50], Train Loss: 0.0278, Val Loss: 0.0605\n",
      "Epoch [41/50], Train Loss: 0.0270, Val Loss: 0.0598\n",
      "Epoch [42/50], Train Loss: 0.0270, Val Loss: 0.0592\n",
      "Epoch [43/50], Train Loss: 0.0268, Val Loss: 0.0587\n",
      "Epoch [44/50], Train Loss: 0.0267, Val Loss: 0.0582\n",
      "Epoch [45/50], Train Loss: 0.0263, Val Loss: 0.0576\n",
      "Epoch [46/50], Train Loss: 0.0262, Val Loss: 0.0571\n",
      "Epoch [47/50], Train Loss: 0.0261, Val Loss: 0.0565\n",
      "Epoch [48/50], Train Loss: 0.0257, Val Loss: 0.0560\n",
      "Epoch [49/50], Train Loss: 0.0253, Val Loss: 0.0554\n",
      "Epoch [50/50], Train Loss: 0.0256, Val Loss: 0.0549\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=64, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1422, Val Loss: 0.3614\n",
      "Epoch [2/50], Train Loss: 0.1193, Val Loss: 0.3197\n",
      "Epoch [3/50], Train Loss: 0.1011, Val Loss: 0.2844\n",
      "Epoch [4/50], Train Loss: 0.0872, Val Loss: 0.2546\n",
      "Epoch [5/50], Train Loss: 0.0764, Val Loss: 0.2294\n",
      "Epoch [6/50], Train Loss: 0.0679, Val Loss: 0.2080\n",
      "Epoch [7/50], Train Loss: 0.0595, Val Loss: 0.1897\n",
      "Epoch [8/50], Train Loss: 0.0549, Val Loss: 0.1741\n",
      "Epoch [9/50], Train Loss: 0.0510, Val Loss: 0.1607\n",
      "Epoch [10/50], Train Loss: 0.0477, Val Loss: 0.1492\n",
      "Epoch [11/50], Train Loss: 0.0442, Val Loss: 0.1395\n",
      "Epoch [12/50], Train Loss: 0.0420, Val Loss: 0.1308\n",
      "Epoch [13/50], Train Loss: 0.0397, Val Loss: 0.1235\n",
      "Epoch [14/50], Train Loss: 0.0390, Val Loss: 0.1171\n",
      "Epoch [15/50], Train Loss: 0.0391, Val Loss: 0.1116\n",
      "Epoch [16/50], Train Loss: 0.0367, Val Loss: 0.1067\n",
      "Epoch [17/50], Train Loss: 0.0357, Val Loss: 0.1023\n",
      "Epoch [18/50], Train Loss: 0.0358, Val Loss: 0.0987\n",
      "Epoch [19/50], Train Loss: 0.0347, Val Loss: 0.0954\n",
      "Epoch [20/50], Train Loss: 0.0342, Val Loss: 0.0925\n",
      "Epoch [21/50], Train Loss: 0.0346, Val Loss: 0.0899\n",
      "Epoch [22/50], Train Loss: 0.0337, Val Loss: 0.0874\n",
      "Epoch [23/50], Train Loss: 0.0341, Val Loss: 0.0855\n",
      "Epoch [24/50], Train Loss: 0.0326, Val Loss: 0.0836\n",
      "Epoch [25/50], Train Loss: 0.0322, Val Loss: 0.0817\n",
      "Epoch [26/50], Train Loss: 0.0322, Val Loss: 0.0802\n",
      "Epoch [27/50], Train Loss: 0.0322, Val Loss: 0.0788\n",
      "Epoch [28/50], Train Loss: 0.0321, Val Loss: 0.0776\n",
      "Epoch [29/50], Train Loss: 0.0317, Val Loss: 0.0763\n",
      "Epoch [30/50], Train Loss: 0.0311, Val Loss: 0.0754\n",
      "Epoch [31/50], Train Loss: 0.0312, Val Loss: 0.0742\n",
      "Epoch [32/50], Train Loss: 0.0308, Val Loss: 0.0732\n",
      "Epoch [33/50], Train Loss: 0.0317, Val Loss: 0.0723\n",
      "Epoch [34/50], Train Loss: 0.0312, Val Loss: 0.0715\n",
      "Epoch [35/50], Train Loss: 0.0306, Val Loss: 0.0707\n",
      "Epoch [36/50], Train Loss: 0.0300, Val Loss: 0.0699\n",
      "Epoch [37/50], Train Loss: 0.0307, Val Loss: 0.0691\n",
      "Epoch [38/50], Train Loss: 0.0300, Val Loss: 0.0683\n",
      "Epoch [39/50], Train Loss: 0.0290, Val Loss: 0.0675\n",
      "Epoch [40/50], Train Loss: 0.0296, Val Loss: 0.0667\n",
      "Epoch [41/50], Train Loss: 0.0296, Val Loss: 0.0661\n",
      "Epoch [42/50], Train Loss: 0.0290, Val Loss: 0.0653\n",
      "Epoch [43/50], Train Loss: 0.0285, Val Loss: 0.0647\n",
      "Epoch [44/50], Train Loss: 0.0274, Val Loss: 0.0641\n",
      "Epoch [45/50], Train Loss: 0.0286, Val Loss: 0.0636\n",
      "Epoch [46/50], Train Loss: 0.0282, Val Loss: 0.0630\n",
      "Epoch [47/50], Train Loss: 0.0280, Val Loss: 0.0625\n",
      "Epoch [48/50], Train Loss: 0.0274, Val Loss: 0.0620\n",
      "Epoch [49/50], Train Loss: 0.0280, Val Loss: 0.0614\n",
      "Epoch [50/50], Train Loss: 0.0272, Val Loss: 0.0608\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=64, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0901, Val Loss: 0.2385\n",
      "Epoch [2/50], Train Loss: 0.0747, Val Loss: 0.2088\n",
      "Epoch [3/50], Train Loss: 0.0633, Val Loss: 0.1850\n",
      "Epoch [4/50], Train Loss: 0.0550, Val Loss: 0.1658\n",
      "Epoch [5/50], Train Loss: 0.0490, Val Loss: 0.1503\n",
      "Epoch [6/50], Train Loss: 0.0446, Val Loss: 0.1379\n",
      "Epoch [7/50], Train Loss: 0.0415, Val Loss: 0.1279\n",
      "Epoch [8/50], Train Loss: 0.0392, Val Loss: 0.1199\n",
      "Epoch [9/50], Train Loss: 0.0376, Val Loss: 0.1133\n",
      "Epoch [10/50], Train Loss: 0.0365, Val Loss: 0.1081\n",
      "Epoch [11/50], Train Loss: 0.0357, Val Loss: 0.1038\n",
      "Epoch [12/50], Train Loss: 0.0351, Val Loss: 0.1003\n",
      "Epoch [13/50], Train Loss: 0.0346, Val Loss: 0.0974\n",
      "Epoch [14/50], Train Loss: 0.0342, Val Loss: 0.0949\n",
      "Epoch [15/50], Train Loss: 0.0340, Val Loss: 0.0929\n",
      "Epoch [16/50], Train Loss: 0.0337, Val Loss: 0.0912\n",
      "Epoch [17/50], Train Loss: 0.0335, Val Loss: 0.0897\n",
      "Epoch [18/50], Train Loss: 0.0333, Val Loss: 0.0884\n",
      "Epoch [19/50], Train Loss: 0.0331, Val Loss: 0.0873\n",
      "Epoch [20/50], Train Loss: 0.0330, Val Loss: 0.0863\n",
      "Epoch [21/50], Train Loss: 0.0328, Val Loss: 0.0853\n",
      "Epoch [22/50], Train Loss: 0.0327, Val Loss: 0.0845\n",
      "Epoch [23/50], Train Loss: 0.0325, Val Loss: 0.0837\n",
      "Epoch [24/50], Train Loss: 0.0324, Val Loss: 0.0830\n",
      "Epoch [25/50], Train Loss: 0.0322, Val Loss: 0.0823\n",
      "Epoch [26/50], Train Loss: 0.0321, Val Loss: 0.0816\n",
      "Epoch [27/50], Train Loss: 0.0319, Val Loss: 0.0810\n",
      "Epoch [28/50], Train Loss: 0.0318, Val Loss: 0.0803\n",
      "Epoch [29/50], Train Loss: 0.0316, Val Loss: 0.0797\n",
      "Epoch [30/50], Train Loss: 0.0315, Val Loss: 0.0791\n",
      "Epoch [31/50], Train Loss: 0.0313, Val Loss: 0.0785\n",
      "Epoch [32/50], Train Loss: 0.0312, Val Loss: 0.0780\n",
      "Epoch [33/50], Train Loss: 0.0310, Val Loss: 0.0774\n",
      "Epoch [34/50], Train Loss: 0.0309, Val Loss: 0.0768\n",
      "Epoch [35/50], Train Loss: 0.0307, Val Loss: 0.0762\n",
      "Epoch [36/50], Train Loss: 0.0306, Val Loss: 0.0757\n",
      "Epoch [37/50], Train Loss: 0.0304, Val Loss: 0.0751\n",
      "Epoch [38/50], Train Loss: 0.0303, Val Loss: 0.0745\n",
      "Epoch [39/50], Train Loss: 0.0301, Val Loss: 0.0740\n",
      "Epoch [40/50], Train Loss: 0.0300, Val Loss: 0.0734\n",
      "Epoch [41/50], Train Loss: 0.0298, Val Loss: 0.0729\n",
      "Epoch [42/50], Train Loss: 0.0297, Val Loss: 0.0723\n",
      "Epoch [43/50], Train Loss: 0.0295, Val Loss: 0.0717\n",
      "Epoch [44/50], Train Loss: 0.0294, Val Loss: 0.0712\n",
      "Epoch [45/50], Train Loss: 0.0293, Val Loss: 0.0706\n",
      "Epoch [46/50], Train Loss: 0.0291, Val Loss: 0.0701\n",
      "Epoch [47/50], Train Loss: 0.0290, Val Loss: 0.0695\n",
      "Epoch [48/50], Train Loss: 0.0288, Val Loss: 0.0689\n",
      "Epoch [49/50], Train Loss: 0.0287, Val Loss: 0.0684\n",
      "Epoch [50/50], Train Loss: 0.0285, Val Loss: 0.0678\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=64, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1012, Val Loss: 0.2591\n",
      "Epoch [2/50], Train Loss: 0.0854, Val Loss: 0.2284\n",
      "Epoch [3/50], Train Loss: 0.0729, Val Loss: 0.2031\n",
      "Epoch [4/50], Train Loss: 0.0639, Val Loss: 0.1821\n",
      "Epoch [5/50], Train Loss: 0.0565, Val Loss: 0.1646\n",
      "Epoch [6/50], Train Loss: 0.0515, Val Loss: 0.1503\n",
      "Epoch [7/50], Train Loss: 0.0475, Val Loss: 0.1383\n",
      "Epoch [8/50], Train Loss: 0.0445, Val Loss: 0.1285\n",
      "Epoch [9/50], Train Loss: 0.0419, Val Loss: 0.1202\n",
      "Epoch [10/50], Train Loss: 0.0409, Val Loss: 0.1133\n",
      "Epoch [11/50], Train Loss: 0.0389, Val Loss: 0.1076\n",
      "Epoch [12/50], Train Loss: 0.0382, Val Loss: 0.1029\n",
      "Epoch [13/50], Train Loss: 0.0370, Val Loss: 0.0988\n",
      "Epoch [14/50], Train Loss: 0.0364, Val Loss: 0.0956\n",
      "Epoch [15/50], Train Loss: 0.0356, Val Loss: 0.0927\n",
      "Epoch [16/50], Train Loss: 0.0357, Val Loss: 0.0903\n",
      "Epoch [17/50], Train Loss: 0.0352, Val Loss: 0.0881\n",
      "Epoch [18/50], Train Loss: 0.0350, Val Loss: 0.0864\n",
      "Epoch [19/50], Train Loss: 0.0348, Val Loss: 0.0849\n",
      "Epoch [20/50], Train Loss: 0.0347, Val Loss: 0.0837\n",
      "Epoch [21/50], Train Loss: 0.0348, Val Loss: 0.0824\n",
      "Epoch [22/50], Train Loss: 0.0341, Val Loss: 0.0814\n",
      "Epoch [23/50], Train Loss: 0.0343, Val Loss: 0.0804\n",
      "Epoch [24/50], Train Loss: 0.0344, Val Loss: 0.0796\n",
      "Epoch [25/50], Train Loss: 0.0337, Val Loss: 0.0787\n",
      "Epoch [26/50], Train Loss: 0.0339, Val Loss: 0.0780\n",
      "Epoch [27/50], Train Loss: 0.0331, Val Loss: 0.0772\n",
      "Epoch [28/50], Train Loss: 0.0335, Val Loss: 0.0766\n",
      "Epoch [29/50], Train Loss: 0.0325, Val Loss: 0.0760\n",
      "Epoch [30/50], Train Loss: 0.0335, Val Loss: 0.0752\n",
      "Epoch [31/50], Train Loss: 0.0327, Val Loss: 0.0746\n",
      "Epoch [32/50], Train Loss: 0.0331, Val Loss: 0.0739\n",
      "Epoch [33/50], Train Loss: 0.0330, Val Loss: 0.0734\n",
      "Epoch [34/50], Train Loss: 0.0326, Val Loss: 0.0728\n",
      "Epoch [35/50], Train Loss: 0.0318, Val Loss: 0.0723\n",
      "Epoch [36/50], Train Loss: 0.0328, Val Loss: 0.0717\n",
      "Epoch [37/50], Train Loss: 0.0321, Val Loss: 0.0712\n",
      "Epoch [38/50], Train Loss: 0.0322, Val Loss: 0.0708\n",
      "Epoch [39/50], Train Loss: 0.0319, Val Loss: 0.0703\n",
      "Epoch [40/50], Train Loss: 0.0315, Val Loss: 0.0697\n",
      "Epoch [41/50], Train Loss: 0.0311, Val Loss: 0.0691\n",
      "Epoch [42/50], Train Loss: 0.0313, Val Loss: 0.0686\n",
      "Epoch [43/50], Train Loss: 0.0313, Val Loss: 0.0681\n",
      "Epoch [44/50], Train Loss: 0.0316, Val Loss: 0.0677\n",
      "Epoch [45/50], Train Loss: 0.0307, Val Loss: 0.0672\n",
      "Epoch [46/50], Train Loss: 0.0309, Val Loss: 0.0669\n",
      "Epoch [47/50], Train Loss: 0.0306, Val Loss: 0.0662\n",
      "Epoch [48/50], Train Loss: 0.0307, Val Loss: 0.0657\n",
      "Epoch [49/50], Train Loss: 0.0307, Val Loss: 0.0652\n",
      "Epoch [50/50], Train Loss: 0.0307, Val Loss: 0.0648\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=64, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1206, Val Loss: 0.3312\n",
      "Epoch [2/50], Train Loss: 0.1031, Val Loss: 0.2962\n",
      "Epoch [3/50], Train Loss: 0.0881, Val Loss: 0.2666\n",
      "Epoch [4/50], Train Loss: 0.0773, Val Loss: 0.2418\n",
      "Epoch [5/50], Train Loss: 0.0681, Val Loss: 0.2212\n",
      "Epoch [6/50], Train Loss: 0.0611, Val Loss: 0.2034\n",
      "Epoch [7/50], Train Loss: 0.0575, Val Loss: 0.1883\n",
      "Epoch [8/50], Train Loss: 0.0533, Val Loss: 0.1753\n",
      "Epoch [9/50], Train Loss: 0.0500, Val Loss: 0.1643\n",
      "Epoch [10/50], Train Loss: 0.0482, Val Loss: 0.1549\n",
      "Epoch [11/50], Train Loss: 0.0466, Val Loss: 0.1468\n",
      "Epoch [12/50], Train Loss: 0.0444, Val Loss: 0.1400\n",
      "Epoch [13/50], Train Loss: 0.0447, Val Loss: 0.1340\n",
      "Epoch [14/50], Train Loss: 0.0431, Val Loss: 0.1288\n",
      "Epoch [15/50], Train Loss: 0.0424, Val Loss: 0.1244\n",
      "Epoch [16/50], Train Loss: 0.0409, Val Loss: 0.1208\n",
      "Epoch [17/50], Train Loss: 0.0418, Val Loss: 0.1175\n",
      "Epoch [18/50], Train Loss: 0.0420, Val Loss: 0.1145\n",
      "Epoch [19/50], Train Loss: 0.0405, Val Loss: 0.1120\n",
      "Epoch [20/50], Train Loss: 0.0406, Val Loss: 0.1100\n",
      "Epoch [21/50], Train Loss: 0.0403, Val Loss: 0.1080\n",
      "Epoch [22/50], Train Loss: 0.0401, Val Loss: 0.1062\n",
      "Epoch [23/50], Train Loss: 0.0396, Val Loss: 0.1046\n",
      "Epoch [24/50], Train Loss: 0.0393, Val Loss: 0.1032\n",
      "Epoch [25/50], Train Loss: 0.0393, Val Loss: 0.1019\n",
      "Epoch [26/50], Train Loss: 0.0392, Val Loss: 0.1009\n",
      "Epoch [27/50], Train Loss: 0.0391, Val Loss: 0.0999\n",
      "Epoch [28/50], Train Loss: 0.0389, Val Loss: 0.0991\n",
      "Epoch [29/50], Train Loss: 0.0388, Val Loss: 0.0982\n",
      "Epoch [30/50], Train Loss: 0.0390, Val Loss: 0.0975\n",
      "Epoch [31/50], Train Loss: 0.0393, Val Loss: 0.0969\n",
      "Epoch [32/50], Train Loss: 0.0388, Val Loss: 0.0963\n",
      "Epoch [33/50], Train Loss: 0.0372, Val Loss: 0.0957\n",
      "Epoch [34/50], Train Loss: 0.0372, Val Loss: 0.0949\n",
      "Epoch [35/50], Train Loss: 0.0382, Val Loss: 0.0943\n",
      "Epoch [36/50], Train Loss: 0.0383, Val Loss: 0.0938\n",
      "Epoch [37/50], Train Loss: 0.0381, Val Loss: 0.0933\n",
      "Epoch [38/50], Train Loss: 0.0377, Val Loss: 0.0926\n",
      "Epoch [39/50], Train Loss: 0.0376, Val Loss: 0.0921\n",
      "Epoch [40/50], Train Loss: 0.0378, Val Loss: 0.0917\n",
      "Epoch [41/50], Train Loss: 0.0376, Val Loss: 0.0913\n",
      "Epoch [42/50], Train Loss: 0.0366, Val Loss: 0.0908\n",
      "Epoch [43/50], Train Loss: 0.0367, Val Loss: 0.0903\n",
      "Epoch [44/50], Train Loss: 0.0365, Val Loss: 0.0899\n",
      "Epoch [45/50], Train Loss: 0.0373, Val Loss: 0.0894\n",
      "Epoch [46/50], Train Loss: 0.0363, Val Loss: 0.0890\n",
      "Epoch [47/50], Train Loss: 0.0359, Val Loss: 0.0885\n",
      "Epoch [48/50], Train Loss: 0.0362, Val Loss: 0.0882\n",
      "Epoch [49/50], Train Loss: 0.0361, Val Loss: 0.0881\n",
      "Epoch [50/50], Train Loss: 0.0359, Val Loss: 0.0876\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=64, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1214, Val Loss: 0.3106\n",
      "Epoch [2/50], Train Loss: 0.0997, Val Loss: 0.2723\n",
      "Epoch [3/50], Train Loss: 0.0833, Val Loss: 0.2411\n",
      "Epoch [4/50], Train Loss: 0.0709, Val Loss: 0.2155\n",
      "Epoch [5/50], Train Loss: 0.0615, Val Loss: 0.1944\n",
      "Epoch [6/50], Train Loss: 0.0546, Val Loss: 0.1770\n",
      "Epoch [7/50], Train Loss: 0.0494, Val Loss: 0.1627\n",
      "Epoch [8/50], Train Loss: 0.0456, Val Loss: 0.1509\n",
      "Epoch [9/50], Train Loss: 0.0428, Val Loss: 0.1413\n",
      "Epoch [10/50], Train Loss: 0.0408, Val Loss: 0.1333\n",
      "Epoch [11/50], Train Loss: 0.0394, Val Loss: 0.1268\n",
      "Epoch [12/50], Train Loss: 0.0383, Val Loss: 0.1214\n",
      "Epoch [13/50], Train Loss: 0.0376, Val Loss: 0.1170\n",
      "Epoch [14/50], Train Loss: 0.0370, Val Loss: 0.1134\n",
      "Epoch [15/50], Train Loss: 0.0367, Val Loss: 0.1104\n",
      "Epoch [16/50], Train Loss: 0.0364, Val Loss: 0.1078\n",
      "Epoch [17/50], Train Loss: 0.0361, Val Loss: 0.1057\n",
      "Epoch [18/50], Train Loss: 0.0359, Val Loss: 0.1040\n",
      "Epoch [19/50], Train Loss: 0.0358, Val Loss: 0.1024\n",
      "Epoch [20/50], Train Loss: 0.0356, Val Loss: 0.1011\n",
      "Epoch [21/50], Train Loss: 0.0355, Val Loss: 0.1000\n",
      "Epoch [22/50], Train Loss: 0.0354, Val Loss: 0.0990\n",
      "Epoch [23/50], Train Loss: 0.0353, Val Loss: 0.0981\n",
      "Epoch [24/50], Train Loss: 0.0352, Val Loss: 0.0974\n",
      "Epoch [25/50], Train Loss: 0.0351, Val Loss: 0.0966\n",
      "Epoch [26/50], Train Loss: 0.0350, Val Loss: 0.0960\n",
      "Epoch [27/50], Train Loss: 0.0349, Val Loss: 0.0954\n",
      "Epoch [28/50], Train Loss: 0.0348, Val Loss: 0.0948\n",
      "Epoch [29/50], Train Loss: 0.0347, Val Loss: 0.0943\n",
      "Epoch [30/50], Train Loss: 0.0346, Val Loss: 0.0938\n",
      "Epoch [31/50], Train Loss: 0.0345, Val Loss: 0.0933\n",
      "Epoch [32/50], Train Loss: 0.0344, Val Loss: 0.0928\n",
      "Epoch [33/50], Train Loss: 0.0343, Val Loss: 0.0923\n",
      "Epoch [34/50], Train Loss: 0.0342, Val Loss: 0.0919\n",
      "Epoch [35/50], Train Loss: 0.0341, Val Loss: 0.0914\n",
      "Epoch [36/50], Train Loss: 0.0340, Val Loss: 0.0910\n",
      "Epoch [37/50], Train Loss: 0.0339, Val Loss: 0.0905\n",
      "Epoch [38/50], Train Loss: 0.0338, Val Loss: 0.0901\n",
      "Epoch [39/50], Train Loss: 0.0336, Val Loss: 0.0897\n",
      "Epoch [40/50], Train Loss: 0.0335, Val Loss: 0.0892\n",
      "Epoch [41/50], Train Loss: 0.0334, Val Loss: 0.0888\n",
      "Epoch [42/50], Train Loss: 0.0333, Val Loss: 0.0884\n",
      "Epoch [43/50], Train Loss: 0.0332, Val Loss: 0.0880\n",
      "Epoch [44/50], Train Loss: 0.0331, Val Loss: 0.0875\n",
      "Epoch [45/50], Train Loss: 0.0330, Val Loss: 0.0871\n",
      "Epoch [46/50], Train Loss: 0.0329, Val Loss: 0.0867\n",
      "Epoch [47/50], Train Loss: 0.0328, Val Loss: 0.0862\n",
      "Epoch [48/50], Train Loss: 0.0327, Val Loss: 0.0858\n",
      "Epoch [49/50], Train Loss: 0.0326, Val Loss: 0.0853\n",
      "Epoch [50/50], Train Loss: 0.0325, Val Loss: 0.0849\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=64, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1198, Val Loss: 0.3031\n",
      "Epoch [2/50], Train Loss: 0.0983, Val Loss: 0.2648\n",
      "Epoch [3/50], Train Loss: 0.0820, Val Loss: 0.2339\n",
      "Epoch [4/50], Train Loss: 0.0705, Val Loss: 0.2090\n",
      "Epoch [5/50], Train Loss: 0.0622, Val Loss: 0.1887\n",
      "Epoch [6/50], Train Loss: 0.0552, Val Loss: 0.1718\n",
      "Epoch [7/50], Train Loss: 0.0500, Val Loss: 0.1581\n",
      "Epoch [8/50], Train Loss: 0.0463, Val Loss: 0.1467\n",
      "Epoch [9/50], Train Loss: 0.0434, Val Loss: 0.1375\n",
      "Epoch [10/50], Train Loss: 0.0417, Val Loss: 0.1298\n",
      "Epoch [11/50], Train Loss: 0.0399, Val Loss: 0.1235\n",
      "Epoch [12/50], Train Loss: 0.0393, Val Loss: 0.1184\n",
      "Epoch [13/50], Train Loss: 0.0388, Val Loss: 0.1139\n",
      "Epoch [14/50], Train Loss: 0.0376, Val Loss: 0.1104\n",
      "Epoch [15/50], Train Loss: 0.0374, Val Loss: 0.1075\n",
      "Epoch [16/50], Train Loss: 0.0370, Val Loss: 0.1049\n",
      "Epoch [17/50], Train Loss: 0.0368, Val Loss: 0.1029\n",
      "Epoch [18/50], Train Loss: 0.0370, Val Loss: 0.1011\n",
      "Epoch [19/50], Train Loss: 0.0365, Val Loss: 0.0995\n",
      "Epoch [20/50], Train Loss: 0.0365, Val Loss: 0.0983\n",
      "Epoch [21/50], Train Loss: 0.0363, Val Loss: 0.0971\n",
      "Epoch [22/50], Train Loss: 0.0368, Val Loss: 0.0963\n",
      "Epoch [23/50], Train Loss: 0.0360, Val Loss: 0.0954\n",
      "Epoch [24/50], Train Loss: 0.0357, Val Loss: 0.0946\n",
      "Epoch [25/50], Train Loss: 0.0361, Val Loss: 0.0938\n",
      "Epoch [26/50], Train Loss: 0.0356, Val Loss: 0.0930\n",
      "Epoch [27/50], Train Loss: 0.0354, Val Loss: 0.0922\n",
      "Epoch [28/50], Train Loss: 0.0354, Val Loss: 0.0919\n",
      "Epoch [29/50], Train Loss: 0.0360, Val Loss: 0.0914\n",
      "Epoch [30/50], Train Loss: 0.0355, Val Loss: 0.0908\n",
      "Epoch [31/50], Train Loss: 0.0346, Val Loss: 0.0903\n",
      "Epoch [32/50], Train Loss: 0.0350, Val Loss: 0.0899\n",
      "Epoch [33/50], Train Loss: 0.0350, Val Loss: 0.0894\n",
      "Epoch [34/50], Train Loss: 0.0348, Val Loss: 0.0890\n",
      "Epoch [35/50], Train Loss: 0.0350, Val Loss: 0.0885\n",
      "Epoch [36/50], Train Loss: 0.0345, Val Loss: 0.0881\n",
      "Epoch [37/50], Train Loss: 0.0348, Val Loss: 0.0877\n",
      "Epoch [38/50], Train Loss: 0.0346, Val Loss: 0.0874\n",
      "Epoch [39/50], Train Loss: 0.0345, Val Loss: 0.0869\n",
      "Epoch [40/50], Train Loss: 0.0344, Val Loss: 0.0865\n",
      "Epoch [41/50], Train Loss: 0.0341, Val Loss: 0.0861\n",
      "Epoch [42/50], Train Loss: 0.0342, Val Loss: 0.0859\n",
      "Epoch [43/50], Train Loss: 0.0339, Val Loss: 0.0855\n",
      "Epoch [44/50], Train Loss: 0.0336, Val Loss: 0.0850\n",
      "Epoch [45/50], Train Loss: 0.0336, Val Loss: 0.0846\n",
      "Epoch [46/50], Train Loss: 0.0336, Val Loss: 0.0842\n",
      "Epoch [47/50], Train Loss: 0.0337, Val Loss: 0.0838\n",
      "Epoch [48/50], Train Loss: 0.0338, Val Loss: 0.0833\n",
      "Epoch [49/50], Train Loss: 0.0335, Val Loss: 0.0829\n",
      "Epoch [50/50], Train Loss: 0.0335, Val Loss: 0.0824\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=64, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1084, Val Loss: 0.2649\n",
      "Epoch [2/50], Train Loss: 0.0892, Val Loss: 0.2311\n",
      "Epoch [3/50], Train Loss: 0.0751, Val Loss: 0.2051\n",
      "Epoch [4/50], Train Loss: 0.0667, Val Loss: 0.1838\n",
      "Epoch [5/50], Train Loss: 0.0583, Val Loss: 0.1670\n",
      "Epoch [6/50], Train Loss: 0.0544, Val Loss: 0.1536\n",
      "Epoch [7/50], Train Loss: 0.0513, Val Loss: 0.1430\n",
      "Epoch [8/50], Train Loss: 0.0492, Val Loss: 0.1347\n",
      "Epoch [9/50], Train Loss: 0.0472, Val Loss: 0.1278\n",
      "Epoch [10/50], Train Loss: 0.0456, Val Loss: 0.1222\n",
      "Epoch [11/50], Train Loss: 0.0458, Val Loss: 0.1177\n",
      "Epoch [12/50], Train Loss: 0.0434, Val Loss: 0.1142\n",
      "Epoch [13/50], Train Loss: 0.0437, Val Loss: 0.1115\n",
      "Epoch [14/50], Train Loss: 0.0436, Val Loss: 0.1092\n",
      "Epoch [15/50], Train Loss: 0.0422, Val Loss: 0.1073\n",
      "Epoch [16/50], Train Loss: 0.0422, Val Loss: 0.1059\n",
      "Epoch [17/50], Train Loss: 0.0425, Val Loss: 0.1047\n",
      "Epoch [18/50], Train Loss: 0.0427, Val Loss: 0.1036\n",
      "Epoch [19/50], Train Loss: 0.0416, Val Loss: 0.1024\n",
      "Epoch [20/50], Train Loss: 0.0410, Val Loss: 0.1012\n",
      "Epoch [21/50], Train Loss: 0.0419, Val Loss: 0.1005\n",
      "Epoch [22/50], Train Loss: 0.0418, Val Loss: 0.0999\n",
      "Epoch [23/50], Train Loss: 0.0409, Val Loss: 0.0993\n",
      "Epoch [24/50], Train Loss: 0.0409, Val Loss: 0.0987\n",
      "Epoch [25/50], Train Loss: 0.0411, Val Loss: 0.0982\n",
      "Epoch [26/50], Train Loss: 0.0417, Val Loss: 0.0977\n",
      "Epoch [27/50], Train Loss: 0.0403, Val Loss: 0.0973\n",
      "Epoch [28/50], Train Loss: 0.0404, Val Loss: 0.0971\n",
      "Epoch [29/50], Train Loss: 0.0414, Val Loss: 0.0968\n",
      "Epoch [30/50], Train Loss: 0.0397, Val Loss: 0.0964\n",
      "Epoch [31/50], Train Loss: 0.0404, Val Loss: 0.0961\n",
      "Epoch [32/50], Train Loss: 0.0398, Val Loss: 0.0954\n",
      "Epoch [33/50], Train Loss: 0.0405, Val Loss: 0.0950\n",
      "Epoch [34/50], Train Loss: 0.0398, Val Loss: 0.0949\n",
      "Epoch [35/50], Train Loss: 0.0386, Val Loss: 0.0946\n",
      "Epoch [36/50], Train Loss: 0.0389, Val Loss: 0.0942\n",
      "Epoch [37/50], Train Loss: 0.0392, Val Loss: 0.0939\n",
      "Epoch [38/50], Train Loss: 0.0384, Val Loss: 0.0936\n",
      "Epoch [39/50], Train Loss: 0.0386, Val Loss: 0.0933\n",
      "Epoch [40/50], Train Loss: 0.0395, Val Loss: 0.0928\n",
      "Epoch [41/50], Train Loss: 0.0386, Val Loss: 0.0925\n",
      "Epoch [42/50], Train Loss: 0.0380, Val Loss: 0.0926\n",
      "Epoch [43/50], Train Loss: 0.0377, Val Loss: 0.0921\n",
      "Epoch [44/50], Train Loss: 0.0375, Val Loss: 0.0919\n",
      "Epoch [45/50], Train Loss: 0.0378, Val Loss: 0.0915\n",
      "Epoch [46/50], Train Loss: 0.0383, Val Loss: 0.0912\n",
      "Epoch [47/50], Train Loss: 0.0374, Val Loss: 0.0908\n",
      "Epoch [48/50], Train Loss: 0.0377, Val Loss: 0.0906\n",
      "Epoch [49/50], Train Loss: 0.0372, Val Loss: 0.0902\n",
      "Epoch [50/50], Train Loss: 0.0366, Val Loss: 0.0899\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=128, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1410, Val Loss: 0.3791\n",
      "Epoch [2/50], Train Loss: 0.1175, Val Loss: 0.3338\n",
      "Epoch [3/50], Train Loss: 0.0989, Val Loss: 0.2960\n",
      "Epoch [4/50], Train Loss: 0.0842, Val Loss: 0.2640\n",
      "Epoch [5/50], Train Loss: 0.0725, Val Loss: 0.2370\n",
      "Epoch [6/50], Train Loss: 0.0632, Val Loss: 0.2140\n",
      "Epoch [7/50], Train Loss: 0.0558, Val Loss: 0.1943\n",
      "Epoch [8/50], Train Loss: 0.0500, Val Loss: 0.1775\n",
      "Epoch [9/50], Train Loss: 0.0453, Val Loss: 0.1632\n",
      "Epoch [10/50], Train Loss: 0.0417, Val Loss: 0.1509\n",
      "Epoch [11/50], Train Loss: 0.0389, Val Loss: 0.1403\n",
      "Epoch [12/50], Train Loss: 0.0366, Val Loss: 0.1313\n",
      "Epoch [13/50], Train Loss: 0.0348, Val Loss: 0.1234\n",
      "Epoch [14/50], Train Loss: 0.0334, Val Loss: 0.1167\n",
      "Epoch [15/50], Train Loss: 0.0323, Val Loss: 0.1108\n",
      "Epoch [16/50], Train Loss: 0.0314, Val Loss: 0.1058\n",
      "Epoch [17/50], Train Loss: 0.0307, Val Loss: 0.1013\n",
      "Epoch [18/50], Train Loss: 0.0301, Val Loss: 0.0974\n",
      "Epoch [19/50], Train Loss: 0.0296, Val Loss: 0.0940\n",
      "Epoch [20/50], Train Loss: 0.0292, Val Loss: 0.0910\n",
      "Epoch [21/50], Train Loss: 0.0288, Val Loss: 0.0883\n",
      "Epoch [22/50], Train Loss: 0.0285, Val Loss: 0.0860\n",
      "Epoch [23/50], Train Loss: 0.0282, Val Loss: 0.0838\n",
      "Epoch [24/50], Train Loss: 0.0279, Val Loss: 0.0819\n",
      "Epoch [25/50], Train Loss: 0.0276, Val Loss: 0.0802\n",
      "Epoch [26/50], Train Loss: 0.0274, Val Loss: 0.0786\n",
      "Epoch [27/50], Train Loss: 0.0272, Val Loss: 0.0771\n",
      "Epoch [28/50], Train Loss: 0.0269, Val Loss: 0.0758\n",
      "Epoch [29/50], Train Loss: 0.0267, Val Loss: 0.0746\n",
      "Epoch [30/50], Train Loss: 0.0265, Val Loss: 0.0734\n",
      "Epoch [31/50], Train Loss: 0.0263, Val Loss: 0.0723\n",
      "Epoch [32/50], Train Loss: 0.0261, Val Loss: 0.0713\n",
      "Epoch [33/50], Train Loss: 0.0258, Val Loss: 0.0703\n",
      "Epoch [34/50], Train Loss: 0.0256, Val Loss: 0.0694\n",
      "Epoch [35/50], Train Loss: 0.0254, Val Loss: 0.0685\n",
      "Epoch [36/50], Train Loss: 0.0252, Val Loss: 0.0677\n",
      "Epoch [37/50], Train Loss: 0.0250, Val Loss: 0.0669\n",
      "Epoch [38/50], Train Loss: 0.0248, Val Loss: 0.0661\n",
      "Epoch [39/50], Train Loss: 0.0246, Val Loss: 0.0654\n",
      "Epoch [40/50], Train Loss: 0.0244, Val Loss: 0.0646\n",
      "Epoch [41/50], Train Loss: 0.0242, Val Loss: 0.0639\n",
      "Epoch [42/50], Train Loss: 0.0240, Val Loss: 0.0632\n",
      "Epoch [43/50], Train Loss: 0.0238, Val Loss: 0.0625\n",
      "Epoch [44/50], Train Loss: 0.0236, Val Loss: 0.0618\n",
      "Epoch [45/50], Train Loss: 0.0234, Val Loss: 0.0612\n",
      "Epoch [46/50], Train Loss: 0.0232, Val Loss: 0.0605\n",
      "Epoch [47/50], Train Loss: 0.0230, Val Loss: 0.0599\n",
      "Epoch [48/50], Train Loss: 0.0228, Val Loss: 0.0593\n",
      "Epoch [49/50], Train Loss: 0.0226, Val Loss: 0.0586\n",
      "Epoch [50/50], Train Loss: 0.0224, Val Loss: 0.0580\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=128, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1052, Val Loss: 0.2931\n",
      "Epoch [2/50], Train Loss: 0.0872, Val Loss: 0.2576\n",
      "Epoch [3/50], Train Loss: 0.0742, Val Loss: 0.2281\n",
      "Epoch [4/50], Train Loss: 0.0639, Val Loss: 0.2035\n",
      "Epoch [5/50], Train Loss: 0.0558, Val Loss: 0.1829\n",
      "Epoch [6/50], Train Loss: 0.0494, Val Loss: 0.1656\n",
      "Epoch [7/50], Train Loss: 0.0443, Val Loss: 0.1511\n",
      "Epoch [8/50], Train Loss: 0.0406, Val Loss: 0.1387\n",
      "Epoch [9/50], Train Loss: 0.0378, Val Loss: 0.1283\n",
      "Epoch [10/50], Train Loss: 0.0357, Val Loss: 0.1193\n",
      "Epoch [11/50], Train Loss: 0.0337, Val Loss: 0.1118\n",
      "Epoch [12/50], Train Loss: 0.0325, Val Loss: 0.1053\n",
      "Epoch [13/50], Train Loss: 0.0311, Val Loss: 0.0997\n",
      "Epoch [14/50], Train Loss: 0.0306, Val Loss: 0.0950\n",
      "Epoch [15/50], Train Loss: 0.0298, Val Loss: 0.0908\n",
      "Epoch [16/50], Train Loss: 0.0293, Val Loss: 0.0872\n",
      "Epoch [17/50], Train Loss: 0.0286, Val Loss: 0.0841\n",
      "Epoch [18/50], Train Loss: 0.0284, Val Loss: 0.0813\n",
      "Epoch [19/50], Train Loss: 0.0282, Val Loss: 0.0788\n",
      "Epoch [20/50], Train Loss: 0.0278, Val Loss: 0.0766\n",
      "Epoch [21/50], Train Loss: 0.0272, Val Loss: 0.0748\n",
      "Epoch [22/50], Train Loss: 0.0269, Val Loss: 0.0731\n",
      "Epoch [23/50], Train Loss: 0.0271, Val Loss: 0.0716\n",
      "Epoch [24/50], Train Loss: 0.0265, Val Loss: 0.0702\n",
      "Epoch [25/50], Train Loss: 0.0261, Val Loss: 0.0689\n",
      "Epoch [26/50], Train Loss: 0.0263, Val Loss: 0.0677\n",
      "Epoch [27/50], Train Loss: 0.0264, Val Loss: 0.0667\n",
      "Epoch [28/50], Train Loss: 0.0259, Val Loss: 0.0656\n",
      "Epoch [29/50], Train Loss: 0.0258, Val Loss: 0.0647\n",
      "Epoch [30/50], Train Loss: 0.0257, Val Loss: 0.0639\n",
      "Epoch [31/50], Train Loss: 0.0257, Val Loss: 0.0630\n",
      "Epoch [32/50], Train Loss: 0.0253, Val Loss: 0.0622\n",
      "Epoch [33/50], Train Loss: 0.0252, Val Loss: 0.0615\n",
      "Epoch [34/50], Train Loss: 0.0252, Val Loss: 0.0607\n",
      "Epoch [35/50], Train Loss: 0.0246, Val Loss: 0.0600\n",
      "Epoch [36/50], Train Loss: 0.0244, Val Loss: 0.0594\n",
      "Epoch [37/50], Train Loss: 0.0243, Val Loss: 0.0588\n",
      "Epoch [38/50], Train Loss: 0.0241, Val Loss: 0.0581\n",
      "Epoch [39/50], Train Loss: 0.0239, Val Loss: 0.0575\n",
      "Epoch [40/50], Train Loss: 0.0238, Val Loss: 0.0569\n",
      "Epoch [41/50], Train Loss: 0.0236, Val Loss: 0.0563\n",
      "Epoch [42/50], Train Loss: 0.0234, Val Loss: 0.0556\n",
      "Epoch [43/50], Train Loss: 0.0233, Val Loss: 0.0551\n",
      "Epoch [44/50], Train Loss: 0.0232, Val Loss: 0.0545\n",
      "Epoch [45/50], Train Loss: 0.0228, Val Loss: 0.0539\n",
      "Epoch [46/50], Train Loss: 0.0225, Val Loss: 0.0533\n",
      "Epoch [47/50], Train Loss: 0.0227, Val Loss: 0.0527\n",
      "Epoch [48/50], Train Loss: 0.0223, Val Loss: 0.0522\n",
      "Epoch [49/50], Train Loss: 0.0224, Val Loss: 0.0516\n",
      "Epoch [50/50], Train Loss: 0.0220, Val Loss: 0.0511\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=128, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0813, Val Loss: 0.2315\n",
      "Epoch [2/50], Train Loss: 0.0707, Val Loss: 0.2059\n",
      "Epoch [3/50], Train Loss: 0.0621, Val Loss: 0.1848\n",
      "Epoch [4/50], Train Loss: 0.0551, Val Loss: 0.1668\n",
      "Epoch [5/50], Train Loss: 0.0500, Val Loss: 0.1516\n",
      "Epoch [6/50], Train Loss: 0.0465, Val Loss: 0.1387\n",
      "Epoch [7/50], Train Loss: 0.0432, Val Loss: 0.1277\n",
      "Epoch [8/50], Train Loss: 0.0402, Val Loss: 0.1183\n",
      "Epoch [9/50], Train Loss: 0.0373, Val Loss: 0.1103\n",
      "Epoch [10/50], Train Loss: 0.0361, Val Loss: 0.1034\n",
      "Epoch [11/50], Train Loss: 0.0349, Val Loss: 0.0974\n",
      "Epoch [12/50], Train Loss: 0.0347, Val Loss: 0.0922\n",
      "Epoch [13/50], Train Loss: 0.0328, Val Loss: 0.0878\n",
      "Epoch [14/50], Train Loss: 0.0326, Val Loss: 0.0839\n",
      "Epoch [15/50], Train Loss: 0.0323, Val Loss: 0.0806\n",
      "Epoch [16/50], Train Loss: 0.0315, Val Loss: 0.0776\n",
      "Epoch [17/50], Train Loss: 0.0309, Val Loss: 0.0750\n",
      "Epoch [18/50], Train Loss: 0.0305, Val Loss: 0.0726\n",
      "Epoch [19/50], Train Loss: 0.0307, Val Loss: 0.0706\n",
      "Epoch [20/50], Train Loss: 0.0305, Val Loss: 0.0688\n",
      "Epoch [21/50], Train Loss: 0.0293, Val Loss: 0.0671\n",
      "Epoch [22/50], Train Loss: 0.0297, Val Loss: 0.0657\n",
      "Epoch [23/50], Train Loss: 0.0296, Val Loss: 0.0643\n",
      "Epoch [24/50], Train Loss: 0.0292, Val Loss: 0.0631\n",
      "Epoch [25/50], Train Loss: 0.0286, Val Loss: 0.0619\n",
      "Epoch [26/50], Train Loss: 0.0280, Val Loss: 0.0609\n",
      "Epoch [27/50], Train Loss: 0.0290, Val Loss: 0.0598\n",
      "Epoch [28/50], Train Loss: 0.0287, Val Loss: 0.0589\n",
      "Epoch [29/50], Train Loss: 0.0286, Val Loss: 0.0581\n",
      "Epoch [30/50], Train Loss: 0.0283, Val Loss: 0.0572\n",
      "Epoch [31/50], Train Loss: 0.0276, Val Loss: 0.0566\n",
      "Epoch [32/50], Train Loss: 0.0275, Val Loss: 0.0559\n",
      "Epoch [33/50], Train Loss: 0.0270, Val Loss: 0.0551\n",
      "Epoch [34/50], Train Loss: 0.0273, Val Loss: 0.0544\n",
      "Epoch [35/50], Train Loss: 0.0270, Val Loss: 0.0540\n",
      "Epoch [36/50], Train Loss: 0.0266, Val Loss: 0.0531\n",
      "Epoch [37/50], Train Loss: 0.0264, Val Loss: 0.0526\n",
      "Epoch [38/50], Train Loss: 0.0263, Val Loss: 0.0520\n",
      "Epoch [39/50], Train Loss: 0.0263, Val Loss: 0.0514\n",
      "Epoch [40/50], Train Loss: 0.0256, Val Loss: 0.0508\n",
      "Epoch [41/50], Train Loss: 0.0259, Val Loss: 0.0502\n",
      "Epoch [42/50], Train Loss: 0.0257, Val Loss: 0.0497\n",
      "Epoch [43/50], Train Loss: 0.0259, Val Loss: 0.0490\n",
      "Epoch [44/50], Train Loss: 0.0263, Val Loss: 0.0484\n",
      "Epoch [45/50], Train Loss: 0.0252, Val Loss: 0.0478\n",
      "Epoch [46/50], Train Loss: 0.0247, Val Loss: 0.0473\n",
      "Epoch [47/50], Train Loss: 0.0242, Val Loss: 0.0468\n",
      "Epoch [48/50], Train Loss: 0.0248, Val Loss: 0.0462\n",
      "Epoch [49/50], Train Loss: 0.0242, Val Loss: 0.0458\n",
      "Epoch [50/50], Train Loss: 0.0243, Val Loss: 0.0453\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=128, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1209, Val Loss: 0.3190\n",
      "Epoch [2/50], Train Loss: 0.1000, Val Loss: 0.2802\n",
      "Epoch [3/50], Train Loss: 0.0838, Val Loss: 0.2481\n",
      "Epoch [4/50], Train Loss: 0.0712, Val Loss: 0.2212\n",
      "Epoch [5/50], Train Loss: 0.0615, Val Loss: 0.1986\n",
      "Epoch [6/50], Train Loss: 0.0540, Val Loss: 0.1797\n",
      "Epoch [7/50], Train Loss: 0.0483, Val Loss: 0.1638\n",
      "Epoch [8/50], Train Loss: 0.0439, Val Loss: 0.1504\n",
      "Epoch [9/50], Train Loss: 0.0406, Val Loss: 0.1393\n",
      "Epoch [10/50], Train Loss: 0.0382, Val Loss: 0.1299\n",
      "Epoch [11/50], Train Loss: 0.0363, Val Loss: 0.1221\n",
      "Epoch [12/50], Train Loss: 0.0350, Val Loss: 0.1155\n",
      "Epoch [13/50], Train Loss: 0.0340, Val Loss: 0.1100\n",
      "Epoch [14/50], Train Loss: 0.0332, Val Loss: 0.1054\n",
      "Epoch [15/50], Train Loss: 0.0326, Val Loss: 0.1015\n",
      "Epoch [16/50], Train Loss: 0.0322, Val Loss: 0.0982\n",
      "Epoch [17/50], Train Loss: 0.0319, Val Loss: 0.0954\n",
      "Epoch [18/50], Train Loss: 0.0316, Val Loss: 0.0930\n",
      "Epoch [19/50], Train Loss: 0.0313, Val Loss: 0.0910\n",
      "Epoch [20/50], Train Loss: 0.0311, Val Loss: 0.0892\n",
      "Epoch [21/50], Train Loss: 0.0309, Val Loss: 0.0876\n",
      "Epoch [22/50], Train Loss: 0.0308, Val Loss: 0.0863\n",
      "Epoch [23/50], Train Loss: 0.0306, Val Loss: 0.0850\n",
      "Epoch [24/50], Train Loss: 0.0305, Val Loss: 0.0839\n",
      "Epoch [25/50], Train Loss: 0.0303, Val Loss: 0.0830\n",
      "Epoch [26/50], Train Loss: 0.0302, Val Loss: 0.0821\n",
      "Epoch [27/50], Train Loss: 0.0300, Val Loss: 0.0813\n",
      "Epoch [28/50], Train Loss: 0.0299, Val Loss: 0.0805\n",
      "Epoch [29/50], Train Loss: 0.0297, Val Loss: 0.0798\n",
      "Epoch [30/50], Train Loss: 0.0296, Val Loss: 0.0791\n",
      "Epoch [31/50], Train Loss: 0.0294, Val Loss: 0.0785\n",
      "Epoch [32/50], Train Loss: 0.0293, Val Loss: 0.0778\n",
      "Epoch [33/50], Train Loss: 0.0291, Val Loss: 0.0772\n",
      "Epoch [34/50], Train Loss: 0.0290, Val Loss: 0.0767\n",
      "Epoch [35/50], Train Loss: 0.0289, Val Loss: 0.0761\n",
      "Epoch [36/50], Train Loss: 0.0287, Val Loss: 0.0756\n",
      "Epoch [37/50], Train Loss: 0.0286, Val Loss: 0.0750\n",
      "Epoch [38/50], Train Loss: 0.0284, Val Loss: 0.0745\n",
      "Epoch [39/50], Train Loss: 0.0283, Val Loss: 0.0740\n",
      "Epoch [40/50], Train Loss: 0.0281, Val Loss: 0.0734\n",
      "Epoch [41/50], Train Loss: 0.0280, Val Loss: 0.0729\n",
      "Epoch [42/50], Train Loss: 0.0278, Val Loss: 0.0724\n",
      "Epoch [43/50], Train Loss: 0.0277, Val Loss: 0.0719\n",
      "Epoch [44/50], Train Loss: 0.0275, Val Loss: 0.0714\n",
      "Epoch [45/50], Train Loss: 0.0274, Val Loss: 0.0709\n",
      "Epoch [46/50], Train Loss: 0.0272, Val Loss: 0.0704\n",
      "Epoch [47/50], Train Loss: 0.0271, Val Loss: 0.0699\n",
      "Epoch [48/50], Train Loss: 0.0269, Val Loss: 0.0693\n",
      "Epoch [49/50], Train Loss: 0.0268, Val Loss: 0.0688\n",
      "Epoch [50/50], Train Loss: 0.0266, Val Loss: 0.0683\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=128, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1289, Val Loss: 0.3208\n",
      "Epoch [2/50], Train Loss: 0.1044, Val Loss: 0.2774\n",
      "Epoch [3/50], Train Loss: 0.0864, Val Loss: 0.2419\n",
      "Epoch [4/50], Train Loss: 0.0723, Val Loss: 0.2127\n",
      "Epoch [5/50], Train Loss: 0.0618, Val Loss: 0.1885\n",
      "Epoch [6/50], Train Loss: 0.0544, Val Loss: 0.1686\n",
      "Epoch [7/50], Train Loss: 0.0474, Val Loss: 0.1521\n",
      "Epoch [8/50], Train Loss: 0.0433, Val Loss: 0.1385\n",
      "Epoch [9/50], Train Loss: 0.0405, Val Loss: 0.1273\n",
      "Epoch [10/50], Train Loss: 0.0382, Val Loss: 0.1181\n",
      "Epoch [11/50], Train Loss: 0.0360, Val Loss: 0.1105\n",
      "Epoch [12/50], Train Loss: 0.0353, Val Loss: 0.1042\n",
      "Epoch [13/50], Train Loss: 0.0343, Val Loss: 0.0990\n",
      "Epoch [14/50], Train Loss: 0.0338, Val Loss: 0.0948\n",
      "Epoch [15/50], Train Loss: 0.0334, Val Loss: 0.0912\n",
      "Epoch [16/50], Train Loss: 0.0325, Val Loss: 0.0883\n",
      "Epoch [17/50], Train Loss: 0.0325, Val Loss: 0.0857\n",
      "Epoch [18/50], Train Loss: 0.0327, Val Loss: 0.0836\n",
      "Epoch [19/50], Train Loss: 0.0324, Val Loss: 0.0819\n",
      "Epoch [20/50], Train Loss: 0.0321, Val Loss: 0.0803\n",
      "Epoch [21/50], Train Loss: 0.0322, Val Loss: 0.0790\n",
      "Epoch [22/50], Train Loss: 0.0319, Val Loss: 0.0778\n",
      "Epoch [23/50], Train Loss: 0.0318, Val Loss: 0.0767\n",
      "Epoch [24/50], Train Loss: 0.0314, Val Loss: 0.0759\n",
      "Epoch [25/50], Train Loss: 0.0316, Val Loss: 0.0751\n",
      "Epoch [26/50], Train Loss: 0.0311, Val Loss: 0.0743\n",
      "Epoch [27/50], Train Loss: 0.0314, Val Loss: 0.0736\n",
      "Epoch [28/50], Train Loss: 0.0312, Val Loss: 0.0729\n",
      "Epoch [29/50], Train Loss: 0.0309, Val Loss: 0.0724\n",
      "Epoch [30/50], Train Loss: 0.0304, Val Loss: 0.0716\n",
      "Epoch [31/50], Train Loss: 0.0307, Val Loss: 0.0710\n",
      "Epoch [32/50], Train Loss: 0.0304, Val Loss: 0.0704\n",
      "Epoch [33/50], Train Loss: 0.0303, Val Loss: 0.0700\n",
      "Epoch [34/50], Train Loss: 0.0301, Val Loss: 0.0695\n",
      "Epoch [35/50], Train Loss: 0.0299, Val Loss: 0.0689\n",
      "Epoch [36/50], Train Loss: 0.0301, Val Loss: 0.0685\n",
      "Epoch [37/50], Train Loss: 0.0303, Val Loss: 0.0679\n",
      "Epoch [38/50], Train Loss: 0.0301, Val Loss: 0.0674\n",
      "Epoch [39/50], Train Loss: 0.0296, Val Loss: 0.0669\n",
      "Epoch [40/50], Train Loss: 0.0296, Val Loss: 0.0664\n",
      "Epoch [41/50], Train Loss: 0.0294, Val Loss: 0.0658\n",
      "Epoch [42/50], Train Loss: 0.0290, Val Loss: 0.0654\n",
      "Epoch [43/50], Train Loss: 0.0287, Val Loss: 0.0649\n",
      "Epoch [44/50], Train Loss: 0.0289, Val Loss: 0.0644\n",
      "Epoch [45/50], Train Loss: 0.0290, Val Loss: 0.0639\n",
      "Epoch [46/50], Train Loss: 0.0286, Val Loss: 0.0634\n",
      "Epoch [47/50], Train Loss: 0.0286, Val Loss: 0.0629\n",
      "Epoch [48/50], Train Loss: 0.0285, Val Loss: 0.0625\n",
      "Epoch [49/50], Train Loss: 0.0285, Val Loss: 0.0619\n",
      "Epoch [50/50], Train Loss: 0.0283, Val Loss: 0.0614\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=128, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1445, Val Loss: 0.3582\n",
      "Epoch [2/50], Train Loss: 0.1175, Val Loss: 0.3096\n",
      "Epoch [3/50], Train Loss: 0.0971, Val Loss: 0.2696\n",
      "Epoch [4/50], Train Loss: 0.0810, Val Loss: 0.2369\n",
      "Epoch [5/50], Train Loss: 0.0678, Val Loss: 0.2101\n",
      "Epoch [6/50], Train Loss: 0.0600, Val Loss: 0.1878\n",
      "Epoch [7/50], Train Loss: 0.0536, Val Loss: 0.1695\n",
      "Epoch [8/50], Train Loss: 0.0489, Val Loss: 0.1545\n",
      "Epoch [9/50], Train Loss: 0.0454, Val Loss: 0.1422\n",
      "Epoch [10/50], Train Loss: 0.0429, Val Loss: 0.1321\n",
      "Epoch [11/50], Train Loss: 0.0410, Val Loss: 0.1238\n",
      "Epoch [12/50], Train Loss: 0.0399, Val Loss: 0.1170\n",
      "Epoch [13/50], Train Loss: 0.0396, Val Loss: 0.1115\n",
      "Epoch [14/50], Train Loss: 0.0385, Val Loss: 0.1070\n",
      "Epoch [15/50], Train Loss: 0.0381, Val Loss: 0.1033\n",
      "Epoch [16/50], Train Loss: 0.0377, Val Loss: 0.1002\n",
      "Epoch [17/50], Train Loss: 0.0373, Val Loss: 0.0977\n",
      "Epoch [18/50], Train Loss: 0.0375, Val Loss: 0.0956\n",
      "Epoch [19/50], Train Loss: 0.0371, Val Loss: 0.0935\n",
      "Epoch [20/50], Train Loss: 0.0372, Val Loss: 0.0920\n",
      "Epoch [21/50], Train Loss: 0.0360, Val Loss: 0.0906\n",
      "Epoch [22/50], Train Loss: 0.0357, Val Loss: 0.0893\n",
      "Epoch [23/50], Train Loss: 0.0364, Val Loss: 0.0882\n",
      "Epoch [24/50], Train Loss: 0.0361, Val Loss: 0.0873\n",
      "Epoch [25/50], Train Loss: 0.0361, Val Loss: 0.0866\n",
      "Epoch [26/50], Train Loss: 0.0361, Val Loss: 0.0858\n",
      "Epoch [27/50], Train Loss: 0.0360, Val Loss: 0.0852\n",
      "Epoch [28/50], Train Loss: 0.0352, Val Loss: 0.0843\n",
      "Epoch [29/50], Train Loss: 0.0355, Val Loss: 0.0835\n",
      "Epoch [30/50], Train Loss: 0.0352, Val Loss: 0.0829\n",
      "Epoch [31/50], Train Loss: 0.0348, Val Loss: 0.0827\n",
      "Epoch [32/50], Train Loss: 0.0348, Val Loss: 0.0822\n",
      "Epoch [33/50], Train Loss: 0.0347, Val Loss: 0.0818\n",
      "Epoch [34/50], Train Loss: 0.0355, Val Loss: 0.0815\n",
      "Epoch [35/50], Train Loss: 0.0346, Val Loss: 0.0808\n",
      "Epoch [36/50], Train Loss: 0.0349, Val Loss: 0.0803\n",
      "Epoch [37/50], Train Loss: 0.0345, Val Loss: 0.0799\n",
      "Epoch [38/50], Train Loss: 0.0340, Val Loss: 0.0795\n",
      "Epoch [39/50], Train Loss: 0.0342, Val Loss: 0.0791\n",
      "Epoch [40/50], Train Loss: 0.0336, Val Loss: 0.0787\n",
      "Epoch [41/50], Train Loss: 0.0342, Val Loss: 0.0782\n",
      "Epoch [42/50], Train Loss: 0.0335, Val Loss: 0.0776\n",
      "Epoch [43/50], Train Loss: 0.0331, Val Loss: 0.0772\n",
      "Epoch [44/50], Train Loss: 0.0325, Val Loss: 0.0769\n",
      "Epoch [45/50], Train Loss: 0.0341, Val Loss: 0.0764\n",
      "Epoch [46/50], Train Loss: 0.0335, Val Loss: 0.0761\n",
      "Epoch [47/50], Train Loss: 0.0326, Val Loss: 0.0755\n",
      "Epoch [48/50], Train Loss: 0.0330, Val Loss: 0.0748\n",
      "Epoch [49/50], Train Loss: 0.0330, Val Loss: 0.0744\n",
      "Epoch [50/50], Train Loss: 0.0331, Val Loss: 0.0740\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=128, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1492, Val Loss: 0.3715\n",
      "Epoch [2/50], Train Loss: 0.1160, Val Loss: 0.3146\n",
      "Epoch [3/50], Train Loss: 0.0918, Val Loss: 0.2690\n",
      "Epoch [4/50], Train Loss: 0.0741, Val Loss: 0.2323\n",
      "Epoch [5/50], Train Loss: 0.0614, Val Loss: 0.2027\n",
      "Epoch [6/50], Train Loss: 0.0525, Val Loss: 0.1792\n",
      "Epoch [7/50], Train Loss: 0.0463, Val Loss: 0.1604\n",
      "Epoch [8/50], Train Loss: 0.0421, Val Loss: 0.1457\n",
      "Epoch [9/50], Train Loss: 0.0393, Val Loss: 0.1341\n",
      "Epoch [10/50], Train Loss: 0.0376, Val Loss: 0.1250\n",
      "Epoch [11/50], Train Loss: 0.0364, Val Loss: 0.1179\n",
      "Epoch [12/50], Train Loss: 0.0357, Val Loss: 0.1124\n",
      "Epoch [13/50], Train Loss: 0.0352, Val Loss: 0.1080\n",
      "Epoch [14/50], Train Loss: 0.0349, Val Loss: 0.1046\n",
      "Epoch [15/50], Train Loss: 0.0347, Val Loss: 0.1019\n",
      "Epoch [16/50], Train Loss: 0.0346, Val Loss: 0.0997\n",
      "Epoch [17/50], Train Loss: 0.0344, Val Loss: 0.0980\n",
      "Epoch [18/50], Train Loss: 0.0343, Val Loss: 0.0965\n",
      "Epoch [19/50], Train Loss: 0.0342, Val Loss: 0.0954\n",
      "Epoch [20/50], Train Loss: 0.0341, Val Loss: 0.0944\n",
      "Epoch [21/50], Train Loss: 0.0340, Val Loss: 0.0935\n",
      "Epoch [22/50], Train Loss: 0.0339, Val Loss: 0.0928\n",
      "Epoch [23/50], Train Loss: 0.0338, Val Loss: 0.0921\n",
      "Epoch [24/50], Train Loss: 0.0337, Val Loss: 0.0915\n",
      "Epoch [25/50], Train Loss: 0.0336, Val Loss: 0.0909\n",
      "Epoch [26/50], Train Loss: 0.0335, Val Loss: 0.0904\n",
      "Epoch [27/50], Train Loss: 0.0334, Val Loss: 0.0899\n",
      "Epoch [28/50], Train Loss: 0.0333, Val Loss: 0.0895\n",
      "Epoch [29/50], Train Loss: 0.0332, Val Loss: 0.0890\n",
      "Epoch [30/50], Train Loss: 0.0331, Val Loss: 0.0886\n",
      "Epoch [31/50], Train Loss: 0.0330, Val Loss: 0.0882\n",
      "Epoch [32/50], Train Loss: 0.0329, Val Loss: 0.0877\n",
      "Epoch [33/50], Train Loss: 0.0328, Val Loss: 0.0873\n",
      "Epoch [34/50], Train Loss: 0.0327, Val Loss: 0.0869\n",
      "Epoch [35/50], Train Loss: 0.0326, Val Loss: 0.0865\n",
      "Epoch [36/50], Train Loss: 0.0325, Val Loss: 0.0861\n",
      "Epoch [37/50], Train Loss: 0.0324, Val Loss: 0.0857\n",
      "Epoch [38/50], Train Loss: 0.0323, Val Loss: 0.0852\n",
      "Epoch [39/50], Train Loss: 0.0321, Val Loss: 0.0848\n",
      "Epoch [40/50], Train Loss: 0.0320, Val Loss: 0.0844\n",
      "Epoch [41/50], Train Loss: 0.0319, Val Loss: 0.0840\n",
      "Epoch [42/50], Train Loss: 0.0318, Val Loss: 0.0836\n",
      "Epoch [43/50], Train Loss: 0.0317, Val Loss: 0.0832\n",
      "Epoch [44/50], Train Loss: 0.0316, Val Loss: 0.0828\n",
      "Epoch [45/50], Train Loss: 0.0315, Val Loss: 0.0823\n",
      "Epoch [46/50], Train Loss: 0.0314, Val Loss: 0.0819\n",
      "Epoch [47/50], Train Loss: 0.0313, Val Loss: 0.0815\n",
      "Epoch [48/50], Train Loss: 0.0311, Val Loss: 0.0811\n",
      "Epoch [49/50], Train Loss: 0.0310, Val Loss: 0.0807\n",
      "Epoch [50/50], Train Loss: 0.0309, Val Loss: 0.0802\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=128, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1003, Val Loss: 0.2713\n",
      "Epoch [2/50], Train Loss: 0.0837, Val Loss: 0.2393\n",
      "Epoch [3/50], Train Loss: 0.0714, Val Loss: 0.2131\n",
      "Epoch [4/50], Train Loss: 0.0624, Val Loss: 0.1916\n",
      "Epoch [5/50], Train Loss: 0.0548, Val Loss: 0.1739\n",
      "Epoch [6/50], Train Loss: 0.0494, Val Loss: 0.1595\n",
      "Epoch [7/50], Train Loss: 0.0462, Val Loss: 0.1476\n",
      "Epoch [8/50], Train Loss: 0.0435, Val Loss: 0.1379\n",
      "Epoch [9/50], Train Loss: 0.0418, Val Loss: 0.1300\n",
      "Epoch [10/50], Train Loss: 0.0402, Val Loss: 0.1235\n",
      "Epoch [11/50], Train Loss: 0.0387, Val Loss: 0.1180\n",
      "Epoch [12/50], Train Loss: 0.0384, Val Loss: 0.1137\n",
      "Epoch [13/50], Train Loss: 0.0379, Val Loss: 0.1100\n",
      "Epoch [14/50], Train Loss: 0.0376, Val Loss: 0.1070\n",
      "Epoch [15/50], Train Loss: 0.0375, Val Loss: 0.1046\n",
      "Epoch [16/50], Train Loss: 0.0373, Val Loss: 0.1026\n",
      "Epoch [17/50], Train Loss: 0.0371, Val Loss: 0.1009\n",
      "Epoch [18/50], Train Loss: 0.0365, Val Loss: 0.0995\n",
      "Epoch [19/50], Train Loss: 0.0367, Val Loss: 0.0983\n",
      "Epoch [20/50], Train Loss: 0.0369, Val Loss: 0.0973\n",
      "Epoch [21/50], Train Loss: 0.0364, Val Loss: 0.0963\n",
      "Epoch [22/50], Train Loss: 0.0362, Val Loss: 0.0955\n",
      "Epoch [23/50], Train Loss: 0.0365, Val Loss: 0.0948\n",
      "Epoch [24/50], Train Loss: 0.0362, Val Loss: 0.0941\n",
      "Epoch [25/50], Train Loss: 0.0359, Val Loss: 0.0934\n",
      "Epoch [26/50], Train Loss: 0.0357, Val Loss: 0.0928\n",
      "Epoch [27/50], Train Loss: 0.0358, Val Loss: 0.0922\n",
      "Epoch [28/50], Train Loss: 0.0354, Val Loss: 0.0917\n",
      "Epoch [29/50], Train Loss: 0.0356, Val Loss: 0.0912\n",
      "Epoch [30/50], Train Loss: 0.0357, Val Loss: 0.0906\n",
      "Epoch [31/50], Train Loss: 0.0357, Val Loss: 0.0902\n",
      "Epoch [32/50], Train Loss: 0.0353, Val Loss: 0.0897\n",
      "Epoch [33/50], Train Loss: 0.0354, Val Loss: 0.0891\n",
      "Epoch [34/50], Train Loss: 0.0350, Val Loss: 0.0887\n",
      "Epoch [35/50], Train Loss: 0.0351, Val Loss: 0.0882\n",
      "Epoch [36/50], Train Loss: 0.0349, Val Loss: 0.0878\n",
      "Epoch [37/50], Train Loss: 0.0351, Val Loss: 0.0873\n",
      "Epoch [38/50], Train Loss: 0.0345, Val Loss: 0.0869\n",
      "Epoch [39/50], Train Loss: 0.0348, Val Loss: 0.0864\n",
      "Epoch [40/50], Train Loss: 0.0345, Val Loss: 0.0858\n",
      "Epoch [41/50], Train Loss: 0.0343, Val Loss: 0.0854\n",
      "Epoch [42/50], Train Loss: 0.0341, Val Loss: 0.0850\n",
      "Epoch [43/50], Train Loss: 0.0338, Val Loss: 0.0845\n",
      "Epoch [44/50], Train Loss: 0.0336, Val Loss: 0.0840\n",
      "Epoch [45/50], Train Loss: 0.0336, Val Loss: 0.0835\n",
      "Epoch [46/50], Train Loss: 0.0340, Val Loss: 0.0831\n",
      "Epoch [47/50], Train Loss: 0.0336, Val Loss: 0.0827\n",
      "Epoch [48/50], Train Loss: 0.0333, Val Loss: 0.0822\n",
      "Epoch [49/50], Train Loss: 0.0334, Val Loss: 0.0815\n",
      "Epoch [50/50], Train Loss: 0.0327, Val Loss: 0.0811\n",
      "Testing parameters: lr=0.005, optimizer=sgd, hidden_size=128, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1136, Val Loss: 0.2878\n",
      "Epoch [2/50], Train Loss: 0.0945, Val Loss: 0.2537\n",
      "Epoch [3/50], Train Loss: 0.0801, Val Loss: 0.2261\n",
      "Epoch [4/50], Train Loss: 0.0694, Val Loss: 0.2035\n",
      "Epoch [5/50], Train Loss: 0.0615, Val Loss: 0.1847\n",
      "Epoch [6/50], Train Loss: 0.0547, Val Loss: 0.1694\n",
      "Epoch [7/50], Train Loss: 0.0503, Val Loss: 0.1568\n",
      "Epoch [8/50], Train Loss: 0.0473, Val Loss: 0.1463\n",
      "Epoch [9/50], Train Loss: 0.0455, Val Loss: 0.1375\n",
      "Epoch [10/50], Train Loss: 0.0433, Val Loss: 0.1302\n",
      "Epoch [11/50], Train Loss: 0.0415, Val Loss: 0.1242\n",
      "Epoch [12/50], Train Loss: 0.0407, Val Loss: 0.1192\n",
      "Epoch [13/50], Train Loss: 0.0405, Val Loss: 0.1151\n",
      "Epoch [14/50], Train Loss: 0.0403, Val Loss: 0.1117\n",
      "Epoch [15/50], Train Loss: 0.0393, Val Loss: 0.1088\n",
      "Epoch [16/50], Train Loss: 0.0395, Val Loss: 0.1063\n",
      "Epoch [17/50], Train Loss: 0.0395, Val Loss: 0.1044\n",
      "Epoch [18/50], Train Loss: 0.0389, Val Loss: 0.1027\n",
      "Epoch [19/50], Train Loss: 0.0386, Val Loss: 0.1011\n",
      "Epoch [20/50], Train Loss: 0.0388, Val Loss: 0.1000\n",
      "Epoch [21/50], Train Loss: 0.0386, Val Loss: 0.0988\n",
      "Epoch [22/50], Train Loss: 0.0385, Val Loss: 0.0978\n",
      "Epoch [23/50], Train Loss: 0.0387, Val Loss: 0.0971\n",
      "Epoch [24/50], Train Loss: 0.0382, Val Loss: 0.0964\n",
      "Epoch [25/50], Train Loss: 0.0381, Val Loss: 0.0958\n",
      "Epoch [26/50], Train Loss: 0.0384, Val Loss: 0.0954\n",
      "Epoch [27/50], Train Loss: 0.0387, Val Loss: 0.0947\n",
      "Epoch [28/50], Train Loss: 0.0381, Val Loss: 0.0941\n",
      "Epoch [29/50], Train Loss: 0.0380, Val Loss: 0.0935\n",
      "Epoch [30/50], Train Loss: 0.0382, Val Loss: 0.0932\n",
      "Epoch [31/50], Train Loss: 0.0375, Val Loss: 0.0930\n",
      "Epoch [32/50], Train Loss: 0.0381, Val Loss: 0.0927\n",
      "Epoch [33/50], Train Loss: 0.0374, Val Loss: 0.0923\n",
      "Epoch [34/50], Train Loss: 0.0374, Val Loss: 0.0922\n",
      "Epoch [35/50], Train Loss: 0.0377, Val Loss: 0.0919\n",
      "Epoch [36/50], Train Loss: 0.0372, Val Loss: 0.0915\n",
      "Epoch [37/50], Train Loss: 0.0371, Val Loss: 0.0912\n",
      "Epoch [38/50], Train Loss: 0.0369, Val Loss: 0.0909\n",
      "Epoch [39/50], Train Loss: 0.0369, Val Loss: 0.0906\n",
      "Epoch [40/50], Train Loss: 0.0367, Val Loss: 0.0904\n",
      "Epoch [41/50], Train Loss: 0.0373, Val Loss: 0.0902\n",
      "Epoch [42/50], Train Loss: 0.0371, Val Loss: 0.0899\n",
      "Epoch [43/50], Train Loss: 0.0372, Val Loss: 0.0895\n",
      "Epoch [44/50], Train Loss: 0.0371, Val Loss: 0.0893\n",
      "Epoch [45/50], Train Loss: 0.0359, Val Loss: 0.0889\n",
      "Epoch [46/50], Train Loss: 0.0368, Val Loss: 0.0887\n",
      "Epoch [47/50], Train Loss: 0.0366, Val Loss: 0.0884\n",
      "Epoch [48/50], Train Loss: 0.0368, Val Loss: 0.0881\n",
      "Epoch [49/50], Train Loss: 0.0368, Val Loss: 0.0877\n",
      "Epoch [50/50], Train Loss: 0.0366, Val Loss: 0.0875\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=16, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0234, Val Loss: 0.0322\n",
      "Epoch [2/50], Train Loss: 0.0555, Val Loss: 0.0183\n",
      "Epoch [3/50], Train Loss: 0.0247, Val Loss: 0.0090\n",
      "Epoch [4/50], Train Loss: 0.0131, Val Loss: 0.0059\n",
      "Epoch [5/50], Train Loss: 0.0104, Val Loss: 0.0087\n",
      "Epoch [6/50], Train Loss: 0.0045, Val Loss: 0.0077\n",
      "Epoch [7/50], Train Loss: 0.0034, Val Loss: 0.0019\n",
      "Epoch [8/50], Train Loss: 0.0030, Val Loss: 0.0033\n",
      "Epoch [9/50], Train Loss: 0.0022, Val Loss: 0.0045\n",
      "Epoch [10/50], Train Loss: 0.0024, Val Loss: 0.0082\n",
      "Epoch [11/50], Train Loss: 0.0020, Val Loss: 0.0019\n",
      "Epoch [12/50], Train Loss: 0.0025, Val Loss: 0.0033\n",
      "Epoch [13/50], Train Loss: 0.0018, Val Loss: 0.0023\n",
      "Epoch [14/50], Train Loss: 0.0022, Val Loss: 0.0079\n",
      "Epoch [15/50], Train Loss: 0.0017, Val Loss: 0.0026\n",
      "Epoch [16/50], Train Loss: 0.0023, Val Loss: 0.0036\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=16, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0403, Val Loss: 0.0462\n",
      "Epoch [2/50], Train Loss: 0.0443, Val Loss: 0.0148\n",
      "Epoch [3/50], Train Loss: 0.0329, Val Loss: 0.0131\n",
      "Epoch [4/50], Train Loss: 0.0220, Val Loss: 0.0075\n",
      "Epoch [5/50], Train Loss: 0.0143, Val Loss: 0.0103\n",
      "Epoch [6/50], Train Loss: 0.0096, Val Loss: 0.0072\n",
      "Epoch [7/50], Train Loss: 0.0075, Val Loss: 0.0011\n",
      "Epoch [8/50], Train Loss: 0.0071, Val Loss: 0.0025\n",
      "Epoch [9/50], Train Loss: 0.0052, Val Loss: 0.0038\n",
      "Epoch [10/50], Train Loss: 0.0056, Val Loss: 0.0046\n",
      "Epoch [11/50], Train Loss: 0.0045, Val Loss: 0.0011\n",
      "Epoch [12/50], Train Loss: 0.0043, Val Loss: 0.0010\n",
      "Epoch [13/50], Train Loss: 0.0043, Val Loss: 0.0016\n",
      "Epoch [14/50], Train Loss: 0.0046, Val Loss: 0.0057\n",
      "Epoch [15/50], Train Loss: 0.0042, Val Loss: 0.0043\n",
      "Epoch [16/50], Train Loss: 0.0047, Val Loss: 0.0019\n",
      "Epoch [17/50], Train Loss: 0.0041, Val Loss: 0.0031\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=16, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0652, Val Loss: 0.0343\n",
      "Epoch [2/50], Train Loss: 0.0592, Val Loss: 0.0178\n",
      "Epoch [3/50], Train Loss: 0.0316, Val Loss: 0.0071\n",
      "Epoch [4/50], Train Loss: 0.0206, Val Loss: 0.0093\n",
      "Epoch [5/50], Train Loss: 0.0169, Val Loss: 0.0167\n",
      "Epoch [6/50], Train Loss: 0.0130, Val Loss: 0.0061\n",
      "Epoch [7/50], Train Loss: 0.0125, Val Loss: 0.0016\n",
      "Epoch [8/50], Train Loss: 0.0135, Val Loss: 0.0034\n",
      "Epoch [9/50], Train Loss: 0.0105, Val Loss: 0.0162\n",
      "Epoch [10/50], Train Loss: 0.0104, Val Loss: 0.0136\n",
      "Epoch [11/50], Train Loss: 0.0105, Val Loss: 0.0017\n",
      "Epoch [12/50], Train Loss: 0.0107, Val Loss: 0.0045\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=16, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0265, Val Loss: 0.0555\n",
      "Epoch [2/50], Train Loss: 0.0701, Val Loss: 0.0380\n",
      "Epoch [3/50], Train Loss: 0.0381, Val Loss: 0.0163\n",
      "Epoch [4/50], Train Loss: 0.0291, Val Loss: 0.0248\n",
      "Epoch [5/50], Train Loss: 0.0138, Val Loss: 0.0112\n",
      "Epoch [6/50], Train Loss: 0.0074, Val Loss: 0.0105\n",
      "Epoch [7/50], Train Loss: 0.0187, Val Loss: 0.0114\n",
      "Epoch [8/50], Train Loss: 0.0079, Val Loss: 0.0065\n",
      "Epoch [9/50], Train Loss: 0.0085, Val Loss: 0.0281\n",
      "Epoch [10/50], Train Loss: 0.0082, Val Loss: 0.0046\n",
      "Epoch [11/50], Train Loss: 0.0054, Val Loss: 0.0030\n",
      "Epoch [12/50], Train Loss: 0.0066, Val Loss: 0.0149\n",
      "Epoch [13/50], Train Loss: 0.0085, Val Loss: 0.0035\n",
      "Epoch [14/50], Train Loss: 0.0061, Val Loss: 0.0035\n",
      "Epoch [15/50], Train Loss: 0.0055, Val Loss: 0.0094\n",
      "Epoch [16/50], Train Loss: 0.0076, Val Loss: 0.0045\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=16, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0330, Val Loss: 0.0763\n",
      "Epoch [2/50], Train Loss: 0.0659, Val Loss: 0.0193\n",
      "Epoch [3/50], Train Loss: 0.0435, Val Loss: 0.0145\n",
      "Epoch [4/50], Train Loss: 0.0233, Val Loss: 0.0078\n",
      "Epoch [5/50], Train Loss: 0.0166, Val Loss: 0.0156\n",
      "Epoch [6/50], Train Loss: 0.0137, Val Loss: 0.0032\n",
      "Epoch [7/50], Train Loss: 0.0080, Val Loss: 0.0099\n",
      "Epoch [8/50], Train Loss: 0.0064, Val Loss: 0.0076\n",
      "Epoch [9/50], Train Loss: 0.0068, Val Loss: 0.0071\n",
      "Epoch [10/50], Train Loss: 0.0056, Val Loss: 0.0027\n",
      "Epoch [11/50], Train Loss: 0.0063, Val Loss: 0.0084\n",
      "Epoch [12/50], Train Loss: 0.0081, Val Loss: 0.0057\n",
      "Epoch [13/50], Train Loss: 0.0058, Val Loss: 0.0046\n",
      "Epoch [14/50], Train Loss: 0.0068, Val Loss: 0.0106\n",
      "Epoch [15/50], Train Loss: 0.0086, Val Loss: 0.0175\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=16, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0481, Val Loss: 0.0839\n",
      "Epoch [2/50], Train Loss: 0.0571, Val Loss: 0.0268\n",
      "Epoch [3/50], Train Loss: 0.0481, Val Loss: 0.0072\n",
      "Epoch [4/50], Train Loss: 0.0291, Val Loss: 0.0039\n",
      "Epoch [5/50], Train Loss: 0.0222, Val Loss: 0.0133\n",
      "Epoch [6/50], Train Loss: 0.0137, Val Loss: 0.0063\n",
      "Epoch [7/50], Train Loss: 0.0143, Val Loss: 0.0146\n",
      "Epoch [8/50], Train Loss: 0.0135, Val Loss: 0.0032\n",
      "Epoch [9/50], Train Loss: 0.0120, Val Loss: 0.0078\n",
      "Epoch [10/50], Train Loss: 0.0118, Val Loss: 0.0232\n",
      "Epoch [11/50], Train Loss: 0.0211, Val Loss: 0.0143\n",
      "Epoch [12/50], Train Loss: 0.0111, Val Loss: 0.0033\n",
      "Epoch [13/50], Train Loss: 0.0109, Val Loss: 0.0254\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=16, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0219, Val Loss: 0.1008\n",
      "Epoch [2/50], Train Loss: 0.0645, Val Loss: 0.0551\n",
      "Epoch [3/50], Train Loss: 0.0612, Val Loss: 0.0646\n",
      "Epoch [4/50], Train Loss: 0.0452, Val Loss: 0.0141\n",
      "Epoch [5/50], Train Loss: 0.0378, Val Loss: 0.0109\n",
      "Epoch [6/50], Train Loss: 0.0164, Val Loss: 0.0147\n",
      "Epoch [7/50], Train Loss: 0.0234, Val Loss: 0.0368\n",
      "Epoch [8/50], Train Loss: 0.0161, Val Loss: 0.0117\n",
      "Epoch [9/50], Train Loss: 0.0046, Val Loss: 0.0135\n",
      "Epoch [10/50], Train Loss: 0.0026, Val Loss: 0.0095\n",
      "Epoch [11/50], Train Loss: 0.0024, Val Loss: 0.0092\n",
      "Epoch [12/50], Train Loss: 0.0029, Val Loss: 0.0107\n",
      "Epoch [13/50], Train Loss: 0.0029, Val Loss: 0.0085\n",
      "Epoch [14/50], Train Loss: 0.0042, Val Loss: 0.0067\n",
      "Epoch [15/50], Train Loss: 0.0041, Val Loss: 0.0086\n",
      "Epoch [16/50], Train Loss: 0.0046, Val Loss: 0.0050\n",
      "Epoch [17/50], Train Loss: 0.0090, Val Loss: 0.0366\n",
      "Epoch [18/50], Train Loss: 0.0095, Val Loss: 0.0080\n",
      "Epoch [19/50], Train Loss: 0.0068, Val Loss: 0.0136\n",
      "Epoch [20/50], Train Loss: 0.0055, Val Loss: 0.0049\n",
      "Epoch [21/50], Train Loss: 0.0055, Val Loss: 0.0279\n",
      "Epoch [22/50], Train Loss: 0.0051, Val Loss: 0.0090\n",
      "Epoch [23/50], Train Loss: 0.0037, Val Loss: 0.0028\n",
      "Epoch [24/50], Train Loss: 0.0044, Val Loss: 0.0157\n",
      "Epoch [25/50], Train Loss: 0.0035, Val Loss: 0.0057\n",
      "Epoch [26/50], Train Loss: 0.0024, Val Loss: 0.0082\n",
      "Epoch [27/50], Train Loss: 0.0034, Val Loss: 0.0095\n",
      "Epoch [28/50], Train Loss: 0.0056, Val Loss: 0.0115\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=16, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0411, Val Loss: 0.1030\n",
      "Epoch [2/50], Train Loss: 0.0629, Val Loss: 0.0573\n",
      "Epoch [3/50], Train Loss: 0.0597, Val Loss: 0.0562\n",
      "Epoch [4/50], Train Loss: 0.0428, Val Loss: 0.0203\n",
      "Epoch [5/50], Train Loss: 0.0426, Val Loss: 0.0098\n",
      "Epoch [6/50], Train Loss: 0.0336, Val Loss: 0.0180\n",
      "Epoch [7/50], Train Loss: 0.0226, Val Loss: 0.0295\n",
      "Epoch [8/50], Train Loss: 0.0182, Val Loss: 0.0128\n",
      "Epoch [9/50], Train Loss: 0.0116, Val Loss: 0.0082\n",
      "Epoch [10/50], Train Loss: 0.0127, Val Loss: 0.0137\n",
      "Epoch [11/50], Train Loss: 0.0178, Val Loss: 0.0602\n",
      "Epoch [12/50], Train Loss: 0.0284, Val Loss: 0.0340\n",
      "Epoch [13/50], Train Loss: 0.0205, Val Loss: 0.0135\n",
      "Epoch [14/50], Train Loss: 0.0251, Val Loss: 0.0971\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=16, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0472, Val Loss: 0.1041\n",
      "Epoch [2/50], Train Loss: 0.0592, Val Loss: 0.0821\n",
      "Epoch [3/50], Train Loss: 0.0482, Val Loss: 0.0733\n",
      "Epoch [4/50], Train Loss: 0.0604, Val Loss: 0.0711\n",
      "Epoch [5/50], Train Loss: 0.0401, Val Loss: 0.0236\n",
      "Epoch [6/50], Train Loss: 0.0264, Val Loss: 0.0102\n",
      "Epoch [7/50], Train Loss: 0.0228, Val Loss: 0.0281\n",
      "Epoch [8/50], Train Loss: 0.0185, Val Loss: 0.0097\n",
      "Epoch [9/50], Train Loss: 0.0138, Val Loss: 0.0116\n",
      "Epoch [10/50], Train Loss: 0.0223, Val Loss: 0.0110\n",
      "Epoch [11/50], Train Loss: 0.0150, Val Loss: 0.0322\n",
      "Epoch [12/50], Train Loss: 0.0145, Val Loss: 0.0272\n",
      "Epoch [13/50], Train Loss: 0.0097, Val Loss: 0.0089\n",
      "Epoch [14/50], Train Loss: 0.0150, Val Loss: 0.0190\n",
      "Epoch [15/50], Train Loss: 0.0150, Val Loss: 0.0304\n",
      "Epoch [16/50], Train Loss: 0.0252, Val Loss: 0.0127\n",
      "Epoch [17/50], Train Loss: 0.0122, Val Loss: 0.0434\n",
      "Epoch [18/50], Train Loss: 0.0196, Val Loss: 0.0104\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=32, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0266, Val Loss: 0.0370\n",
      "Epoch [2/50], Train Loss: 0.0482, Val Loss: 0.0049\n",
      "Epoch [3/50], Train Loss: 0.0253, Val Loss: 0.0145\n",
      "Epoch [4/50], Train Loss: 0.0151, Val Loss: 0.0200\n",
      "Epoch [5/50], Train Loss: 0.0055, Val Loss: 0.0029\n",
      "Epoch [6/50], Train Loss: 0.0048, Val Loss: 0.0112\n",
      "Epoch [7/50], Train Loss: 0.0046, Val Loss: 0.0048\n",
      "Epoch [8/50], Train Loss: 0.0032, Val Loss: 0.0013\n",
      "Epoch [9/50], Train Loss: 0.0048, Val Loss: 0.0090\n",
      "Epoch [10/50], Train Loss: 0.0046, Val Loss: 0.0077\n",
      "Epoch [11/50], Train Loss: 0.0034, Val Loss: 0.0023\n",
      "Epoch [12/50], Train Loss: 0.0044, Val Loss: 0.0031\n",
      "Epoch [13/50], Train Loss: 0.0042, Val Loss: 0.0060\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=32, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0241, Val Loss: 0.0603\n",
      "Epoch [2/50], Train Loss: 0.0561, Val Loss: 0.0087\n",
      "Epoch [3/50], Train Loss: 0.0383, Val Loss: 0.0135\n",
      "Epoch [4/50], Train Loss: 0.0159, Val Loss: 0.0161\n",
      "Epoch [5/50], Train Loss: 0.0134, Val Loss: 0.0086\n",
      "Epoch [6/50], Train Loss: 0.0058, Val Loss: 0.0020\n",
      "Epoch [7/50], Train Loss: 0.0040, Val Loss: 0.0014\n",
      "Epoch [8/50], Train Loss: 0.0033, Val Loss: 0.0024\n",
      "Epoch [9/50], Train Loss: 0.0037, Val Loss: 0.0034\n",
      "Epoch [10/50], Train Loss: 0.0032, Val Loss: 0.0017\n",
      "Epoch [11/50], Train Loss: 0.0031, Val Loss: 0.0011\n",
      "Epoch [12/50], Train Loss: 0.0031, Val Loss: 0.0037\n",
      "Epoch [13/50], Train Loss: 0.0031, Val Loss: 0.0040\n",
      "Epoch [14/50], Train Loss: 0.0030, Val Loss: 0.0011\n",
      "Epoch [15/50], Train Loss: 0.0027, Val Loss: 0.0013\n",
      "Epoch [16/50], Train Loss: 0.0028, Val Loss: 0.0029\n",
      "Epoch [17/50], Train Loss: 0.0029, Val Loss: 0.0031\n",
      "Epoch [18/50], Train Loss: 0.0028, Val Loss: 0.0017\n",
      "Epoch [19/50], Train Loss: 0.0027, Val Loss: 0.0022\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=32, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0359, Val Loss: 0.0606\n",
      "Epoch [2/50], Train Loss: 0.0528, Val Loss: 0.0077\n",
      "Epoch [3/50], Train Loss: 0.0294, Val Loss: 0.0047\n",
      "Epoch [4/50], Train Loss: 0.0176, Val Loss: 0.0076\n",
      "Epoch [5/50], Train Loss: 0.0112, Val Loss: 0.0129\n",
      "Epoch [6/50], Train Loss: 0.0105, Val Loss: 0.0287\n",
      "Epoch [7/50], Train Loss: 0.0091, Val Loss: 0.0023\n",
      "Epoch [8/50], Train Loss: 0.0127, Val Loss: 0.0086\n",
      "Epoch [9/50], Train Loss: 0.0089, Val Loss: 0.0271\n",
      "Epoch [10/50], Train Loss: 0.0070, Val Loss: 0.0054\n",
      "Epoch [11/50], Train Loss: 0.0086, Val Loss: 0.0039\n",
      "Epoch [12/50], Train Loss: 0.0072, Val Loss: 0.0082\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=32, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0157, Val Loss: 0.0528\n",
      "Epoch [2/50], Train Loss: 0.0518, Val Loss: 0.0679\n",
      "Epoch [3/50], Train Loss: 0.0394, Val Loss: 0.0158\n",
      "Epoch [4/50], Train Loss: 0.0417, Val Loss: 0.0090\n",
      "Epoch [5/50], Train Loss: 0.0196, Val Loss: 0.0071\n",
      "Epoch [6/50], Train Loss: 0.0242, Val Loss: 0.0146\n",
      "Epoch [7/50], Train Loss: 0.0082, Val Loss: 0.0231\n",
      "Epoch [8/50], Train Loss: 0.0149, Val Loss: 0.0099\n",
      "Epoch [9/50], Train Loss: 0.0101, Val Loss: 0.0271\n",
      "Epoch [10/50], Train Loss: 0.0057, Val Loss: 0.0152\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=32, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0305, Val Loss: 0.0801\n",
      "Epoch [2/50], Train Loss: 0.0604, Val Loss: 0.0730\n",
      "Epoch [3/50], Train Loss: 0.0433, Val Loss: 0.0098\n",
      "Epoch [4/50], Train Loss: 0.0243, Val Loss: 0.0169\n",
      "Epoch [5/50], Train Loss: 0.0144, Val Loss: 0.0031\n",
      "Epoch [6/50], Train Loss: 0.0108, Val Loss: 0.0159\n",
      "Epoch [7/50], Train Loss: 0.0158, Val Loss: 0.0054\n",
      "Epoch [8/50], Train Loss: 0.0113, Val Loss: 0.0091\n",
      "Epoch [9/50], Train Loss: 0.0100, Val Loss: 0.0063\n",
      "Epoch [10/50], Train Loss: 0.0239, Val Loss: 0.0144\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=32, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0317, Val Loss: 0.0494\n",
      "Epoch [2/50], Train Loss: 0.0443, Val Loss: 0.0165\n",
      "Epoch [3/50], Train Loss: 0.0381, Val Loss: 0.0054\n",
      "Epoch [4/50], Train Loss: 0.0254, Val Loss: 0.0055\n",
      "Epoch [5/50], Train Loss: 0.0210, Val Loss: 0.0044\n",
      "Epoch [6/50], Train Loss: 0.0219, Val Loss: 0.0283\n",
      "Epoch [7/50], Train Loss: 0.0235, Val Loss: 0.0145\n",
      "Epoch [8/50], Train Loss: 0.0126, Val Loss: 0.0088\n",
      "Epoch [9/50], Train Loss: 0.0149, Val Loss: 0.0034\n",
      "Epoch [10/50], Train Loss: 0.0234, Val Loss: 0.0120\n",
      "Epoch [11/50], Train Loss: 0.0155, Val Loss: 0.0654\n",
      "Epoch [12/50], Train Loss: 0.0070, Val Loss: 0.0026\n",
      "Epoch [13/50], Train Loss: 0.0078, Val Loss: 0.0071\n",
      "Epoch [14/50], Train Loss: 0.0101, Val Loss: 0.0129\n",
      "Epoch [15/50], Train Loss: 0.0273, Val Loss: 0.0208\n",
      "Epoch [16/50], Train Loss: 0.0131, Val Loss: 0.0052\n",
      "Epoch [17/50], Train Loss: 0.0097, Val Loss: 0.0195\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=32, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0216, Val Loss: 0.0727\n",
      "Epoch [2/50], Train Loss: 0.0559, Val Loss: 0.1192\n",
      "Epoch [3/50], Train Loss: 0.0433, Val Loss: 0.0609\n",
      "Epoch [4/50], Train Loss: 0.0370, Val Loss: 0.0361\n",
      "Epoch [5/50], Train Loss: 0.0280, Val Loss: 0.0177\n",
      "Epoch [6/50], Train Loss: 0.0298, Val Loss: 0.0284\n",
      "Epoch [7/50], Train Loss: 0.0237, Val Loss: 0.0317\n",
      "Epoch [8/50], Train Loss: 0.0222, Val Loss: 0.0158\n",
      "Epoch [9/50], Train Loss: 0.0140, Val Loss: 0.0195\n",
      "Epoch [10/50], Train Loss: 0.0328, Val Loss: 0.0084\n",
      "Epoch [11/50], Train Loss: 0.0166, Val Loss: 0.0026\n",
      "Epoch [12/50], Train Loss: 0.0160, Val Loss: 0.0180\n",
      "Epoch [13/50], Train Loss: 0.0097, Val Loss: 0.0126\n",
      "Epoch [14/50], Train Loss: 0.0059, Val Loss: 0.0041\n",
      "Epoch [15/50], Train Loss: 0.0067, Val Loss: 0.0090\n",
      "Epoch [16/50], Train Loss: 0.0228, Val Loss: 0.0094\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=32, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0273, Val Loss: 0.0543\n",
      "Epoch [2/50], Train Loss: 0.0498, Val Loss: 0.0815\n",
      "Epoch [3/50], Train Loss: 0.0473, Val Loss: 0.0277\n",
      "Epoch [4/50], Train Loss: 0.0414, Val Loss: 0.0392\n",
      "Epoch [5/50], Train Loss: 0.0330, Val Loss: 0.0165\n",
      "Epoch [6/50], Train Loss: 0.0315, Val Loss: 0.0093\n",
      "Epoch [7/50], Train Loss: 0.0377, Val Loss: 0.0348\n",
      "Epoch [8/50], Train Loss: 0.0422, Val Loss: 0.0046\n",
      "Epoch [9/50], Train Loss: 0.0280, Val Loss: 0.0112\n",
      "Epoch [10/50], Train Loss: 0.0120, Val Loss: 0.0025\n",
      "Epoch [11/50], Train Loss: 0.0127, Val Loss: 0.0118\n",
      "Epoch [12/50], Train Loss: 0.0337, Val Loss: 0.0345\n",
      "Epoch [13/50], Train Loss: 0.0183, Val Loss: 0.0337\n",
      "Epoch [14/50], Train Loss: 0.0133, Val Loss: 0.0150\n",
      "Epoch [15/50], Train Loss: 0.0122, Val Loss: 0.0073\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=32, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0306, Val Loss: 0.0569\n",
      "Epoch [2/50], Train Loss: 0.0515, Val Loss: 0.0749\n",
      "Epoch [3/50], Train Loss: 0.0419, Val Loss: 0.0693\n",
      "Epoch [4/50], Train Loss: 0.0431, Val Loss: 0.0123\n",
      "Epoch [5/50], Train Loss: 0.0308, Val Loss: 0.0254\n",
      "Epoch [6/50], Train Loss: 0.0259, Val Loss: 0.0080\n",
      "Epoch [7/50], Train Loss: 0.0266, Val Loss: 0.0109\n",
      "Epoch [8/50], Train Loss: 0.0136, Val Loss: 0.0043\n",
      "Epoch [9/50], Train Loss: 0.0394, Val Loss: 0.0302\n",
      "Epoch [10/50], Train Loss: 0.0366, Val Loss: 0.1207\n",
      "Epoch [11/50], Train Loss: 0.0200, Val Loss: 0.0341\n",
      "Epoch [12/50], Train Loss: 0.0178, Val Loss: 0.0103\n",
      "Epoch [13/50], Train Loss: 0.0195, Val Loss: 0.0302\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=64, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0160, Val Loss: 0.0595\n",
      "Epoch [2/50], Train Loss: 0.0399, Val Loss: 0.0501\n",
      "Epoch [3/50], Train Loss: 0.0331, Val Loss: 0.0138\n",
      "Epoch [4/50], Train Loss: 0.0285, Val Loss: 0.0205\n",
      "Epoch [5/50], Train Loss: 0.0139, Val Loss: 0.0188\n",
      "Epoch [6/50], Train Loss: 0.0065, Val Loss: 0.0109\n",
      "Epoch [7/50], Train Loss: 0.0059, Val Loss: 0.0111\n",
      "Epoch [8/50], Train Loss: 0.0052, Val Loss: 0.0038\n",
      "Epoch [9/50], Train Loss: 0.0034, Val Loss: 0.0050\n",
      "Epoch [10/50], Train Loss: 0.0027, Val Loss: 0.0055\n",
      "Epoch [11/50], Train Loss: 0.0036, Val Loss: 0.0059\n",
      "Epoch [12/50], Train Loss: 0.0032, Val Loss: 0.0036\n",
      "Epoch [13/50], Train Loss: 0.0037, Val Loss: 0.0074\n",
      "Epoch [14/50], Train Loss: 0.0037, Val Loss: 0.0026\n",
      "Epoch [15/50], Train Loss: 0.0043, Val Loss: 0.0036\n",
      "Epoch [16/50], Train Loss: 0.0044, Val Loss: 0.0050\n",
      "Epoch [17/50], Train Loss: 0.0035, Val Loss: 0.0086\n",
      "Epoch [18/50], Train Loss: 0.0033, Val Loss: 0.0066\n",
      "Epoch [19/50], Train Loss: 0.0027, Val Loss: 0.0038\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=64, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0177, Val Loss: 0.0531\n",
      "Epoch [2/50], Train Loss: 0.0374, Val Loss: 0.0191\n",
      "Epoch [3/50], Train Loss: 0.0278, Val Loss: 0.0110\n",
      "Epoch [4/50], Train Loss: 0.0199, Val Loss: 0.0225\n",
      "Epoch [5/50], Train Loss: 0.0112, Val Loss: 0.0166\n",
      "Epoch [6/50], Train Loss: 0.0053, Val Loss: 0.0029\n",
      "Epoch [7/50], Train Loss: 0.0051, Val Loss: 0.0034\n",
      "Epoch [8/50], Train Loss: 0.0060, Val Loss: 0.0057\n",
      "Epoch [9/50], Train Loss: 0.0054, Val Loss: 0.0042\n",
      "Epoch [10/50], Train Loss: 0.0046, Val Loss: 0.0069\n",
      "Epoch [11/50], Train Loss: 0.0082, Val Loss: 0.0151\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=64, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0229, Val Loss: 0.0482\n",
      "Epoch [2/50], Train Loss: 0.0404, Val Loss: 0.0331\n",
      "Epoch [3/50], Train Loss: 0.0250, Val Loss: 0.0216\n",
      "Epoch [4/50], Train Loss: 0.0210, Val Loss: 0.0113\n",
      "Epoch [5/50], Train Loss: 0.0134, Val Loss: 0.0134\n",
      "Epoch [6/50], Train Loss: 0.0068, Val Loss: 0.0012\n",
      "Epoch [7/50], Train Loss: 0.0069, Val Loss: 0.0036\n",
      "Epoch [8/50], Train Loss: 0.0061, Val Loss: 0.0023\n",
      "Epoch [9/50], Train Loss: 0.0061, Val Loss: 0.0035\n",
      "Epoch [10/50], Train Loss: 0.0046, Val Loss: 0.0044\n",
      "Epoch [11/50], Train Loss: 0.0066, Val Loss: 0.0071\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=64, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0186, Val Loss: 0.0364\n",
      "Epoch [2/50], Train Loss: 0.0346, Val Loss: 0.0400\n",
      "Epoch [3/50], Train Loss: 0.0421, Val Loss: 0.0106\n",
      "Epoch [4/50], Train Loss: 0.0382, Val Loss: 0.0053\n",
      "Epoch [5/50], Train Loss: 0.0280, Val Loss: 0.0052\n",
      "Epoch [6/50], Train Loss: 0.0182, Val Loss: 0.0170\n",
      "Epoch [7/50], Train Loss: 0.0148, Val Loss: 0.0125\n",
      "Epoch [8/50], Train Loss: 0.0079, Val Loss: 0.0090\n",
      "Epoch [9/50], Train Loss: 0.0123, Val Loss: 0.0175\n",
      "Epoch [10/50], Train Loss: 0.0123, Val Loss: 0.0226\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=64, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0220, Val Loss: 0.0480\n",
      "Epoch [2/50], Train Loss: 0.0370, Val Loss: 0.0359\n",
      "Epoch [3/50], Train Loss: 0.0431, Val Loss: 0.0297\n",
      "Epoch [4/50], Train Loss: 0.0372, Val Loss: 0.0032\n",
      "Epoch [5/50], Train Loss: 0.0306, Val Loss: 0.0169\n",
      "Epoch [6/50], Train Loss: 0.0219, Val Loss: 0.0210\n",
      "Epoch [7/50], Train Loss: 0.0105, Val Loss: 0.0103\n",
      "Epoch [8/50], Train Loss: 0.0196, Val Loss: 0.0052\n",
      "Epoch [9/50], Train Loss: 0.0332, Val Loss: 0.0022\n",
      "Epoch [10/50], Train Loss: 0.0146, Val Loss: 0.0162\n",
      "Epoch [11/50], Train Loss: 0.0096, Val Loss: 0.0016\n",
      "Epoch [12/50], Train Loss: 0.0137, Val Loss: 0.0121\n",
      "Epoch [13/50], Train Loss: 0.0175, Val Loss: 0.0164\n",
      "Epoch [14/50], Train Loss: 0.0187, Val Loss: 0.0464\n",
      "Epoch [15/50], Train Loss: 0.0149, Val Loss: 0.0044\n",
      "Epoch [16/50], Train Loss: 0.0080, Val Loss: 0.0027\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=64, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0297, Val Loss: 0.0257\n",
      "Epoch [2/50], Train Loss: 0.0466, Val Loss: 0.0175\n",
      "Epoch [3/50], Train Loss: 0.0440, Val Loss: 0.0054\n",
      "Epoch [4/50], Train Loss: 0.0235, Val Loss: 0.0105\n",
      "Epoch [5/50], Train Loss: 0.0186, Val Loss: 0.0068\n",
      "Epoch [6/50], Train Loss: 0.0221, Val Loss: 0.0296\n",
      "Epoch [7/50], Train Loss: 0.0206, Val Loss: 0.0053\n",
      "Epoch [8/50], Train Loss: 0.0110, Val Loss: 0.0027\n",
      "Epoch [9/50], Train Loss: 0.0152, Val Loss: 0.0074\n",
      "Epoch [10/50], Train Loss: 0.0148, Val Loss: 0.0022\n",
      "Epoch [11/50], Train Loss: 0.0192, Val Loss: 0.0091\n",
      "Epoch [12/50], Train Loss: 0.0176, Val Loss: 0.0035\n",
      "Epoch [13/50], Train Loss: 0.0151, Val Loss: 0.0194\n",
      "Epoch [14/50], Train Loss: 0.0159, Val Loss: 0.0040\n",
      "Epoch [15/50], Train Loss: 0.0102, Val Loss: 0.0040\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=64, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0184, Val Loss: 0.0404\n",
      "Epoch [2/50], Train Loss: 0.0381, Val Loss: 0.0503\n",
      "Epoch [3/50], Train Loss: 0.0401, Val Loss: 0.0513\n",
      "Epoch [4/50], Train Loss: 0.0367, Val Loss: 0.0346\n",
      "Epoch [5/50], Train Loss: 0.0388, Val Loss: 0.0212\n",
      "Epoch [6/50], Train Loss: 0.0253, Val Loss: 0.0174\n",
      "Epoch [7/50], Train Loss: 0.0296, Val Loss: 0.0440\n",
      "Epoch [8/50], Train Loss: 0.0251, Val Loss: 0.0222\n",
      "Epoch [9/50], Train Loss: 0.0140, Val Loss: 0.0117\n",
      "Epoch [10/50], Train Loss: 0.0355, Val Loss: 0.0123\n",
      "Epoch [11/50], Train Loss: 0.0241, Val Loss: 0.0309\n",
      "Epoch [12/50], Train Loss: 0.0300, Val Loss: 0.0398\n",
      "Epoch [13/50], Train Loss: 0.0234, Val Loss: 0.0092\n",
      "Epoch [14/50], Train Loss: 0.0142, Val Loss: 0.0068\n",
      "Epoch [15/50], Train Loss: 0.0191, Val Loss: 0.0065\n",
      "Epoch [16/50], Train Loss: 0.0218, Val Loss: 0.0109\n",
      "Epoch [17/50], Train Loss: 0.0210, Val Loss: 0.0234\n",
      "Epoch [18/50], Train Loss: 0.0111, Val Loss: 0.0148\n",
      "Epoch [19/50], Train Loss: 0.0158, Val Loss: 0.0109\n",
      "Epoch [20/50], Train Loss: 0.0111, Val Loss: 0.0041\n",
      "Epoch [21/50], Train Loss: 0.0177, Val Loss: 0.0162\n",
      "Epoch [22/50], Train Loss: 0.0181, Val Loss: 0.0037\n",
      "Epoch [23/50], Train Loss: 0.0045, Val Loss: 0.0028\n",
      "Epoch [24/50], Train Loss: 0.0092, Val Loss: 0.0166\n",
      "Epoch [25/50], Train Loss: 0.0126, Val Loss: 0.0363\n",
      "Epoch [26/50], Train Loss: 0.0131, Val Loss: 0.0085\n",
      "Epoch [27/50], Train Loss: 0.0041, Val Loss: 0.0255\n",
      "Epoch [28/50], Train Loss: 0.0058, Val Loss: 0.0167\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=64, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0245, Val Loss: 0.0605\n",
      "Epoch [2/50], Train Loss: 0.0409, Val Loss: 0.0416\n",
      "Epoch [3/50], Train Loss: 0.0466, Val Loss: 0.0399\n",
      "Epoch [4/50], Train Loss: 0.0540, Val Loss: 0.0315\n",
      "Epoch [5/50], Train Loss: 0.0450, Val Loss: 0.0769\n",
      "Epoch [6/50], Train Loss: 0.0331, Val Loss: 0.0486\n",
      "Epoch [7/50], Train Loss: 0.0251, Val Loss: 0.0376\n",
      "Epoch [8/50], Train Loss: 0.0222, Val Loss: 0.0209\n",
      "Epoch [9/50], Train Loss: 0.0310, Val Loss: 0.0037\n",
      "Epoch [10/50], Train Loss: 0.0228, Val Loss: 0.0110\n",
      "Epoch [11/50], Train Loss: 0.0220, Val Loss: 0.0140\n",
      "Epoch [12/50], Train Loss: 0.0244, Val Loss: 0.0851\n",
      "Epoch [13/50], Train Loss: 0.0102, Val Loss: 0.0182\n",
      "Epoch [14/50], Train Loss: 0.0157, Val Loss: 0.0532\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=64, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0300, Val Loss: 0.0527\n",
      "Epoch [2/50], Train Loss: 0.0429, Val Loss: 0.0388\n",
      "Epoch [3/50], Train Loss: 0.0560, Val Loss: 0.0447\n",
      "Epoch [4/50], Train Loss: 0.0463, Val Loss: 0.0372\n",
      "Epoch [5/50], Train Loss: 0.0327, Val Loss: 0.0079\n",
      "Epoch [6/50], Train Loss: 0.0280, Val Loss: 0.0040\n",
      "Epoch [7/50], Train Loss: 0.0224, Val Loss: 0.0416\n",
      "Epoch [8/50], Train Loss: 0.0300, Val Loss: 0.0117\n",
      "Epoch [9/50], Train Loss: 0.0192, Val Loss: 0.0068\n",
      "Epoch [10/50], Train Loss: 0.0172, Val Loss: 0.0230\n",
      "Epoch [11/50], Train Loss: 0.0176, Val Loss: 0.0223\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=128, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0120, Val Loss: 0.0283\n",
      "Epoch [2/50], Train Loss: 0.0182, Val Loss: 0.0307\n",
      "Epoch [3/50], Train Loss: 0.0193, Val Loss: 0.0274\n",
      "Epoch [4/50], Train Loss: 0.0198, Val Loss: 0.0295\n",
      "Epoch [5/50], Train Loss: 0.0205, Val Loss: 0.0369\n",
      "Epoch [6/50], Train Loss: 0.0123, Val Loss: 0.0256\n",
      "Epoch [7/50], Train Loss: 0.0095, Val Loss: 0.0119\n",
      "Epoch [8/50], Train Loss: 0.0064, Val Loss: 0.0073\n",
      "Epoch [9/50], Train Loss: 0.0035, Val Loss: 0.0075\n",
      "Epoch [10/50], Train Loss: 0.0070, Val Loss: 0.0045\n",
      "Epoch [11/50], Train Loss: 0.0040, Val Loss: 0.0067\n",
      "Epoch [12/50], Train Loss: 0.0026, Val Loss: 0.0067\n",
      "Epoch [13/50], Train Loss: 0.0033, Val Loss: 0.0036\n",
      "Epoch [14/50], Train Loss: 0.0037, Val Loss: 0.0045\n",
      "Epoch [15/50], Train Loss: 0.0041, Val Loss: 0.0054\n",
      "Epoch [16/50], Train Loss: 0.0048, Val Loss: 0.0050\n",
      "Epoch [17/50], Train Loss: 0.0050, Val Loss: 0.0051\n",
      "Epoch [18/50], Train Loss: 0.0047, Val Loss: 0.0020\n",
      "Epoch [19/50], Train Loss: 0.0030, Val Loss: 0.0023\n",
      "Epoch [20/50], Train Loss: 0.0050, Val Loss: 0.0029\n",
      "Epoch [21/50], Train Loss: 0.0033, Val Loss: 0.0021\n",
      "Epoch [22/50], Train Loss: 0.0052, Val Loss: 0.0042\n",
      "Epoch [23/50], Train Loss: 0.0038, Val Loss: 0.0018\n",
      "Epoch [24/50], Train Loss: 0.0032, Val Loss: 0.0025\n",
      "Epoch [25/50], Train Loss: 0.0024, Val Loss: 0.0015\n",
      "Epoch [26/50], Train Loss: 0.0034, Val Loss: 0.0059\n",
      "Epoch [27/50], Train Loss: 0.0048, Val Loss: 0.0074\n",
      "Epoch [28/50], Train Loss: 0.0026, Val Loss: 0.0086\n",
      "Epoch [29/50], Train Loss: 0.0022, Val Loss: 0.0061\n",
      "Epoch [30/50], Train Loss: 0.0041, Val Loss: 0.0201\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=128, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0160, Val Loss: 0.0282\n",
      "Epoch [2/50], Train Loss: 0.0219, Val Loss: 0.0307\n",
      "Epoch [3/50], Train Loss: 0.0219, Val Loss: 0.0193\n",
      "Epoch [4/50], Train Loss: 0.0228, Val Loss: 0.0285\n",
      "Epoch [5/50], Train Loss: 0.0176, Val Loss: 0.0297\n",
      "Epoch [6/50], Train Loss: 0.0090, Val Loss: 0.0083\n",
      "Epoch [7/50], Train Loss: 0.0081, Val Loss: 0.0154\n",
      "Epoch [8/50], Train Loss: 0.0028, Val Loss: 0.0039\n",
      "Epoch [9/50], Train Loss: 0.0045, Val Loss: 0.0052\n",
      "Epoch [10/50], Train Loss: 0.0055, Val Loss: 0.0023\n",
      "Epoch [11/50], Train Loss: 0.0051, Val Loss: 0.0095\n",
      "Epoch [12/50], Train Loss: 0.0068, Val Loss: 0.0076\n",
      "Epoch [13/50], Train Loss: 0.0082, Val Loss: 0.0186\n",
      "Epoch [14/50], Train Loss: 0.0056, Val Loss: 0.0069\n",
      "Epoch [15/50], Train Loss: 0.0036, Val Loss: 0.0110\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=128, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0185, Val Loss: 0.0377\n",
      "Epoch [2/50], Train Loss: 0.0264, Val Loss: 0.0290\n",
      "Epoch [3/50], Train Loss: 0.0295, Val Loss: 0.0359\n",
      "Epoch [4/50], Train Loss: 0.0244, Val Loss: 0.0198\n",
      "Epoch [5/50], Train Loss: 0.0186, Val Loss: 0.0272\n",
      "Epoch [6/50], Train Loss: 0.0133, Val Loss: 0.0030\n",
      "Epoch [7/50], Train Loss: 0.0074, Val Loss: 0.0041\n",
      "Epoch [8/50], Train Loss: 0.0082, Val Loss: 0.0075\n",
      "Epoch [9/50], Train Loss: 0.0072, Val Loss: 0.0106\n",
      "Epoch [10/50], Train Loss: 0.0078, Val Loss: 0.0070\n",
      "Epoch [11/50], Train Loss: 0.0054, Val Loss: 0.0043\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=128, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0165, Val Loss: 0.0453\n",
      "Epoch [2/50], Train Loss: 0.0254, Val Loss: 0.0292\n",
      "Epoch [3/50], Train Loss: 0.0204, Val Loss: 0.0194\n",
      "Epoch [4/50], Train Loss: 0.0170, Val Loss: 0.0213\n",
      "Epoch [5/50], Train Loss: 0.0143, Val Loss: 0.0194\n",
      "Epoch [6/50], Train Loss: 0.0246, Val Loss: 0.0138\n",
      "Epoch [7/50], Train Loss: 0.0220, Val Loss: 0.0179\n",
      "Epoch [8/50], Train Loss: 0.0279, Val Loss: 0.0180\n",
      "Epoch [9/50], Train Loss: 0.0261, Val Loss: 0.0207\n",
      "Epoch [10/50], Train Loss: 0.0150, Val Loss: 0.0205\n",
      "Epoch [11/50], Train Loss: 0.0203, Val Loss: 0.0242\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=128, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0140, Val Loss: 0.0272\n",
      "Epoch [2/50], Train Loss: 0.0311, Val Loss: 0.0385\n",
      "Epoch [3/50], Train Loss: 0.0264, Val Loss: 0.0234\n",
      "Epoch [4/50], Train Loss: 0.0258, Val Loss: 0.0305\n",
      "Epoch [5/50], Train Loss: 0.0257, Val Loss: 0.0385\n",
      "Epoch [6/50], Train Loss: 0.0229, Val Loss: 0.0286\n",
      "Epoch [7/50], Train Loss: 0.0227, Val Loss: 0.0153\n",
      "Epoch [8/50], Train Loss: 0.0194, Val Loss: 0.0193\n",
      "Epoch [9/50], Train Loss: 0.0103, Val Loss: 0.0240\n",
      "Epoch [10/50], Train Loss: 0.0103, Val Loss: 0.0163\n",
      "Epoch [11/50], Train Loss: 0.0126, Val Loss: 0.0168\n",
      "Epoch [12/50], Train Loss: 0.0113, Val Loss: 0.0070\n",
      "Epoch [13/50], Train Loss: 0.0044, Val Loss: 0.0032\n",
      "Epoch [14/50], Train Loss: 0.0067, Val Loss: 0.0077\n",
      "Epoch [15/50], Train Loss: 0.0077, Val Loss: 0.0033\n",
      "Epoch [16/50], Train Loss: 0.0153, Val Loss: 0.0087\n",
      "Epoch [17/50], Train Loss: 0.0144, Val Loss: 0.0336\n",
      "Epoch [18/50], Train Loss: 0.0091, Val Loss: 0.0133\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=128, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0198, Val Loss: 0.0324\n",
      "Epoch [2/50], Train Loss: 0.0265, Val Loss: 0.0417\n",
      "Epoch [3/50], Train Loss: 0.0243, Val Loss: 0.0136\n",
      "Epoch [4/50], Train Loss: 0.0183, Val Loss: 0.0224\n",
      "Epoch [5/50], Train Loss: 0.0209, Val Loss: 0.0191\n",
      "Epoch [6/50], Train Loss: 0.0251, Val Loss: 0.0156\n",
      "Epoch [7/50], Train Loss: 0.0197, Val Loss: 0.0222\n",
      "Epoch [8/50], Train Loss: 0.0215, Val Loss: 0.0323\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=128, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0153, Val Loss: 0.0132\n",
      "Epoch [2/50], Train Loss: 0.0288, Val Loss: 0.0258\n",
      "Epoch [3/50], Train Loss: 0.0324, Val Loss: 0.0375\n",
      "Epoch [4/50], Train Loss: 0.0276, Val Loss: 0.0315\n",
      "Epoch [5/50], Train Loss: 0.0277, Val Loss: 0.0271\n",
      "Epoch [6/50], Train Loss: 0.0278, Val Loss: 0.0169\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=128, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0184, Val Loss: 0.0246\n",
      "Epoch [2/50], Train Loss: 0.0313, Val Loss: 0.0461\n",
      "Epoch [3/50], Train Loss: 0.0266, Val Loss: 0.0412\n",
      "Epoch [4/50], Train Loss: 0.0233, Val Loss: 0.0147\n",
      "Epoch [5/50], Train Loss: 0.0260, Val Loss: 0.0094\n",
      "Epoch [6/50], Train Loss: 0.0200, Val Loss: 0.0150\n",
      "Epoch [7/50], Train Loss: 0.0261, Val Loss: 0.0114\n",
      "Epoch [8/50], Train Loss: 0.0273, Val Loss: 0.0081\n",
      "Epoch [9/50], Train Loss: 0.0338, Val Loss: 0.0187\n",
      "Epoch [10/50], Train Loss: 0.0223, Val Loss: 0.0113\n",
      "Epoch [11/50], Train Loss: 0.0245, Val Loss: 0.0167\n",
      "Epoch [12/50], Train Loss: 0.0294, Val Loss: 0.0155\n",
      "Epoch [13/50], Train Loss: 0.0222, Val Loss: 0.0183\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.005, optimizer=adamw, hidden_size=128, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0211, Val Loss: 0.0422\n",
      "Epoch [2/50], Train Loss: 0.0277, Val Loss: 0.0591\n",
      "Epoch [3/50], Train Loss: 0.0232, Val Loss: 0.0291\n",
      "Epoch [4/50], Train Loss: 0.0217, Val Loss: 0.0170\n",
      "Epoch [5/50], Train Loss: 0.0224, Val Loss: 0.0263\n",
      "Epoch [6/50], Train Loss: 0.0229, Val Loss: 0.0255\n",
      "Epoch [7/50], Train Loss: 0.0285, Val Loss: 0.0183\n",
      "Epoch [8/50], Train Loss: 0.0341, Val Loss: 0.0204\n",
      "Epoch [9/50], Train Loss: 0.0313, Val Loss: 0.0184\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=16, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.2114, Val Loss: 0.4202\n",
      "Epoch [2/50], Train Loss: 0.1100, Val Loss: 0.2179\n",
      "Epoch [3/50], Train Loss: 0.0411, Val Loss: 0.0567\n",
      "Epoch [4/50], Train Loss: 0.0240, Val Loss: 0.0245\n",
      "Epoch [5/50], Train Loss: 0.0225, Val Loss: 0.0227\n",
      "Epoch [6/50], Train Loss: 0.0187, Val Loss: 0.0165\n",
      "Epoch [7/50], Train Loss: 0.0162, Val Loss: 0.0122\n",
      "Epoch [8/50], Train Loss: 0.0138, Val Loss: 0.0089\n",
      "Epoch [9/50], Train Loss: 0.0116, Val Loss: 0.0066\n",
      "Epoch [10/50], Train Loss: 0.0097, Val Loss: 0.0052\n",
      "Epoch [11/50], Train Loss: 0.0080, Val Loss: 0.0046\n",
      "Epoch [12/50], Train Loss: 0.0064, Val Loss: 0.0046\n",
      "Epoch [13/50], Train Loss: 0.0050, Val Loss: 0.0048\n",
      "Epoch [14/50], Train Loss: 0.0039, Val Loss: 0.0048\n",
      "Epoch [15/50], Train Loss: 0.0032, Val Loss: 0.0042\n",
      "Epoch [16/50], Train Loss: 0.0027, Val Loss: 0.0036\n",
      "Epoch [17/50], Train Loss: 0.0024, Val Loss: 0.0031\n",
      "Epoch [18/50], Train Loss: 0.0022, Val Loss: 0.0028\n",
      "Epoch [19/50], Train Loss: 0.0021, Val Loss: 0.0025\n",
      "Epoch [20/50], Train Loss: 0.0020, Val Loss: 0.0024\n",
      "Epoch [21/50], Train Loss: 0.0019, Val Loss: 0.0023\n",
      "Epoch [22/50], Train Loss: 0.0019, Val Loss: 0.0022\n",
      "Epoch [23/50], Train Loss: 0.0019, Val Loss: 0.0022\n",
      "Epoch [24/50], Train Loss: 0.0019, Val Loss: 0.0022\n",
      "Epoch [25/50], Train Loss: 0.0018, Val Loss: 0.0021\n",
      "Epoch [26/50], Train Loss: 0.0018, Val Loss: 0.0021\n",
      "Epoch [27/50], Train Loss: 0.0018, Val Loss: 0.0021\n",
      "Epoch [28/50], Train Loss: 0.0018, Val Loss: 0.0021\n",
      "Epoch [29/50], Train Loss: 0.0018, Val Loss: 0.0021\n",
      "Epoch [30/50], Train Loss: 0.0017, Val Loss: 0.0020\n",
      "Epoch [31/50], Train Loss: 0.0017, Val Loss: 0.0020\n",
      "Epoch [32/50], Train Loss: 0.0017, Val Loss: 0.0020\n",
      "Epoch [33/50], Train Loss: 0.0017, Val Loss: 0.0020\n",
      "Epoch [34/50], Train Loss: 0.0017, Val Loss: 0.0020\n",
      "Epoch [35/50], Train Loss: 0.0017, Val Loss: 0.0020\n",
      "Epoch [36/50], Train Loss: 0.0017, Val Loss: 0.0019\n",
      "Epoch [37/50], Train Loss: 0.0016, Val Loss: 0.0019\n",
      "Epoch [38/50], Train Loss: 0.0016, Val Loss: 0.0019\n",
      "Epoch [39/50], Train Loss: 0.0016, Val Loss: 0.0019\n",
      "Epoch [40/50], Train Loss: 0.0016, Val Loss: 0.0019\n",
      "Epoch [41/50], Train Loss: 0.0016, Val Loss: 0.0019\n",
      "Epoch [42/50], Train Loss: 0.0016, Val Loss: 0.0019\n",
      "Epoch [43/50], Train Loss: 0.0016, Val Loss: 0.0019\n",
      "Epoch [44/50], Train Loss: 0.0016, Val Loss: 0.0018\n",
      "Epoch [45/50], Train Loss: 0.0016, Val Loss: 0.0018\n",
      "Epoch [46/50], Train Loss: 0.0015, Val Loss: 0.0018\n",
      "Epoch [47/50], Train Loss: 0.0015, Val Loss: 0.0018\n",
      "Epoch [48/50], Train Loss: 0.0015, Val Loss: 0.0018\n",
      "Epoch [49/50], Train Loss: 0.0015, Val Loss: 0.0018\n",
      "Epoch [50/50], Train Loss: 0.0015, Val Loss: 0.0018\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=16, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1262, Val Loss: 0.1857\n",
      "Epoch [2/50], Train Loss: 0.0649, Val Loss: 0.0734\n",
      "Epoch [3/50], Train Loss: 0.0427, Val Loss: 0.0394\n",
      "Epoch [4/50], Train Loss: 0.0343, Val Loss: 0.0354\n",
      "Epoch [5/50], Train Loss: 0.0286, Val Loss: 0.0353\n",
      "Epoch [6/50], Train Loss: 0.0237, Val Loss: 0.0324\n",
      "Epoch [7/50], Train Loss: 0.0225, Val Loss: 0.0260\n",
      "Epoch [8/50], Train Loss: 0.0194, Val Loss: 0.0229\n",
      "Epoch [9/50], Train Loss: 0.0180, Val Loss: 0.0158\n",
      "Epoch [10/50], Train Loss: 0.0158, Val Loss: 0.0113\n",
      "Epoch [11/50], Train Loss: 0.0124, Val Loss: 0.0057\n",
      "Epoch [12/50], Train Loss: 0.0103, Val Loss: 0.0030\n",
      "Epoch [13/50], Train Loss: 0.0085, Val Loss: 0.0030\n",
      "Epoch [14/50], Train Loss: 0.0080, Val Loss: 0.0037\n",
      "Epoch [15/50], Train Loss: 0.0077, Val Loss: 0.0028\n",
      "Epoch [16/50], Train Loss: 0.0076, Val Loss: 0.0039\n",
      "Epoch [17/50], Train Loss: 0.0072, Val Loss: 0.0025\n",
      "Epoch [18/50], Train Loss: 0.0075, Val Loss: 0.0025\n",
      "Epoch [19/50], Train Loss: 0.0074, Val Loss: 0.0036\n",
      "Epoch [20/50], Train Loss: 0.0068, Val Loss: 0.0027\n",
      "Epoch [21/50], Train Loss: 0.0071, Val Loss: 0.0026\n",
      "Epoch [22/50], Train Loss: 0.0068, Val Loss: 0.0037\n",
      "Epoch [23/50], Train Loss: 0.0062, Val Loss: 0.0022\n",
      "Epoch [24/50], Train Loss: 0.0062, Val Loss: 0.0019\n",
      "Epoch [25/50], Train Loss: 0.0065, Val Loss: 0.0023\n",
      "Epoch [26/50], Train Loss: 0.0059, Val Loss: 0.0031\n",
      "Epoch [27/50], Train Loss: 0.0056, Val Loss: 0.0026\n",
      "Epoch [28/50], Train Loss: 0.0057, Val Loss: 0.0025\n",
      "Epoch [29/50], Train Loss: 0.0058, Val Loss: 0.0026\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=16, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1138, Val Loss: 0.2398\n",
      "Epoch [2/50], Train Loss: 0.0687, Val Loss: 0.1561\n",
      "Epoch [3/50], Train Loss: 0.0531, Val Loss: 0.1118\n",
      "Epoch [4/50], Train Loss: 0.0460, Val Loss: 0.0862\n",
      "Epoch [5/50], Train Loss: 0.0404, Val Loss: 0.0675\n",
      "Epoch [6/50], Train Loss: 0.0357, Val Loss: 0.0561\n",
      "Epoch [7/50], Train Loss: 0.0326, Val Loss: 0.0452\n",
      "Epoch [8/50], Train Loss: 0.0290, Val Loss: 0.0347\n",
      "Epoch [9/50], Train Loss: 0.0260, Val Loss: 0.0311\n",
      "Epoch [10/50], Train Loss: 0.0237, Val Loss: 0.0255\n",
      "Epoch [11/50], Train Loss: 0.0218, Val Loss: 0.0231\n",
      "Epoch [12/50], Train Loss: 0.0204, Val Loss: 0.0188\n",
      "Epoch [13/50], Train Loss: 0.0178, Val Loss: 0.0192\n",
      "Epoch [14/50], Train Loss: 0.0181, Val Loss: 0.0168\n",
      "Epoch [15/50], Train Loss: 0.0164, Val Loss: 0.0134\n",
      "Epoch [16/50], Train Loss: 0.0152, Val Loss: 0.0161\n",
      "Epoch [17/50], Train Loss: 0.0150, Val Loss: 0.0114\n",
      "Epoch [18/50], Train Loss: 0.0150, Val Loss: 0.0151\n",
      "Epoch [19/50], Train Loss: 0.0140, Val Loss: 0.0106\n",
      "Epoch [20/50], Train Loss: 0.0120, Val Loss: 0.0101\n",
      "Epoch [21/50], Train Loss: 0.0142, Val Loss: 0.0101\n",
      "Epoch [22/50], Train Loss: 0.0129, Val Loss: 0.0078\n",
      "Epoch [23/50], Train Loss: 0.0121, Val Loss: 0.0100\n",
      "Epoch [24/50], Train Loss: 0.0116, Val Loss: 0.0073\n",
      "Epoch [25/50], Train Loss: 0.0118, Val Loss: 0.0108\n",
      "Epoch [26/50], Train Loss: 0.0109, Val Loss: 0.0082\n",
      "Epoch [27/50], Train Loss: 0.0103, Val Loss: 0.0083\n",
      "Epoch [28/50], Train Loss: 0.0109, Val Loss: 0.0056\n",
      "Epoch [29/50], Train Loss: 0.0108, Val Loss: 0.0076\n",
      "Epoch [30/50], Train Loss: 0.0101, Val Loss: 0.0079\n",
      "Epoch [31/50], Train Loss: 0.0103, Val Loss: 0.0048\n",
      "Epoch [32/50], Train Loss: 0.0098, Val Loss: 0.0052\n",
      "Epoch [33/50], Train Loss: 0.0096, Val Loss: 0.0107\n",
      "Epoch [34/50], Train Loss: 0.0090, Val Loss: 0.0043\n",
      "Epoch [35/50], Train Loss: 0.0094, Val Loss: 0.0049\n",
      "Epoch [36/50], Train Loss: 0.0089, Val Loss: 0.0084\n",
      "Epoch [37/50], Train Loss: 0.0085, Val Loss: 0.0058\n",
      "Epoch [38/50], Train Loss: 0.0084, Val Loss: 0.0054\n",
      "Epoch [39/50], Train Loss: 0.0087, Val Loss: 0.0045\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=16, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.2102, Val Loss: 0.2287\n",
      "Epoch [2/50], Train Loss: 0.0509, Val Loss: 0.0632\n",
      "Epoch [3/50], Train Loss: 0.0388, Val Loss: 0.0638\n",
      "Epoch [4/50], Train Loss: 0.0352, Val Loss: 0.0597\n",
      "Epoch [5/50], Train Loss: 0.0319, Val Loss: 0.0517\n",
      "Epoch [6/50], Train Loss: 0.0288, Val Loss: 0.0435\n",
      "Epoch [7/50], Train Loss: 0.0251, Val Loss: 0.0341\n",
      "Epoch [8/50], Train Loss: 0.0210, Val Loss: 0.0247\n",
      "Epoch [9/50], Train Loss: 0.0166, Val Loss: 0.0171\n",
      "Epoch [10/50], Train Loss: 0.0131, Val Loss: 0.0135\n",
      "Epoch [11/50], Train Loss: 0.0108, Val Loss: 0.0135\n",
      "Epoch [12/50], Train Loss: 0.0086, Val Loss: 0.0134\n",
      "Epoch [13/50], Train Loss: 0.0068, Val Loss: 0.0119\n",
      "Epoch [14/50], Train Loss: 0.0055, Val Loss: 0.0100\n",
      "Epoch [15/50], Train Loss: 0.0045, Val Loss: 0.0087\n",
      "Epoch [16/50], Train Loss: 0.0037, Val Loss: 0.0084\n",
      "Epoch [17/50], Train Loss: 0.0033, Val Loss: 0.0087\n",
      "Epoch [18/50], Train Loss: 0.0029, Val Loss: 0.0081\n",
      "Epoch [19/50], Train Loss: 0.0027, Val Loss: 0.0070\n",
      "Epoch [20/50], Train Loss: 0.0025, Val Loss: 0.0064\n",
      "Epoch [21/50], Train Loss: 0.0025, Val Loss: 0.0069\n",
      "Epoch [22/50], Train Loss: 0.0024, Val Loss: 0.0075\n",
      "Epoch [23/50], Train Loss: 0.0022, Val Loss: 0.0065\n",
      "Epoch [24/50], Train Loss: 0.0021, Val Loss: 0.0051\n",
      "Epoch [25/50], Train Loss: 0.0021, Val Loss: 0.0050\n",
      "Epoch [26/50], Train Loss: 0.0023, Val Loss: 0.0066\n",
      "Epoch [27/50], Train Loss: 0.0022, Val Loss: 0.0075\n",
      "Epoch [28/50], Train Loss: 0.0020, Val Loss: 0.0054\n",
      "Epoch [29/50], Train Loss: 0.0021, Val Loss: 0.0035\n",
      "Epoch [30/50], Train Loss: 0.0021, Val Loss: 0.0037\n",
      "Epoch [31/50], Train Loss: 0.0025, Val Loss: 0.0071\n",
      "Epoch [32/50], Train Loss: 0.0023, Val Loss: 0.0092\n",
      "Epoch [33/50], Train Loss: 0.0022, Val Loss: 0.0046\n",
      "Epoch [34/50], Train Loss: 0.0028, Val Loss: 0.0024\n",
      "Epoch [35/50], Train Loss: 0.0025, Val Loss: 0.0028\n",
      "Epoch [36/50], Train Loss: 0.0031, Val Loss: 0.0096\n",
      "Epoch [37/50], Train Loss: 0.0027, Val Loss: 0.0113\n",
      "Epoch [38/50], Train Loss: 0.0031, Val Loss: 0.0035\n",
      "Epoch [39/50], Train Loss: 0.0042, Val Loss: 0.0029\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=16, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1047, Val Loss: 0.1726\n",
      "Epoch [2/50], Train Loss: 0.0449, Val Loss: 0.0607\n",
      "Epoch [3/50], Train Loss: 0.0427, Val Loss: 0.0495\n",
      "Epoch [4/50], Train Loss: 0.0322, Val Loss: 0.0268\n",
      "Epoch [5/50], Train Loss: 0.0274, Val Loss: 0.0132\n",
      "Epoch [6/50], Train Loss: 0.0232, Val Loss: 0.0096\n",
      "Epoch [7/50], Train Loss: 0.0206, Val Loss: 0.0085\n",
      "Epoch [8/50], Train Loss: 0.0189, Val Loss: 0.0080\n",
      "Epoch [9/50], Train Loss: 0.0176, Val Loss: 0.0076\n",
      "Epoch [10/50], Train Loss: 0.0154, Val Loss: 0.0072\n",
      "Epoch [11/50], Train Loss: 0.0127, Val Loss: 0.0044\n",
      "Epoch [12/50], Train Loss: 0.0102, Val Loss: 0.0038\n",
      "Epoch [13/50], Train Loss: 0.0086, Val Loss: 0.0055\n",
      "Epoch [14/50], Train Loss: 0.0093, Val Loss: 0.0023\n",
      "Epoch [15/50], Train Loss: 0.0088, Val Loss: 0.0025\n",
      "Epoch [16/50], Train Loss: 0.0092, Val Loss: 0.0115\n",
      "Epoch [17/50], Train Loss: 0.0092, Val Loss: 0.0058\n",
      "Epoch [18/50], Train Loss: 0.0104, Val Loss: 0.0031\n",
      "Epoch [19/50], Train Loss: 0.0086, Val Loss: 0.0048\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=16, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0784, Val Loss: 0.1409\n",
      "Epoch [2/50], Train Loss: 0.0564, Val Loss: 0.0999\n",
      "Epoch [3/50], Train Loss: 0.0466, Val Loss: 0.0787\n",
      "Epoch [4/50], Train Loss: 0.0396, Val Loss: 0.0606\n",
      "Epoch [5/50], Train Loss: 0.0343, Val Loss: 0.0393\n",
      "Epoch [6/50], Train Loss: 0.0277, Val Loss: 0.0284\n",
      "Epoch [7/50], Train Loss: 0.0246, Val Loss: 0.0232\n",
      "Epoch [8/50], Train Loss: 0.0211, Val Loss: 0.0126\n",
      "Epoch [9/50], Train Loss: 0.0181, Val Loss: 0.0102\n",
      "Epoch [10/50], Train Loss: 0.0167, Val Loss: 0.0170\n",
      "Epoch [11/50], Train Loss: 0.0172, Val Loss: 0.0097\n",
      "Epoch [12/50], Train Loss: 0.0161, Val Loss: 0.0066\n",
      "Epoch [13/50], Train Loss: 0.0155, Val Loss: 0.0201\n",
      "Epoch [14/50], Train Loss: 0.0149, Val Loss: 0.0125\n",
      "Epoch [15/50], Train Loss: 0.0163, Val Loss: 0.0055\n",
      "Epoch [16/50], Train Loss: 0.0141, Val Loss: 0.0140\n",
      "Epoch [17/50], Train Loss: 0.0142, Val Loss: 0.0139\n",
      "Epoch [18/50], Train Loss: 0.0136, Val Loss: 0.0058\n",
      "Epoch [19/50], Train Loss: 0.0126, Val Loss: 0.0106\n",
      "Epoch [20/50], Train Loss: 0.0126, Val Loss: 0.0141\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=16, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1056, Val Loss: 0.1846\n",
      "Epoch [2/50], Train Loss: 0.0334, Val Loss: 0.0604\n",
      "Epoch [3/50], Train Loss: 0.0409, Val Loss: 0.0656\n",
      "Epoch [4/50], Train Loss: 0.0315, Val Loss: 0.0477\n",
      "Epoch [5/50], Train Loss: 0.0250, Val Loss: 0.0304\n",
      "Epoch [6/50], Train Loss: 0.0163, Val Loss: 0.0175\n",
      "Epoch [7/50], Train Loss: 0.0106, Val Loss: 0.0175\n",
      "Epoch [8/50], Train Loss: 0.0060, Val Loss: 0.0130\n",
      "Epoch [9/50], Train Loss: 0.0041, Val Loss: 0.0068\n",
      "Epoch [10/50], Train Loss: 0.0033, Val Loss: 0.0096\n",
      "Epoch [11/50], Train Loss: 0.0033, Val Loss: 0.0149\n",
      "Epoch [12/50], Train Loss: 0.0025, Val Loss: 0.0075\n",
      "Epoch [13/50], Train Loss: 0.0029, Val Loss: 0.0050\n",
      "Epoch [14/50], Train Loss: 0.0036, Val Loss: 0.0134\n",
      "Epoch [15/50], Train Loss: 0.0025, Val Loss: 0.0097\n",
      "Epoch [16/50], Train Loss: 0.0030, Val Loss: 0.0051\n",
      "Epoch [17/50], Train Loss: 0.0031, Val Loss: 0.0091\n",
      "Epoch [18/50], Train Loss: 0.0032, Val Loss: 0.0133\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=16, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1444, Val Loss: 0.1929\n",
      "Epoch [2/50], Train Loss: 0.0515, Val Loss: 0.0749\n",
      "Epoch [3/50], Train Loss: 0.0559, Val Loss: 0.0830\n",
      "Epoch [4/50], Train Loss: 0.0481, Val Loss: 0.0734\n",
      "Epoch [5/50], Train Loss: 0.0437, Val Loss: 0.0650\n",
      "Epoch [6/50], Train Loss: 0.0400, Val Loss: 0.0557\n",
      "Epoch [7/50], Train Loss: 0.0340, Val Loss: 0.0423\n",
      "Epoch [8/50], Train Loss: 0.0293, Val Loss: 0.0291\n",
      "Epoch [9/50], Train Loss: 0.0205, Val Loss: 0.0172\n",
      "Epoch [10/50], Train Loss: 0.0157, Val Loss: 0.0156\n",
      "Epoch [11/50], Train Loss: 0.0132, Val Loss: 0.0120\n",
      "Epoch [12/50], Train Loss: 0.0126, Val Loss: 0.0095\n",
      "Epoch [13/50], Train Loss: 0.0122, Val Loss: 0.0175\n",
      "Epoch [14/50], Train Loss: 0.0109, Val Loss: 0.0111\n",
      "Epoch [15/50], Train Loss: 0.0106, Val Loss: 0.0069\n",
      "Epoch [16/50], Train Loss: 0.0103, Val Loss: 0.0081\n",
      "Epoch [17/50], Train Loss: 0.0093, Val Loss: 0.0154\n",
      "Epoch [18/50], Train Loss: 0.0094, Val Loss: 0.0126\n",
      "Epoch [19/50], Train Loss: 0.0087, Val Loss: 0.0070\n",
      "Epoch [20/50], Train Loss: 0.0091, Val Loss: 0.0102\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=16, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0943, Val Loss: 0.1126\n",
      "Epoch [2/50], Train Loss: 0.0778, Val Loss: 0.1013\n",
      "Epoch [3/50], Train Loss: 0.0654, Val Loss: 0.0737\n",
      "Epoch [4/50], Train Loss: 0.0534, Val Loss: 0.0429\n",
      "Epoch [5/50], Train Loss: 0.0458, Val Loss: 0.0326\n",
      "Epoch [6/50], Train Loss: 0.0412, Val Loss: 0.0379\n",
      "Epoch [7/50], Train Loss: 0.0374, Val Loss: 0.0309\n",
      "Epoch [8/50], Train Loss: 0.0351, Val Loss: 0.0182\n",
      "Epoch [9/50], Train Loss: 0.0333, Val Loss: 0.0092\n",
      "Epoch [10/50], Train Loss: 0.0287, Val Loss: 0.0241\n",
      "Epoch [11/50], Train Loss: 0.0246, Val Loss: 0.0242\n",
      "Epoch [12/50], Train Loss: 0.0251, Val Loss: 0.0151\n",
      "Epoch [13/50], Train Loss: 0.0257, Val Loss: 0.0034\n",
      "Epoch [14/50], Train Loss: 0.0239, Val Loss: 0.0254\n",
      "Epoch [15/50], Train Loss: 0.0225, Val Loss: 0.0147\n",
      "Epoch [16/50], Train Loss: 0.0209, Val Loss: 0.0095\n",
      "Epoch [17/50], Train Loss: 0.0192, Val Loss: 0.0203\n",
      "Epoch [18/50], Train Loss: 0.0179, Val Loss: 0.0171\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=32, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0577, Val Loss: 0.0772\n",
      "Epoch [2/50], Train Loss: 0.0380, Val Loss: 0.0493\n",
      "Epoch [3/50], Train Loss: 0.0294, Val Loss: 0.0338\n",
      "Epoch [4/50], Train Loss: 0.0242, Val Loss: 0.0213\n",
      "Epoch [5/50], Train Loss: 0.0190, Val Loss: 0.0112\n",
      "Epoch [6/50], Train Loss: 0.0144, Val Loss: 0.0058\n",
      "Epoch [7/50], Train Loss: 0.0107, Val Loss: 0.0044\n",
      "Epoch [8/50], Train Loss: 0.0086, Val Loss: 0.0043\n",
      "Epoch [9/50], Train Loss: 0.0070, Val Loss: 0.0039\n",
      "Epoch [10/50], Train Loss: 0.0053, Val Loss: 0.0031\n",
      "Epoch [11/50], Train Loss: 0.0035, Val Loss: 0.0023\n",
      "Epoch [12/50], Train Loss: 0.0026, Val Loss: 0.0025\n",
      "Epoch [13/50], Train Loss: 0.0020, Val Loss: 0.0035\n",
      "Epoch [14/50], Train Loss: 0.0024, Val Loss: 0.0037\n",
      "Epoch [15/50], Train Loss: 0.0020, Val Loss: 0.0023\n",
      "Epoch [16/50], Train Loss: 0.0022, Val Loss: 0.0021\n",
      "Epoch [17/50], Train Loss: 0.0019, Val Loss: 0.0029\n",
      "Epoch [18/50], Train Loss: 0.0022, Val Loss: 0.0039\n",
      "Epoch [19/50], Train Loss: 0.0020, Val Loss: 0.0032\n",
      "Epoch [20/50], Train Loss: 0.0020, Val Loss: 0.0022\n",
      "Epoch [21/50], Train Loss: 0.0020, Val Loss: 0.0024\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=32, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1103, Val Loss: 0.1632\n",
      "Epoch [2/50], Train Loss: 0.0339, Val Loss: 0.0329\n",
      "Epoch [3/50], Train Loss: 0.0375, Val Loss: 0.0406\n",
      "Epoch [4/50], Train Loss: 0.0283, Val Loss: 0.0238\n",
      "Epoch [5/50], Train Loss: 0.0233, Val Loss: 0.0166\n",
      "Epoch [6/50], Train Loss: 0.0193, Val Loss: 0.0082\n",
      "Epoch [7/50], Train Loss: 0.0151, Val Loss: 0.0039\n",
      "Epoch [8/50], Train Loss: 0.0122, Val Loss: 0.0050\n",
      "Epoch [9/50], Train Loss: 0.0098, Val Loss: 0.0033\n",
      "Epoch [10/50], Train Loss: 0.0078, Val Loss: 0.0019\n",
      "Epoch [11/50], Train Loss: 0.0070, Val Loss: 0.0017\n",
      "Epoch [12/50], Train Loss: 0.0062, Val Loss: 0.0023\n",
      "Epoch [13/50], Train Loss: 0.0056, Val Loss: 0.0021\n",
      "Epoch [14/50], Train Loss: 0.0056, Val Loss: 0.0031\n",
      "Epoch [15/50], Train Loss: 0.0053, Val Loss: 0.0028\n",
      "Epoch [16/50], Train Loss: 0.0052, Val Loss: 0.0016\n",
      "Epoch [17/50], Train Loss: 0.0051, Val Loss: 0.0017\n",
      "Epoch [18/50], Train Loss: 0.0049, Val Loss: 0.0035\n",
      "Epoch [19/50], Train Loss: 0.0050, Val Loss: 0.0038\n",
      "Epoch [20/50], Train Loss: 0.0053, Val Loss: 0.0025\n",
      "Epoch [21/50], Train Loss: 0.0053, Val Loss: 0.0014\n",
      "Epoch [22/50], Train Loss: 0.0050, Val Loss: 0.0040\n",
      "Epoch [23/50], Train Loss: 0.0050, Val Loss: 0.0055\n",
      "Epoch [24/50], Train Loss: 0.0045, Val Loss: 0.0014\n",
      "Epoch [25/50], Train Loss: 0.0044, Val Loss: 0.0014\n",
      "Epoch [26/50], Train Loss: 0.0043, Val Loss: 0.0026\n",
      "Epoch [27/50], Train Loss: 0.0041, Val Loss: 0.0018\n",
      "Epoch [28/50], Train Loss: 0.0042, Val Loss: 0.0013\n",
      "Epoch [29/50], Train Loss: 0.0040, Val Loss: 0.0020\n",
      "Epoch [30/50], Train Loss: 0.0042, Val Loss: 0.0042\n",
      "Epoch [31/50], Train Loss: 0.0039, Val Loss: 0.0014\n",
      "Epoch [32/50], Train Loss: 0.0040, Val Loss: 0.0013\n",
      "Epoch [33/50], Train Loss: 0.0041, Val Loss: 0.0024\n",
      "Epoch [34/50], Train Loss: 0.0045, Val Loss: 0.0076\n",
      "Epoch [35/50], Train Loss: 0.0040, Val Loss: 0.0012\n",
      "Epoch [36/50], Train Loss: 0.0046, Val Loss: 0.0025\n",
      "Epoch [37/50], Train Loss: 0.0040, Val Loss: 0.0016\n",
      "Epoch [38/50], Train Loss: 0.0051, Val Loss: 0.0110\n",
      "Epoch [39/50], Train Loss: 0.0039, Val Loss: 0.0013\n",
      "Epoch [40/50], Train Loss: 0.0046, Val Loss: 0.0019\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=32, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0668, Val Loss: 0.1346\n",
      "Epoch [2/50], Train Loss: 0.0405, Val Loss: 0.0780\n",
      "Epoch [3/50], Train Loss: 0.0347, Val Loss: 0.0572\n",
      "Epoch [4/50], Train Loss: 0.0295, Val Loss: 0.0408\n",
      "Epoch [5/50], Train Loss: 0.0224, Val Loss: 0.0250\n",
      "Epoch [6/50], Train Loss: 0.0174, Val Loss: 0.0170\n",
      "Epoch [7/50], Train Loss: 0.0134, Val Loss: 0.0120\n",
      "Epoch [8/50], Train Loss: 0.0114, Val Loss: 0.0055\n",
      "Epoch [9/50], Train Loss: 0.0097, Val Loss: 0.0049\n",
      "Epoch [10/50], Train Loss: 0.0090, Val Loss: 0.0047\n",
      "Epoch [11/50], Train Loss: 0.0087, Val Loss: 0.0039\n",
      "Epoch [12/50], Train Loss: 0.0085, Val Loss: 0.0040\n",
      "Epoch [13/50], Train Loss: 0.0079, Val Loss: 0.0047\n",
      "Epoch [14/50], Train Loss: 0.0076, Val Loss: 0.0058\n",
      "Epoch [15/50], Train Loss: 0.0072, Val Loss: 0.0025\n",
      "Epoch [16/50], Train Loss: 0.0075, Val Loss: 0.0025\n",
      "Epoch [17/50], Train Loss: 0.0073, Val Loss: 0.0077\n",
      "Epoch [18/50], Train Loss: 0.0072, Val Loss: 0.0028\n",
      "Epoch [19/50], Train Loss: 0.0072, Val Loss: 0.0016\n",
      "Epoch [20/50], Train Loss: 0.0066, Val Loss: 0.0031\n",
      "Epoch [21/50], Train Loss: 0.0064, Val Loss: 0.0071\n",
      "Epoch [22/50], Train Loss: 0.0064, Val Loss: 0.0018\n",
      "Epoch [23/50], Train Loss: 0.0069, Val Loss: 0.0020\n",
      "Epoch [24/50], Train Loss: 0.0063, Val Loss: 0.0083\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=32, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0537, Val Loss: 0.0250\n",
      "Epoch [2/50], Train Loss: 0.0496, Val Loss: 0.0513\n",
      "Epoch [3/50], Train Loss: 0.0245, Val Loss: 0.0138\n",
      "Epoch [4/50], Train Loss: 0.0187, Val Loss: 0.0051\n",
      "Epoch [5/50], Train Loss: 0.0139, Val Loss: 0.0058\n",
      "Epoch [6/50], Train Loss: 0.0120, Val Loss: 0.0109\n",
      "Epoch [7/50], Train Loss: 0.0088, Val Loss: 0.0115\n",
      "Epoch [8/50], Train Loss: 0.0067, Val Loss: 0.0056\n",
      "Epoch [9/50], Train Loss: 0.0043, Val Loss: 0.0017\n",
      "Epoch [10/50], Train Loss: 0.0027, Val Loss: 0.0027\n",
      "Epoch [11/50], Train Loss: 0.0027, Val Loss: 0.0121\n",
      "Epoch [12/50], Train Loss: 0.0023, Val Loss: 0.0074\n",
      "Epoch [13/50], Train Loss: 0.0027, Val Loss: 0.0017\n",
      "Epoch [14/50], Train Loss: 0.0022, Val Loss: 0.0032\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=32, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0466, Val Loss: 0.0457\n",
      "Epoch [2/50], Train Loss: 0.0549, Val Loss: 0.0693\n",
      "Epoch [3/50], Train Loss: 0.0312, Val Loss: 0.0415\n",
      "Epoch [4/50], Train Loss: 0.0255, Val Loss: 0.0194\n",
      "Epoch [5/50], Train Loss: 0.0173, Val Loss: 0.0110\n",
      "Epoch [6/50], Train Loss: 0.0123, Val Loss: 0.0081\n",
      "Epoch [7/50], Train Loss: 0.0087, Val Loss: 0.0034\n",
      "Epoch [8/50], Train Loss: 0.0076, Val Loss: 0.0055\n",
      "Epoch [9/50], Train Loss: 0.0062, Val Loss: 0.0032\n",
      "Epoch [10/50], Train Loss: 0.0063, Val Loss: 0.0088\n",
      "Epoch [11/50], Train Loss: 0.0063, Val Loss: 0.0077\n",
      "Epoch [12/50], Train Loss: 0.0066, Val Loss: 0.0027\n",
      "Epoch [13/50], Train Loss: 0.0062, Val Loss: 0.0046\n",
      "Epoch [14/50], Train Loss: 0.0057, Val Loss: 0.0070\n",
      "Epoch [15/50], Train Loss: 0.0059, Val Loss: 0.0081\n",
      "Epoch [16/50], Train Loss: 0.0061, Val Loss: 0.0032\n",
      "Epoch [17/50], Train Loss: 0.0059, Val Loss: 0.0055\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=32, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0643, Val Loss: 0.1202\n",
      "Epoch [2/50], Train Loss: 0.0446, Val Loss: 0.0688\n",
      "Epoch [3/50], Train Loss: 0.0344, Val Loss: 0.0337\n",
      "Epoch [4/50], Train Loss: 0.0242, Val Loss: 0.0150\n",
      "Epoch [5/50], Train Loss: 0.0191, Val Loss: 0.0092\n",
      "Epoch [6/50], Train Loss: 0.0121, Val Loss: 0.0050\n",
      "Epoch [7/50], Train Loss: 0.0112, Val Loss: 0.0080\n",
      "Epoch [8/50], Train Loss: 0.0103, Val Loss: 0.0069\n",
      "Epoch [9/50], Train Loss: 0.0109, Val Loss: 0.0022\n",
      "Epoch [10/50], Train Loss: 0.0092, Val Loss: 0.0088\n",
      "Epoch [11/50], Train Loss: 0.0094, Val Loss: 0.0171\n",
      "Epoch [12/50], Train Loss: 0.0105, Val Loss: 0.0040\n",
      "Epoch [13/50], Train Loss: 0.0128, Val Loss: 0.0064\n",
      "Epoch [14/50], Train Loss: 0.0093, Val Loss: 0.0198\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=32, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0608, Val Loss: 0.0696\n",
      "Epoch [2/50], Train Loss: 0.0584, Val Loss: 0.0904\n",
      "Epoch [3/50], Train Loss: 0.0401, Val Loss: 0.0628\n",
      "Epoch [4/50], Train Loss: 0.0356, Val Loss: 0.0401\n",
      "Epoch [5/50], Train Loss: 0.0260, Val Loss: 0.0120\n",
      "Epoch [6/50], Train Loss: 0.0148, Val Loss: 0.0032\n",
      "Epoch [7/50], Train Loss: 0.0073, Val Loss: 0.0133\n",
      "Epoch [8/50], Train Loss: 0.0076, Val Loss: 0.0101\n",
      "Epoch [9/50], Train Loss: 0.0064, Val Loss: 0.0038\n",
      "Epoch [10/50], Train Loss: 0.0041, Val Loss: 0.0095\n",
      "Epoch [11/50], Train Loss: 0.0057, Val Loss: 0.0098\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=32, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0449, Val Loss: 0.0623\n",
      "Epoch [2/50], Train Loss: 0.0658, Val Loss: 0.0915\n",
      "Epoch [3/50], Train Loss: 0.0420, Val Loss: 0.0724\n",
      "Epoch [4/50], Train Loss: 0.0332, Val Loss: 0.0363\n",
      "Epoch [5/50], Train Loss: 0.0231, Val Loss: 0.0096\n",
      "Epoch [6/50], Train Loss: 0.0125, Val Loss: 0.0134\n",
      "Epoch [7/50], Train Loss: 0.0103, Val Loss: 0.0148\n",
      "Epoch [8/50], Train Loss: 0.0112, Val Loss: 0.0065\n",
      "Epoch [9/50], Train Loss: 0.0098, Val Loss: 0.0032\n",
      "Epoch [10/50], Train Loss: 0.0107, Val Loss: 0.0220\n",
      "Epoch [11/50], Train Loss: 0.0138, Val Loss: 0.0044\n",
      "Epoch [12/50], Train Loss: 0.0089, Val Loss: 0.0037\n",
      "Epoch [13/50], Train Loss: 0.0090, Val Loss: 0.0139\n",
      "Epoch [14/50], Train Loss: 0.0116, Val Loss: 0.0024\n",
      "Epoch [15/50], Train Loss: 0.0081, Val Loss: 0.0071\n",
      "Epoch [16/50], Train Loss: 0.0080, Val Loss: 0.0120\n",
      "Epoch [17/50], Train Loss: 0.0097, Val Loss: 0.0023\n",
      "Epoch [18/50], Train Loss: 0.0079, Val Loss: 0.0184\n",
      "Epoch [19/50], Train Loss: 0.0092, Val Loss: 0.0046\n",
      "Epoch [20/50], Train Loss: 0.0084, Val Loss: 0.0033\n",
      "Epoch [21/50], Train Loss: 0.0080, Val Loss: 0.0240\n",
      "Epoch [22/50], Train Loss: 0.0075, Val Loss: 0.0032\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=32, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0722, Val Loss: 0.0542\n",
      "Epoch [2/50], Train Loss: 0.0862, Val Loss: 0.1031\n",
      "Epoch [3/50], Train Loss: 0.0518, Val Loss: 0.0673\n",
      "Epoch [4/50], Train Loss: 0.0448, Val Loss: 0.0351\n",
      "Epoch [5/50], Train Loss: 0.0361, Val Loss: 0.0155\n",
      "Epoch [6/50], Train Loss: 0.0289, Val Loss: 0.0071\n",
      "Epoch [7/50], Train Loss: 0.0222, Val Loss: 0.0191\n",
      "Epoch [8/50], Train Loss: 0.0208, Val Loss: 0.0145\n",
      "Epoch [9/50], Train Loss: 0.0195, Val Loss: 0.0145\n",
      "Epoch [10/50], Train Loss: 0.0188, Val Loss: 0.0049\n",
      "Epoch [11/50], Train Loss: 0.0193, Val Loss: 0.0330\n",
      "Epoch [12/50], Train Loss: 0.0175, Val Loss: 0.0034\n",
      "Epoch [13/50], Train Loss: 0.0171, Val Loss: 0.0019\n",
      "Epoch [14/50], Train Loss: 0.0157, Val Loss: 0.0250\n",
      "Epoch [15/50], Train Loss: 0.0156, Val Loss: 0.0057\n",
      "Epoch [16/50], Train Loss: 0.0175, Val Loss: 0.0063\n",
      "Epoch [17/50], Train Loss: 0.0133, Val Loss: 0.0205\n",
      "Epoch [18/50], Train Loss: 0.0140, Val Loss: 0.0068\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=64, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0565, Val Loss: 0.0461\n",
      "Epoch [2/50], Train Loss: 0.0399, Val Loss: 0.0380\n",
      "Epoch [3/50], Train Loss: 0.0241, Val Loss: 0.0141\n",
      "Epoch [4/50], Train Loss: 0.0171, Val Loss: 0.0030\n",
      "Epoch [5/50], Train Loss: 0.0091, Val Loss: 0.0131\n",
      "Epoch [6/50], Train Loss: 0.0079, Val Loss: 0.0036\n",
      "Epoch [7/50], Train Loss: 0.0040, Val Loss: 0.0027\n",
      "Epoch [8/50], Train Loss: 0.0026, Val Loss: 0.0039\n",
      "Epoch [9/50], Train Loss: 0.0021, Val Loss: 0.0051\n",
      "Epoch [10/50], Train Loss: 0.0023, Val Loss: 0.0047\n",
      "Epoch [11/50], Train Loss: 0.0023, Val Loss: 0.0023\n",
      "Epoch [12/50], Train Loss: 0.0019, Val Loss: 0.0025\n",
      "Epoch [13/50], Train Loss: 0.0022, Val Loss: 0.0065\n",
      "Epoch [14/50], Train Loss: 0.0020, Val Loss: 0.0027\n",
      "Epoch [15/50], Train Loss: 0.0024, Val Loss: 0.0027\n",
      "Epoch [16/50], Train Loss: 0.0017, Val Loss: 0.0027\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=64, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0465, Val Loss: 0.0233\n",
      "Epoch [2/50], Train Loss: 0.0388, Val Loss: 0.0285\n",
      "Epoch [3/50], Train Loss: 0.0226, Val Loss: 0.0120\n",
      "Epoch [4/50], Train Loss: 0.0156, Val Loss: 0.0025\n",
      "Epoch [5/50], Train Loss: 0.0092, Val Loss: 0.0067\n",
      "Epoch [6/50], Train Loss: 0.0077, Val Loss: 0.0031\n",
      "Epoch [7/50], Train Loss: 0.0052, Val Loss: 0.0021\n",
      "Epoch [8/50], Train Loss: 0.0046, Val Loss: 0.0030\n",
      "Epoch [9/50], Train Loss: 0.0042, Val Loss: 0.0040\n",
      "Epoch [10/50], Train Loss: 0.0039, Val Loss: 0.0037\n",
      "Epoch [11/50], Train Loss: 0.0040, Val Loss: 0.0020\n",
      "Epoch [12/50], Train Loss: 0.0038, Val Loss: 0.0018\n",
      "Epoch [13/50], Train Loss: 0.0034, Val Loss: 0.0027\n",
      "Epoch [14/50], Train Loss: 0.0034, Val Loss: 0.0032\n",
      "Epoch [15/50], Train Loss: 0.0035, Val Loss: 0.0023\n",
      "Epoch [16/50], Train Loss: 0.0034, Val Loss: 0.0018\n",
      "Epoch [17/50], Train Loss: 0.0031, Val Loss: 0.0020\n",
      "Epoch [18/50], Train Loss: 0.0032, Val Loss: 0.0021\n",
      "Epoch [19/50], Train Loss: 0.0032, Val Loss: 0.0031\n",
      "Epoch [20/50], Train Loss: 0.0032, Val Loss: 0.0015\n",
      "Epoch [21/50], Train Loss: 0.0030, Val Loss: 0.0014\n",
      "Epoch [22/50], Train Loss: 0.0028, Val Loss: 0.0023\n",
      "Epoch [23/50], Train Loss: 0.0033, Val Loss: 0.0048\n",
      "Epoch [24/50], Train Loss: 0.0031, Val Loss: 0.0037\n",
      "Epoch [25/50], Train Loss: 0.0030, Val Loss: 0.0014\n",
      "Epoch [26/50], Train Loss: 0.0029, Val Loss: 0.0025\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=64, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0637, Val Loss: 0.0453\n",
      "Epoch [2/50], Train Loss: 0.0455, Val Loss: 0.0391\n",
      "Epoch [3/50], Train Loss: 0.0304, Val Loss: 0.0239\n",
      "Epoch [4/50], Train Loss: 0.0235, Val Loss: 0.0082\n",
      "Epoch [5/50], Train Loss: 0.0164, Val Loss: 0.0039\n",
      "Epoch [6/50], Train Loss: 0.0109, Val Loss: 0.0043\n",
      "Epoch [7/50], Train Loss: 0.0097, Val Loss: 0.0057\n",
      "Epoch [8/50], Train Loss: 0.0086, Val Loss: 0.0026\n",
      "Epoch [9/50], Train Loss: 0.0094, Val Loss: 0.0046\n",
      "Epoch [10/50], Train Loss: 0.0084, Val Loss: 0.0039\n",
      "Epoch [11/50], Train Loss: 0.0081, Val Loss: 0.0051\n",
      "Epoch [12/50], Train Loss: 0.0083, Val Loss: 0.0027\n",
      "Epoch [13/50], Train Loss: 0.0084, Val Loss: 0.0039\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=64, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0308, Val Loss: 0.0731\n",
      "Epoch [2/50], Train Loss: 0.0632, Val Loss: 0.0407\n",
      "Epoch [3/50], Train Loss: 0.0320, Val Loss: 0.0171\n",
      "Epoch [4/50], Train Loss: 0.0207, Val Loss: 0.0053\n",
      "Epoch [5/50], Train Loss: 0.0135, Val Loss: 0.0050\n",
      "Epoch [6/50], Train Loss: 0.0070, Val Loss: 0.0122\n",
      "Epoch [7/50], Train Loss: 0.0053, Val Loss: 0.0072\n",
      "Epoch [8/50], Train Loss: 0.0041, Val Loss: 0.0026\n",
      "Epoch [9/50], Train Loss: 0.0027, Val Loss: 0.0039\n",
      "Epoch [10/50], Train Loss: 0.0024, Val Loss: 0.0038\n",
      "Epoch [11/50], Train Loss: 0.0023, Val Loss: 0.0015\n",
      "Epoch [12/50], Train Loss: 0.0026, Val Loss: 0.0039\n",
      "Epoch [13/50], Train Loss: 0.0037, Val Loss: 0.0152\n",
      "Epoch [14/50], Train Loss: 0.0031, Val Loss: 0.0098\n",
      "Epoch [15/50], Train Loss: 0.0022, Val Loss: 0.0017\n",
      "Epoch [16/50], Train Loss: 0.0021, Val Loss: 0.0023\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=64, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0296, Val Loss: 0.0649\n",
      "Epoch [2/50], Train Loss: 0.0552, Val Loss: 0.0133\n",
      "Epoch [3/50], Train Loss: 0.0356, Val Loss: 0.0030\n",
      "Epoch [4/50], Train Loss: 0.0179, Val Loss: 0.0084\n",
      "Epoch [5/50], Train Loss: 0.0167, Val Loss: 0.0108\n",
      "Epoch [6/50], Train Loss: 0.0095, Val Loss: 0.0047\n",
      "Epoch [7/50], Train Loss: 0.0074, Val Loss: 0.0078\n",
      "Epoch [8/50], Train Loss: 0.0050, Val Loss: 0.0025\n",
      "Epoch [9/50], Train Loss: 0.0054, Val Loss: 0.0064\n",
      "Epoch [10/50], Train Loss: 0.0042, Val Loss: 0.0037\n",
      "Epoch [11/50], Train Loss: 0.0048, Val Loss: 0.0056\n",
      "Epoch [12/50], Train Loss: 0.0045, Val Loss: 0.0064\n",
      "Epoch [13/50], Train Loss: 0.0053, Val Loss: 0.0052\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=64, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0454, Val Loss: 0.0425\n",
      "Epoch [2/50], Train Loss: 0.0568, Val Loss: 0.0436\n",
      "Epoch [3/50], Train Loss: 0.0322, Val Loss: 0.0073\n",
      "Epoch [4/50], Train Loss: 0.0200, Val Loss: 0.0080\n",
      "Epoch [5/50], Train Loss: 0.0161, Val Loss: 0.0075\n",
      "Epoch [6/50], Train Loss: 0.0105, Val Loss: 0.0063\n",
      "Epoch [7/50], Train Loss: 0.0087, Val Loss: 0.0028\n",
      "Epoch [8/50], Train Loss: 0.0098, Val Loss: 0.0178\n",
      "Epoch [9/50], Train Loss: 0.0108, Val Loss: 0.0038\n",
      "Epoch [10/50], Train Loss: 0.0089, Val Loss: 0.0049\n",
      "Epoch [11/50], Train Loss: 0.0094, Val Loss: 0.0200\n",
      "Epoch [12/50], Train Loss: 0.0110, Val Loss: 0.0037\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=64, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0268, Val Loss: 0.0874\n",
      "Epoch [2/50], Train Loss: 0.0638, Val Loss: 0.0520\n",
      "Epoch [3/50], Train Loss: 0.0488, Val Loss: 0.0311\n",
      "Epoch [4/50], Train Loss: 0.0271, Val Loss: 0.0153\n",
      "Epoch [5/50], Train Loss: 0.0163, Val Loss: 0.0189\n",
      "Epoch [6/50], Train Loss: 0.0110, Val Loss: 0.0062\n",
      "Epoch [7/50], Train Loss: 0.0083, Val Loss: 0.0043\n",
      "Epoch [8/50], Train Loss: 0.0068, Val Loss: 0.0064\n",
      "Epoch [9/50], Train Loss: 0.0047, Val Loss: 0.0062\n",
      "Epoch [10/50], Train Loss: 0.0068, Val Loss: 0.0136\n",
      "Epoch [11/50], Train Loss: 0.0168, Val Loss: 0.0107\n",
      "Epoch [12/50], Train Loss: 0.0062, Val Loss: 0.0035\n",
      "Epoch [13/50], Train Loss: 0.0055, Val Loss: 0.0171\n",
      "Epoch [14/50], Train Loss: 0.0048, Val Loss: 0.0098\n",
      "Epoch [15/50], Train Loss: 0.0035, Val Loss: 0.0037\n",
      "Epoch [16/50], Train Loss: 0.0044, Val Loss: 0.0074\n",
      "Epoch [17/50], Train Loss: 0.0095, Val Loss: 0.0077\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=64, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0302, Val Loss: 0.0933\n",
      "Epoch [2/50], Train Loss: 0.0635, Val Loss: 0.0459\n",
      "Epoch [3/50], Train Loss: 0.0470, Val Loss: 0.0170\n",
      "Epoch [4/50], Train Loss: 0.0242, Val Loss: 0.0117\n",
      "Epoch [5/50], Train Loss: 0.0129, Val Loss: 0.0067\n",
      "Epoch [6/50], Train Loss: 0.0132, Val Loss: 0.0071\n",
      "Epoch [7/50], Train Loss: 0.0094, Val Loss: 0.0084\n",
      "Epoch [8/50], Train Loss: 0.0127, Val Loss: 0.0166\n",
      "Epoch [9/50], Train Loss: 0.0062, Val Loss: 0.0112\n",
      "Epoch [10/50], Train Loss: 0.0071, Val Loss: 0.0045\n",
      "Epoch [11/50], Train Loss: 0.0067, Val Loss: 0.0143\n",
      "Epoch [12/50], Train Loss: 0.0057, Val Loss: 0.0110\n",
      "Epoch [13/50], Train Loss: 0.0048, Val Loss: 0.0021\n",
      "Epoch [14/50], Train Loss: 0.0050, Val Loss: 0.0035\n",
      "Epoch [15/50], Train Loss: 0.0073, Val Loss: 0.0211\n",
      "Epoch [16/50], Train Loss: 0.0057, Val Loss: 0.0052\n",
      "Epoch [17/50], Train Loss: 0.0068, Val Loss: 0.0070\n",
      "Epoch [18/50], Train Loss: 0.0074, Val Loss: 0.0236\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=64, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0478, Val Loss: 0.1217\n",
      "Epoch [2/50], Train Loss: 0.0714, Val Loss: 0.0201\n",
      "Epoch [3/50], Train Loss: 0.0584, Val Loss: 0.0125\n",
      "Epoch [4/50], Train Loss: 0.0324, Val Loss: 0.0084\n",
      "Epoch [5/50], Train Loss: 0.0264, Val Loss: 0.0186\n",
      "Epoch [6/50], Train Loss: 0.0228, Val Loss: 0.0128\n",
      "Epoch [7/50], Train Loss: 0.0112, Val Loss: 0.0031\n",
      "Epoch [8/50], Train Loss: 0.0115, Val Loss: 0.0088\n",
      "Epoch [9/50], Train Loss: 0.0108, Val Loss: 0.0021\n",
      "Epoch [10/50], Train Loss: 0.0110, Val Loss: 0.0029\n",
      "Epoch [11/50], Train Loss: 0.0116, Val Loss: 0.0112\n",
      "Epoch [12/50], Train Loss: 0.0104, Val Loss: 0.0043\n",
      "Epoch [13/50], Train Loss: 0.0120, Val Loss: 0.0122\n",
      "Epoch [14/50], Train Loss: 0.0193, Val Loss: 0.0399\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=128, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0233, Val Loss: 0.0310\n",
      "Epoch [2/50], Train Loss: 0.0496, Val Loss: 0.0169\n",
      "Epoch [3/50], Train Loss: 0.0239, Val Loss: 0.0037\n",
      "Epoch [4/50], Train Loss: 0.0120, Val Loss: 0.0072\n",
      "Epoch [5/50], Train Loss: 0.0094, Val Loss: 0.0032\n",
      "Epoch [6/50], Train Loss: 0.0044, Val Loss: 0.0019\n",
      "Epoch [7/50], Train Loss: 0.0044, Val Loss: 0.0073\n",
      "Epoch [8/50], Train Loss: 0.0028, Val Loss: 0.0058\n",
      "Epoch [9/50], Train Loss: 0.0029, Val Loss: 0.0030\n",
      "Epoch [10/50], Train Loss: 0.0025, Val Loss: 0.0020\n",
      "Epoch [11/50], Train Loss: 0.0032, Val Loss: 0.0063\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=128, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0343, Val Loss: 0.0619\n",
      "Epoch [2/50], Train Loss: 0.0457, Val Loss: 0.0067\n",
      "Epoch [3/50], Train Loss: 0.0268, Val Loss: 0.0032\n",
      "Epoch [4/50], Train Loss: 0.0140, Val Loss: 0.0069\n",
      "Epoch [5/50], Train Loss: 0.0106, Val Loss: 0.0028\n",
      "Epoch [6/50], Train Loss: 0.0060, Val Loss: 0.0039\n",
      "Epoch [7/50], Train Loss: 0.0057, Val Loss: 0.0072\n",
      "Epoch [8/50], Train Loss: 0.0033, Val Loss: 0.0015\n",
      "Epoch [9/50], Train Loss: 0.0036, Val Loss: 0.0031\n",
      "Epoch [10/50], Train Loss: 0.0033, Val Loss: 0.0030\n",
      "Epoch [11/50], Train Loss: 0.0033, Val Loss: 0.0022\n",
      "Epoch [12/50], Train Loss: 0.0030, Val Loss: 0.0029\n",
      "Epoch [13/50], Train Loss: 0.0034, Val Loss: 0.0029\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=128, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0326, Val Loss: 0.0535\n",
      "Epoch [2/50], Train Loss: 0.0580, Val Loss: 0.0333\n",
      "Epoch [3/50], Train Loss: 0.0290, Val Loss: 0.0136\n",
      "Epoch [4/50], Train Loss: 0.0187, Val Loss: 0.0018\n",
      "Epoch [5/50], Train Loss: 0.0113, Val Loss: 0.0056\n",
      "Epoch [6/50], Train Loss: 0.0083, Val Loss: 0.0034\n",
      "Epoch [7/50], Train Loss: 0.0087, Val Loss: 0.0089\n",
      "Epoch [8/50], Train Loss: 0.0076, Val Loss: 0.0020\n",
      "Epoch [9/50], Train Loss: 0.0075, Val Loss: 0.0030\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=128, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0212, Val Loss: 0.0770\n",
      "Epoch [2/50], Train Loss: 0.0592, Val Loss: 0.0051\n",
      "Epoch [3/50], Train Loss: 0.0399, Val Loss: 0.0082\n",
      "Epoch [4/50], Train Loss: 0.0124, Val Loss: 0.0232\n",
      "Epoch [5/50], Train Loss: 0.0207, Val Loss: 0.0244\n",
      "Epoch [6/50], Train Loss: 0.0137, Val Loss: 0.0084\n",
      "Epoch [7/50], Train Loss: 0.0059, Val Loss: 0.0065\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=128, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0211, Val Loss: 0.0777\n",
      "Epoch [2/50], Train Loss: 0.0560, Val Loss: 0.0071\n",
      "Epoch [3/50], Train Loss: 0.0390, Val Loss: 0.0047\n",
      "Epoch [4/50], Train Loss: 0.0131, Val Loss: 0.0151\n",
      "Epoch [5/50], Train Loss: 0.0261, Val Loss: 0.0361\n",
      "Epoch [6/50], Train Loss: 0.0146, Val Loss: 0.0129\n",
      "Epoch [7/50], Train Loss: 0.0091, Val Loss: 0.0023\n",
      "Epoch [8/50], Train Loss: 0.0054, Val Loss: 0.0041\n",
      "Epoch [9/50], Train Loss: 0.0063, Val Loss: 0.0181\n",
      "Epoch [10/50], Train Loss: 0.0065, Val Loss: 0.0042\n",
      "Epoch [11/50], Train Loss: 0.0065, Val Loss: 0.0070\n",
      "Epoch [12/50], Train Loss: 0.0063, Val Loss: 0.0253\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=128, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0238, Val Loss: 0.0754\n",
      "Epoch [2/50], Train Loss: 0.0591, Val Loss: 0.0060\n",
      "Epoch [3/50], Train Loss: 0.0370, Val Loss: 0.0103\n",
      "Epoch [4/50], Train Loss: 0.0132, Val Loss: 0.0072\n",
      "Epoch [5/50], Train Loss: 0.0093, Val Loss: 0.0069\n",
      "Epoch [6/50], Train Loss: 0.0106, Val Loss: 0.0075\n",
      "Epoch [7/50], Train Loss: 0.0087, Val Loss: 0.0094\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=128, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0195, Val Loss: 0.0843\n",
      "Epoch [2/50], Train Loss: 0.0589, Val Loss: 0.0740\n",
      "Epoch [3/50], Train Loss: 0.0506, Val Loss: 0.0155\n",
      "Epoch [4/50], Train Loss: 0.0366, Val Loss: 0.0170\n",
      "Epoch [5/50], Train Loss: 0.0183, Val Loss: 0.0089\n",
      "Epoch [6/50], Train Loss: 0.0105, Val Loss: 0.0091\n",
      "Epoch [7/50], Train Loss: 0.0217, Val Loss: 0.0123\n",
      "Epoch [8/50], Train Loss: 0.0355, Val Loss: 0.0443\n",
      "Epoch [9/50], Train Loss: 0.0141, Val Loss: 0.0158\n",
      "Epoch [10/50], Train Loss: 0.0093, Val Loss: 0.0232\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=128, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0207, Val Loss: 0.0869\n",
      "Epoch [2/50], Train Loss: 0.0518, Val Loss: 0.0879\n",
      "Epoch [3/50], Train Loss: 0.0455, Val Loss: 0.0104\n",
      "Epoch [4/50], Train Loss: 0.0407, Val Loss: 0.0222\n",
      "Epoch [5/50], Train Loss: 0.0217, Val Loss: 0.0194\n",
      "Epoch [6/50], Train Loss: 0.0177, Val Loss: 0.0106\n",
      "Epoch [7/50], Train Loss: 0.0140, Val Loss: 0.0127\n",
      "Epoch [8/50], Train Loss: 0.0296, Val Loss: 0.0366\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adam, hidden_size=128, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0264, Val Loss: 0.0705\n",
      "Epoch [2/50], Train Loss: 0.0628, Val Loss: 0.0822\n",
      "Epoch [3/50], Train Loss: 0.0484, Val Loss: 0.0080\n",
      "Epoch [4/50], Train Loss: 0.0340, Val Loss: 0.0148\n",
      "Epoch [5/50], Train Loss: 0.0217, Val Loss: 0.0048\n",
      "Epoch [6/50], Train Loss: 0.0150, Val Loss: 0.0254\n",
      "Epoch [7/50], Train Loss: 0.0314, Val Loss: 0.0261\n",
      "Epoch [8/50], Train Loss: 0.0405, Val Loss: 0.0337\n",
      "Epoch [9/50], Train Loss: 0.0187, Val Loss: 0.0145\n",
      "Epoch [10/50], Train Loss: 0.0120, Val Loss: 0.0064\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=16, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.2194, Val Loss: 0.4351\n",
      "Epoch [2/50], Train Loss: 0.2087, Val Loss: 0.4187\n",
      "Epoch [3/50], Train Loss: 0.1987, Val Loss: 0.4033\n",
      "Epoch [4/50], Train Loss: 0.1893, Val Loss: 0.3887\n",
      "Epoch [5/50], Train Loss: 0.1805, Val Loss: 0.3749\n",
      "Epoch [6/50], Train Loss: 0.1723, Val Loss: 0.3619\n",
      "Epoch [7/50], Train Loss: 0.1645, Val Loss: 0.3495\n",
      "Epoch [8/50], Train Loss: 0.1572, Val Loss: 0.3378\n",
      "Epoch [9/50], Train Loss: 0.1503, Val Loss: 0.3267\n",
      "Epoch [10/50], Train Loss: 0.1438, Val Loss: 0.3161\n",
      "Epoch [11/50], Train Loss: 0.1377, Val Loss: 0.3060\n",
      "Epoch [12/50], Train Loss: 0.1320, Val Loss: 0.2965\n",
      "Epoch [13/50], Train Loss: 0.1265, Val Loss: 0.2874\n",
      "Epoch [14/50], Train Loss: 0.1214, Val Loss: 0.2787\n",
      "Epoch [15/50], Train Loss: 0.1166, Val Loss: 0.2705\n",
      "Epoch [16/50], Train Loss: 0.1120, Val Loss: 0.2627\n",
      "Epoch [17/50], Train Loss: 0.1077, Val Loss: 0.2552\n",
      "Epoch [18/50], Train Loss: 0.1036, Val Loss: 0.2480\n",
      "Epoch [19/50], Train Loss: 0.0997, Val Loss: 0.2412\n",
      "Epoch [20/50], Train Loss: 0.0961, Val Loss: 0.2347\n",
      "Epoch [21/50], Train Loss: 0.0926, Val Loss: 0.2285\n",
      "Epoch [22/50], Train Loss: 0.0893, Val Loss: 0.2225\n",
      "Epoch [23/50], Train Loss: 0.0862, Val Loss: 0.2169\n",
      "Epoch [24/50], Train Loss: 0.0833, Val Loss: 0.2114\n",
      "Epoch [25/50], Train Loss: 0.0805, Val Loss: 0.2063\n",
      "Epoch [26/50], Train Loss: 0.0779, Val Loss: 0.2013\n",
      "Epoch [27/50], Train Loss: 0.0754, Val Loss: 0.1965\n",
      "Epoch [28/50], Train Loss: 0.0730, Val Loss: 0.1920\n",
      "Epoch [29/50], Train Loss: 0.0708, Val Loss: 0.1876\n",
      "Epoch [30/50], Train Loss: 0.0687, Val Loss: 0.1835\n",
      "Epoch [31/50], Train Loss: 0.0667, Val Loss: 0.1795\n",
      "Epoch [32/50], Train Loss: 0.0648, Val Loss: 0.1756\n",
      "Epoch [33/50], Train Loss: 0.0630, Val Loss: 0.1719\n",
      "Epoch [34/50], Train Loss: 0.0613, Val Loss: 0.1684\n",
      "Epoch [35/50], Train Loss: 0.0597, Val Loss: 0.1651\n",
      "Epoch [36/50], Train Loss: 0.0581, Val Loss: 0.1618\n",
      "Epoch [37/50], Train Loss: 0.0567, Val Loss: 0.1587\n",
      "Epoch [38/50], Train Loss: 0.0553, Val Loss: 0.1557\n",
      "Epoch [39/50], Train Loss: 0.0540, Val Loss: 0.1529\n",
      "Epoch [40/50], Train Loss: 0.0528, Val Loss: 0.1502\n",
      "Epoch [41/50], Train Loss: 0.0516, Val Loss: 0.1475\n",
      "Epoch [42/50], Train Loss: 0.0505, Val Loss: 0.1450\n",
      "Epoch [43/50], Train Loss: 0.0494, Val Loss: 0.1426\n",
      "Epoch [44/50], Train Loss: 0.0484, Val Loss: 0.1402\n",
      "Epoch [45/50], Train Loss: 0.0475, Val Loss: 0.1380\n",
      "Epoch [46/50], Train Loss: 0.0466, Val Loss: 0.1358\n",
      "Epoch [47/50], Train Loss: 0.0457, Val Loss: 0.1338\n",
      "Epoch [48/50], Train Loss: 0.0449, Val Loss: 0.1318\n",
      "Epoch [49/50], Train Loss: 0.0442, Val Loss: 0.1299\n",
      "Epoch [50/50], Train Loss: 0.0434, Val Loss: 0.1280\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=16, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1212, Val Loss: 0.3418\n",
      "Epoch [2/50], Train Loss: 0.1175, Val Loss: 0.3349\n",
      "Epoch [3/50], Train Loss: 0.1131, Val Loss: 0.3282\n",
      "Epoch [4/50], Train Loss: 0.1118, Val Loss: 0.3217\n",
      "Epoch [5/50], Train Loss: 0.1069, Val Loss: 0.3155\n",
      "Epoch [6/50], Train Loss: 0.1050, Val Loss: 0.3094\n",
      "Epoch [7/50], Train Loss: 0.1028, Val Loss: 0.3035\n",
      "Epoch [8/50], Train Loss: 0.0998, Val Loss: 0.2978\n",
      "Epoch [9/50], Train Loss: 0.0966, Val Loss: 0.2923\n",
      "Epoch [10/50], Train Loss: 0.0943, Val Loss: 0.2869\n",
      "Epoch [11/50], Train Loss: 0.0927, Val Loss: 0.2817\n",
      "Epoch [12/50], Train Loss: 0.0887, Val Loss: 0.2767\n",
      "Epoch [13/50], Train Loss: 0.0881, Val Loss: 0.2719\n",
      "Epoch [14/50], Train Loss: 0.0855, Val Loss: 0.2671\n",
      "Epoch [15/50], Train Loss: 0.0839, Val Loss: 0.2626\n",
      "Epoch [16/50], Train Loss: 0.0805, Val Loss: 0.2581\n",
      "Epoch [17/50], Train Loss: 0.0791, Val Loss: 0.2538\n",
      "Epoch [18/50], Train Loss: 0.0771, Val Loss: 0.2496\n",
      "Epoch [19/50], Train Loss: 0.0756, Val Loss: 0.2456\n",
      "Epoch [20/50], Train Loss: 0.0741, Val Loss: 0.2417\n",
      "Epoch [21/50], Train Loss: 0.0732, Val Loss: 0.2379\n",
      "Epoch [22/50], Train Loss: 0.0716, Val Loss: 0.2342\n",
      "Epoch [23/50], Train Loss: 0.0696, Val Loss: 0.2306\n",
      "Epoch [24/50], Train Loss: 0.0691, Val Loss: 0.2271\n",
      "Epoch [25/50], Train Loss: 0.0677, Val Loss: 0.2237\n",
      "Epoch [26/50], Train Loss: 0.0666, Val Loss: 0.2205\n",
      "Epoch [27/50], Train Loss: 0.0647, Val Loss: 0.2174\n",
      "Epoch [28/50], Train Loss: 0.0637, Val Loss: 0.2143\n",
      "Epoch [29/50], Train Loss: 0.0627, Val Loss: 0.2113\n",
      "Epoch [30/50], Train Loss: 0.0617, Val Loss: 0.2084\n",
      "Epoch [31/50], Train Loss: 0.0608, Val Loss: 0.2057\n",
      "Epoch [32/50], Train Loss: 0.0598, Val Loss: 0.2029\n",
      "Epoch [33/50], Train Loss: 0.0593, Val Loss: 0.2003\n",
      "Epoch [34/50], Train Loss: 0.0584, Val Loss: 0.1978\n",
      "Epoch [35/50], Train Loss: 0.0580, Val Loss: 0.1953\n",
      "Epoch [36/50], Train Loss: 0.0564, Val Loss: 0.1929\n",
      "Epoch [37/50], Train Loss: 0.0553, Val Loss: 0.1906\n",
      "Epoch [38/50], Train Loss: 0.0536, Val Loss: 0.1883\n",
      "Epoch [39/50], Train Loss: 0.0541, Val Loss: 0.1861\n",
      "Epoch [40/50], Train Loss: 0.0539, Val Loss: 0.1840\n",
      "Epoch [41/50], Train Loss: 0.0532, Val Loss: 0.1819\n",
      "Epoch [42/50], Train Loss: 0.0516, Val Loss: 0.1800\n",
      "Epoch [43/50], Train Loss: 0.0512, Val Loss: 0.1780\n",
      "Epoch [44/50], Train Loss: 0.0501, Val Loss: 0.1761\n",
      "Epoch [45/50], Train Loss: 0.0500, Val Loss: 0.1743\n",
      "Epoch [46/50], Train Loss: 0.0500, Val Loss: 0.1725\n",
      "Epoch [47/50], Train Loss: 0.0498, Val Loss: 0.1707\n",
      "Epoch [48/50], Train Loss: 0.0489, Val Loss: 0.1691\n",
      "Epoch [49/50], Train Loss: 0.0488, Val Loss: 0.1675\n",
      "Epoch [50/50], Train Loss: 0.0484, Val Loss: 0.1659\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=16, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.2057, Val Loss: 0.4305\n",
      "Epoch [2/50], Train Loss: 0.2012, Val Loss: 0.4146\n",
      "Epoch [3/50], Train Loss: 0.1912, Val Loss: 0.3996\n",
      "Epoch [4/50], Train Loss: 0.1815, Val Loss: 0.3855\n",
      "Epoch [5/50], Train Loss: 0.1727, Val Loss: 0.3722\n",
      "Epoch [6/50], Train Loss: 0.1677, Val Loss: 0.3594\n",
      "Epoch [7/50], Train Loss: 0.1643, Val Loss: 0.3472\n",
      "Epoch [8/50], Train Loss: 0.1543, Val Loss: 0.3357\n",
      "Epoch [9/50], Train Loss: 0.1486, Val Loss: 0.3246\n",
      "Epoch [10/50], Train Loss: 0.1417, Val Loss: 0.3142\n",
      "Epoch [11/50], Train Loss: 0.1357, Val Loss: 0.3041\n",
      "Epoch [12/50], Train Loss: 0.1328, Val Loss: 0.2946\n",
      "Epoch [13/50], Train Loss: 0.1236, Val Loss: 0.2856\n",
      "Epoch [14/50], Train Loss: 0.1226, Val Loss: 0.2769\n",
      "Epoch [15/50], Train Loss: 0.1172, Val Loss: 0.2686\n",
      "Epoch [16/50], Train Loss: 0.1108, Val Loss: 0.2606\n",
      "Epoch [17/50], Train Loss: 0.1093, Val Loss: 0.2530\n",
      "Epoch [18/50], Train Loss: 0.1038, Val Loss: 0.2457\n",
      "Epoch [19/50], Train Loss: 0.1016, Val Loss: 0.2389\n",
      "Epoch [20/50], Train Loss: 0.1005, Val Loss: 0.2321\n",
      "Epoch [21/50], Train Loss: 0.0961, Val Loss: 0.2257\n",
      "Epoch [22/50], Train Loss: 0.0921, Val Loss: 0.2197\n",
      "Epoch [23/50], Train Loss: 0.0915, Val Loss: 0.2138\n",
      "Epoch [24/50], Train Loss: 0.0859, Val Loss: 0.2082\n",
      "Epoch [25/50], Train Loss: 0.0839, Val Loss: 0.2028\n",
      "Epoch [26/50], Train Loss: 0.0826, Val Loss: 0.1977\n",
      "Epoch [27/50], Train Loss: 0.0806, Val Loss: 0.1928\n",
      "Epoch [28/50], Train Loss: 0.0794, Val Loss: 0.1879\n",
      "Epoch [29/50], Train Loss: 0.0767, Val Loss: 0.1835\n",
      "Epoch [30/50], Train Loss: 0.0720, Val Loss: 0.1792\n",
      "Epoch [31/50], Train Loss: 0.0727, Val Loss: 0.1750\n",
      "Epoch [32/50], Train Loss: 0.0713, Val Loss: 0.1710\n",
      "Epoch [33/50], Train Loss: 0.0683, Val Loss: 0.1671\n",
      "Epoch [34/50], Train Loss: 0.0678, Val Loss: 0.1634\n",
      "Epoch [35/50], Train Loss: 0.0650, Val Loss: 0.1599\n",
      "Epoch [36/50], Train Loss: 0.0639, Val Loss: 0.1565\n",
      "Epoch [37/50], Train Loss: 0.0612, Val Loss: 0.1532\n",
      "Epoch [38/50], Train Loss: 0.0628, Val Loss: 0.1501\n",
      "Epoch [39/50], Train Loss: 0.0596, Val Loss: 0.1470\n",
      "Epoch [40/50], Train Loss: 0.0599, Val Loss: 0.1441\n",
      "Epoch [41/50], Train Loss: 0.0577, Val Loss: 0.1413\n",
      "Epoch [42/50], Train Loss: 0.0584, Val Loss: 0.1386\n",
      "Epoch [43/50], Train Loss: 0.0575, Val Loss: 0.1360\n",
      "Epoch [44/50], Train Loss: 0.0569, Val Loss: 0.1334\n",
      "Epoch [45/50], Train Loss: 0.0554, Val Loss: 0.1310\n",
      "Epoch [46/50], Train Loss: 0.0535, Val Loss: 0.1287\n",
      "Epoch [47/50], Train Loss: 0.0536, Val Loss: 0.1264\n",
      "Epoch [48/50], Train Loss: 0.0513, Val Loss: 0.1242\n",
      "Epoch [49/50], Train Loss: 0.0516, Val Loss: 0.1222\n",
      "Epoch [50/50], Train Loss: 0.0529, Val Loss: 0.1201\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=16, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.2643, Val Loss: 0.6006\n",
      "Epoch [2/50], Train Loss: 0.2512, Val Loss: 0.5802\n",
      "Epoch [3/50], Train Loss: 0.2391, Val Loss: 0.5610\n",
      "Epoch [4/50], Train Loss: 0.2277, Val Loss: 0.5428\n",
      "Epoch [5/50], Train Loss: 0.2170, Val Loss: 0.5255\n",
      "Epoch [6/50], Train Loss: 0.2071, Val Loss: 0.5091\n",
      "Epoch [7/50], Train Loss: 0.1977, Val Loss: 0.4935\n",
      "Epoch [8/50], Train Loss: 0.1889, Val Loss: 0.4786\n",
      "Epoch [9/50], Train Loss: 0.1806, Val Loss: 0.4645\n",
      "Epoch [10/50], Train Loss: 0.1727, Val Loss: 0.4509\n",
      "Epoch [11/50], Train Loss: 0.1653, Val Loss: 0.4380\n",
      "Epoch [12/50], Train Loss: 0.1584, Val Loss: 0.4256\n",
      "Epoch [13/50], Train Loss: 0.1518, Val Loss: 0.4138\n",
      "Epoch [14/50], Train Loss: 0.1455, Val Loss: 0.4024\n",
      "Epoch [15/50], Train Loss: 0.1396, Val Loss: 0.3915\n",
      "Epoch [16/50], Train Loss: 0.1340, Val Loss: 0.3810\n",
      "Epoch [17/50], Train Loss: 0.1287, Val Loss: 0.3710\n",
      "Epoch [18/50], Train Loss: 0.1237, Val Loss: 0.3613\n",
      "Epoch [19/50], Train Loss: 0.1189, Val Loss: 0.3520\n",
      "Epoch [20/50], Train Loss: 0.1143, Val Loss: 0.3431\n",
      "Epoch [21/50], Train Loss: 0.1101, Val Loss: 0.3345\n",
      "Epoch [22/50], Train Loss: 0.1060, Val Loss: 0.3262\n",
      "Epoch [23/50], Train Loss: 0.1021, Val Loss: 0.3182\n",
      "Epoch [24/50], Train Loss: 0.0984, Val Loss: 0.3105\n",
      "Epoch [25/50], Train Loss: 0.0949, Val Loss: 0.3031\n",
      "Epoch [26/50], Train Loss: 0.0916, Val Loss: 0.2959\n",
      "Epoch [27/50], Train Loss: 0.0884, Val Loss: 0.2890\n",
      "Epoch [28/50], Train Loss: 0.0854, Val Loss: 0.2824\n",
      "Epoch [29/50], Train Loss: 0.0826, Val Loss: 0.2760\n",
      "Epoch [30/50], Train Loss: 0.0799, Val Loss: 0.2698\n",
      "Epoch [31/50], Train Loss: 0.0773, Val Loss: 0.2638\n",
      "Epoch [32/50], Train Loss: 0.0749, Val Loss: 0.2580\n",
      "Epoch [33/50], Train Loss: 0.0725, Val Loss: 0.2525\n",
      "Epoch [34/50], Train Loss: 0.0703, Val Loss: 0.2471\n",
      "Epoch [35/50], Train Loss: 0.0683, Val Loss: 0.2419\n",
      "Epoch [36/50], Train Loss: 0.0663, Val Loss: 0.2369\n",
      "Epoch [37/50], Train Loss: 0.0644, Val Loss: 0.2320\n",
      "Epoch [38/50], Train Loss: 0.0626, Val Loss: 0.2273\n",
      "Epoch [39/50], Train Loss: 0.0609, Val Loss: 0.2228\n",
      "Epoch [40/50], Train Loss: 0.0593, Val Loss: 0.2185\n",
      "Epoch [41/50], Train Loss: 0.0578, Val Loss: 0.2143\n",
      "Epoch [42/50], Train Loss: 0.0563, Val Loss: 0.2102\n",
      "Epoch [43/50], Train Loss: 0.0549, Val Loss: 0.2063\n",
      "Epoch [44/50], Train Loss: 0.0536, Val Loss: 0.2025\n",
      "Epoch [45/50], Train Loss: 0.0524, Val Loss: 0.1988\n",
      "Epoch [46/50], Train Loss: 0.0512, Val Loss: 0.1952\n",
      "Epoch [47/50], Train Loss: 0.0501, Val Loss: 0.1918\n",
      "Epoch [48/50], Train Loss: 0.0491, Val Loss: 0.1885\n",
      "Epoch [49/50], Train Loss: 0.0481, Val Loss: 0.1853\n",
      "Epoch [50/50], Train Loss: 0.0471, Val Loss: 0.1823\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=16, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.2212, Val Loss: 0.5205\n",
      "Epoch [2/50], Train Loss: 0.2119, Val Loss: 0.5013\n",
      "Epoch [3/50], Train Loss: 0.2010, Val Loss: 0.4833\n",
      "Epoch [4/50], Train Loss: 0.1905, Val Loss: 0.4665\n",
      "Epoch [5/50], Train Loss: 0.1817, Val Loss: 0.4504\n",
      "Epoch [6/50], Train Loss: 0.1731, Val Loss: 0.4353\n",
      "Epoch [7/50], Train Loss: 0.1658, Val Loss: 0.4210\n",
      "Epoch [8/50], Train Loss: 0.1572, Val Loss: 0.4075\n",
      "Epoch [9/50], Train Loss: 0.1510, Val Loss: 0.3947\n",
      "Epoch [10/50], Train Loss: 0.1453, Val Loss: 0.3823\n",
      "Epoch [11/50], Train Loss: 0.1397, Val Loss: 0.3706\n",
      "Epoch [12/50], Train Loss: 0.1340, Val Loss: 0.3595\n",
      "Epoch [13/50], Train Loss: 0.1271, Val Loss: 0.3489\n",
      "Epoch [14/50], Train Loss: 0.1238, Val Loss: 0.3387\n",
      "Epoch [15/50], Train Loss: 0.1194, Val Loss: 0.3290\n",
      "Epoch [16/50], Train Loss: 0.1146, Val Loss: 0.3197\n",
      "Epoch [17/50], Train Loss: 0.1114, Val Loss: 0.3109\n",
      "Epoch [18/50], Train Loss: 0.1070, Val Loss: 0.3024\n",
      "Epoch [19/50], Train Loss: 0.1037, Val Loss: 0.2944\n",
      "Epoch [20/50], Train Loss: 0.0983, Val Loss: 0.2867\n",
      "Epoch [21/50], Train Loss: 0.0960, Val Loss: 0.2793\n",
      "Epoch [22/50], Train Loss: 0.0927, Val Loss: 0.2722\n",
      "Epoch [23/50], Train Loss: 0.0904, Val Loss: 0.2654\n",
      "Epoch [24/50], Train Loss: 0.0875, Val Loss: 0.2589\n",
      "Epoch [25/50], Train Loss: 0.0857, Val Loss: 0.2527\n",
      "Epoch [26/50], Train Loss: 0.0818, Val Loss: 0.2467\n",
      "Epoch [27/50], Train Loss: 0.0796, Val Loss: 0.2410\n",
      "Epoch [28/50], Train Loss: 0.0777, Val Loss: 0.2354\n",
      "Epoch [29/50], Train Loss: 0.0752, Val Loss: 0.2302\n",
      "Epoch [30/50], Train Loss: 0.0731, Val Loss: 0.2251\n",
      "Epoch [31/50], Train Loss: 0.0723, Val Loss: 0.2202\n",
      "Epoch [32/50], Train Loss: 0.0699, Val Loss: 0.2155\n",
      "Epoch [33/50], Train Loss: 0.0678, Val Loss: 0.2109\n",
      "Epoch [34/50], Train Loss: 0.0672, Val Loss: 0.2066\n",
      "Epoch [35/50], Train Loss: 0.0652, Val Loss: 0.2025\n",
      "Epoch [36/50], Train Loss: 0.0637, Val Loss: 0.1985\n",
      "Epoch [37/50], Train Loss: 0.0621, Val Loss: 0.1946\n",
      "Epoch [38/50], Train Loss: 0.0620, Val Loss: 0.1910\n",
      "Epoch [39/50], Train Loss: 0.0592, Val Loss: 0.1875\n",
      "Epoch [40/50], Train Loss: 0.0594, Val Loss: 0.1841\n",
      "Epoch [41/50], Train Loss: 0.0583, Val Loss: 0.1808\n",
      "Epoch [42/50], Train Loss: 0.0573, Val Loss: 0.1777\n",
      "Epoch [43/50], Train Loss: 0.0568, Val Loss: 0.1746\n",
      "Epoch [44/50], Train Loss: 0.0550, Val Loss: 0.1717\n",
      "Epoch [45/50], Train Loss: 0.0547, Val Loss: 0.1690\n",
      "Epoch [46/50], Train Loss: 0.0541, Val Loss: 0.1663\n",
      "Epoch [47/50], Train Loss: 0.0540, Val Loss: 0.1638\n",
      "Epoch [48/50], Train Loss: 0.0527, Val Loss: 0.1613\n",
      "Epoch [49/50], Train Loss: 0.0512, Val Loss: 0.1589\n",
      "Epoch [50/50], Train Loss: 0.0506, Val Loss: 0.1567\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=16, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.2479, Val Loss: 0.5070\n",
      "Epoch [2/50], Train Loss: 0.2378, Val Loss: 0.4914\n",
      "Epoch [3/50], Train Loss: 0.2313, Val Loss: 0.4764\n",
      "Epoch [4/50], Train Loss: 0.2218, Val Loss: 0.4621\n",
      "Epoch [5/50], Train Loss: 0.2137, Val Loss: 0.4486\n",
      "Epoch [6/50], Train Loss: 0.2061, Val Loss: 0.4356\n",
      "Epoch [7/50], Train Loss: 0.1969, Val Loss: 0.4234\n",
      "Epoch [8/50], Train Loss: 0.1921, Val Loss: 0.4118\n",
      "Epoch [9/50], Train Loss: 0.1824, Val Loss: 0.4009\n",
      "Epoch [10/50], Train Loss: 0.1780, Val Loss: 0.3902\n",
      "Epoch [11/50], Train Loss: 0.1708, Val Loss: 0.3799\n",
      "Epoch [12/50], Train Loss: 0.1647, Val Loss: 0.3702\n",
      "Epoch [13/50], Train Loss: 0.1607, Val Loss: 0.3608\n",
      "Epoch [14/50], Train Loss: 0.1571, Val Loss: 0.3517\n",
      "Epoch [15/50], Train Loss: 0.1512, Val Loss: 0.3431\n",
      "Epoch [16/50], Train Loss: 0.1460, Val Loss: 0.3348\n",
      "Epoch [17/50], Train Loss: 0.1425, Val Loss: 0.3268\n",
      "Epoch [18/50], Train Loss: 0.1390, Val Loss: 0.3193\n",
      "Epoch [19/50], Train Loss: 0.1320, Val Loss: 0.3120\n",
      "Epoch [20/50], Train Loss: 0.1299, Val Loss: 0.3048\n",
      "Epoch [21/50], Train Loss: 0.1290, Val Loss: 0.2981\n",
      "Epoch [22/50], Train Loss: 0.1237, Val Loss: 0.2916\n",
      "Epoch [23/50], Train Loss: 0.1209, Val Loss: 0.2851\n",
      "Epoch [24/50], Train Loss: 0.1178, Val Loss: 0.2791\n",
      "Epoch [25/50], Train Loss: 0.1121, Val Loss: 0.2732\n",
      "Epoch [26/50], Train Loss: 0.1107, Val Loss: 0.2674\n",
      "Epoch [27/50], Train Loss: 0.1074, Val Loss: 0.2619\n",
      "Epoch [28/50], Train Loss: 0.1060, Val Loss: 0.2566\n",
      "Epoch [29/50], Train Loss: 0.1021, Val Loss: 0.2515\n",
      "Epoch [30/50], Train Loss: 0.1032, Val Loss: 0.2466\n",
      "Epoch [31/50], Train Loss: 0.0984, Val Loss: 0.2418\n",
      "Epoch [32/50], Train Loss: 0.0957, Val Loss: 0.2371\n",
      "Epoch [33/50], Train Loss: 0.0944, Val Loss: 0.2326\n",
      "Epoch [34/50], Train Loss: 0.0909, Val Loss: 0.2282\n",
      "Epoch [35/50], Train Loss: 0.0896, Val Loss: 0.2239\n",
      "Epoch [36/50], Train Loss: 0.0876, Val Loss: 0.2199\n",
      "Epoch [37/50], Train Loss: 0.0865, Val Loss: 0.2160\n",
      "Epoch [38/50], Train Loss: 0.0837, Val Loss: 0.2122\n",
      "Epoch [39/50], Train Loss: 0.0828, Val Loss: 0.2084\n",
      "Epoch [40/50], Train Loss: 0.0822, Val Loss: 0.2049\n",
      "Epoch [41/50], Train Loss: 0.0807, Val Loss: 0.2014\n",
      "Epoch [42/50], Train Loss: 0.0790, Val Loss: 0.1980\n",
      "Epoch [43/50], Train Loss: 0.0795, Val Loss: 0.1948\n",
      "Epoch [44/50], Train Loss: 0.0782, Val Loss: 0.1917\n",
      "Epoch [45/50], Train Loss: 0.0735, Val Loss: 0.1886\n",
      "Epoch [46/50], Train Loss: 0.0745, Val Loss: 0.1857\n",
      "Epoch [47/50], Train Loss: 0.0714, Val Loss: 0.1828\n",
      "Epoch [48/50], Train Loss: 0.0712, Val Loss: 0.1800\n",
      "Epoch [49/50], Train Loss: 0.0691, Val Loss: 0.1773\n",
      "Epoch [50/50], Train Loss: 0.0696, Val Loss: 0.1747\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=16, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1609, Val Loss: 0.3711\n",
      "Epoch [2/50], Train Loss: 0.1541, Val Loss: 0.3610\n",
      "Epoch [3/50], Train Loss: 0.1477, Val Loss: 0.3515\n",
      "Epoch [4/50], Train Loss: 0.1416, Val Loss: 0.3424\n",
      "Epoch [5/50], Train Loss: 0.1359, Val Loss: 0.3336\n",
      "Epoch [6/50], Train Loss: 0.1305, Val Loss: 0.3252\n",
      "Epoch [7/50], Train Loss: 0.1254, Val Loss: 0.3172\n",
      "Epoch [8/50], Train Loss: 0.1206, Val Loss: 0.3095\n",
      "Epoch [9/50], Train Loss: 0.1161, Val Loss: 0.3021\n",
      "Epoch [10/50], Train Loss: 0.1117, Val Loss: 0.2950\n",
      "Epoch [11/50], Train Loss: 0.1077, Val Loss: 0.2882\n",
      "Epoch [12/50], Train Loss: 0.1038, Val Loss: 0.2816\n",
      "Epoch [13/50], Train Loss: 0.1001, Val Loss: 0.2753\n",
      "Epoch [14/50], Train Loss: 0.0967, Val Loss: 0.2693\n",
      "Epoch [15/50], Train Loss: 0.0934, Val Loss: 0.2635\n",
      "Epoch [16/50], Train Loss: 0.0903, Val Loss: 0.2579\n",
      "Epoch [17/50], Train Loss: 0.0874, Val Loss: 0.2525\n",
      "Epoch [18/50], Train Loss: 0.0846, Val Loss: 0.2473\n",
      "Epoch [19/50], Train Loss: 0.0820, Val Loss: 0.2423\n",
      "Epoch [20/50], Train Loss: 0.0795, Val Loss: 0.2375\n",
      "Epoch [21/50], Train Loss: 0.0771, Val Loss: 0.2329\n",
      "Epoch [22/50], Train Loss: 0.0749, Val Loss: 0.2284\n",
      "Epoch [23/50], Train Loss: 0.0728, Val Loss: 0.2241\n",
      "Epoch [24/50], Train Loss: 0.0708, Val Loss: 0.2200\n",
      "Epoch [25/50], Train Loss: 0.0689, Val Loss: 0.2160\n",
      "Epoch [26/50], Train Loss: 0.0671, Val Loss: 0.2122\n",
      "Epoch [27/50], Train Loss: 0.0654, Val Loss: 0.2085\n",
      "Epoch [28/50], Train Loss: 0.0638, Val Loss: 0.2049\n",
      "Epoch [29/50], Train Loss: 0.0623, Val Loss: 0.2015\n",
      "Epoch [30/50], Train Loss: 0.0608, Val Loss: 0.1982\n",
      "Epoch [31/50], Train Loss: 0.0595, Val Loss: 0.1950\n",
      "Epoch [32/50], Train Loss: 0.0582, Val Loss: 0.1919\n",
      "Epoch [33/50], Train Loss: 0.0570, Val Loss: 0.1890\n",
      "Epoch [34/50], Train Loss: 0.0558, Val Loss: 0.1861\n",
      "Epoch [35/50], Train Loss: 0.0548, Val Loss: 0.1834\n",
      "Epoch [36/50], Train Loss: 0.0537, Val Loss: 0.1807\n",
      "Epoch [37/50], Train Loss: 0.0528, Val Loss: 0.1782\n",
      "Epoch [38/50], Train Loss: 0.0518, Val Loss: 0.1757\n",
      "Epoch [39/50], Train Loss: 0.0510, Val Loss: 0.1733\n",
      "Epoch [40/50], Train Loss: 0.0502, Val Loss: 0.1710\n",
      "Epoch [41/50], Train Loss: 0.0494, Val Loss: 0.1688\n",
      "Epoch [42/50], Train Loss: 0.0487, Val Loss: 0.1667\n",
      "Epoch [43/50], Train Loss: 0.0480, Val Loss: 0.1646\n",
      "Epoch [44/50], Train Loss: 0.0473, Val Loss: 0.1626\n",
      "Epoch [45/50], Train Loss: 0.0467, Val Loss: 0.1607\n",
      "Epoch [46/50], Train Loss: 0.0461, Val Loss: 0.1588\n",
      "Epoch [47/50], Train Loss: 0.0456, Val Loss: 0.1571\n",
      "Epoch [48/50], Train Loss: 0.0450, Val Loss: 0.1553\n",
      "Epoch [49/50], Train Loss: 0.0445, Val Loss: 0.1537\n",
      "Epoch [50/50], Train Loss: 0.0441, Val Loss: 0.1521\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=16, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1234, Val Loss: 0.2735\n",
      "Epoch [2/50], Train Loss: 0.1176, Val Loss: 0.2659\n",
      "Epoch [3/50], Train Loss: 0.1146, Val Loss: 0.2588\n",
      "Epoch [4/50], Train Loss: 0.1092, Val Loss: 0.2520\n",
      "Epoch [5/50], Train Loss: 0.1071, Val Loss: 0.2453\n",
      "Epoch [6/50], Train Loss: 0.1028, Val Loss: 0.2391\n",
      "Epoch [7/50], Train Loss: 0.0989, Val Loss: 0.2333\n",
      "Epoch [8/50], Train Loss: 0.0967, Val Loss: 0.2277\n",
      "Epoch [9/50], Train Loss: 0.0931, Val Loss: 0.2225\n",
      "Epoch [10/50], Train Loss: 0.0906, Val Loss: 0.2174\n",
      "Epoch [11/50], Train Loss: 0.0888, Val Loss: 0.2125\n",
      "Epoch [12/50], Train Loss: 0.0857, Val Loss: 0.2079\n",
      "Epoch [13/50], Train Loss: 0.0847, Val Loss: 0.2036\n",
      "Epoch [14/50], Train Loss: 0.0816, Val Loss: 0.1995\n",
      "Epoch [15/50], Train Loss: 0.0796, Val Loss: 0.1955\n",
      "Epoch [16/50], Train Loss: 0.0767, Val Loss: 0.1917\n",
      "Epoch [17/50], Train Loss: 0.0755, Val Loss: 0.1882\n",
      "Epoch [18/50], Train Loss: 0.0750, Val Loss: 0.1848\n",
      "Epoch [19/50], Train Loss: 0.0710, Val Loss: 0.1815\n",
      "Epoch [20/50], Train Loss: 0.0702, Val Loss: 0.1784\n",
      "Epoch [21/50], Train Loss: 0.0691, Val Loss: 0.1755\n",
      "Epoch [22/50], Train Loss: 0.0679, Val Loss: 0.1726\n",
      "Epoch [23/50], Train Loss: 0.0662, Val Loss: 0.1698\n",
      "Epoch [24/50], Train Loss: 0.0648, Val Loss: 0.1673\n",
      "Epoch [25/50], Train Loss: 0.0637, Val Loss: 0.1648\n",
      "Epoch [26/50], Train Loss: 0.0627, Val Loss: 0.1624\n",
      "Epoch [27/50], Train Loss: 0.0625, Val Loss: 0.1601\n",
      "Epoch [28/50], Train Loss: 0.0604, Val Loss: 0.1580\n",
      "Epoch [29/50], Train Loss: 0.0587, Val Loss: 0.1558\n",
      "Epoch [30/50], Train Loss: 0.0591, Val Loss: 0.1539\n",
      "Epoch [31/50], Train Loss: 0.0575, Val Loss: 0.1521\n",
      "Epoch [32/50], Train Loss: 0.0578, Val Loss: 0.1503\n",
      "Epoch [33/50], Train Loss: 0.0565, Val Loss: 0.1486\n",
      "Epoch [34/50], Train Loss: 0.0557, Val Loss: 0.1470\n",
      "Epoch [35/50], Train Loss: 0.0554, Val Loss: 0.1453\n",
      "Epoch [36/50], Train Loss: 0.0547, Val Loss: 0.1438\n",
      "Epoch [37/50], Train Loss: 0.0532, Val Loss: 0.1423\n",
      "Epoch [38/50], Train Loss: 0.0519, Val Loss: 0.1410\n",
      "Epoch [39/50], Train Loss: 0.0520, Val Loss: 0.1396\n",
      "Epoch [40/50], Train Loss: 0.0513, Val Loss: 0.1384\n",
      "Epoch [41/50], Train Loss: 0.0508, Val Loss: 0.1373\n",
      "Epoch [42/50], Train Loss: 0.0510, Val Loss: 0.1361\n",
      "Epoch [43/50], Train Loss: 0.0491, Val Loss: 0.1350\n",
      "Epoch [44/50], Train Loss: 0.0488, Val Loss: 0.1340\n",
      "Epoch [45/50], Train Loss: 0.0484, Val Loss: 0.1330\n",
      "Epoch [46/50], Train Loss: 0.0484, Val Loss: 0.1320\n",
      "Epoch [47/50], Train Loss: 0.0483, Val Loss: 0.1311\n",
      "Epoch [48/50], Train Loss: 0.0481, Val Loss: 0.1303\n",
      "Epoch [49/50], Train Loss: 0.0478, Val Loss: 0.1295\n",
      "Epoch [50/50], Train Loss: 0.0479, Val Loss: 0.1287\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=16, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1204, Val Loss: 0.2748\n",
      "Epoch [2/50], Train Loss: 0.1171, Val Loss: 0.2689\n",
      "Epoch [3/50], Train Loss: 0.1146, Val Loss: 0.2633\n",
      "Epoch [4/50], Train Loss: 0.1094, Val Loss: 0.2581\n",
      "Epoch [5/50], Train Loss: 0.1100, Val Loss: 0.2530\n",
      "Epoch [6/50], Train Loss: 0.1050, Val Loss: 0.2482\n",
      "Epoch [7/50], Train Loss: 0.1050, Val Loss: 0.2434\n",
      "Epoch [8/50], Train Loss: 0.0985, Val Loss: 0.2390\n",
      "Epoch [9/50], Train Loss: 0.0941, Val Loss: 0.2349\n",
      "Epoch [10/50], Train Loss: 0.0941, Val Loss: 0.2311\n",
      "Epoch [11/50], Train Loss: 0.0922, Val Loss: 0.2273\n",
      "Epoch [12/50], Train Loss: 0.0904, Val Loss: 0.2236\n",
      "Epoch [13/50], Train Loss: 0.0896, Val Loss: 0.2201\n",
      "Epoch [14/50], Train Loss: 0.0852, Val Loss: 0.2167\n",
      "Epoch [15/50], Train Loss: 0.0848, Val Loss: 0.2136\n",
      "Epoch [16/50], Train Loss: 0.0811, Val Loss: 0.2107\n",
      "Epoch [17/50], Train Loss: 0.0810, Val Loss: 0.2078\n",
      "Epoch [18/50], Train Loss: 0.0807, Val Loss: 0.2050\n",
      "Epoch [19/50], Train Loss: 0.0782, Val Loss: 0.2023\n",
      "Epoch [20/50], Train Loss: 0.0776, Val Loss: 0.1998\n",
      "Epoch [21/50], Train Loss: 0.0776, Val Loss: 0.1972\n",
      "Epoch [22/50], Train Loss: 0.0772, Val Loss: 0.1948\n",
      "Epoch [23/50], Train Loss: 0.0733, Val Loss: 0.1925\n",
      "Epoch [24/50], Train Loss: 0.0733, Val Loss: 0.1902\n",
      "Epoch [25/50], Train Loss: 0.0701, Val Loss: 0.1882\n",
      "Epoch [26/50], Train Loss: 0.0715, Val Loss: 0.1860\n",
      "Epoch [27/50], Train Loss: 0.0703, Val Loss: 0.1842\n",
      "Epoch [28/50], Train Loss: 0.0710, Val Loss: 0.1822\n",
      "Epoch [29/50], Train Loss: 0.0675, Val Loss: 0.1804\n",
      "Epoch [30/50], Train Loss: 0.0689, Val Loss: 0.1787\n",
      "Epoch [31/50], Train Loss: 0.0678, Val Loss: 0.1770\n",
      "Epoch [32/50], Train Loss: 0.0663, Val Loss: 0.1753\n",
      "Epoch [33/50], Train Loss: 0.0641, Val Loss: 0.1738\n",
      "Epoch [34/50], Train Loss: 0.0669, Val Loss: 0.1722\n",
      "Epoch [35/50], Train Loss: 0.0646, Val Loss: 0.1707\n",
      "Epoch [36/50], Train Loss: 0.0654, Val Loss: 0.1693\n",
      "Epoch [37/50], Train Loss: 0.0637, Val Loss: 0.1680\n",
      "Epoch [38/50], Train Loss: 0.0639, Val Loss: 0.1667\n",
      "Epoch [39/50], Train Loss: 0.0629, Val Loss: 0.1653\n",
      "Epoch [40/50], Train Loss: 0.0623, Val Loss: 0.1641\n",
      "Epoch [41/50], Train Loss: 0.0611, Val Loss: 0.1630\n",
      "Epoch [42/50], Train Loss: 0.0630, Val Loss: 0.1618\n",
      "Epoch [43/50], Train Loss: 0.0600, Val Loss: 0.1607\n",
      "Epoch [44/50], Train Loss: 0.0617, Val Loss: 0.1595\n",
      "Epoch [45/50], Train Loss: 0.0592, Val Loss: 0.1584\n",
      "Epoch [46/50], Train Loss: 0.0603, Val Loss: 0.1573\n",
      "Epoch [47/50], Train Loss: 0.0588, Val Loss: 0.1564\n",
      "Epoch [48/50], Train Loss: 0.0582, Val Loss: 0.1554\n",
      "Epoch [49/50], Train Loss: 0.0590, Val Loss: 0.1544\n",
      "Epoch [50/50], Train Loss: 0.0594, Val Loss: 0.1534\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=32, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1788, Val Loss: 0.4127\n",
      "Epoch [2/50], Train Loss: 0.1720, Val Loss: 0.4013\n",
      "Epoch [3/50], Train Loss: 0.1655, Val Loss: 0.3905\n",
      "Epoch [4/50], Train Loss: 0.1594, Val Loss: 0.3801\n",
      "Epoch [5/50], Train Loss: 0.1535, Val Loss: 0.3702\n",
      "Epoch [6/50], Train Loss: 0.1479, Val Loss: 0.3606\n",
      "Epoch [7/50], Train Loss: 0.1426, Val Loss: 0.3514\n",
      "Epoch [8/50], Train Loss: 0.1376, Val Loss: 0.3426\n",
      "Epoch [9/50], Train Loss: 0.1327, Val Loss: 0.3341\n",
      "Epoch [10/50], Train Loss: 0.1281, Val Loss: 0.3260\n",
      "Epoch [11/50], Train Loss: 0.1238, Val Loss: 0.3181\n",
      "Epoch [12/50], Train Loss: 0.1196, Val Loss: 0.3106\n",
      "Epoch [13/50], Train Loss: 0.1156, Val Loss: 0.3033\n",
      "Epoch [14/50], Train Loss: 0.1118, Val Loss: 0.2963\n",
      "Epoch [15/50], Train Loss: 0.1081, Val Loss: 0.2895\n",
      "Epoch [16/50], Train Loss: 0.1046, Val Loss: 0.2829\n",
      "Epoch [17/50], Train Loss: 0.1013, Val Loss: 0.2766\n",
      "Epoch [18/50], Train Loss: 0.0981, Val Loss: 0.2705\n",
      "Epoch [19/50], Train Loss: 0.0951, Val Loss: 0.2646\n",
      "Epoch [20/50], Train Loss: 0.0922, Val Loss: 0.2590\n",
      "Epoch [21/50], Train Loss: 0.0894, Val Loss: 0.2535\n",
      "Epoch [22/50], Train Loss: 0.0867, Val Loss: 0.2481\n",
      "Epoch [23/50], Train Loss: 0.0841, Val Loss: 0.2430\n",
      "Epoch [24/50], Train Loss: 0.0817, Val Loss: 0.2380\n",
      "Epoch [25/50], Train Loss: 0.0794, Val Loss: 0.2332\n",
      "Epoch [26/50], Train Loss: 0.0771, Val Loss: 0.2286\n",
      "Epoch [27/50], Train Loss: 0.0750, Val Loss: 0.2240\n",
      "Epoch [28/50], Train Loss: 0.0729, Val Loss: 0.2196\n",
      "Epoch [29/50], Train Loss: 0.0709, Val Loss: 0.2154\n",
      "Epoch [30/50], Train Loss: 0.0690, Val Loss: 0.2113\n",
      "Epoch [31/50], Train Loss: 0.0672, Val Loss: 0.2073\n",
      "Epoch [32/50], Train Loss: 0.0655, Val Loss: 0.2035\n",
      "Epoch [33/50], Train Loss: 0.0638, Val Loss: 0.1998\n",
      "Epoch [34/50], Train Loss: 0.0622, Val Loss: 0.1961\n",
      "Epoch [35/50], Train Loss: 0.0607, Val Loss: 0.1926\n",
      "Epoch [36/50], Train Loss: 0.0592, Val Loss: 0.1892\n",
      "Epoch [37/50], Train Loss: 0.0578, Val Loss: 0.1859\n",
      "Epoch [38/50], Train Loss: 0.0565, Val Loss: 0.1827\n",
      "Epoch [39/50], Train Loss: 0.0552, Val Loss: 0.1796\n",
      "Epoch [40/50], Train Loss: 0.0540, Val Loss: 0.1765\n",
      "Epoch [41/50], Train Loss: 0.0528, Val Loss: 0.1736\n",
      "Epoch [42/50], Train Loss: 0.0516, Val Loss: 0.1708\n",
      "Epoch [43/50], Train Loss: 0.0506, Val Loss: 0.1680\n",
      "Epoch [44/50], Train Loss: 0.0495, Val Loss: 0.1653\n",
      "Epoch [45/50], Train Loss: 0.0485, Val Loss: 0.1627\n",
      "Epoch [46/50], Train Loss: 0.0476, Val Loss: 0.1602\n",
      "Epoch [47/50], Train Loss: 0.0466, Val Loss: 0.1577\n",
      "Epoch [48/50], Train Loss: 0.0457, Val Loss: 0.1553\n",
      "Epoch [49/50], Train Loss: 0.0449, Val Loss: 0.1530\n",
      "Epoch [50/50], Train Loss: 0.0441, Val Loss: 0.1507\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=32, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1838, Val Loss: 0.4521\n",
      "Epoch [2/50], Train Loss: 0.1759, Val Loss: 0.4404\n",
      "Epoch [3/50], Train Loss: 0.1694, Val Loss: 0.4292\n",
      "Epoch [4/50], Train Loss: 0.1639, Val Loss: 0.4183\n",
      "Epoch [5/50], Train Loss: 0.1595, Val Loss: 0.4078\n",
      "Epoch [6/50], Train Loss: 0.1534, Val Loss: 0.3977\n",
      "Epoch [7/50], Train Loss: 0.1478, Val Loss: 0.3881\n",
      "Epoch [8/50], Train Loss: 0.1436, Val Loss: 0.3788\n",
      "Epoch [9/50], Train Loss: 0.1388, Val Loss: 0.3699\n",
      "Epoch [10/50], Train Loss: 0.1339, Val Loss: 0.3613\n",
      "Epoch [11/50], Train Loss: 0.1300, Val Loss: 0.3529\n",
      "Epoch [12/50], Train Loss: 0.1261, Val Loss: 0.3449\n",
      "Epoch [13/50], Train Loss: 0.1213, Val Loss: 0.3371\n",
      "Epoch [14/50], Train Loss: 0.1183, Val Loss: 0.3296\n",
      "Epoch [15/50], Train Loss: 0.1149, Val Loss: 0.3224\n",
      "Epoch [16/50], Train Loss: 0.1112, Val Loss: 0.3153\n",
      "Epoch [17/50], Train Loss: 0.1079, Val Loss: 0.3085\n",
      "Epoch [18/50], Train Loss: 0.1050, Val Loss: 0.3020\n",
      "Epoch [19/50], Train Loss: 0.1013, Val Loss: 0.2957\n",
      "Epoch [20/50], Train Loss: 0.0989, Val Loss: 0.2896\n",
      "Epoch [21/50], Train Loss: 0.0964, Val Loss: 0.2836\n",
      "Epoch [22/50], Train Loss: 0.0929, Val Loss: 0.2778\n",
      "Epoch [23/50], Train Loss: 0.0908, Val Loss: 0.2723\n",
      "Epoch [24/50], Train Loss: 0.0891, Val Loss: 0.2669\n",
      "Epoch [25/50], Train Loss: 0.0864, Val Loss: 0.2616\n",
      "Epoch [26/50], Train Loss: 0.0838, Val Loss: 0.2565\n",
      "Epoch [27/50], Train Loss: 0.0813, Val Loss: 0.2517\n",
      "Epoch [28/50], Train Loss: 0.0802, Val Loss: 0.2469\n",
      "Epoch [29/50], Train Loss: 0.0771, Val Loss: 0.2422\n",
      "Epoch [30/50], Train Loss: 0.0753, Val Loss: 0.2377\n",
      "Epoch [31/50], Train Loss: 0.0741, Val Loss: 0.2334\n",
      "Epoch [32/50], Train Loss: 0.0724, Val Loss: 0.2291\n",
      "Epoch [33/50], Train Loss: 0.0705, Val Loss: 0.2251\n",
      "Epoch [34/50], Train Loss: 0.0693, Val Loss: 0.2211\n",
      "Epoch [35/50], Train Loss: 0.0678, Val Loss: 0.2172\n",
      "Epoch [36/50], Train Loss: 0.0663, Val Loss: 0.2135\n",
      "Epoch [37/50], Train Loss: 0.0640, Val Loss: 0.2098\n",
      "Epoch [38/50], Train Loss: 0.0629, Val Loss: 0.2063\n",
      "Epoch [39/50], Train Loss: 0.0624, Val Loss: 0.2029\n",
      "Epoch [40/50], Train Loss: 0.0600, Val Loss: 0.1996\n",
      "Epoch [41/50], Train Loss: 0.0599, Val Loss: 0.1963\n",
      "Epoch [42/50], Train Loss: 0.0582, Val Loss: 0.1931\n",
      "Epoch [43/50], Train Loss: 0.0567, Val Loss: 0.1901\n",
      "Epoch [44/50], Train Loss: 0.0564, Val Loss: 0.1871\n",
      "Epoch [45/50], Train Loss: 0.0549, Val Loss: 0.1842\n",
      "Epoch [46/50], Train Loss: 0.0532, Val Loss: 0.1814\n",
      "Epoch [47/50], Train Loss: 0.0530, Val Loss: 0.1787\n",
      "Epoch [48/50], Train Loss: 0.0526, Val Loss: 0.1761\n",
      "Epoch [49/50], Train Loss: 0.0514, Val Loss: 0.1735\n",
      "Epoch [50/50], Train Loss: 0.0511, Val Loss: 0.1710\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=32, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1074, Val Loss: 0.3124\n",
      "Epoch [2/50], Train Loss: 0.1042, Val Loss: 0.3044\n",
      "Epoch [3/50], Train Loss: 0.1007, Val Loss: 0.2968\n",
      "Epoch [4/50], Train Loss: 0.0996, Val Loss: 0.2896\n",
      "Epoch [5/50], Train Loss: 0.0944, Val Loss: 0.2825\n",
      "Epoch [6/50], Train Loss: 0.0912, Val Loss: 0.2756\n",
      "Epoch [7/50], Train Loss: 0.0905, Val Loss: 0.2690\n",
      "Epoch [8/50], Train Loss: 0.0869, Val Loss: 0.2627\n",
      "Epoch [9/50], Train Loss: 0.0847, Val Loss: 0.2565\n",
      "Epoch [10/50], Train Loss: 0.0818, Val Loss: 0.2506\n",
      "Epoch [11/50], Train Loss: 0.0805, Val Loss: 0.2448\n",
      "Epoch [12/50], Train Loss: 0.0779, Val Loss: 0.2392\n",
      "Epoch [13/50], Train Loss: 0.0753, Val Loss: 0.2339\n",
      "Epoch [14/50], Train Loss: 0.0738, Val Loss: 0.2287\n",
      "Epoch [15/50], Train Loss: 0.0726, Val Loss: 0.2237\n",
      "Epoch [16/50], Train Loss: 0.0691, Val Loss: 0.2189\n",
      "Epoch [17/50], Train Loss: 0.0682, Val Loss: 0.2142\n",
      "Epoch [18/50], Train Loss: 0.0665, Val Loss: 0.2098\n",
      "Epoch [19/50], Train Loss: 0.0656, Val Loss: 0.2054\n",
      "Epoch [20/50], Train Loss: 0.0627, Val Loss: 0.2012\n",
      "Epoch [21/50], Train Loss: 0.0633, Val Loss: 0.1971\n",
      "Epoch [22/50], Train Loss: 0.0606, Val Loss: 0.1931\n",
      "Epoch [23/50], Train Loss: 0.0598, Val Loss: 0.1893\n",
      "Epoch [24/50], Train Loss: 0.0594, Val Loss: 0.1856\n",
      "Epoch [25/50], Train Loss: 0.0574, Val Loss: 0.1820\n",
      "Epoch [26/50], Train Loss: 0.0568, Val Loss: 0.1786\n",
      "Epoch [27/50], Train Loss: 0.0558, Val Loss: 0.1753\n",
      "Epoch [28/50], Train Loss: 0.0551, Val Loss: 0.1720\n",
      "Epoch [29/50], Train Loss: 0.0542, Val Loss: 0.1689\n",
      "Epoch [30/50], Train Loss: 0.0517, Val Loss: 0.1659\n",
      "Epoch [31/50], Train Loss: 0.0528, Val Loss: 0.1630\n",
      "Epoch [32/50], Train Loss: 0.0518, Val Loss: 0.1602\n",
      "Epoch [33/50], Train Loss: 0.0510, Val Loss: 0.1575\n",
      "Epoch [34/50], Train Loss: 0.0508, Val Loss: 0.1548\n",
      "Epoch [35/50], Train Loss: 0.0509, Val Loss: 0.1522\n",
      "Epoch [36/50], Train Loss: 0.0487, Val Loss: 0.1497\n",
      "Epoch [37/50], Train Loss: 0.0485, Val Loss: 0.1473\n",
      "Epoch [38/50], Train Loss: 0.0478, Val Loss: 0.1450\n",
      "Epoch [39/50], Train Loss: 0.0456, Val Loss: 0.1428\n",
      "Epoch [40/50], Train Loss: 0.0462, Val Loss: 0.1406\n",
      "Epoch [41/50], Train Loss: 0.0469, Val Loss: 0.1385\n",
      "Epoch [42/50], Train Loss: 0.0457, Val Loss: 0.1364\n",
      "Epoch [43/50], Train Loss: 0.0455, Val Loss: 0.1345\n",
      "Epoch [44/50], Train Loss: 0.0445, Val Loss: 0.1325\n",
      "Epoch [45/50], Train Loss: 0.0454, Val Loss: 0.1306\n",
      "Epoch [46/50], Train Loss: 0.0426, Val Loss: 0.1288\n",
      "Epoch [47/50], Train Loss: 0.0434, Val Loss: 0.1270\n",
      "Epoch [48/50], Train Loss: 0.0426, Val Loss: 0.1254\n",
      "Epoch [49/50], Train Loss: 0.0412, Val Loss: 0.1237\n",
      "Epoch [50/50], Train Loss: 0.0427, Val Loss: 0.1222\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=32, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1075, Val Loss: 0.3209\n",
      "Epoch [2/50], Train Loss: 0.1026, Val Loss: 0.3106\n",
      "Epoch [3/50], Train Loss: 0.0980, Val Loss: 0.3008\n",
      "Epoch [4/50], Train Loss: 0.0937, Val Loss: 0.2915\n",
      "Epoch [5/50], Train Loss: 0.0897, Val Loss: 0.2827\n",
      "Epoch [6/50], Train Loss: 0.0859, Val Loss: 0.2742\n",
      "Epoch [7/50], Train Loss: 0.0824, Val Loss: 0.2661\n",
      "Epoch [8/50], Train Loss: 0.0791, Val Loss: 0.2585\n",
      "Epoch [9/50], Train Loss: 0.0760, Val Loss: 0.2511\n",
      "Epoch [10/50], Train Loss: 0.0731, Val Loss: 0.2441\n",
      "Epoch [11/50], Train Loss: 0.0704, Val Loss: 0.2374\n",
      "Epoch [12/50], Train Loss: 0.0678, Val Loss: 0.2310\n",
      "Epoch [13/50], Train Loss: 0.0655, Val Loss: 0.2249\n",
      "Epoch [14/50], Train Loss: 0.0633, Val Loss: 0.2191\n",
      "Epoch [15/50], Train Loss: 0.0612, Val Loss: 0.2135\n",
      "Epoch [16/50], Train Loss: 0.0592, Val Loss: 0.2082\n",
      "Epoch [17/50], Train Loss: 0.0574, Val Loss: 0.2031\n",
      "Epoch [18/50], Train Loss: 0.0557, Val Loss: 0.1982\n",
      "Epoch [19/50], Train Loss: 0.0541, Val Loss: 0.1935\n",
      "Epoch [20/50], Train Loss: 0.0526, Val Loss: 0.1891\n",
      "Epoch [21/50], Train Loss: 0.0512, Val Loss: 0.1848\n",
      "Epoch [22/50], Train Loss: 0.0499, Val Loss: 0.1807\n",
      "Epoch [23/50], Train Loss: 0.0487, Val Loss: 0.1768\n",
      "Epoch [24/50], Train Loss: 0.0476, Val Loss: 0.1731\n",
      "Epoch [25/50], Train Loss: 0.0465, Val Loss: 0.1695\n",
      "Epoch [26/50], Train Loss: 0.0455, Val Loss: 0.1661\n",
      "Epoch [27/50], Train Loss: 0.0446, Val Loss: 0.1628\n",
      "Epoch [28/50], Train Loss: 0.0437, Val Loss: 0.1597\n",
      "Epoch [29/50], Train Loss: 0.0429, Val Loss: 0.1566\n",
      "Epoch [30/50], Train Loss: 0.0422, Val Loss: 0.1538\n",
      "Epoch [31/50], Train Loss: 0.0415, Val Loss: 0.1510\n",
      "Epoch [32/50], Train Loss: 0.0408, Val Loss: 0.1484\n",
      "Epoch [33/50], Train Loss: 0.0402, Val Loss: 0.1458\n",
      "Epoch [34/50], Train Loss: 0.0396, Val Loss: 0.1434\n",
      "Epoch [35/50], Train Loss: 0.0391, Val Loss: 0.1411\n",
      "Epoch [36/50], Train Loss: 0.0386, Val Loss: 0.1389\n",
      "Epoch [37/50], Train Loss: 0.0381, Val Loss: 0.1367\n",
      "Epoch [38/50], Train Loss: 0.0377, Val Loss: 0.1347\n",
      "Epoch [39/50], Train Loss: 0.0373, Val Loss: 0.1327\n",
      "Epoch [40/50], Train Loss: 0.0369, Val Loss: 0.1308\n",
      "Epoch [41/50], Train Loss: 0.0366, Val Loss: 0.1290\n",
      "Epoch [42/50], Train Loss: 0.0362, Val Loss: 0.1273\n",
      "Epoch [43/50], Train Loss: 0.0359, Val Loss: 0.1256\n",
      "Epoch [44/50], Train Loss: 0.0357, Val Loss: 0.1241\n",
      "Epoch [45/50], Train Loss: 0.0354, Val Loss: 0.1225\n",
      "Epoch [46/50], Train Loss: 0.0351, Val Loss: 0.1211\n",
      "Epoch [47/50], Train Loss: 0.0349, Val Loss: 0.1197\n",
      "Epoch [48/50], Train Loss: 0.0347, Val Loss: 0.1183\n",
      "Epoch [49/50], Train Loss: 0.0345, Val Loss: 0.1170\n",
      "Epoch [50/50], Train Loss: 0.0343, Val Loss: 0.1158\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=32, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1580, Val Loss: 0.4233\n",
      "Epoch [2/50], Train Loss: 0.1505, Val Loss: 0.4112\n",
      "Epoch [3/50], Train Loss: 0.1455, Val Loss: 0.3996\n",
      "Epoch [4/50], Train Loss: 0.1388, Val Loss: 0.3886\n",
      "Epoch [5/50], Train Loss: 0.1335, Val Loss: 0.3780\n",
      "Epoch [6/50], Train Loss: 0.1282, Val Loss: 0.3678\n",
      "Epoch [7/50], Train Loss: 0.1240, Val Loss: 0.3581\n",
      "Epoch [8/50], Train Loss: 0.1187, Val Loss: 0.3487\n",
      "Epoch [9/50], Train Loss: 0.1150, Val Loss: 0.3397\n",
      "Epoch [10/50], Train Loss: 0.1105, Val Loss: 0.3310\n",
      "Epoch [11/50], Train Loss: 0.1063, Val Loss: 0.3227\n",
      "Epoch [12/50], Train Loss: 0.1024, Val Loss: 0.3148\n",
      "Epoch [13/50], Train Loss: 0.0991, Val Loss: 0.3071\n",
      "Epoch [14/50], Train Loss: 0.0957, Val Loss: 0.2996\n",
      "Epoch [15/50], Train Loss: 0.0929, Val Loss: 0.2925\n",
      "Epoch [16/50], Train Loss: 0.0895, Val Loss: 0.2856\n",
      "Epoch [17/50], Train Loss: 0.0872, Val Loss: 0.2790\n",
      "Epoch [18/50], Train Loss: 0.0843, Val Loss: 0.2727\n",
      "Epoch [19/50], Train Loss: 0.0819, Val Loss: 0.2665\n",
      "Epoch [20/50], Train Loss: 0.0792, Val Loss: 0.2606\n",
      "Epoch [21/50], Train Loss: 0.0768, Val Loss: 0.2549\n",
      "Epoch [22/50], Train Loss: 0.0751, Val Loss: 0.2494\n",
      "Epoch [23/50], Train Loss: 0.0727, Val Loss: 0.2440\n",
      "Epoch [24/50], Train Loss: 0.0707, Val Loss: 0.2389\n",
      "Epoch [25/50], Train Loss: 0.0692, Val Loss: 0.2340\n",
      "Epoch [26/50], Train Loss: 0.0677, Val Loss: 0.2292\n",
      "Epoch [27/50], Train Loss: 0.0659, Val Loss: 0.2246\n",
      "Epoch [28/50], Train Loss: 0.0636, Val Loss: 0.2201\n",
      "Epoch [29/50], Train Loss: 0.0627, Val Loss: 0.2158\n",
      "Epoch [30/50], Train Loss: 0.0611, Val Loss: 0.2117\n",
      "Epoch [31/50], Train Loss: 0.0596, Val Loss: 0.2077\n",
      "Epoch [32/50], Train Loss: 0.0576, Val Loss: 0.2038\n",
      "Epoch [33/50], Train Loss: 0.0573, Val Loss: 0.2001\n",
      "Epoch [34/50], Train Loss: 0.0558, Val Loss: 0.1965\n",
      "Epoch [35/50], Train Loss: 0.0555, Val Loss: 0.1930\n",
      "Epoch [36/50], Train Loss: 0.0538, Val Loss: 0.1896\n",
      "Epoch [37/50], Train Loss: 0.0527, Val Loss: 0.1864\n",
      "Epoch [38/50], Train Loss: 0.0524, Val Loss: 0.1832\n",
      "Epoch [39/50], Train Loss: 0.0507, Val Loss: 0.1801\n",
      "Epoch [40/50], Train Loss: 0.0501, Val Loss: 0.1772\n",
      "Epoch [41/50], Train Loss: 0.0492, Val Loss: 0.1744\n",
      "Epoch [42/50], Train Loss: 0.0489, Val Loss: 0.1716\n",
      "Epoch [43/50], Train Loss: 0.0478, Val Loss: 0.1690\n",
      "Epoch [44/50], Train Loss: 0.0478, Val Loss: 0.1664\n",
      "Epoch [45/50], Train Loss: 0.0465, Val Loss: 0.1639\n",
      "Epoch [46/50], Train Loss: 0.0462, Val Loss: 0.1615\n",
      "Epoch [47/50], Train Loss: 0.0457, Val Loss: 0.1592\n",
      "Epoch [48/50], Train Loss: 0.0453, Val Loss: 0.1569\n",
      "Epoch [49/50], Train Loss: 0.0449, Val Loss: 0.1548\n",
      "Epoch [50/50], Train Loss: 0.0442, Val Loss: 0.1527\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=32, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1875, Val Loss: 0.4362\n",
      "Epoch [2/50], Train Loss: 0.1786, Val Loss: 0.4210\n",
      "Epoch [3/50], Train Loss: 0.1719, Val Loss: 0.4064\n",
      "Epoch [4/50], Train Loss: 0.1613, Val Loss: 0.3927\n",
      "Epoch [5/50], Train Loss: 0.1558, Val Loss: 0.3797\n",
      "Epoch [6/50], Train Loss: 0.1487, Val Loss: 0.3670\n",
      "Epoch [7/50], Train Loss: 0.1421, Val Loss: 0.3552\n",
      "Epoch [8/50], Train Loss: 0.1346, Val Loss: 0.3440\n",
      "Epoch [9/50], Train Loss: 0.1298, Val Loss: 0.3333\n",
      "Epoch [10/50], Train Loss: 0.1261, Val Loss: 0.3229\n",
      "Epoch [11/50], Train Loss: 0.1219, Val Loss: 0.3132\n",
      "Epoch [12/50], Train Loss: 0.1160, Val Loss: 0.3037\n",
      "Epoch [13/50], Train Loss: 0.1110, Val Loss: 0.2948\n",
      "Epoch [14/50], Train Loss: 0.1052, Val Loss: 0.2863\n",
      "Epoch [15/50], Train Loss: 0.1021, Val Loss: 0.2781\n",
      "Epoch [16/50], Train Loss: 0.0986, Val Loss: 0.2706\n",
      "Epoch [17/50], Train Loss: 0.0964, Val Loss: 0.2631\n",
      "Epoch [18/50], Train Loss: 0.0925, Val Loss: 0.2561\n",
      "Epoch [19/50], Train Loss: 0.0916, Val Loss: 0.2494\n",
      "Epoch [20/50], Train Loss: 0.0886, Val Loss: 0.2429\n",
      "Epoch [21/50], Train Loss: 0.0854, Val Loss: 0.2367\n",
      "Epoch [22/50], Train Loss: 0.0822, Val Loss: 0.2307\n",
      "Epoch [23/50], Train Loss: 0.0801, Val Loss: 0.2251\n",
      "Epoch [24/50], Train Loss: 0.0766, Val Loss: 0.2197\n",
      "Epoch [25/50], Train Loss: 0.0757, Val Loss: 0.2146\n",
      "Epoch [26/50], Train Loss: 0.0737, Val Loss: 0.2098\n",
      "Epoch [27/50], Train Loss: 0.0730, Val Loss: 0.2051\n",
      "Epoch [28/50], Train Loss: 0.0693, Val Loss: 0.2006\n",
      "Epoch [29/50], Train Loss: 0.0677, Val Loss: 0.1963\n",
      "Epoch [30/50], Train Loss: 0.0694, Val Loss: 0.1922\n",
      "Epoch [31/50], Train Loss: 0.0669, Val Loss: 0.1882\n",
      "Epoch [32/50], Train Loss: 0.0659, Val Loss: 0.1845\n",
      "Epoch [33/50], Train Loss: 0.0628, Val Loss: 0.1809\n",
      "Epoch [34/50], Train Loss: 0.0623, Val Loss: 0.1776\n",
      "Epoch [35/50], Train Loss: 0.0603, Val Loss: 0.1744\n",
      "Epoch [36/50], Train Loss: 0.0593, Val Loss: 0.1712\n",
      "Epoch [37/50], Train Loss: 0.0608, Val Loss: 0.1683\n",
      "Epoch [38/50], Train Loss: 0.0592, Val Loss: 0.1654\n",
      "Epoch [39/50], Train Loss: 0.0590, Val Loss: 0.1627\n",
      "Epoch [40/50], Train Loss: 0.0584, Val Loss: 0.1600\n",
      "Epoch [41/50], Train Loss: 0.0577, Val Loss: 0.1574\n",
      "Epoch [42/50], Train Loss: 0.0574, Val Loss: 0.1550\n",
      "Epoch [43/50], Train Loss: 0.0555, Val Loss: 0.1527\n",
      "Epoch [44/50], Train Loss: 0.0553, Val Loss: 0.1505\n",
      "Epoch [45/50], Train Loss: 0.0548, Val Loss: 0.1483\n",
      "Epoch [46/50], Train Loss: 0.0549, Val Loss: 0.1462\n",
      "Epoch [47/50], Train Loss: 0.0540, Val Loss: 0.1444\n",
      "Epoch [48/50], Train Loss: 0.0550, Val Loss: 0.1426\n",
      "Epoch [49/50], Train Loss: 0.0540, Val Loss: 0.1408\n",
      "Epoch [50/50], Train Loss: 0.0523, Val Loss: 0.1391\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=32, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1298, Val Loss: 0.3524\n",
      "Epoch [2/50], Train Loss: 0.1245, Val Loss: 0.3429\n",
      "Epoch [3/50], Train Loss: 0.1196, Val Loss: 0.3339\n",
      "Epoch [4/50], Train Loss: 0.1149, Val Loss: 0.3254\n",
      "Epoch [5/50], Train Loss: 0.1105, Val Loss: 0.3172\n",
      "Epoch [6/50], Train Loss: 0.1064, Val Loss: 0.3094\n",
      "Epoch [7/50], Train Loss: 0.1026, Val Loss: 0.3020\n",
      "Epoch [8/50], Train Loss: 0.0989, Val Loss: 0.2949\n",
      "Epoch [9/50], Train Loss: 0.0955, Val Loss: 0.2882\n",
      "Epoch [10/50], Train Loss: 0.0922, Val Loss: 0.2817\n",
      "Epoch [11/50], Train Loss: 0.0892, Val Loss: 0.2754\n",
      "Epoch [12/50], Train Loss: 0.0863, Val Loss: 0.2695\n",
      "Epoch [13/50], Train Loss: 0.0836, Val Loss: 0.2638\n",
      "Epoch [14/50], Train Loss: 0.0810, Val Loss: 0.2583\n",
      "Epoch [15/50], Train Loss: 0.0786, Val Loss: 0.2530\n",
      "Epoch [16/50], Train Loss: 0.0762, Val Loss: 0.2479\n",
      "Epoch [17/50], Train Loss: 0.0741, Val Loss: 0.2430\n",
      "Epoch [18/50], Train Loss: 0.0720, Val Loss: 0.2384\n",
      "Epoch [19/50], Train Loss: 0.0701, Val Loss: 0.2338\n",
      "Epoch [20/50], Train Loss: 0.0682, Val Loss: 0.2295\n",
      "Epoch [21/50], Train Loss: 0.0665, Val Loss: 0.2253\n",
      "Epoch [22/50], Train Loss: 0.0648, Val Loss: 0.2213\n",
      "Epoch [23/50], Train Loss: 0.0632, Val Loss: 0.2174\n",
      "Epoch [24/50], Train Loss: 0.0617, Val Loss: 0.2137\n",
      "Epoch [25/50], Train Loss: 0.0603, Val Loss: 0.2101\n",
      "Epoch [26/50], Train Loss: 0.0590, Val Loss: 0.2066\n",
      "Epoch [27/50], Train Loss: 0.0577, Val Loss: 0.2032\n",
      "Epoch [28/50], Train Loss: 0.0565, Val Loss: 0.2000\n",
      "Epoch [29/50], Train Loss: 0.0554, Val Loss: 0.1969\n",
      "Epoch [30/50], Train Loss: 0.0543, Val Loss: 0.1938\n",
      "Epoch [31/50], Train Loss: 0.0533, Val Loss: 0.1910\n",
      "Epoch [32/50], Train Loss: 0.0523, Val Loss: 0.1881\n",
      "Epoch [33/50], Train Loss: 0.0514, Val Loss: 0.1854\n",
      "Epoch [34/50], Train Loss: 0.0506, Val Loss: 0.1828\n",
      "Epoch [35/50], Train Loss: 0.0497, Val Loss: 0.1803\n",
      "Epoch [36/50], Train Loss: 0.0490, Val Loss: 0.1778\n",
      "Epoch [37/50], Train Loss: 0.0482, Val Loss: 0.1755\n",
      "Epoch [38/50], Train Loss: 0.0475, Val Loss: 0.1732\n",
      "Epoch [39/50], Train Loss: 0.0469, Val Loss: 0.1710\n",
      "Epoch [40/50], Train Loss: 0.0462, Val Loss: 0.1688\n",
      "Epoch [41/50], Train Loss: 0.0457, Val Loss: 0.1668\n",
      "Epoch [42/50], Train Loss: 0.0451, Val Loss: 0.1648\n",
      "Epoch [43/50], Train Loss: 0.0446, Val Loss: 0.1628\n",
      "Epoch [44/50], Train Loss: 0.0441, Val Loss: 0.1610\n",
      "Epoch [45/50], Train Loss: 0.0436, Val Loss: 0.1592\n",
      "Epoch [46/50], Train Loss: 0.0431, Val Loss: 0.1574\n",
      "Epoch [47/50], Train Loss: 0.0427, Val Loss: 0.1558\n",
      "Epoch [48/50], Train Loss: 0.0423, Val Loss: 0.1541\n",
      "Epoch [49/50], Train Loss: 0.0419, Val Loss: 0.1525\n",
      "Epoch [50/50], Train Loss: 0.0416, Val Loss: 0.1510\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=32, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1976, Val Loss: 0.4441\n",
      "Epoch [2/50], Train Loss: 0.1898, Val Loss: 0.4286\n",
      "Epoch [3/50], Train Loss: 0.1805, Val Loss: 0.4138\n",
      "Epoch [4/50], Train Loss: 0.1717, Val Loss: 0.3998\n",
      "Epoch [5/50], Train Loss: 0.1648, Val Loss: 0.3863\n",
      "Epoch [6/50], Train Loss: 0.1555, Val Loss: 0.3737\n",
      "Epoch [7/50], Train Loss: 0.1494, Val Loss: 0.3616\n",
      "Epoch [8/50], Train Loss: 0.1414, Val Loss: 0.3501\n",
      "Epoch [9/50], Train Loss: 0.1361, Val Loss: 0.3391\n",
      "Epoch [10/50], Train Loss: 0.1302, Val Loss: 0.3286\n",
      "Epoch [11/50], Train Loss: 0.1252, Val Loss: 0.3186\n",
      "Epoch [12/50], Train Loss: 0.1196, Val Loss: 0.3091\n",
      "Epoch [13/50], Train Loss: 0.1140, Val Loss: 0.2999\n",
      "Epoch [14/50], Train Loss: 0.1106, Val Loss: 0.2910\n",
      "Epoch [15/50], Train Loss: 0.1046, Val Loss: 0.2826\n",
      "Epoch [16/50], Train Loss: 0.0996, Val Loss: 0.2747\n",
      "Epoch [17/50], Train Loss: 0.0964, Val Loss: 0.2670\n",
      "Epoch [18/50], Train Loss: 0.0938, Val Loss: 0.2597\n",
      "Epoch [19/50], Train Loss: 0.0901, Val Loss: 0.2526\n",
      "Epoch [20/50], Train Loss: 0.0872, Val Loss: 0.2458\n",
      "Epoch [21/50], Train Loss: 0.0835, Val Loss: 0.2393\n",
      "Epoch [22/50], Train Loss: 0.0806, Val Loss: 0.2331\n",
      "Epoch [23/50], Train Loss: 0.0779, Val Loss: 0.2272\n",
      "Epoch [24/50], Train Loss: 0.0749, Val Loss: 0.2216\n",
      "Epoch [25/50], Train Loss: 0.0729, Val Loss: 0.2161\n",
      "Epoch [26/50], Train Loss: 0.0702, Val Loss: 0.2109\n",
      "Epoch [27/50], Train Loss: 0.0691, Val Loss: 0.2059\n",
      "Epoch [28/50], Train Loss: 0.0661, Val Loss: 0.2012\n",
      "Epoch [29/50], Train Loss: 0.0649, Val Loss: 0.1966\n",
      "Epoch [30/50], Train Loss: 0.0626, Val Loss: 0.1922\n",
      "Epoch [31/50], Train Loss: 0.0602, Val Loss: 0.1881\n",
      "Epoch [32/50], Train Loss: 0.0595, Val Loss: 0.1840\n",
      "Epoch [33/50], Train Loss: 0.0574, Val Loss: 0.1803\n",
      "Epoch [34/50], Train Loss: 0.0562, Val Loss: 0.1766\n",
      "Epoch [35/50], Train Loss: 0.0564, Val Loss: 0.1730\n",
      "Epoch [36/50], Train Loss: 0.0545, Val Loss: 0.1697\n",
      "Epoch [37/50], Train Loss: 0.0532, Val Loss: 0.1665\n",
      "Epoch [38/50], Train Loss: 0.0518, Val Loss: 0.1634\n",
      "Epoch [39/50], Train Loss: 0.0506, Val Loss: 0.1604\n",
      "Epoch [40/50], Train Loss: 0.0502, Val Loss: 0.1576\n",
      "Epoch [41/50], Train Loss: 0.0489, Val Loss: 0.1549\n",
      "Epoch [42/50], Train Loss: 0.0486, Val Loss: 0.1523\n",
      "Epoch [43/50], Train Loss: 0.0477, Val Loss: 0.1499\n",
      "Epoch [44/50], Train Loss: 0.0464, Val Loss: 0.1476\n",
      "Epoch [45/50], Train Loss: 0.0461, Val Loss: 0.1453\n",
      "Epoch [46/50], Train Loss: 0.0450, Val Loss: 0.1431\n",
      "Epoch [47/50], Train Loss: 0.0449, Val Loss: 0.1411\n",
      "Epoch [48/50], Train Loss: 0.0444, Val Loss: 0.1391\n",
      "Epoch [49/50], Train Loss: 0.0441, Val Loss: 0.1372\n",
      "Epoch [50/50], Train Loss: 0.0435, Val Loss: 0.1353\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=32, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1803, Val Loss: 0.4041\n",
      "Epoch [2/50], Train Loss: 0.1729, Val Loss: 0.3907\n",
      "Epoch [3/50], Train Loss: 0.1665, Val Loss: 0.3780\n",
      "Epoch [4/50], Train Loss: 0.1579, Val Loss: 0.3663\n",
      "Epoch [5/50], Train Loss: 0.1516, Val Loss: 0.3551\n",
      "Epoch [6/50], Train Loss: 0.1424, Val Loss: 0.3448\n",
      "Epoch [7/50], Train Loss: 0.1388, Val Loss: 0.3350\n",
      "Epoch [8/50], Train Loss: 0.1332, Val Loss: 0.3257\n",
      "Epoch [9/50], Train Loss: 0.1277, Val Loss: 0.3167\n",
      "Epoch [10/50], Train Loss: 0.1212, Val Loss: 0.3084\n",
      "Epoch [11/50], Train Loss: 0.1179, Val Loss: 0.3005\n",
      "Epoch [12/50], Train Loss: 0.1149, Val Loss: 0.2930\n",
      "Epoch [13/50], Train Loss: 0.1111, Val Loss: 0.2859\n",
      "Epoch [14/50], Train Loss: 0.1069, Val Loss: 0.2791\n",
      "Epoch [15/50], Train Loss: 0.1044, Val Loss: 0.2727\n",
      "Epoch [16/50], Train Loss: 0.1001, Val Loss: 0.2665\n",
      "Epoch [17/50], Train Loss: 0.0976, Val Loss: 0.2605\n",
      "Epoch [18/50], Train Loss: 0.0928, Val Loss: 0.2549\n",
      "Epoch [19/50], Train Loss: 0.0903, Val Loss: 0.2496\n",
      "Epoch [20/50], Train Loss: 0.0892, Val Loss: 0.2445\n",
      "Epoch [21/50], Train Loss: 0.0866, Val Loss: 0.2397\n",
      "Epoch [22/50], Train Loss: 0.0837, Val Loss: 0.2349\n",
      "Epoch [23/50], Train Loss: 0.0817, Val Loss: 0.2304\n",
      "Epoch [24/50], Train Loss: 0.0801, Val Loss: 0.2260\n",
      "Epoch [25/50], Train Loss: 0.0766, Val Loss: 0.2220\n",
      "Epoch [26/50], Train Loss: 0.0765, Val Loss: 0.2180\n",
      "Epoch [27/50], Train Loss: 0.0743, Val Loss: 0.2143\n",
      "Epoch [28/50], Train Loss: 0.0733, Val Loss: 0.2106\n",
      "Epoch [29/50], Train Loss: 0.0720, Val Loss: 0.2072\n",
      "Epoch [30/50], Train Loss: 0.0694, Val Loss: 0.2038\n",
      "Epoch [31/50], Train Loss: 0.0686, Val Loss: 0.2006\n",
      "Epoch [32/50], Train Loss: 0.0686, Val Loss: 0.1975\n",
      "Epoch [33/50], Train Loss: 0.0674, Val Loss: 0.1945\n",
      "Epoch [34/50], Train Loss: 0.0662, Val Loss: 0.1917\n",
      "Epoch [35/50], Train Loss: 0.0641, Val Loss: 0.1891\n",
      "Epoch [36/50], Train Loss: 0.0646, Val Loss: 0.1864\n",
      "Epoch [37/50], Train Loss: 0.0617, Val Loss: 0.1839\n",
      "Epoch [38/50], Train Loss: 0.0620, Val Loss: 0.1815\n",
      "Epoch [39/50], Train Loss: 0.0600, Val Loss: 0.1791\n",
      "Epoch [40/50], Train Loss: 0.0601, Val Loss: 0.1768\n",
      "Epoch [41/50], Train Loss: 0.0603, Val Loss: 0.1746\n",
      "Epoch [42/50], Train Loss: 0.0588, Val Loss: 0.1726\n",
      "Epoch [43/50], Train Loss: 0.0562, Val Loss: 0.1704\n",
      "Epoch [44/50], Train Loss: 0.0567, Val Loss: 0.1685\n",
      "Epoch [45/50], Train Loss: 0.0563, Val Loss: 0.1667\n",
      "Epoch [46/50], Train Loss: 0.0556, Val Loss: 0.1649\n",
      "Epoch [47/50], Train Loss: 0.0536, Val Loss: 0.1631\n",
      "Epoch [48/50], Train Loss: 0.0554, Val Loss: 0.1614\n",
      "Epoch [49/50], Train Loss: 0.0537, Val Loss: 0.1597\n",
      "Epoch [50/50], Train Loss: 0.0528, Val Loss: 0.1582\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=64, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1416, Val Loss: 0.3968\n",
      "Epoch [2/50], Train Loss: 0.1365, Val Loss: 0.3861\n",
      "Epoch [3/50], Train Loss: 0.1317, Val Loss: 0.3757\n",
      "Epoch [4/50], Train Loss: 0.1270, Val Loss: 0.3658\n",
      "Epoch [5/50], Train Loss: 0.1226, Val Loss: 0.3562\n",
      "Epoch [6/50], Train Loss: 0.1184, Val Loss: 0.3470\n",
      "Epoch [7/50], Train Loss: 0.1144, Val Loss: 0.3381\n",
      "Epoch [8/50], Train Loss: 0.1106, Val Loss: 0.3296\n",
      "Epoch [9/50], Train Loss: 0.1070, Val Loss: 0.3213\n",
      "Epoch [10/50], Train Loss: 0.1035, Val Loss: 0.3134\n",
      "Epoch [11/50], Train Loss: 0.1002, Val Loss: 0.3058\n",
      "Epoch [12/50], Train Loss: 0.0971, Val Loss: 0.2984\n",
      "Epoch [13/50], Train Loss: 0.0941, Val Loss: 0.2913\n",
      "Epoch [14/50], Train Loss: 0.0912, Val Loss: 0.2844\n",
      "Epoch [15/50], Train Loss: 0.0884, Val Loss: 0.2778\n",
      "Epoch [16/50], Train Loss: 0.0858, Val Loss: 0.2714\n",
      "Epoch [17/50], Train Loss: 0.0833, Val Loss: 0.2652\n",
      "Epoch [18/50], Train Loss: 0.0809, Val Loss: 0.2592\n",
      "Epoch [19/50], Train Loss: 0.0787, Val Loss: 0.2534\n",
      "Epoch [20/50], Train Loss: 0.0765, Val Loss: 0.2479\n",
      "Epoch [21/50], Train Loss: 0.0744, Val Loss: 0.2425\n",
      "Epoch [22/50], Train Loss: 0.0724, Val Loss: 0.2372\n",
      "Epoch [23/50], Train Loss: 0.0705, Val Loss: 0.2322\n",
      "Epoch [24/50], Train Loss: 0.0687, Val Loss: 0.2273\n",
      "Epoch [25/50], Train Loss: 0.0670, Val Loss: 0.2226\n",
      "Epoch [26/50], Train Loss: 0.0653, Val Loss: 0.2180\n",
      "Epoch [27/50], Train Loss: 0.0637, Val Loss: 0.2136\n",
      "Epoch [28/50], Train Loss: 0.0622, Val Loss: 0.2093\n",
      "Epoch [29/50], Train Loss: 0.0608, Val Loss: 0.2052\n",
      "Epoch [30/50], Train Loss: 0.0594, Val Loss: 0.2012\n",
      "Epoch [31/50], Train Loss: 0.0581, Val Loss: 0.1973\n",
      "Epoch [32/50], Train Loss: 0.0569, Val Loss: 0.1936\n",
      "Epoch [33/50], Train Loss: 0.0557, Val Loss: 0.1899\n",
      "Epoch [34/50], Train Loss: 0.0545, Val Loss: 0.1864\n",
      "Epoch [35/50], Train Loss: 0.0534, Val Loss: 0.1830\n",
      "Epoch [36/50], Train Loss: 0.0524, Val Loss: 0.1797\n",
      "Epoch [37/50], Train Loss: 0.0514, Val Loss: 0.1765\n",
      "Epoch [38/50], Train Loss: 0.0504, Val Loss: 0.1734\n",
      "Epoch [39/50], Train Loss: 0.0495, Val Loss: 0.1704\n",
      "Epoch [40/50], Train Loss: 0.0487, Val Loss: 0.1675\n",
      "Epoch [41/50], Train Loss: 0.0478, Val Loss: 0.1647\n",
      "Epoch [42/50], Train Loss: 0.0470, Val Loss: 0.1620\n",
      "Epoch [43/50], Train Loss: 0.0463, Val Loss: 0.1593\n",
      "Epoch [44/50], Train Loss: 0.0456, Val Loss: 0.1568\n",
      "Epoch [45/50], Train Loss: 0.0449, Val Loss: 0.1543\n",
      "Epoch [46/50], Train Loss: 0.0442, Val Loss: 0.1519\n",
      "Epoch [47/50], Train Loss: 0.0436, Val Loss: 0.1496\n",
      "Epoch [48/50], Train Loss: 0.0430, Val Loss: 0.1473\n",
      "Epoch [49/50], Train Loss: 0.0424, Val Loss: 0.1451\n",
      "Epoch [50/50], Train Loss: 0.0419, Val Loss: 0.1430\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=64, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1144, Val Loss: 0.3230\n",
      "Epoch [2/50], Train Loss: 0.1106, Val Loss: 0.3145\n",
      "Epoch [3/50], Train Loss: 0.1068, Val Loss: 0.3063\n",
      "Epoch [4/50], Train Loss: 0.1032, Val Loss: 0.2984\n",
      "Epoch [5/50], Train Loss: 0.0996, Val Loss: 0.2907\n",
      "Epoch [6/50], Train Loss: 0.0959, Val Loss: 0.2835\n",
      "Epoch [7/50], Train Loss: 0.0938, Val Loss: 0.2765\n",
      "Epoch [8/50], Train Loss: 0.0903, Val Loss: 0.2697\n",
      "Epoch [9/50], Train Loss: 0.0876, Val Loss: 0.2632\n",
      "Epoch [10/50], Train Loss: 0.0853, Val Loss: 0.2570\n",
      "Epoch [11/50], Train Loss: 0.0823, Val Loss: 0.2510\n",
      "Epoch [12/50], Train Loss: 0.0795, Val Loss: 0.2453\n",
      "Epoch [13/50], Train Loss: 0.0772, Val Loss: 0.2397\n",
      "Epoch [14/50], Train Loss: 0.0750, Val Loss: 0.2344\n",
      "Epoch [15/50], Train Loss: 0.0734, Val Loss: 0.2292\n",
      "Epoch [16/50], Train Loss: 0.0713, Val Loss: 0.2243\n",
      "Epoch [17/50], Train Loss: 0.0698, Val Loss: 0.2195\n",
      "Epoch [18/50], Train Loss: 0.0672, Val Loss: 0.2148\n",
      "Epoch [19/50], Train Loss: 0.0663, Val Loss: 0.2104\n",
      "Epoch [20/50], Train Loss: 0.0646, Val Loss: 0.2060\n",
      "Epoch [21/50], Train Loss: 0.0630, Val Loss: 0.2018\n",
      "Epoch [22/50], Train Loss: 0.0613, Val Loss: 0.1978\n",
      "Epoch [23/50], Train Loss: 0.0601, Val Loss: 0.1939\n",
      "Epoch [24/50], Train Loss: 0.0585, Val Loss: 0.1902\n",
      "Epoch [25/50], Train Loss: 0.0569, Val Loss: 0.1866\n",
      "Epoch [26/50], Train Loss: 0.0562, Val Loss: 0.1831\n",
      "Epoch [27/50], Train Loss: 0.0553, Val Loss: 0.1797\n",
      "Epoch [28/50], Train Loss: 0.0538, Val Loss: 0.1765\n",
      "Epoch [29/50], Train Loss: 0.0530, Val Loss: 0.1734\n",
      "Epoch [30/50], Train Loss: 0.0519, Val Loss: 0.1703\n",
      "Epoch [31/50], Train Loss: 0.0511, Val Loss: 0.1674\n",
      "Epoch [32/50], Train Loss: 0.0504, Val Loss: 0.1645\n",
      "Epoch [33/50], Train Loss: 0.0490, Val Loss: 0.1618\n",
      "Epoch [34/50], Train Loss: 0.0485, Val Loss: 0.1591\n",
      "Epoch [35/50], Train Loss: 0.0473, Val Loss: 0.1565\n",
      "Epoch [36/50], Train Loss: 0.0470, Val Loss: 0.1541\n",
      "Epoch [37/50], Train Loss: 0.0462, Val Loss: 0.1516\n",
      "Epoch [38/50], Train Loss: 0.0454, Val Loss: 0.1493\n",
      "Epoch [39/50], Train Loss: 0.0451, Val Loss: 0.1471\n",
      "Epoch [40/50], Train Loss: 0.0447, Val Loss: 0.1449\n",
      "Epoch [41/50], Train Loss: 0.0440, Val Loss: 0.1428\n",
      "Epoch [42/50], Train Loss: 0.0430, Val Loss: 0.1407\n",
      "Epoch [43/50], Train Loss: 0.0430, Val Loss: 0.1387\n",
      "Epoch [44/50], Train Loss: 0.0423, Val Loss: 0.1368\n",
      "Epoch [45/50], Train Loss: 0.0417, Val Loss: 0.1349\n",
      "Epoch [46/50], Train Loss: 0.0416, Val Loss: 0.1331\n",
      "Epoch [47/50], Train Loss: 0.0411, Val Loss: 0.1314\n",
      "Epoch [48/50], Train Loss: 0.0406, Val Loss: 0.1297\n",
      "Epoch [49/50], Train Loss: 0.0403, Val Loss: 0.1281\n",
      "Epoch [50/50], Train Loss: 0.0398, Val Loss: 0.1265\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=64, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1738, Val Loss: 0.4462\n",
      "Epoch [2/50], Train Loss: 0.1695, Val Loss: 0.4344\n",
      "Epoch [3/50], Train Loss: 0.1641, Val Loss: 0.4230\n",
      "Epoch [4/50], Train Loss: 0.1570, Val Loss: 0.4121\n",
      "Epoch [5/50], Train Loss: 0.1519, Val Loss: 0.4017\n",
      "Epoch [6/50], Train Loss: 0.1471, Val Loss: 0.3916\n",
      "Epoch [7/50], Train Loss: 0.1412, Val Loss: 0.3819\n",
      "Epoch [8/50], Train Loss: 0.1374, Val Loss: 0.3726\n",
      "Epoch [9/50], Train Loss: 0.1323, Val Loss: 0.3636\n",
      "Epoch [10/50], Train Loss: 0.1273, Val Loss: 0.3549\n",
      "Epoch [11/50], Train Loss: 0.1228, Val Loss: 0.3465\n",
      "Epoch [12/50], Train Loss: 0.1192, Val Loss: 0.3385\n",
      "Epoch [13/50], Train Loss: 0.1160, Val Loss: 0.3307\n",
      "Epoch [14/50], Train Loss: 0.1126, Val Loss: 0.3232\n",
      "Epoch [15/50], Train Loss: 0.1086, Val Loss: 0.3160\n",
      "Epoch [16/50], Train Loss: 0.1066, Val Loss: 0.3089\n",
      "Epoch [17/50], Train Loss: 0.1031, Val Loss: 0.3020\n",
      "Epoch [18/50], Train Loss: 0.0997, Val Loss: 0.2955\n",
      "Epoch [19/50], Train Loss: 0.0968, Val Loss: 0.2891\n",
      "Epoch [20/50], Train Loss: 0.0960, Val Loss: 0.2829\n",
      "Epoch [21/50], Train Loss: 0.0910, Val Loss: 0.2770\n",
      "Epoch [22/50], Train Loss: 0.0893, Val Loss: 0.2713\n",
      "Epoch [23/50], Train Loss: 0.0870, Val Loss: 0.2657\n",
      "Epoch [24/50], Train Loss: 0.0848, Val Loss: 0.2603\n",
      "Epoch [25/50], Train Loss: 0.0825, Val Loss: 0.2550\n",
      "Epoch [26/50], Train Loss: 0.0793, Val Loss: 0.2500\n",
      "Epoch [27/50], Train Loss: 0.0773, Val Loss: 0.2450\n",
      "Epoch [28/50], Train Loss: 0.0760, Val Loss: 0.2403\n",
      "Epoch [29/50], Train Loss: 0.0746, Val Loss: 0.2356\n",
      "Epoch [30/50], Train Loss: 0.0733, Val Loss: 0.2312\n",
      "Epoch [31/50], Train Loss: 0.0715, Val Loss: 0.2269\n",
      "Epoch [32/50], Train Loss: 0.0696, Val Loss: 0.2227\n",
      "Epoch [33/50], Train Loss: 0.0684, Val Loss: 0.2187\n",
      "Epoch [34/50], Train Loss: 0.0674, Val Loss: 0.2147\n",
      "Epoch [35/50], Train Loss: 0.0655, Val Loss: 0.2109\n",
      "Epoch [36/50], Train Loss: 0.0635, Val Loss: 0.2072\n",
      "Epoch [37/50], Train Loss: 0.0637, Val Loss: 0.2036\n",
      "Epoch [38/50], Train Loss: 0.0621, Val Loss: 0.2002\n",
      "Epoch [39/50], Train Loss: 0.0600, Val Loss: 0.1968\n",
      "Epoch [40/50], Train Loss: 0.0601, Val Loss: 0.1936\n",
      "Epoch [41/50], Train Loss: 0.0579, Val Loss: 0.1904\n",
      "Epoch [42/50], Train Loss: 0.0587, Val Loss: 0.1874\n",
      "Epoch [43/50], Train Loss: 0.0558, Val Loss: 0.1844\n",
      "Epoch [44/50], Train Loss: 0.0556, Val Loss: 0.1815\n",
      "Epoch [45/50], Train Loss: 0.0524, Val Loss: 0.1788\n",
      "Epoch [46/50], Train Loss: 0.0537, Val Loss: 0.1760\n",
      "Epoch [47/50], Train Loss: 0.0533, Val Loss: 0.1734\n",
      "Epoch [48/50], Train Loss: 0.0527, Val Loss: 0.1709\n",
      "Epoch [49/50], Train Loss: 0.0512, Val Loss: 0.1684\n",
      "Epoch [50/50], Train Loss: 0.0507, Val Loss: 0.1660\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=64, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1210, Val Loss: 0.3357\n",
      "Epoch [2/50], Train Loss: 0.1155, Val Loss: 0.3250\n",
      "Epoch [3/50], Train Loss: 0.1103, Val Loss: 0.3148\n",
      "Epoch [4/50], Train Loss: 0.1054, Val Loss: 0.3051\n",
      "Epoch [5/50], Train Loss: 0.1009, Val Loss: 0.2958\n",
      "Epoch [6/50], Train Loss: 0.0966, Val Loss: 0.2870\n",
      "Epoch [7/50], Train Loss: 0.0926, Val Loss: 0.2787\n",
      "Epoch [8/50], Train Loss: 0.0889, Val Loss: 0.2707\n",
      "Epoch [9/50], Train Loss: 0.0854, Val Loss: 0.2630\n",
      "Epoch [10/50], Train Loss: 0.0821, Val Loss: 0.2558\n",
      "Epoch [11/50], Train Loss: 0.0790, Val Loss: 0.2488\n",
      "Epoch [12/50], Train Loss: 0.0761, Val Loss: 0.2422\n",
      "Epoch [13/50], Train Loss: 0.0734, Val Loss: 0.2358\n",
      "Epoch [14/50], Train Loss: 0.0708, Val Loss: 0.2298\n",
      "Epoch [15/50], Train Loss: 0.0684, Val Loss: 0.2240\n",
      "Epoch [16/50], Train Loss: 0.0662, Val Loss: 0.2184\n",
      "Epoch [17/50], Train Loss: 0.0641, Val Loss: 0.2131\n",
      "Epoch [18/50], Train Loss: 0.0621, Val Loss: 0.2081\n",
      "Epoch [19/50], Train Loss: 0.0602, Val Loss: 0.2032\n",
      "Epoch [20/50], Train Loss: 0.0585, Val Loss: 0.1986\n",
      "Epoch [21/50], Train Loss: 0.0569, Val Loss: 0.1941\n",
      "Epoch [22/50], Train Loss: 0.0554, Val Loss: 0.1899\n",
      "Epoch [23/50], Train Loss: 0.0539, Val Loss: 0.1858\n",
      "Epoch [24/50], Train Loss: 0.0526, Val Loss: 0.1820\n",
      "Epoch [25/50], Train Loss: 0.0514, Val Loss: 0.1782\n",
      "Epoch [26/50], Train Loss: 0.0502, Val Loss: 0.1747\n",
      "Epoch [27/50], Train Loss: 0.0491, Val Loss: 0.1713\n",
      "Epoch [28/50], Train Loss: 0.0481, Val Loss: 0.1680\n",
      "Epoch [29/50], Train Loss: 0.0471, Val Loss: 0.1649\n",
      "Epoch [30/50], Train Loss: 0.0462, Val Loss: 0.1619\n",
      "Epoch [31/50], Train Loss: 0.0454, Val Loss: 0.1590\n",
      "Epoch [32/50], Train Loss: 0.0446, Val Loss: 0.1563\n",
      "Epoch [33/50], Train Loss: 0.0439, Val Loss: 0.1537\n",
      "Epoch [34/50], Train Loss: 0.0432, Val Loss: 0.1511\n",
      "Epoch [35/50], Train Loss: 0.0426, Val Loss: 0.1487\n",
      "Epoch [36/50], Train Loss: 0.0420, Val Loss: 0.1464\n",
      "Epoch [37/50], Train Loss: 0.0414, Val Loss: 0.1442\n",
      "Epoch [38/50], Train Loss: 0.0409, Val Loss: 0.1421\n",
      "Epoch [39/50], Train Loss: 0.0404, Val Loss: 0.1401\n",
      "Epoch [40/50], Train Loss: 0.0400, Val Loss: 0.1381\n",
      "Epoch [41/50], Train Loss: 0.0396, Val Loss: 0.1362\n",
      "Epoch [42/50], Train Loss: 0.0392, Val Loss: 0.1345\n",
      "Epoch [43/50], Train Loss: 0.0388, Val Loss: 0.1328\n",
      "Epoch [44/50], Train Loss: 0.0385, Val Loss: 0.1311\n",
      "Epoch [45/50], Train Loss: 0.0382, Val Loss: 0.1295\n",
      "Epoch [46/50], Train Loss: 0.0379, Val Loss: 0.1280\n",
      "Epoch [47/50], Train Loss: 0.0376, Val Loss: 0.1266\n",
      "Epoch [48/50], Train Loss: 0.0373, Val Loss: 0.1252\n",
      "Epoch [49/50], Train Loss: 0.0371, Val Loss: 0.1238\n",
      "Epoch [50/50], Train Loss: 0.0369, Val Loss: 0.1226\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=64, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1616, Val Loss: 0.4333\n",
      "Epoch [2/50], Train Loss: 0.1530, Val Loss: 0.4196\n",
      "Epoch [3/50], Train Loss: 0.1467, Val Loss: 0.4067\n",
      "Epoch [4/50], Train Loss: 0.1401, Val Loss: 0.3944\n",
      "Epoch [5/50], Train Loss: 0.1346, Val Loss: 0.3826\n",
      "Epoch [6/50], Train Loss: 0.1282, Val Loss: 0.3713\n",
      "Epoch [7/50], Train Loss: 0.1239, Val Loss: 0.3605\n",
      "Epoch [8/50], Train Loss: 0.1182, Val Loss: 0.3503\n",
      "Epoch [9/50], Train Loss: 0.1135, Val Loss: 0.3405\n",
      "Epoch [10/50], Train Loss: 0.1089, Val Loss: 0.3311\n",
      "Epoch [11/50], Train Loss: 0.1046, Val Loss: 0.3221\n",
      "Epoch [12/50], Train Loss: 0.1009, Val Loss: 0.3135\n",
      "Epoch [13/50], Train Loss: 0.0968, Val Loss: 0.3052\n",
      "Epoch [14/50], Train Loss: 0.0931, Val Loss: 0.2972\n",
      "Epoch [15/50], Train Loss: 0.0898, Val Loss: 0.2896\n",
      "Epoch [16/50], Train Loss: 0.0876, Val Loss: 0.2822\n",
      "Epoch [17/50], Train Loss: 0.0841, Val Loss: 0.2752\n",
      "Epoch [18/50], Train Loss: 0.0817, Val Loss: 0.2684\n",
      "Epoch [19/50], Train Loss: 0.0783, Val Loss: 0.2620\n",
      "Epoch [20/50], Train Loss: 0.0771, Val Loss: 0.2558\n",
      "Epoch [21/50], Train Loss: 0.0744, Val Loss: 0.2497\n",
      "Epoch [22/50], Train Loss: 0.0716, Val Loss: 0.2440\n",
      "Epoch [23/50], Train Loss: 0.0697, Val Loss: 0.2385\n",
      "Epoch [24/50], Train Loss: 0.0679, Val Loss: 0.2331\n",
      "Epoch [25/50], Train Loss: 0.0657, Val Loss: 0.2280\n",
      "Epoch [26/50], Train Loss: 0.0641, Val Loss: 0.2231\n",
      "Epoch [27/50], Train Loss: 0.0626, Val Loss: 0.2184\n",
      "Epoch [28/50], Train Loss: 0.0614, Val Loss: 0.2138\n",
      "Epoch [29/50], Train Loss: 0.0592, Val Loss: 0.2094\n",
      "Epoch [30/50], Train Loss: 0.0578, Val Loss: 0.2052\n",
      "Epoch [31/50], Train Loss: 0.0563, Val Loss: 0.2012\n",
      "Epoch [32/50], Train Loss: 0.0558, Val Loss: 0.1972\n",
      "Epoch [33/50], Train Loss: 0.0549, Val Loss: 0.1934\n",
      "Epoch [34/50], Train Loss: 0.0532, Val Loss: 0.1898\n",
      "Epoch [35/50], Train Loss: 0.0520, Val Loss: 0.1863\n",
      "Epoch [36/50], Train Loss: 0.0505, Val Loss: 0.1830\n",
      "Epoch [37/50], Train Loss: 0.0503, Val Loss: 0.1798\n",
      "Epoch [38/50], Train Loss: 0.0491, Val Loss: 0.1767\n",
      "Epoch [39/50], Train Loss: 0.0485, Val Loss: 0.1738\n",
      "Epoch [40/50], Train Loss: 0.0480, Val Loss: 0.1709\n",
      "Epoch [41/50], Train Loss: 0.0471, Val Loss: 0.1682\n",
      "Epoch [42/50], Train Loss: 0.0470, Val Loss: 0.1655\n",
      "Epoch [43/50], Train Loss: 0.0455, Val Loss: 0.1629\n",
      "Epoch [44/50], Train Loss: 0.0453, Val Loss: 0.1605\n",
      "Epoch [45/50], Train Loss: 0.0450, Val Loss: 0.1581\n",
      "Epoch [46/50], Train Loss: 0.0441, Val Loss: 0.1558\n",
      "Epoch [47/50], Train Loss: 0.0436, Val Loss: 0.1536\n",
      "Epoch [48/50], Train Loss: 0.0433, Val Loss: 0.1516\n",
      "Epoch [49/50], Train Loss: 0.0427, Val Loss: 0.1496\n",
      "Epoch [50/50], Train Loss: 0.0422, Val Loss: 0.1476\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=64, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1192, Val Loss: 0.3495\n",
      "Epoch [2/50], Train Loss: 0.1150, Val Loss: 0.3402\n",
      "Epoch [3/50], Train Loss: 0.1114, Val Loss: 0.3314\n",
      "Epoch [4/50], Train Loss: 0.1082, Val Loss: 0.3228\n",
      "Epoch [5/50], Train Loss: 0.1046, Val Loss: 0.3146\n",
      "Epoch [6/50], Train Loss: 0.1003, Val Loss: 0.3068\n",
      "Epoch [7/50], Train Loss: 0.0963, Val Loss: 0.2993\n",
      "Epoch [8/50], Train Loss: 0.0943, Val Loss: 0.2921\n",
      "Epoch [9/50], Train Loss: 0.0907, Val Loss: 0.2851\n",
      "Epoch [10/50], Train Loss: 0.0872, Val Loss: 0.2783\n",
      "Epoch [11/50], Train Loss: 0.0869, Val Loss: 0.2717\n",
      "Epoch [12/50], Train Loss: 0.0834, Val Loss: 0.2656\n",
      "Epoch [13/50], Train Loss: 0.0810, Val Loss: 0.2596\n",
      "Epoch [14/50], Train Loss: 0.0788, Val Loss: 0.2537\n",
      "Epoch [15/50], Train Loss: 0.0770, Val Loss: 0.2481\n",
      "Epoch [16/50], Train Loss: 0.0746, Val Loss: 0.2426\n",
      "Epoch [17/50], Train Loss: 0.0715, Val Loss: 0.2375\n",
      "Epoch [18/50], Train Loss: 0.0707, Val Loss: 0.2325\n",
      "Epoch [19/50], Train Loss: 0.0695, Val Loss: 0.2276\n",
      "Epoch [20/50], Train Loss: 0.0679, Val Loss: 0.2229\n",
      "Epoch [21/50], Train Loss: 0.0653, Val Loss: 0.2184\n",
      "Epoch [22/50], Train Loss: 0.0640, Val Loss: 0.2141\n",
      "Epoch [23/50], Train Loss: 0.0638, Val Loss: 0.2099\n",
      "Epoch [24/50], Train Loss: 0.0631, Val Loss: 0.2058\n",
      "Epoch [25/50], Train Loss: 0.0610, Val Loss: 0.2019\n",
      "Epoch [26/50], Train Loss: 0.0601, Val Loss: 0.1981\n",
      "Epoch [27/50], Train Loss: 0.0588, Val Loss: 0.1945\n",
      "Epoch [28/50], Train Loss: 0.0573, Val Loss: 0.1910\n",
      "Epoch [29/50], Train Loss: 0.0559, Val Loss: 0.1875\n",
      "Epoch [30/50], Train Loss: 0.0559, Val Loss: 0.1842\n",
      "Epoch [31/50], Train Loss: 0.0535, Val Loss: 0.1811\n",
      "Epoch [32/50], Train Loss: 0.0539, Val Loss: 0.1780\n",
      "Epoch [33/50], Train Loss: 0.0537, Val Loss: 0.1750\n",
      "Epoch [34/50], Train Loss: 0.0530, Val Loss: 0.1721\n",
      "Epoch [35/50], Train Loss: 0.0514, Val Loss: 0.1694\n",
      "Epoch [36/50], Train Loss: 0.0503, Val Loss: 0.1668\n",
      "Epoch [37/50], Train Loss: 0.0511, Val Loss: 0.1642\n",
      "Epoch [38/50], Train Loss: 0.0498, Val Loss: 0.1617\n",
      "Epoch [39/50], Train Loss: 0.0491, Val Loss: 0.1594\n",
      "Epoch [40/50], Train Loss: 0.0484, Val Loss: 0.1570\n",
      "Epoch [41/50], Train Loss: 0.0484, Val Loss: 0.1548\n",
      "Epoch [42/50], Train Loss: 0.0467, Val Loss: 0.1527\n",
      "Epoch [43/50], Train Loss: 0.0478, Val Loss: 0.1506\n",
      "Epoch [44/50], Train Loss: 0.0465, Val Loss: 0.1486\n",
      "Epoch [45/50], Train Loss: 0.0472, Val Loss: 0.1466\n",
      "Epoch [46/50], Train Loss: 0.0460, Val Loss: 0.1447\n",
      "Epoch [47/50], Train Loss: 0.0449, Val Loss: 0.1429\n",
      "Epoch [48/50], Train Loss: 0.0457, Val Loss: 0.1411\n",
      "Epoch [49/50], Train Loss: 0.0450, Val Loss: 0.1394\n",
      "Epoch [50/50], Train Loss: 0.0434, Val Loss: 0.1378\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=64, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0936, Val Loss: 0.2750\n",
      "Epoch [2/50], Train Loss: 0.0898, Val Loss: 0.2679\n",
      "Epoch [3/50], Train Loss: 0.0863, Val Loss: 0.2610\n",
      "Epoch [4/50], Train Loss: 0.0831, Val Loss: 0.2545\n",
      "Epoch [5/50], Train Loss: 0.0800, Val Loss: 0.2482\n",
      "Epoch [6/50], Train Loss: 0.0771, Val Loss: 0.2422\n",
      "Epoch [7/50], Train Loss: 0.0744, Val Loss: 0.2365\n",
      "Epoch [8/50], Train Loss: 0.0718, Val Loss: 0.2310\n",
      "Epoch [9/50], Train Loss: 0.0694, Val Loss: 0.2257\n",
      "Epoch [10/50], Train Loss: 0.0672, Val Loss: 0.2207\n",
      "Epoch [11/50], Train Loss: 0.0651, Val Loss: 0.2159\n",
      "Epoch [12/50], Train Loss: 0.0631, Val Loss: 0.2112\n",
      "Epoch [13/50], Train Loss: 0.0612, Val Loss: 0.2068\n",
      "Epoch [14/50], Train Loss: 0.0595, Val Loss: 0.2026\n",
      "Epoch [15/50], Train Loss: 0.0579, Val Loss: 0.1985\n",
      "Epoch [16/50], Train Loss: 0.0564, Val Loss: 0.1946\n",
      "Epoch [17/50], Train Loss: 0.0549, Val Loss: 0.1908\n",
      "Epoch [18/50], Train Loss: 0.0536, Val Loss: 0.1872\n",
      "Epoch [19/50], Train Loss: 0.0523, Val Loss: 0.1838\n",
      "Epoch [20/50], Train Loss: 0.0512, Val Loss: 0.1805\n",
      "Epoch [21/50], Train Loss: 0.0501, Val Loss: 0.1773\n",
      "Epoch [22/50], Train Loss: 0.0491, Val Loss: 0.1743\n",
      "Epoch [23/50], Train Loss: 0.0481, Val Loss: 0.1714\n",
      "Epoch [24/50], Train Loss: 0.0472, Val Loss: 0.1686\n",
      "Epoch [25/50], Train Loss: 0.0464, Val Loss: 0.1659\n",
      "Epoch [26/50], Train Loss: 0.0456, Val Loss: 0.1634\n",
      "Epoch [27/50], Train Loss: 0.0449, Val Loss: 0.1609\n",
      "Epoch [28/50], Train Loss: 0.0442, Val Loss: 0.1585\n",
      "Epoch [29/50], Train Loss: 0.0436, Val Loss: 0.1563\n",
      "Epoch [30/50], Train Loss: 0.0430, Val Loss: 0.1541\n",
      "Epoch [31/50], Train Loss: 0.0424, Val Loss: 0.1520\n",
      "Epoch [32/50], Train Loss: 0.0419, Val Loss: 0.1500\n",
      "Epoch [33/50], Train Loss: 0.0414, Val Loss: 0.1481\n",
      "Epoch [34/50], Train Loss: 0.0410, Val Loss: 0.1463\n",
      "Epoch [35/50], Train Loss: 0.0406, Val Loss: 0.1445\n",
      "Epoch [36/50], Train Loss: 0.0402, Val Loss: 0.1428\n",
      "Epoch [37/50], Train Loss: 0.0398, Val Loss: 0.1412\n",
      "Epoch [38/50], Train Loss: 0.0395, Val Loss: 0.1396\n",
      "Epoch [39/50], Train Loss: 0.0392, Val Loss: 0.1381\n",
      "Epoch [40/50], Train Loss: 0.0389, Val Loss: 0.1367\n",
      "Epoch [41/50], Train Loss: 0.0386, Val Loss: 0.1353\n",
      "Epoch [42/50], Train Loss: 0.0384, Val Loss: 0.1340\n",
      "Epoch [43/50], Train Loss: 0.0381, Val Loss: 0.1327\n",
      "Epoch [44/50], Train Loss: 0.0379, Val Loss: 0.1315\n",
      "Epoch [45/50], Train Loss: 0.0377, Val Loss: 0.1304\n",
      "Epoch [46/50], Train Loss: 0.0375, Val Loss: 0.1292\n",
      "Epoch [47/50], Train Loss: 0.0374, Val Loss: 0.1282\n",
      "Epoch [48/50], Train Loss: 0.0372, Val Loss: 0.1271\n",
      "Epoch [49/50], Train Loss: 0.0370, Val Loss: 0.1261\n",
      "Epoch [50/50], Train Loss: 0.0369, Val Loss: 0.1252\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=64, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1193, Val Loss: 0.3225\n",
      "Epoch [2/50], Train Loss: 0.1146, Val Loss: 0.3139\n",
      "Epoch [3/50], Train Loss: 0.1099, Val Loss: 0.3058\n",
      "Epoch [4/50], Train Loss: 0.1060, Val Loss: 0.2980\n",
      "Epoch [5/50], Train Loss: 0.1018, Val Loss: 0.2906\n",
      "Epoch [6/50], Train Loss: 0.0981, Val Loss: 0.2834\n",
      "Epoch [7/50], Train Loss: 0.0950, Val Loss: 0.2766\n",
      "Epoch [8/50], Train Loss: 0.0915, Val Loss: 0.2700\n",
      "Epoch [9/50], Train Loss: 0.0879, Val Loss: 0.2637\n",
      "Epoch [10/50], Train Loss: 0.0849, Val Loss: 0.2576\n",
      "Epoch [11/50], Train Loss: 0.0821, Val Loss: 0.2518\n",
      "Epoch [12/50], Train Loss: 0.0800, Val Loss: 0.2463\n",
      "Epoch [13/50], Train Loss: 0.0773, Val Loss: 0.2409\n",
      "Epoch [14/50], Train Loss: 0.0750, Val Loss: 0.2358\n",
      "Epoch [15/50], Train Loss: 0.0723, Val Loss: 0.2309\n",
      "Epoch [16/50], Train Loss: 0.0700, Val Loss: 0.2261\n",
      "Epoch [17/50], Train Loss: 0.0684, Val Loss: 0.2215\n",
      "Epoch [18/50], Train Loss: 0.0662, Val Loss: 0.2171\n",
      "Epoch [19/50], Train Loss: 0.0646, Val Loss: 0.2129\n",
      "Epoch [20/50], Train Loss: 0.0630, Val Loss: 0.2088\n",
      "Epoch [21/50], Train Loss: 0.0616, Val Loss: 0.2049\n",
      "Epoch [22/50], Train Loss: 0.0598, Val Loss: 0.2011\n",
      "Epoch [23/50], Train Loss: 0.0586, Val Loss: 0.1975\n",
      "Epoch [24/50], Train Loss: 0.0574, Val Loss: 0.1940\n",
      "Epoch [25/50], Train Loss: 0.0556, Val Loss: 0.1907\n",
      "Epoch [26/50], Train Loss: 0.0545, Val Loss: 0.1875\n",
      "Epoch [27/50], Train Loss: 0.0535, Val Loss: 0.1844\n",
      "Epoch [28/50], Train Loss: 0.0526, Val Loss: 0.1813\n",
      "Epoch [29/50], Train Loss: 0.0515, Val Loss: 0.1784\n",
      "Epoch [30/50], Train Loss: 0.0503, Val Loss: 0.1756\n",
      "Epoch [31/50], Train Loss: 0.0499, Val Loss: 0.1729\n",
      "Epoch [32/50], Train Loss: 0.0489, Val Loss: 0.1704\n",
      "Epoch [33/50], Train Loss: 0.0483, Val Loss: 0.1679\n",
      "Epoch [34/50], Train Loss: 0.0479, Val Loss: 0.1655\n",
      "Epoch [35/50], Train Loss: 0.0464, Val Loss: 0.1632\n",
      "Epoch [36/50], Train Loss: 0.0464, Val Loss: 0.1610\n",
      "Epoch [37/50], Train Loss: 0.0455, Val Loss: 0.1588\n",
      "Epoch [38/50], Train Loss: 0.0448, Val Loss: 0.1567\n",
      "Epoch [39/50], Train Loss: 0.0451, Val Loss: 0.1547\n",
      "Epoch [40/50], Train Loss: 0.0440, Val Loss: 0.1528\n",
      "Epoch [41/50], Train Loss: 0.0436, Val Loss: 0.1510\n",
      "Epoch [42/50], Train Loss: 0.0429, Val Loss: 0.1492\n",
      "Epoch [43/50], Train Loss: 0.0428, Val Loss: 0.1475\n",
      "Epoch [44/50], Train Loss: 0.0426, Val Loss: 0.1459\n",
      "Epoch [45/50], Train Loss: 0.0414, Val Loss: 0.1443\n",
      "Epoch [46/50], Train Loss: 0.0421, Val Loss: 0.1428\n",
      "Epoch [47/50], Train Loss: 0.0410, Val Loss: 0.1413\n",
      "Epoch [48/50], Train Loss: 0.0415, Val Loss: 0.1399\n",
      "Epoch [49/50], Train Loss: 0.0405, Val Loss: 0.1386\n",
      "Epoch [50/50], Train Loss: 0.0404, Val Loss: 0.1373\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=64, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1187, Val Loss: 0.3001\n",
      "Epoch [2/50], Train Loss: 0.1140, Val Loss: 0.2910\n",
      "Epoch [3/50], Train Loss: 0.1089, Val Loss: 0.2824\n",
      "Epoch [4/50], Train Loss: 0.1057, Val Loss: 0.2742\n",
      "Epoch [5/50], Train Loss: 0.1001, Val Loss: 0.2665\n",
      "Epoch [6/50], Train Loss: 0.0966, Val Loss: 0.2592\n",
      "Epoch [7/50], Train Loss: 0.0936, Val Loss: 0.2522\n",
      "Epoch [8/50], Train Loss: 0.0907, Val Loss: 0.2456\n",
      "Epoch [9/50], Train Loss: 0.0866, Val Loss: 0.2394\n",
      "Epoch [10/50], Train Loss: 0.0839, Val Loss: 0.2334\n",
      "Epoch [11/50], Train Loss: 0.0800, Val Loss: 0.2276\n",
      "Epoch [12/50], Train Loss: 0.0776, Val Loss: 0.2223\n",
      "Epoch [13/50], Train Loss: 0.0771, Val Loss: 0.2171\n",
      "Epoch [14/50], Train Loss: 0.0730, Val Loss: 0.2122\n",
      "Epoch [15/50], Train Loss: 0.0715, Val Loss: 0.2075\n",
      "Epoch [16/50], Train Loss: 0.0697, Val Loss: 0.2031\n",
      "Epoch [17/50], Train Loss: 0.0688, Val Loss: 0.1988\n",
      "Epoch [18/50], Train Loss: 0.0659, Val Loss: 0.1948\n",
      "Epoch [19/50], Train Loss: 0.0647, Val Loss: 0.1909\n",
      "Epoch [20/50], Train Loss: 0.0632, Val Loss: 0.1871\n",
      "Epoch [21/50], Train Loss: 0.0605, Val Loss: 0.1837\n",
      "Epoch [22/50], Train Loss: 0.0599, Val Loss: 0.1803\n",
      "Epoch [23/50], Train Loss: 0.0593, Val Loss: 0.1770\n",
      "Epoch [24/50], Train Loss: 0.0586, Val Loss: 0.1740\n",
      "Epoch [25/50], Train Loss: 0.0567, Val Loss: 0.1711\n",
      "Epoch [26/50], Train Loss: 0.0567, Val Loss: 0.1683\n",
      "Epoch [27/50], Train Loss: 0.0547, Val Loss: 0.1656\n",
      "Epoch [28/50], Train Loss: 0.0545, Val Loss: 0.1631\n",
      "Epoch [29/50], Train Loss: 0.0546, Val Loss: 0.1606\n",
      "Epoch [30/50], Train Loss: 0.0530, Val Loss: 0.1582\n",
      "Epoch [31/50], Train Loss: 0.0515, Val Loss: 0.1561\n",
      "Epoch [32/50], Train Loss: 0.0524, Val Loss: 0.1540\n",
      "Epoch [33/50], Train Loss: 0.0503, Val Loss: 0.1520\n",
      "Epoch [34/50], Train Loss: 0.0501, Val Loss: 0.1501\n",
      "Epoch [35/50], Train Loss: 0.0493, Val Loss: 0.1482\n",
      "Epoch [36/50], Train Loss: 0.0481, Val Loss: 0.1465\n",
      "Epoch [37/50], Train Loss: 0.0481, Val Loss: 0.1447\n",
      "Epoch [38/50], Train Loss: 0.0475, Val Loss: 0.1432\n",
      "Epoch [39/50], Train Loss: 0.0479, Val Loss: 0.1416\n",
      "Epoch [40/50], Train Loss: 0.0467, Val Loss: 0.1401\n",
      "Epoch [41/50], Train Loss: 0.0472, Val Loss: 0.1387\n",
      "Epoch [42/50], Train Loss: 0.0487, Val Loss: 0.1374\n",
      "Epoch [43/50], Train Loss: 0.0469, Val Loss: 0.1362\n",
      "Epoch [44/50], Train Loss: 0.0467, Val Loss: 0.1349\n",
      "Epoch [45/50], Train Loss: 0.0457, Val Loss: 0.1336\n",
      "Epoch [46/50], Train Loss: 0.0457, Val Loss: 0.1325\n",
      "Epoch [47/50], Train Loss: 0.0458, Val Loss: 0.1314\n",
      "Epoch [48/50], Train Loss: 0.0449, Val Loss: 0.1303\n",
      "Epoch [49/50], Train Loss: 0.0467, Val Loss: 0.1293\n",
      "Epoch [50/50], Train Loss: 0.0452, Val Loss: 0.1283\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=128, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1197, Val Loss: 0.3486\n",
      "Epoch [2/50], Train Loss: 0.1151, Val Loss: 0.3385\n",
      "Epoch [3/50], Train Loss: 0.1108, Val Loss: 0.3289\n",
      "Epoch [4/50], Train Loss: 0.1067, Val Loss: 0.3196\n",
      "Epoch [5/50], Train Loss: 0.1027, Val Loss: 0.3107\n",
      "Epoch [6/50], Train Loss: 0.0990, Val Loss: 0.3022\n",
      "Epoch [7/50], Train Loss: 0.0955, Val Loss: 0.2940\n",
      "Epoch [8/50], Train Loss: 0.0921, Val Loss: 0.2861\n",
      "Epoch [9/50], Train Loss: 0.0889, Val Loss: 0.2785\n",
      "Epoch [10/50], Train Loss: 0.0859, Val Loss: 0.2712\n",
      "Epoch [11/50], Train Loss: 0.0830, Val Loss: 0.2641\n",
      "Epoch [12/50], Train Loss: 0.0803, Val Loss: 0.2574\n",
      "Epoch [13/50], Train Loss: 0.0776, Val Loss: 0.2508\n",
      "Epoch [14/50], Train Loss: 0.0752, Val Loss: 0.2445\n",
      "Epoch [15/50], Train Loss: 0.0728, Val Loss: 0.2385\n",
      "Epoch [16/50], Train Loss: 0.0706, Val Loss: 0.2327\n",
      "Epoch [17/50], Train Loss: 0.0684, Val Loss: 0.2270\n",
      "Epoch [18/50], Train Loss: 0.0664, Val Loss: 0.2216\n",
      "Epoch [19/50], Train Loss: 0.0645, Val Loss: 0.2164\n",
      "Epoch [20/50], Train Loss: 0.0627, Val Loss: 0.2114\n",
      "Epoch [21/50], Train Loss: 0.0609, Val Loss: 0.2065\n",
      "Epoch [22/50], Train Loss: 0.0593, Val Loss: 0.2018\n",
      "Epoch [23/50], Train Loss: 0.0577, Val Loss: 0.1973\n",
      "Epoch [24/50], Train Loss: 0.0562, Val Loss: 0.1930\n",
      "Epoch [25/50], Train Loss: 0.0548, Val Loss: 0.1888\n",
      "Epoch [26/50], Train Loss: 0.0534, Val Loss: 0.1847\n",
      "Epoch [27/50], Train Loss: 0.0521, Val Loss: 0.1808\n",
      "Epoch [28/50], Train Loss: 0.0509, Val Loss: 0.1770\n",
      "Epoch [29/50], Train Loss: 0.0498, Val Loss: 0.1734\n",
      "Epoch [30/50], Train Loss: 0.0487, Val Loss: 0.1698\n",
      "Epoch [31/50], Train Loss: 0.0476, Val Loss: 0.1664\n",
      "Epoch [32/50], Train Loss: 0.0466, Val Loss: 0.1632\n",
      "Epoch [33/50], Train Loss: 0.0457, Val Loss: 0.1600\n",
      "Epoch [34/50], Train Loss: 0.0448, Val Loss: 0.1569\n",
      "Epoch [35/50], Train Loss: 0.0439, Val Loss: 0.1540\n",
      "Epoch [36/50], Train Loss: 0.0431, Val Loss: 0.1511\n",
      "Epoch [37/50], Train Loss: 0.0424, Val Loss: 0.1484\n",
      "Epoch [38/50], Train Loss: 0.0416, Val Loss: 0.1457\n",
      "Epoch [39/50], Train Loss: 0.0409, Val Loss: 0.1432\n",
      "Epoch [40/50], Train Loss: 0.0403, Val Loss: 0.1407\n",
      "Epoch [41/50], Train Loss: 0.0397, Val Loss: 0.1383\n",
      "Epoch [42/50], Train Loss: 0.0391, Val Loss: 0.1359\n",
      "Epoch [43/50], Train Loss: 0.0385, Val Loss: 0.1337\n",
      "Epoch [44/50], Train Loss: 0.0380, Val Loss: 0.1315\n",
      "Epoch [45/50], Train Loss: 0.0375, Val Loss: 0.1295\n",
      "Epoch [46/50], Train Loss: 0.0370, Val Loss: 0.1274\n",
      "Epoch [47/50], Train Loss: 0.0365, Val Loss: 0.1255\n",
      "Epoch [48/50], Train Loss: 0.0361, Val Loss: 0.1236\n",
      "Epoch [49/50], Train Loss: 0.0357, Val Loss: 0.1218\n",
      "Epoch [50/50], Train Loss: 0.0353, Val Loss: 0.1200\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=128, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1295, Val Loss: 0.3565\n",
      "Epoch [2/50], Train Loss: 0.1241, Val Loss: 0.3462\n",
      "Epoch [3/50], Train Loss: 0.1197, Val Loss: 0.3362\n",
      "Epoch [4/50], Train Loss: 0.1154, Val Loss: 0.3266\n",
      "Epoch [5/50], Train Loss: 0.1112, Val Loss: 0.3174\n",
      "Epoch [6/50], Train Loss: 0.1072, Val Loss: 0.3086\n",
      "Epoch [7/50], Train Loss: 0.1035, Val Loss: 0.3001\n",
      "Epoch [8/50], Train Loss: 0.0997, Val Loss: 0.2919\n",
      "Epoch [9/50], Train Loss: 0.0956, Val Loss: 0.2840\n",
      "Epoch [10/50], Train Loss: 0.0926, Val Loss: 0.2764\n",
      "Epoch [11/50], Train Loss: 0.0895, Val Loss: 0.2691\n",
      "Epoch [12/50], Train Loss: 0.0868, Val Loss: 0.2621\n",
      "Epoch [13/50], Train Loss: 0.0838, Val Loss: 0.2554\n",
      "Epoch [14/50], Train Loss: 0.0809, Val Loss: 0.2488\n",
      "Epoch [15/50], Train Loss: 0.0787, Val Loss: 0.2426\n",
      "Epoch [16/50], Train Loss: 0.0764, Val Loss: 0.2365\n",
      "Epoch [17/50], Train Loss: 0.0737, Val Loss: 0.2307\n",
      "Epoch [18/50], Train Loss: 0.0719, Val Loss: 0.2250\n",
      "Epoch [19/50], Train Loss: 0.0697, Val Loss: 0.2196\n",
      "Epoch [20/50], Train Loss: 0.0677, Val Loss: 0.2143\n",
      "Epoch [21/50], Train Loss: 0.0655, Val Loss: 0.2093\n",
      "Epoch [22/50], Train Loss: 0.0637, Val Loss: 0.2044\n",
      "Epoch [23/50], Train Loss: 0.0621, Val Loss: 0.1997\n",
      "Epoch [24/50], Train Loss: 0.0606, Val Loss: 0.1952\n",
      "Epoch [25/50], Train Loss: 0.0589, Val Loss: 0.1908\n",
      "Epoch [26/50], Train Loss: 0.0572, Val Loss: 0.1866\n",
      "Epoch [27/50], Train Loss: 0.0561, Val Loss: 0.1825\n",
      "Epoch [28/50], Train Loss: 0.0545, Val Loss: 0.1786\n",
      "Epoch [29/50], Train Loss: 0.0533, Val Loss: 0.1748\n",
      "Epoch [30/50], Train Loss: 0.0519, Val Loss: 0.1712\n",
      "Epoch [31/50], Train Loss: 0.0510, Val Loss: 0.1676\n",
      "Epoch [32/50], Train Loss: 0.0497, Val Loss: 0.1642\n",
      "Epoch [33/50], Train Loss: 0.0484, Val Loss: 0.1609\n",
      "Epoch [34/50], Train Loss: 0.0480, Val Loss: 0.1577\n",
      "Epoch [35/50], Train Loss: 0.0468, Val Loss: 0.1547\n",
      "Epoch [36/50], Train Loss: 0.0458, Val Loss: 0.1517\n",
      "Epoch [37/50], Train Loss: 0.0448, Val Loss: 0.1488\n",
      "Epoch [38/50], Train Loss: 0.0444, Val Loss: 0.1461\n",
      "Epoch [39/50], Train Loss: 0.0432, Val Loss: 0.1434\n",
      "Epoch [40/50], Train Loss: 0.0429, Val Loss: 0.1408\n",
      "Epoch [41/50], Train Loss: 0.0423, Val Loss: 0.1383\n",
      "Epoch [42/50], Train Loss: 0.0414, Val Loss: 0.1359\n",
      "Epoch [43/50], Train Loss: 0.0408, Val Loss: 0.1336\n",
      "Epoch [44/50], Train Loss: 0.0401, Val Loss: 0.1313\n",
      "Epoch [45/50], Train Loss: 0.0392, Val Loss: 0.1292\n",
      "Epoch [46/50], Train Loss: 0.0392, Val Loss: 0.1271\n",
      "Epoch [47/50], Train Loss: 0.0386, Val Loss: 0.1250\n",
      "Epoch [48/50], Train Loss: 0.0380, Val Loss: 0.1231\n",
      "Epoch [49/50], Train Loss: 0.0372, Val Loss: 0.1212\n",
      "Epoch [50/50], Train Loss: 0.0374, Val Loss: 0.1194\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=128, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1093, Val Loss: 0.3104\n",
      "Epoch [2/50], Train Loss: 0.1065, Val Loss: 0.3027\n",
      "Epoch [3/50], Train Loss: 0.1024, Val Loss: 0.2953\n",
      "Epoch [4/50], Train Loss: 0.0997, Val Loss: 0.2882\n",
      "Epoch [5/50], Train Loss: 0.0965, Val Loss: 0.2813\n",
      "Epoch [6/50], Train Loss: 0.0939, Val Loss: 0.2746\n",
      "Epoch [7/50], Train Loss: 0.0911, Val Loss: 0.2682\n",
      "Epoch [8/50], Train Loss: 0.0877, Val Loss: 0.2620\n",
      "Epoch [9/50], Train Loss: 0.0845, Val Loss: 0.2560\n",
      "Epoch [10/50], Train Loss: 0.0830, Val Loss: 0.2503\n",
      "Epoch [11/50], Train Loss: 0.0797, Val Loss: 0.2447\n",
      "Epoch [12/50], Train Loss: 0.0779, Val Loss: 0.2394\n",
      "Epoch [13/50], Train Loss: 0.0752, Val Loss: 0.2342\n",
      "Epoch [14/50], Train Loss: 0.0728, Val Loss: 0.2293\n",
      "Epoch [15/50], Train Loss: 0.0718, Val Loss: 0.2244\n",
      "Epoch [16/50], Train Loss: 0.0696, Val Loss: 0.2198\n",
      "Epoch [17/50], Train Loss: 0.0681, Val Loss: 0.2153\n",
      "Epoch [18/50], Train Loss: 0.0659, Val Loss: 0.2109\n",
      "Epoch [19/50], Train Loss: 0.0642, Val Loss: 0.2067\n",
      "Epoch [20/50], Train Loss: 0.0632, Val Loss: 0.2027\n",
      "Epoch [21/50], Train Loss: 0.0616, Val Loss: 0.1988\n",
      "Epoch [22/50], Train Loss: 0.0602, Val Loss: 0.1950\n",
      "Epoch [23/50], Train Loss: 0.0588, Val Loss: 0.1913\n",
      "Epoch [24/50], Train Loss: 0.0573, Val Loss: 0.1877\n",
      "Epoch [25/50], Train Loss: 0.0564, Val Loss: 0.1843\n",
      "Epoch [26/50], Train Loss: 0.0556, Val Loss: 0.1810\n",
      "Epoch [27/50], Train Loss: 0.0541, Val Loss: 0.1778\n",
      "Epoch [28/50], Train Loss: 0.0527, Val Loss: 0.1746\n",
      "Epoch [29/50], Train Loss: 0.0522, Val Loss: 0.1716\n",
      "Epoch [30/50], Train Loss: 0.0512, Val Loss: 0.1686\n",
      "Epoch [31/50], Train Loss: 0.0504, Val Loss: 0.1658\n",
      "Epoch [32/50], Train Loss: 0.0491, Val Loss: 0.1631\n",
      "Epoch [33/50], Train Loss: 0.0481, Val Loss: 0.1605\n",
      "Epoch [34/50], Train Loss: 0.0477, Val Loss: 0.1579\n",
      "Epoch [35/50], Train Loss: 0.0468, Val Loss: 0.1555\n",
      "Epoch [36/50], Train Loss: 0.0460, Val Loss: 0.1531\n",
      "Epoch [37/50], Train Loss: 0.0455, Val Loss: 0.1507\n",
      "Epoch [38/50], Train Loss: 0.0450, Val Loss: 0.1485\n",
      "Epoch [39/50], Train Loss: 0.0443, Val Loss: 0.1463\n",
      "Epoch [40/50], Train Loss: 0.0438, Val Loss: 0.1442\n",
      "Epoch [41/50], Train Loss: 0.0429, Val Loss: 0.1422\n",
      "Epoch [42/50], Train Loss: 0.0424, Val Loss: 0.1402\n",
      "Epoch [43/50], Train Loss: 0.0417, Val Loss: 0.1383\n",
      "Epoch [44/50], Train Loss: 0.0416, Val Loss: 0.1364\n",
      "Epoch [45/50], Train Loss: 0.0410, Val Loss: 0.1346\n",
      "Epoch [46/50], Train Loss: 0.0405, Val Loss: 0.1329\n",
      "Epoch [47/50], Train Loss: 0.0405, Val Loss: 0.1312\n",
      "Epoch [48/50], Train Loss: 0.0397, Val Loss: 0.1296\n",
      "Epoch [49/50], Train Loss: 0.0392, Val Loss: 0.1280\n",
      "Epoch [50/50], Train Loss: 0.0393, Val Loss: 0.1264\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=128, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1129, Val Loss: 0.3204\n",
      "Epoch [2/50], Train Loss: 0.1086, Val Loss: 0.3115\n",
      "Epoch [3/50], Train Loss: 0.1045, Val Loss: 0.3030\n",
      "Epoch [4/50], Train Loss: 0.1006, Val Loss: 0.2948\n",
      "Epoch [5/50], Train Loss: 0.0969, Val Loss: 0.2869\n",
      "Epoch [6/50], Train Loss: 0.0934, Val Loss: 0.2793\n",
      "Epoch [7/50], Train Loss: 0.0900, Val Loss: 0.2720\n",
      "Epoch [8/50], Train Loss: 0.0868, Val Loss: 0.2649\n",
      "Epoch [9/50], Train Loss: 0.0838, Val Loss: 0.2581\n",
      "Epoch [10/50], Train Loss: 0.0809, Val Loss: 0.2515\n",
      "Epoch [11/50], Train Loss: 0.0782, Val Loss: 0.2452\n",
      "Epoch [12/50], Train Loss: 0.0756, Val Loss: 0.2391\n",
      "Epoch [13/50], Train Loss: 0.0731, Val Loss: 0.2332\n",
      "Epoch [14/50], Train Loss: 0.0708, Val Loss: 0.2275\n",
      "Epoch [15/50], Train Loss: 0.0686, Val Loss: 0.2220\n",
      "Epoch [16/50], Train Loss: 0.0665, Val Loss: 0.2167\n",
      "Epoch [17/50], Train Loss: 0.0645, Val Loss: 0.2116\n",
      "Epoch [18/50], Train Loss: 0.0626, Val Loss: 0.2067\n",
      "Epoch [19/50], Train Loss: 0.0608, Val Loss: 0.2020\n",
      "Epoch [20/50], Train Loss: 0.0591, Val Loss: 0.1974\n",
      "Epoch [21/50], Train Loss: 0.0575, Val Loss: 0.1930\n",
      "Epoch [22/50], Train Loss: 0.0560, Val Loss: 0.1887\n",
      "Epoch [23/50], Train Loss: 0.0545, Val Loss: 0.1846\n",
      "Epoch [24/50], Train Loss: 0.0532, Val Loss: 0.1807\n",
      "Epoch [25/50], Train Loss: 0.0519, Val Loss: 0.1769\n",
      "Epoch [26/50], Train Loss: 0.0507, Val Loss: 0.1732\n",
      "Epoch [27/50], Train Loss: 0.0495, Val Loss: 0.1697\n",
      "Epoch [28/50], Train Loss: 0.0484, Val Loss: 0.1663\n",
      "Epoch [29/50], Train Loss: 0.0474, Val Loss: 0.1630\n",
      "Epoch [30/50], Train Loss: 0.0464, Val Loss: 0.1599\n",
      "Epoch [31/50], Train Loss: 0.0455, Val Loss: 0.1568\n",
      "Epoch [32/50], Train Loss: 0.0447, Val Loss: 0.1539\n",
      "Epoch [33/50], Train Loss: 0.0439, Val Loss: 0.1511\n",
      "Epoch [34/50], Train Loss: 0.0431, Val Loss: 0.1484\n",
      "Epoch [35/50], Train Loss: 0.0424, Val Loss: 0.1458\n",
      "Epoch [36/50], Train Loss: 0.0417, Val Loss: 0.1432\n",
      "Epoch [37/50], Train Loss: 0.0411, Val Loss: 0.1408\n",
      "Epoch [38/50], Train Loss: 0.0405, Val Loss: 0.1385\n",
      "Epoch [39/50], Train Loss: 0.0399, Val Loss: 0.1362\n",
      "Epoch [40/50], Train Loss: 0.0394, Val Loss: 0.1341\n",
      "Epoch [41/50], Train Loss: 0.0389, Val Loss: 0.1320\n",
      "Epoch [42/50], Train Loss: 0.0384, Val Loss: 0.1300\n",
      "Epoch [43/50], Train Loss: 0.0380, Val Loss: 0.1281\n",
      "Epoch [44/50], Train Loss: 0.0376, Val Loss: 0.1262\n",
      "Epoch [45/50], Train Loss: 0.0372, Val Loss: 0.1244\n",
      "Epoch [46/50], Train Loss: 0.0368, Val Loss: 0.1227\n",
      "Epoch [47/50], Train Loss: 0.0365, Val Loss: 0.1211\n",
      "Epoch [48/50], Train Loss: 0.0362, Val Loss: 0.1195\n",
      "Epoch [49/50], Train Loss: 0.0359, Val Loss: 0.1180\n",
      "Epoch [50/50], Train Loss: 0.0356, Val Loss: 0.1165\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=128, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1664, Val Loss: 0.4078\n",
      "Epoch [2/50], Train Loss: 0.1591, Val Loss: 0.3943\n",
      "Epoch [3/50], Train Loss: 0.1524, Val Loss: 0.3814\n",
      "Epoch [4/50], Train Loss: 0.1452, Val Loss: 0.3690\n",
      "Epoch [5/50], Train Loss: 0.1388, Val Loss: 0.3572\n",
      "Epoch [6/50], Train Loss: 0.1331, Val Loss: 0.3459\n",
      "Epoch [7/50], Train Loss: 0.1274, Val Loss: 0.3351\n",
      "Epoch [8/50], Train Loss: 0.1225, Val Loss: 0.3247\n",
      "Epoch [9/50], Train Loss: 0.1173, Val Loss: 0.3148\n",
      "Epoch [10/50], Train Loss: 0.1129, Val Loss: 0.3053\n",
      "Epoch [11/50], Train Loss: 0.1084, Val Loss: 0.2961\n",
      "Epoch [12/50], Train Loss: 0.1042, Val Loss: 0.2874\n",
      "Epoch [13/50], Train Loss: 0.1000, Val Loss: 0.2789\n",
      "Epoch [14/50], Train Loss: 0.0961, Val Loss: 0.2708\n",
      "Epoch [15/50], Train Loss: 0.0924, Val Loss: 0.2630\n",
      "Epoch [16/50], Train Loss: 0.0887, Val Loss: 0.2556\n",
      "Epoch [17/50], Train Loss: 0.0854, Val Loss: 0.2484\n",
      "Epoch [18/50], Train Loss: 0.0815, Val Loss: 0.2415\n",
      "Epoch [19/50], Train Loss: 0.0794, Val Loss: 0.2348\n",
      "Epoch [20/50], Train Loss: 0.0767, Val Loss: 0.2284\n",
      "Epoch [21/50], Train Loss: 0.0742, Val Loss: 0.2223\n",
      "Epoch [22/50], Train Loss: 0.0715, Val Loss: 0.2164\n",
      "Epoch [23/50], Train Loss: 0.0690, Val Loss: 0.2107\n",
      "Epoch [24/50], Train Loss: 0.0671, Val Loss: 0.2053\n",
      "Epoch [25/50], Train Loss: 0.0646, Val Loss: 0.2000\n",
      "Epoch [26/50], Train Loss: 0.0634, Val Loss: 0.1950\n",
      "Epoch [27/50], Train Loss: 0.0609, Val Loss: 0.1902\n",
      "Epoch [28/50], Train Loss: 0.0591, Val Loss: 0.1855\n",
      "Epoch [29/50], Train Loss: 0.0579, Val Loss: 0.1810\n",
      "Epoch [30/50], Train Loss: 0.0561, Val Loss: 0.1767\n",
      "Epoch [31/50], Train Loss: 0.0548, Val Loss: 0.1726\n",
      "Epoch [32/50], Train Loss: 0.0531, Val Loss: 0.1686\n",
      "Epoch [33/50], Train Loss: 0.0523, Val Loss: 0.1648\n",
      "Epoch [34/50], Train Loss: 0.0509, Val Loss: 0.1612\n",
      "Epoch [35/50], Train Loss: 0.0495, Val Loss: 0.1576\n",
      "Epoch [36/50], Train Loss: 0.0478, Val Loss: 0.1543\n",
      "Epoch [37/50], Train Loss: 0.0470, Val Loss: 0.1510\n",
      "Epoch [38/50], Train Loss: 0.0465, Val Loss: 0.1479\n",
      "Epoch [39/50], Train Loss: 0.0454, Val Loss: 0.1449\n",
      "Epoch [40/50], Train Loss: 0.0440, Val Loss: 0.1420\n",
      "Epoch [41/50], Train Loss: 0.0436, Val Loss: 0.1392\n",
      "Epoch [42/50], Train Loss: 0.0429, Val Loss: 0.1366\n",
      "Epoch [43/50], Train Loss: 0.0416, Val Loss: 0.1340\n",
      "Epoch [44/50], Train Loss: 0.0415, Val Loss: 0.1316\n",
      "Epoch [45/50], Train Loss: 0.0410, Val Loss: 0.1293\n",
      "Epoch [46/50], Train Loss: 0.0399, Val Loss: 0.1270\n",
      "Epoch [47/50], Train Loss: 0.0392, Val Loss: 0.1249\n",
      "Epoch [48/50], Train Loss: 0.0391, Val Loss: 0.1228\n",
      "Epoch [49/50], Train Loss: 0.0385, Val Loss: 0.1208\n",
      "Epoch [50/50], Train Loss: 0.0382, Val Loss: 0.1189\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=128, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1431, Val Loss: 0.3972\n",
      "Epoch [2/50], Train Loss: 0.1349, Val Loss: 0.3851\n",
      "Epoch [3/50], Train Loss: 0.1304, Val Loss: 0.3734\n",
      "Epoch [4/50], Train Loss: 0.1247, Val Loss: 0.3623\n",
      "Epoch [5/50], Train Loss: 0.1185, Val Loss: 0.3517\n",
      "Epoch [6/50], Train Loss: 0.1149, Val Loss: 0.3416\n",
      "Epoch [7/50], Train Loss: 0.1099, Val Loss: 0.3319\n",
      "Epoch [8/50], Train Loss: 0.1056, Val Loss: 0.3225\n",
      "Epoch [9/50], Train Loss: 0.1014, Val Loss: 0.3136\n",
      "Epoch [10/50], Train Loss: 0.0975, Val Loss: 0.3050\n",
      "Epoch [11/50], Train Loss: 0.0945, Val Loss: 0.2969\n",
      "Epoch [12/50], Train Loss: 0.0907, Val Loss: 0.2891\n",
      "Epoch [13/50], Train Loss: 0.0874, Val Loss: 0.2815\n",
      "Epoch [14/50], Train Loss: 0.0850, Val Loss: 0.2743\n",
      "Epoch [15/50], Train Loss: 0.0817, Val Loss: 0.2673\n",
      "Epoch [16/50], Train Loss: 0.0790, Val Loss: 0.2607\n",
      "Epoch [17/50], Train Loss: 0.0766, Val Loss: 0.2542\n",
      "Epoch [18/50], Train Loss: 0.0737, Val Loss: 0.2480\n",
      "Epoch [19/50], Train Loss: 0.0721, Val Loss: 0.2421\n",
      "Epoch [20/50], Train Loss: 0.0703, Val Loss: 0.2363\n",
      "Epoch [21/50], Train Loss: 0.0678, Val Loss: 0.2308\n",
      "Epoch [22/50], Train Loss: 0.0655, Val Loss: 0.2255\n",
      "Epoch [23/50], Train Loss: 0.0640, Val Loss: 0.2204\n",
      "Epoch [24/50], Train Loss: 0.0623, Val Loss: 0.2156\n",
      "Epoch [25/50], Train Loss: 0.0611, Val Loss: 0.2108\n",
      "Epoch [26/50], Train Loss: 0.0603, Val Loss: 0.2063\n",
      "Epoch [27/50], Train Loss: 0.0588, Val Loss: 0.2019\n",
      "Epoch [28/50], Train Loss: 0.0565, Val Loss: 0.1977\n",
      "Epoch [29/50], Train Loss: 0.0552, Val Loss: 0.1937\n",
      "Epoch [30/50], Train Loss: 0.0548, Val Loss: 0.1897\n",
      "Epoch [31/50], Train Loss: 0.0534, Val Loss: 0.1859\n",
      "Epoch [32/50], Train Loss: 0.0527, Val Loss: 0.1824\n",
      "Epoch [33/50], Train Loss: 0.0518, Val Loss: 0.1789\n",
      "Epoch [34/50], Train Loss: 0.0502, Val Loss: 0.1756\n",
      "Epoch [35/50], Train Loss: 0.0497, Val Loss: 0.1724\n",
      "Epoch [36/50], Train Loss: 0.0489, Val Loss: 0.1693\n",
      "Epoch [37/50], Train Loss: 0.0483, Val Loss: 0.1664\n",
      "Epoch [38/50], Train Loss: 0.0473, Val Loss: 0.1635\n",
      "Epoch [39/50], Train Loss: 0.0462, Val Loss: 0.1608\n",
      "Epoch [40/50], Train Loss: 0.0470, Val Loss: 0.1582\n",
      "Epoch [41/50], Train Loss: 0.0456, Val Loss: 0.1556\n",
      "Epoch [42/50], Train Loss: 0.0447, Val Loss: 0.1532\n",
      "Epoch [43/50], Train Loss: 0.0444, Val Loss: 0.1509\n",
      "Epoch [44/50], Train Loss: 0.0437, Val Loss: 0.1486\n",
      "Epoch [45/50], Train Loss: 0.0438, Val Loss: 0.1465\n",
      "Epoch [46/50], Train Loss: 0.0431, Val Loss: 0.1444\n",
      "Epoch [47/50], Train Loss: 0.0429, Val Loss: 0.1424\n",
      "Epoch [48/50], Train Loss: 0.0429, Val Loss: 0.1404\n",
      "Epoch [49/50], Train Loss: 0.0421, Val Loss: 0.1386\n",
      "Epoch [50/50], Train Loss: 0.0419, Val Loss: 0.1368\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=128, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1252, Val Loss: 0.3330\n",
      "Epoch [2/50], Train Loss: 0.1205, Val Loss: 0.3245\n",
      "Epoch [3/50], Train Loss: 0.1161, Val Loss: 0.3163\n",
      "Epoch [4/50], Train Loss: 0.1119, Val Loss: 0.3085\n",
      "Epoch [5/50], Train Loss: 0.1079, Val Loss: 0.3009\n",
      "Epoch [6/50], Train Loss: 0.1041, Val Loss: 0.2936\n",
      "Epoch [7/50], Train Loss: 0.1004, Val Loss: 0.2866\n",
      "Epoch [8/50], Train Loss: 0.0970, Val Loss: 0.2798\n",
      "Epoch [9/50], Train Loss: 0.0937, Val Loss: 0.2733\n",
      "Epoch [10/50], Train Loss: 0.0906, Val Loss: 0.2670\n",
      "Epoch [11/50], Train Loss: 0.0877, Val Loss: 0.2609\n",
      "Epoch [12/50], Train Loss: 0.0848, Val Loss: 0.2551\n",
      "Epoch [13/50], Train Loss: 0.0822, Val Loss: 0.2494\n",
      "Epoch [14/50], Train Loss: 0.0796, Val Loss: 0.2440\n",
      "Epoch [15/50], Train Loss: 0.0772, Val Loss: 0.2387\n",
      "Epoch [16/50], Train Loss: 0.0749, Val Loss: 0.2337\n",
      "Epoch [17/50], Train Loss: 0.0727, Val Loss: 0.2288\n",
      "Epoch [18/50], Train Loss: 0.0706, Val Loss: 0.2240\n",
      "Epoch [19/50], Train Loss: 0.0687, Val Loss: 0.2195\n",
      "Epoch [20/50], Train Loss: 0.0668, Val Loss: 0.2151\n",
      "Epoch [21/50], Train Loss: 0.0650, Val Loss: 0.2108\n",
      "Epoch [22/50], Train Loss: 0.0633, Val Loss: 0.2067\n",
      "Epoch [23/50], Train Loss: 0.0617, Val Loss: 0.2028\n",
      "Epoch [24/50], Train Loss: 0.0602, Val Loss: 0.1990\n",
      "Epoch [25/50], Train Loss: 0.0587, Val Loss: 0.1953\n",
      "Epoch [26/50], Train Loss: 0.0574, Val Loss: 0.1918\n",
      "Epoch [27/50], Train Loss: 0.0561, Val Loss: 0.1883\n",
      "Epoch [28/50], Train Loss: 0.0549, Val Loss: 0.1850\n",
      "Epoch [29/50], Train Loss: 0.0537, Val Loss: 0.1818\n",
      "Epoch [30/50], Train Loss: 0.0526, Val Loss: 0.1788\n",
      "Epoch [31/50], Train Loss: 0.0516, Val Loss: 0.1758\n",
      "Epoch [32/50], Train Loss: 0.0506, Val Loss: 0.1730\n",
      "Epoch [33/50], Train Loss: 0.0497, Val Loss: 0.1702\n",
      "Epoch [34/50], Train Loss: 0.0488, Val Loss: 0.1676\n",
      "Epoch [35/50], Train Loss: 0.0480, Val Loss: 0.1650\n",
      "Epoch [36/50], Train Loss: 0.0472, Val Loss: 0.1625\n",
      "Epoch [37/50], Train Loss: 0.0464, Val Loss: 0.1601\n",
      "Epoch [38/50], Train Loss: 0.0457, Val Loss: 0.1579\n",
      "Epoch [39/50], Train Loss: 0.0451, Val Loss: 0.1557\n",
      "Epoch [40/50], Train Loss: 0.0445, Val Loss: 0.1535\n",
      "Epoch [41/50], Train Loss: 0.0439, Val Loss: 0.1515\n",
      "Epoch [42/50], Train Loss: 0.0433, Val Loss: 0.1495\n",
      "Epoch [43/50], Train Loss: 0.0428, Val Loss: 0.1476\n",
      "Epoch [44/50], Train Loss: 0.0423, Val Loss: 0.1458\n",
      "Epoch [45/50], Train Loss: 0.0419, Val Loss: 0.1440\n",
      "Epoch [46/50], Train Loss: 0.0414, Val Loss: 0.1424\n",
      "Epoch [47/50], Train Loss: 0.0410, Val Loss: 0.1407\n",
      "Epoch [48/50], Train Loss: 0.0407, Val Loss: 0.1392\n",
      "Epoch [49/50], Train Loss: 0.0403, Val Loss: 0.1376\n",
      "Epoch [50/50], Train Loss: 0.0400, Val Loss: 0.1362\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=128, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1100, Val Loss: 0.3043\n",
      "Epoch [2/50], Train Loss: 0.1055, Val Loss: 0.2961\n",
      "Epoch [3/50], Train Loss: 0.1019, Val Loss: 0.2882\n",
      "Epoch [4/50], Train Loss: 0.0978, Val Loss: 0.2807\n",
      "Epoch [5/50], Train Loss: 0.0940, Val Loss: 0.2734\n",
      "Epoch [6/50], Train Loss: 0.0905, Val Loss: 0.2665\n",
      "Epoch [7/50], Train Loss: 0.0871, Val Loss: 0.2599\n",
      "Epoch [8/50], Train Loss: 0.0840, Val Loss: 0.2536\n",
      "Epoch [9/50], Train Loss: 0.0811, Val Loss: 0.2475\n",
      "Epoch [10/50], Train Loss: 0.0785, Val Loss: 0.2416\n",
      "Epoch [11/50], Train Loss: 0.0760, Val Loss: 0.2360\n",
      "Epoch [12/50], Train Loss: 0.0734, Val Loss: 0.2306\n",
      "Epoch [13/50], Train Loss: 0.0713, Val Loss: 0.2255\n",
      "Epoch [14/50], Train Loss: 0.0688, Val Loss: 0.2206\n",
      "Epoch [15/50], Train Loss: 0.0666, Val Loss: 0.2158\n",
      "Epoch [16/50], Train Loss: 0.0646, Val Loss: 0.2112\n",
      "Epoch [17/50], Train Loss: 0.0626, Val Loss: 0.2069\n",
      "Epoch [18/50], Train Loss: 0.0611, Val Loss: 0.2026\n",
      "Epoch [19/50], Train Loss: 0.0599, Val Loss: 0.1986\n",
      "Epoch [20/50], Train Loss: 0.0583, Val Loss: 0.1947\n",
      "Epoch [21/50], Train Loss: 0.0568, Val Loss: 0.1910\n",
      "Epoch [22/50], Train Loss: 0.0553, Val Loss: 0.1875\n",
      "Epoch [23/50], Train Loss: 0.0543, Val Loss: 0.1841\n",
      "Epoch [24/50], Train Loss: 0.0529, Val Loss: 0.1808\n",
      "Epoch [25/50], Train Loss: 0.0516, Val Loss: 0.1776\n",
      "Epoch [26/50], Train Loss: 0.0510, Val Loss: 0.1746\n",
      "Epoch [27/50], Train Loss: 0.0498, Val Loss: 0.1717\n",
      "Epoch [28/50], Train Loss: 0.0489, Val Loss: 0.1689\n",
      "Epoch [29/50], Train Loss: 0.0480, Val Loss: 0.1662\n",
      "Epoch [30/50], Train Loss: 0.0473, Val Loss: 0.1636\n",
      "Epoch [31/50], Train Loss: 0.0465, Val Loss: 0.1611\n",
      "Epoch [32/50], Train Loss: 0.0458, Val Loss: 0.1588\n",
      "Epoch [33/50], Train Loss: 0.0450, Val Loss: 0.1565\n",
      "Epoch [34/50], Train Loss: 0.0450, Val Loss: 0.1543\n",
      "Epoch [35/50], Train Loss: 0.0441, Val Loss: 0.1522\n",
      "Epoch [36/50], Train Loss: 0.0435, Val Loss: 0.1502\n",
      "Epoch [37/50], Train Loss: 0.0433, Val Loss: 0.1483\n",
      "Epoch [38/50], Train Loss: 0.0425, Val Loss: 0.1465\n",
      "Epoch [39/50], Train Loss: 0.0423, Val Loss: 0.1447\n",
      "Epoch [40/50], Train Loss: 0.0417, Val Loss: 0.1430\n",
      "Epoch [41/50], Train Loss: 0.0413, Val Loss: 0.1414\n",
      "Epoch [42/50], Train Loss: 0.0410, Val Loss: 0.1398\n",
      "Epoch [43/50], Train Loss: 0.0409, Val Loss: 0.1383\n",
      "Epoch [44/50], Train Loss: 0.0406, Val Loss: 0.1369\n",
      "Epoch [45/50], Train Loss: 0.0400, Val Loss: 0.1355\n",
      "Epoch [46/50], Train Loss: 0.0398, Val Loss: 0.1342\n",
      "Epoch [47/50], Train Loss: 0.0399, Val Loss: 0.1329\n",
      "Epoch [48/50], Train Loss: 0.0393, Val Loss: 0.1317\n",
      "Epoch [49/50], Train Loss: 0.0390, Val Loss: 0.1305\n",
      "Epoch [50/50], Train Loss: 0.0393, Val Loss: 0.1294\n",
      "Testing parameters: lr=0.001, optimizer=sgd, hidden_size=128, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1432, Val Loss: 0.3890\n",
      "Epoch [2/50], Train Loss: 0.1370, Val Loss: 0.3770\n",
      "Epoch [3/50], Train Loss: 0.1321, Val Loss: 0.3654\n",
      "Epoch [4/50], Train Loss: 0.1259, Val Loss: 0.3544\n",
      "Epoch [5/50], Train Loss: 0.1213, Val Loss: 0.3438\n",
      "Epoch [6/50], Train Loss: 0.1163, Val Loss: 0.3337\n",
      "Epoch [7/50], Train Loss: 0.1114, Val Loss: 0.3241\n",
      "Epoch [8/50], Train Loss: 0.1067, Val Loss: 0.3148\n",
      "Epoch [9/50], Train Loss: 0.1022, Val Loss: 0.3061\n",
      "Epoch [10/50], Train Loss: 0.0989, Val Loss: 0.2976\n",
      "Epoch [11/50], Train Loss: 0.0942, Val Loss: 0.2895\n",
      "Epoch [12/50], Train Loss: 0.0914, Val Loss: 0.2818\n",
      "Epoch [13/50], Train Loss: 0.0884, Val Loss: 0.2743\n",
      "Epoch [14/50], Train Loss: 0.0856, Val Loss: 0.2672\n",
      "Epoch [15/50], Train Loss: 0.0823, Val Loss: 0.2604\n",
      "Epoch [16/50], Train Loss: 0.0779, Val Loss: 0.2538\n",
      "Epoch [17/50], Train Loss: 0.0773, Val Loss: 0.2475\n",
      "Epoch [18/50], Train Loss: 0.0745, Val Loss: 0.2415\n",
      "Epoch [19/50], Train Loss: 0.0722, Val Loss: 0.2357\n",
      "Epoch [20/50], Train Loss: 0.0699, Val Loss: 0.2301\n",
      "Epoch [21/50], Train Loss: 0.0681, Val Loss: 0.2248\n",
      "Epoch [22/50], Train Loss: 0.0664, Val Loss: 0.2196\n",
      "Epoch [23/50], Train Loss: 0.0645, Val Loss: 0.2147\n",
      "Epoch [24/50], Train Loss: 0.0633, Val Loss: 0.2099\n",
      "Epoch [25/50], Train Loss: 0.0615, Val Loss: 0.2053\n",
      "Epoch [26/50], Train Loss: 0.0603, Val Loss: 0.2010\n",
      "Epoch [27/50], Train Loss: 0.0585, Val Loss: 0.1968\n",
      "Epoch [28/50], Train Loss: 0.0568, Val Loss: 0.1928\n",
      "Epoch [29/50], Train Loss: 0.0557, Val Loss: 0.1890\n",
      "Epoch [30/50], Train Loss: 0.0551, Val Loss: 0.1853\n",
      "Epoch [31/50], Train Loss: 0.0549, Val Loss: 0.1817\n",
      "Epoch [32/50], Train Loss: 0.0528, Val Loss: 0.1783\n",
      "Epoch [33/50], Train Loss: 0.0514, Val Loss: 0.1750\n",
      "Epoch [34/50], Train Loss: 0.0512, Val Loss: 0.1719\n",
      "Epoch [35/50], Train Loss: 0.0501, Val Loss: 0.1688\n",
      "Epoch [36/50], Train Loss: 0.0503, Val Loss: 0.1660\n",
      "Epoch [37/50], Train Loss: 0.0478, Val Loss: 0.1632\n",
      "Epoch [38/50], Train Loss: 0.0477, Val Loss: 0.1606\n",
      "Epoch [39/50], Train Loss: 0.0469, Val Loss: 0.1580\n",
      "Epoch [40/50], Train Loss: 0.0465, Val Loss: 0.1556\n",
      "Epoch [41/50], Train Loss: 0.0467, Val Loss: 0.1533\n",
      "Epoch [42/50], Train Loss: 0.0456, Val Loss: 0.1510\n",
      "Epoch [43/50], Train Loss: 0.0450, Val Loss: 0.1489\n",
      "Epoch [44/50], Train Loss: 0.0446, Val Loss: 0.1468\n",
      "Epoch [45/50], Train Loss: 0.0450, Val Loss: 0.1448\n",
      "Epoch [46/50], Train Loss: 0.0441, Val Loss: 0.1429\n",
      "Epoch [47/50], Train Loss: 0.0445, Val Loss: 0.1412\n",
      "Epoch [48/50], Train Loss: 0.0436, Val Loss: 0.1394\n",
      "Epoch [49/50], Train Loss: 0.0431, Val Loss: 0.1378\n",
      "Epoch [50/50], Train Loss: 0.0415, Val Loss: 0.1362\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=16, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0653, Val Loss: 0.1659\n",
      "Epoch [2/50], Train Loss: 0.0278, Val Loss: 0.0846\n",
      "Epoch [3/50], Train Loss: 0.0228, Val Loss: 0.0578\n",
      "Epoch [4/50], Train Loss: 0.0219, Val Loss: 0.0493\n",
      "Epoch [5/50], Train Loss: 0.0194, Val Loss: 0.0421\n",
      "Epoch [6/50], Train Loss: 0.0166, Val Loss: 0.0342\n",
      "Epoch [7/50], Train Loss: 0.0136, Val Loss: 0.0261\n",
      "Epoch [8/50], Train Loss: 0.0104, Val Loss: 0.0181\n",
      "Epoch [9/50], Train Loss: 0.0072, Val Loss: 0.0113\n",
      "Epoch [10/50], Train Loss: 0.0046, Val Loss: 0.0072\n",
      "Epoch [11/50], Train Loss: 0.0033, Val Loss: 0.0054\n",
      "Epoch [12/50], Train Loss: 0.0026, Val Loss: 0.0048\n",
      "Epoch [13/50], Train Loss: 0.0022, Val Loss: 0.0044\n",
      "Epoch [14/50], Train Loss: 0.0020, Val Loss: 0.0041\n",
      "Epoch [15/50], Train Loss: 0.0018, Val Loss: 0.0039\n",
      "Epoch [16/50], Train Loss: 0.0018, Val Loss: 0.0037\n",
      "Epoch [17/50], Train Loss: 0.0017, Val Loss: 0.0036\n",
      "Epoch [18/50], Train Loss: 0.0017, Val Loss: 0.0035\n",
      "Epoch [19/50], Train Loss: 0.0017, Val Loss: 0.0034\n",
      "Epoch [20/50], Train Loss: 0.0017, Val Loss: 0.0033\n",
      "Epoch [21/50], Train Loss: 0.0016, Val Loss: 0.0033\n",
      "Epoch [22/50], Train Loss: 0.0016, Val Loss: 0.0032\n",
      "Epoch [23/50], Train Loss: 0.0016, Val Loss: 0.0032\n",
      "Epoch [24/50], Train Loss: 0.0016, Val Loss: 0.0031\n",
      "Epoch [25/50], Train Loss: 0.0016, Val Loss: 0.0031\n",
      "Epoch [26/50], Train Loss: 0.0016, Val Loss: 0.0030\n",
      "Epoch [27/50], Train Loss: 0.0016, Val Loss: 0.0030\n",
      "Epoch [28/50], Train Loss: 0.0016, Val Loss: 0.0029\n",
      "Epoch [29/50], Train Loss: 0.0016, Val Loss: 0.0029\n",
      "Epoch [30/50], Train Loss: 0.0015, Val Loss: 0.0028\n",
      "Epoch [31/50], Train Loss: 0.0015, Val Loss: 0.0028\n",
      "Epoch [32/50], Train Loss: 0.0015, Val Loss: 0.0027\n",
      "Epoch [33/50], Train Loss: 0.0015, Val Loss: 0.0027\n",
      "Epoch [34/50], Train Loss: 0.0015, Val Loss: 0.0026\n",
      "Epoch [35/50], Train Loss: 0.0015, Val Loss: 0.0026\n",
      "Epoch [36/50], Train Loss: 0.0015, Val Loss: 0.0026\n",
      "Epoch [37/50], Train Loss: 0.0015, Val Loss: 0.0025\n",
      "Epoch [38/50], Train Loss: 0.0015, Val Loss: 0.0025\n",
      "Epoch [39/50], Train Loss: 0.0015, Val Loss: 0.0024\n",
      "Epoch [40/50], Train Loss: 0.0015, Val Loss: 0.0024\n",
      "Epoch [41/50], Train Loss: 0.0014, Val Loss: 0.0024\n",
      "Epoch [42/50], Train Loss: 0.0014, Val Loss: 0.0023\n",
      "Epoch [43/50], Train Loss: 0.0014, Val Loss: 0.0022\n",
      "Epoch [44/50], Train Loss: 0.0014, Val Loss: 0.0022\n",
      "Epoch [45/50], Train Loss: 0.0014, Val Loss: 0.0023\n",
      "Epoch [46/50], Train Loss: 0.0014, Val Loss: 0.0023\n",
      "Epoch [47/50], Train Loss: 0.0014, Val Loss: 0.0021\n",
      "Epoch [48/50], Train Loss: 0.0014, Val Loss: 0.0019\n",
      "Epoch [49/50], Train Loss: 0.0014, Val Loss: 0.0022\n",
      "Epoch [50/50], Train Loss: 0.0014, Val Loss: 0.0025\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=16, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1041, Val Loss: 0.1885\n",
      "Epoch [2/50], Train Loss: 0.0559, Val Loss: 0.1112\n",
      "Epoch [3/50], Train Loss: 0.0403, Val Loss: 0.0782\n",
      "Epoch [4/50], Train Loss: 0.0354, Val Loss: 0.0683\n",
      "Epoch [5/50], Train Loss: 0.0327, Val Loss: 0.0590\n",
      "Epoch [6/50], Train Loss: 0.0274, Val Loss: 0.0478\n",
      "Epoch [7/50], Train Loss: 0.0257, Val Loss: 0.0389\n",
      "Epoch [8/50], Train Loss: 0.0220, Val Loss: 0.0301\n",
      "Epoch [9/50], Train Loss: 0.0192, Val Loss: 0.0232\n",
      "Epoch [10/50], Train Loss: 0.0161, Val Loss: 0.0198\n",
      "Epoch [11/50], Train Loss: 0.0150, Val Loss: 0.0168\n",
      "Epoch [12/50], Train Loss: 0.0136, Val Loss: 0.0136\n",
      "Epoch [13/50], Train Loss: 0.0125, Val Loss: 0.0109\n",
      "Epoch [14/50], Train Loss: 0.0109, Val Loss: 0.0094\n",
      "Epoch [15/50], Train Loss: 0.0095, Val Loss: 0.0081\n",
      "Epoch [16/50], Train Loss: 0.0084, Val Loss: 0.0070\n",
      "Epoch [17/50], Train Loss: 0.0084, Val Loss: 0.0063\n",
      "Epoch [18/50], Train Loss: 0.0078, Val Loss: 0.0048\n",
      "Epoch [19/50], Train Loss: 0.0067, Val Loss: 0.0050\n",
      "Epoch [20/50], Train Loss: 0.0065, Val Loss: 0.0042\n",
      "Epoch [21/50], Train Loss: 0.0064, Val Loss: 0.0041\n",
      "Epoch [22/50], Train Loss: 0.0059, Val Loss: 0.0034\n",
      "Epoch [23/50], Train Loss: 0.0061, Val Loss: 0.0041\n",
      "Epoch [24/50], Train Loss: 0.0059, Val Loss: 0.0038\n",
      "Epoch [25/50], Train Loss: 0.0057, Val Loss: 0.0038\n",
      "Epoch [26/50], Train Loss: 0.0058, Val Loss: 0.0038\n",
      "Epoch [27/50], Train Loss: 0.0059, Val Loss: 0.0033\n",
      "Epoch [28/50], Train Loss: 0.0054, Val Loss: 0.0033\n",
      "Epoch [29/50], Train Loss: 0.0053, Val Loss: 0.0033\n",
      "Epoch [30/50], Train Loss: 0.0056, Val Loss: 0.0033\n",
      "Epoch [31/50], Train Loss: 0.0051, Val Loss: 0.0038\n",
      "Epoch [32/50], Train Loss: 0.0048, Val Loss: 0.0036\n",
      "Epoch [33/50], Train Loss: 0.0049, Val Loss: 0.0034\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=16, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1164, Val Loss: 0.1559\n",
      "Epoch [2/50], Train Loss: 0.0669, Val Loss: 0.0732\n",
      "Epoch [3/50], Train Loss: 0.0587, Val Loss: 0.0601\n",
      "Epoch [4/50], Train Loss: 0.0530, Val Loss: 0.0506\n",
      "Epoch [5/50], Train Loss: 0.0460, Val Loss: 0.0386\n",
      "Epoch [6/50], Train Loss: 0.0446, Val Loss: 0.0344\n",
      "Epoch [7/50], Train Loss: 0.0386, Val Loss: 0.0257\n",
      "Epoch [8/50], Train Loss: 0.0353, Val Loss: 0.0232\n",
      "Epoch [9/50], Train Loss: 0.0348, Val Loss: 0.0182\n",
      "Epoch [10/50], Train Loss: 0.0304, Val Loss: 0.0153\n",
      "Epoch [11/50], Train Loss: 0.0289, Val Loss: 0.0135\n",
      "Epoch [12/50], Train Loss: 0.0266, Val Loss: 0.0140\n",
      "Epoch [13/50], Train Loss: 0.0249, Val Loss: 0.0083\n",
      "Epoch [14/50], Train Loss: 0.0227, Val Loss: 0.0103\n",
      "Epoch [15/50], Train Loss: 0.0206, Val Loss: 0.0083\n",
      "Epoch [16/50], Train Loss: 0.0210, Val Loss: 0.0065\n",
      "Epoch [17/50], Train Loss: 0.0205, Val Loss: 0.0053\n",
      "Epoch [18/50], Train Loss: 0.0203, Val Loss: 0.0054\n",
      "Epoch [19/50], Train Loss: 0.0182, Val Loss: 0.0046\n",
      "Epoch [20/50], Train Loss: 0.0177, Val Loss: 0.0082\n",
      "Epoch [21/50], Train Loss: 0.0164, Val Loss: 0.0074\n",
      "Epoch [22/50], Train Loss: 0.0162, Val Loss: 0.0047\n",
      "Epoch [23/50], Train Loss: 0.0155, Val Loss: 0.0052\n",
      "Epoch [24/50], Train Loss: 0.0165, Val Loss: 0.0051\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=16, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1154, Val Loss: 0.2018\n",
      "Epoch [2/50], Train Loss: 0.0456, Val Loss: 0.0773\n",
      "Epoch [3/50], Train Loss: 0.0357, Val Loss: 0.0591\n",
      "Epoch [4/50], Train Loss: 0.0327, Val Loss: 0.0510\n",
      "Epoch [5/50], Train Loss: 0.0282, Val Loss: 0.0382\n",
      "Epoch [6/50], Train Loss: 0.0235, Val Loss: 0.0255\n",
      "Epoch [7/50], Train Loss: 0.0180, Val Loss: 0.0153\n",
      "Epoch [8/50], Train Loss: 0.0127, Val Loss: 0.0107\n",
      "Epoch [9/50], Train Loss: 0.0096, Val Loss: 0.0108\n",
      "Epoch [10/50], Train Loss: 0.0073, Val Loss: 0.0103\n",
      "Epoch [11/50], Train Loss: 0.0050, Val Loss: 0.0079\n",
      "Epoch [12/50], Train Loss: 0.0033, Val Loss: 0.0052\n",
      "Epoch [13/50], Train Loss: 0.0025, Val Loss: 0.0049\n",
      "Epoch [14/50], Train Loss: 0.0026, Val Loss: 0.0093\n",
      "Epoch [15/50], Train Loss: 0.0024, Val Loss: 0.0089\n",
      "Epoch [16/50], Train Loss: 0.0022, Val Loss: 0.0044\n",
      "Epoch [17/50], Train Loss: 0.0023, Val Loss: 0.0034\n",
      "Epoch [18/50], Train Loss: 0.0024, Val Loss: 0.0078\n",
      "Epoch [19/50], Train Loss: 0.0026, Val Loss: 0.0108\n",
      "Epoch [20/50], Train Loss: 0.0022, Val Loss: 0.0047\n",
      "Epoch [21/50], Train Loss: 0.0027, Val Loss: 0.0030\n",
      "Epoch [22/50], Train Loss: 0.0023, Val Loss: 0.0054\n",
      "Epoch [23/50], Train Loss: 0.0028, Val Loss: 0.0125\n",
      "Epoch [24/50], Train Loss: 0.0023, Val Loss: 0.0063\n",
      "Epoch [25/50], Train Loss: 0.0030, Val Loss: 0.0029\n",
      "Epoch [26/50], Train Loss: 0.0023, Val Loss: 0.0036\n",
      "Epoch [27/50], Train Loss: 0.0030, Val Loss: 0.0135\n",
      "Epoch [28/50], Train Loss: 0.0028, Val Loss: 0.0080\n",
      "Epoch [29/50], Train Loss: 0.0033, Val Loss: 0.0028\n",
      "Epoch [30/50], Train Loss: 0.0027, Val Loss: 0.0036\n",
      "Epoch [31/50], Train Loss: 0.0033, Val Loss: 0.0139\n",
      "Epoch [32/50], Train Loss: 0.0034, Val Loss: 0.0081\n",
      "Epoch [33/50], Train Loss: 0.0035, Val Loss: 0.0030\n",
      "Epoch [34/50], Train Loss: 0.0029, Val Loss: 0.0045\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=16, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1653, Val Loss: 0.2268\n",
      "Epoch [2/50], Train Loss: 0.0559, Val Loss: 0.0838\n",
      "Epoch [3/50], Train Loss: 0.0411, Val Loss: 0.0605\n",
      "Epoch [4/50], Train Loss: 0.0375, Val Loss: 0.0504\n",
      "Epoch [5/50], Train Loss: 0.0325, Val Loss: 0.0410\n",
      "Epoch [6/50], Train Loss: 0.0259, Val Loss: 0.0313\n",
      "Epoch [7/50], Train Loss: 0.0207, Val Loss: 0.0227\n",
      "Epoch [8/50], Train Loss: 0.0167, Val Loss: 0.0147\n",
      "Epoch [9/50], Train Loss: 0.0136, Val Loss: 0.0112\n",
      "Epoch [10/50], Train Loss: 0.0122, Val Loss: 0.0093\n",
      "Epoch [11/50], Train Loss: 0.0117, Val Loss: 0.0070\n",
      "Epoch [12/50], Train Loss: 0.0112, Val Loss: 0.0059\n",
      "Epoch [13/50], Train Loss: 0.0103, Val Loss: 0.0084\n",
      "Epoch [14/50], Train Loss: 0.0103, Val Loss: 0.0075\n",
      "Epoch [15/50], Train Loss: 0.0102, Val Loss: 0.0038\n",
      "Epoch [16/50], Train Loss: 0.0100, Val Loss: 0.0040\n",
      "Epoch [17/50], Train Loss: 0.0094, Val Loss: 0.0075\n",
      "Epoch [18/50], Train Loss: 0.0095, Val Loss: 0.0077\n",
      "Epoch [19/50], Train Loss: 0.0088, Val Loss: 0.0035\n",
      "Epoch [20/50], Train Loss: 0.0097, Val Loss: 0.0033\n",
      "Epoch [21/50], Train Loss: 0.0086, Val Loss: 0.0088\n",
      "Epoch [22/50], Train Loss: 0.0083, Val Loss: 0.0059\n",
      "Epoch [23/50], Train Loss: 0.0091, Val Loss: 0.0035\n",
      "Epoch [24/50], Train Loss: 0.0089, Val Loss: 0.0038\n",
      "Epoch [25/50], Train Loss: 0.0090, Val Loss: 0.0090\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=16, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0842, Val Loss: 0.1242\n",
      "Epoch [2/50], Train Loss: 0.0594, Val Loss: 0.1004\n",
      "Epoch [3/50], Train Loss: 0.0509, Val Loss: 0.0838\n",
      "Epoch [4/50], Train Loss: 0.0455, Val Loss: 0.0715\n",
      "Epoch [5/50], Train Loss: 0.0389, Val Loss: 0.0545\n",
      "Epoch [6/50], Train Loss: 0.0347, Val Loss: 0.0428\n",
      "Epoch [7/50], Train Loss: 0.0307, Val Loss: 0.0274\n",
      "Epoch [8/50], Train Loss: 0.0247, Val Loss: 0.0197\n",
      "Epoch [9/50], Train Loss: 0.0225, Val Loss: 0.0165\n",
      "Epoch [10/50], Train Loss: 0.0204, Val Loss: 0.0168\n",
      "Epoch [11/50], Train Loss: 0.0198, Val Loss: 0.0193\n",
      "Epoch [12/50], Train Loss: 0.0198, Val Loss: 0.0159\n",
      "Epoch [13/50], Train Loss: 0.0186, Val Loss: 0.0133\n",
      "Epoch [14/50], Train Loss: 0.0172, Val Loss: 0.0141\n",
      "Epoch [15/50], Train Loss: 0.0162, Val Loss: 0.0117\n",
      "Epoch [16/50], Train Loss: 0.0161, Val Loss: 0.0141\n",
      "Epoch [17/50], Train Loss: 0.0163, Val Loss: 0.0104\n",
      "Epoch [18/50], Train Loss: 0.0166, Val Loss: 0.0106\n",
      "Epoch [19/50], Train Loss: 0.0159, Val Loss: 0.0079\n",
      "Epoch [20/50], Train Loss: 0.0149, Val Loss: 0.0116\n",
      "Epoch [21/50], Train Loss: 0.0147, Val Loss: 0.0147\n",
      "Epoch [22/50], Train Loss: 0.0142, Val Loss: 0.0078\n",
      "Epoch [23/50], Train Loss: 0.0128, Val Loss: 0.0073\n",
      "Epoch [24/50], Train Loss: 0.0137, Val Loss: 0.0131\n",
      "Epoch [25/50], Train Loss: 0.0123, Val Loss: 0.0087\n",
      "Epoch [26/50], Train Loss: 0.0131, Val Loss: 0.0064\n",
      "Epoch [27/50], Train Loss: 0.0115, Val Loss: 0.0098\n",
      "Epoch [28/50], Train Loss: 0.0129, Val Loss: 0.0150\n",
      "Epoch [29/50], Train Loss: 0.0117, Val Loss: 0.0065\n",
      "Epoch [30/50], Train Loss: 0.0114, Val Loss: 0.0044\n",
      "Epoch [31/50], Train Loss: 0.0119, Val Loss: 0.0124\n",
      "Epoch [32/50], Train Loss: 0.0107, Val Loss: 0.0066\n",
      "Epoch [33/50], Train Loss: 0.0110, Val Loss: 0.0050\n",
      "Epoch [34/50], Train Loss: 0.0106, Val Loss: 0.0146\n",
      "Epoch [35/50], Train Loss: 0.0111, Val Loss: 0.0056\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=16, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0633, Val Loss: 0.0586\n",
      "Epoch [2/50], Train Loss: 0.0468, Val Loss: 0.0682\n",
      "Epoch [3/50], Train Loss: 0.0350, Val Loss: 0.0502\n",
      "Epoch [4/50], Train Loss: 0.0303, Val Loss: 0.0324\n",
      "Epoch [5/50], Train Loss: 0.0239, Val Loss: 0.0191\n",
      "Epoch [6/50], Train Loss: 0.0189, Val Loss: 0.0178\n",
      "Epoch [7/50], Train Loss: 0.0163, Val Loss: 0.0179\n",
      "Epoch [8/50], Train Loss: 0.0137, Val Loss: 0.0182\n",
      "Epoch [9/50], Train Loss: 0.0112, Val Loss: 0.0266\n",
      "Epoch [10/50], Train Loss: 0.0082, Val Loss: 0.0310\n",
      "Epoch [11/50], Train Loss: 0.0057, Val Loss: 0.0213\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=16, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0749, Val Loss: 0.1506\n",
      "Epoch [2/50], Train Loss: 0.0435, Val Loss: 0.0908\n",
      "Epoch [3/50], Train Loss: 0.0445, Val Loss: 0.0899\n",
      "Epoch [4/50], Train Loss: 0.0398, Val Loss: 0.0806\n",
      "Epoch [5/50], Train Loss: 0.0349, Val Loss: 0.0670\n",
      "Epoch [6/50], Train Loss: 0.0286, Val Loss: 0.0451\n",
      "Epoch [7/50], Train Loss: 0.0221, Val Loss: 0.0192\n",
      "Epoch [8/50], Train Loss: 0.0165, Val Loss: 0.0090\n",
      "Epoch [9/50], Train Loss: 0.0119, Val Loss: 0.0092\n",
      "Epoch [10/50], Train Loss: 0.0096, Val Loss: 0.0079\n",
      "Epoch [11/50], Train Loss: 0.0081, Val Loss: 0.0048\n",
      "Epoch [12/50], Train Loss: 0.0083, Val Loss: 0.0064\n",
      "Epoch [13/50], Train Loss: 0.0079, Val Loss: 0.0076\n",
      "Epoch [14/50], Train Loss: 0.0080, Val Loss: 0.0046\n",
      "Epoch [15/50], Train Loss: 0.0082, Val Loss: 0.0060\n",
      "Epoch [16/50], Train Loss: 0.0073, Val Loss: 0.0047\n",
      "Epoch [17/50], Train Loss: 0.0085, Val Loss: 0.0109\n",
      "Epoch [18/50], Train Loss: 0.0081, Val Loss: 0.0072\n",
      "Epoch [19/50], Train Loss: 0.0087, Val Loss: 0.0082\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=16, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0813, Val Loss: 0.0916\n",
      "Epoch [2/50], Train Loss: 0.0624, Val Loss: 0.0796\n",
      "Epoch [3/50], Train Loss: 0.0512, Val Loss: 0.0628\n",
      "Epoch [4/50], Train Loss: 0.0447, Val Loss: 0.0346\n",
      "Epoch [5/50], Train Loss: 0.0373, Val Loss: 0.0215\n",
      "Epoch [6/50], Train Loss: 0.0340, Val Loss: 0.0271\n",
      "Epoch [7/50], Train Loss: 0.0308, Val Loss: 0.0195\n",
      "Epoch [8/50], Train Loss: 0.0264, Val Loss: 0.0134\n",
      "Epoch [9/50], Train Loss: 0.0239, Val Loss: 0.0165\n",
      "Epoch [10/50], Train Loss: 0.0228, Val Loss: 0.0211\n",
      "Epoch [11/50], Train Loss: 0.0201, Val Loss: 0.0093\n",
      "Epoch [12/50], Train Loss: 0.0226, Val Loss: 0.0033\n",
      "Epoch [13/50], Train Loss: 0.0209, Val Loss: 0.0281\n",
      "Epoch [14/50], Train Loss: 0.0195, Val Loss: 0.0208\n",
      "Epoch [15/50], Train Loss: 0.0232, Val Loss: 0.0100\n",
      "Epoch [16/50], Train Loss: 0.0189, Val Loss: 0.0124\n",
      "Epoch [17/50], Train Loss: 0.0187, Val Loss: 0.0255\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=32, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0810, Val Loss: 0.1218\n",
      "Epoch [2/50], Train Loss: 0.0259, Val Loss: 0.0343\n",
      "Epoch [3/50], Train Loss: 0.0273, Val Loss: 0.0300\n",
      "Epoch [4/50], Train Loss: 0.0190, Val Loss: 0.0153\n",
      "Epoch [5/50], Train Loss: 0.0143, Val Loss: 0.0059\n",
      "Epoch [6/50], Train Loss: 0.0094, Val Loss: 0.0021\n",
      "Epoch [7/50], Train Loss: 0.0056, Val Loss: 0.0024\n",
      "Epoch [8/50], Train Loss: 0.0041, Val Loss: 0.0034\n",
      "Epoch [9/50], Train Loss: 0.0029, Val Loss: 0.0021\n",
      "Epoch [10/50], Train Loss: 0.0023, Val Loss: 0.0019\n",
      "Epoch [11/50], Train Loss: 0.0021, Val Loss: 0.0020\n",
      "Epoch [12/50], Train Loss: 0.0019, Val Loss: 0.0021\n",
      "Epoch [13/50], Train Loss: 0.0019, Val Loss: 0.0023\n",
      "Epoch [14/50], Train Loss: 0.0018, Val Loss: 0.0017\n",
      "Epoch [15/50], Train Loss: 0.0018, Val Loss: 0.0020\n",
      "Epoch [16/50], Train Loss: 0.0017, Val Loss: 0.0016\n",
      "Epoch [17/50], Train Loss: 0.0017, Val Loss: 0.0020\n",
      "Epoch [18/50], Train Loss: 0.0017, Val Loss: 0.0019\n",
      "Epoch [19/50], Train Loss: 0.0017, Val Loss: 0.0016\n",
      "Epoch [20/50], Train Loss: 0.0017, Val Loss: 0.0016\n",
      "Epoch [21/50], Train Loss: 0.0016, Val Loss: 0.0016\n",
      "Epoch [22/50], Train Loss: 0.0016, Val Loss: 0.0017\n",
      "Epoch [23/50], Train Loss: 0.0016, Val Loss: 0.0017\n",
      "Epoch [24/50], Train Loss: 0.0016, Val Loss: 0.0016\n",
      "Epoch [25/50], Train Loss: 0.0016, Val Loss: 0.0015\n",
      "Epoch [26/50], Train Loss: 0.0016, Val Loss: 0.0015\n",
      "Epoch [27/50], Train Loss: 0.0015, Val Loss: 0.0015\n",
      "Epoch [28/50], Train Loss: 0.0015, Val Loss: 0.0016\n",
      "Epoch [29/50], Train Loss: 0.0015, Val Loss: 0.0015\n",
      "Epoch [30/50], Train Loss: 0.0015, Val Loss: 0.0015\n",
      "Epoch [31/50], Train Loss: 0.0015, Val Loss: 0.0014\n",
      "Epoch [32/50], Train Loss: 0.0015, Val Loss: 0.0015\n",
      "Epoch [33/50], Train Loss: 0.0015, Val Loss: 0.0015\n",
      "Epoch [34/50], Train Loss: 0.0015, Val Loss: 0.0014\n",
      "Epoch [35/50], Train Loss: 0.0015, Val Loss: 0.0014\n",
      "Epoch [36/50], Train Loss: 0.0015, Val Loss: 0.0014\n",
      "Epoch [37/50], Train Loss: 0.0015, Val Loss: 0.0014\n",
      "Epoch [38/50], Train Loss: 0.0015, Val Loss: 0.0014\n",
      "Epoch [39/50], Train Loss: 0.0015, Val Loss: 0.0013\n",
      "Epoch [40/50], Train Loss: 0.0014, Val Loss: 0.0013\n",
      "Epoch [41/50], Train Loss: 0.0014, Val Loss: 0.0013\n",
      "Epoch [42/50], Train Loss: 0.0015, Val Loss: 0.0013\n",
      "Epoch [43/50], Train Loss: 0.0015, Val Loss: 0.0012\n",
      "Epoch [44/50], Train Loss: 0.0015, Val Loss: 0.0012\n",
      "Epoch [45/50], Train Loss: 0.0015, Val Loss: 0.0012\n",
      "Epoch [46/50], Train Loss: 0.0015, Val Loss: 0.0011\n",
      "Epoch [47/50], Train Loss: 0.0015, Val Loss: 0.0011\n",
      "Epoch [48/50], Train Loss: 0.0015, Val Loss: 0.0011\n",
      "Epoch [49/50], Train Loss: 0.0014, Val Loss: 0.0011\n",
      "Epoch [50/50], Train Loss: 0.0014, Val Loss: 0.0010\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=32, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1360, Val Loss: 0.1247\n",
      "Epoch [2/50], Train Loss: 0.0353, Val Loss: 0.0220\n",
      "Epoch [3/50], Train Loss: 0.0318, Val Loss: 0.0210\n",
      "Epoch [4/50], Train Loss: 0.0242, Val Loss: 0.0138\n",
      "Epoch [5/50], Train Loss: 0.0213, Val Loss: 0.0099\n",
      "Epoch [6/50], Train Loss: 0.0174, Val Loss: 0.0067\n",
      "Epoch [7/50], Train Loss: 0.0147, Val Loss: 0.0050\n",
      "Epoch [8/50], Train Loss: 0.0111, Val Loss: 0.0039\n",
      "Epoch [9/50], Train Loss: 0.0096, Val Loss: 0.0039\n",
      "Epoch [10/50], Train Loss: 0.0085, Val Loss: 0.0020\n",
      "Epoch [11/50], Train Loss: 0.0078, Val Loss: 0.0020\n",
      "Epoch [12/50], Train Loss: 0.0071, Val Loss: 0.0029\n",
      "Epoch [13/50], Train Loss: 0.0075, Val Loss: 0.0020\n",
      "Epoch [14/50], Train Loss: 0.0065, Val Loss: 0.0025\n",
      "Epoch [15/50], Train Loss: 0.0065, Val Loss: 0.0037\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=32, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0769, Val Loss: 0.1383\n",
      "Epoch [2/50], Train Loss: 0.0369, Val Loss: 0.0562\n",
      "Epoch [3/50], Train Loss: 0.0357, Val Loss: 0.0417\n",
      "Epoch [4/50], Train Loss: 0.0280, Val Loss: 0.0269\n",
      "Epoch [5/50], Train Loss: 0.0226, Val Loss: 0.0143\n",
      "Epoch [6/50], Train Loss: 0.0184, Val Loss: 0.0072\n",
      "Epoch [7/50], Train Loss: 0.0144, Val Loss: 0.0047\n",
      "Epoch [8/50], Train Loss: 0.0129, Val Loss: 0.0030\n",
      "Epoch [9/50], Train Loss: 0.0113, Val Loss: 0.0047\n",
      "Epoch [10/50], Train Loss: 0.0102, Val Loss: 0.0029\n",
      "Epoch [11/50], Train Loss: 0.0099, Val Loss: 0.0034\n",
      "Epoch [12/50], Train Loss: 0.0105, Val Loss: 0.0023\n",
      "Epoch [13/50], Train Loss: 0.0096, Val Loss: 0.0080\n",
      "Epoch [14/50], Train Loss: 0.0092, Val Loss: 0.0050\n",
      "Epoch [15/50], Train Loss: 0.0094, Val Loss: 0.0018\n",
      "Epoch [16/50], Train Loss: 0.0093, Val Loss: 0.0040\n",
      "Epoch [17/50], Train Loss: 0.0088, Val Loss: 0.0068\n",
      "Epoch [18/50], Train Loss: 0.0085, Val Loss: 0.0026\n",
      "Epoch [19/50], Train Loss: 0.0083, Val Loss: 0.0024\n",
      "Epoch [20/50], Train Loss: 0.0080, Val Loss: 0.0035\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=32, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0561, Val Loss: 0.0593\n",
      "Epoch [2/50], Train Loss: 0.0414, Val Loss: 0.0520\n",
      "Epoch [3/50], Train Loss: 0.0314, Val Loss: 0.0336\n",
      "Epoch [4/50], Train Loss: 0.0254, Val Loss: 0.0147\n",
      "Epoch [5/50], Train Loss: 0.0177, Val Loss: 0.0039\n",
      "Epoch [6/50], Train Loss: 0.0103, Val Loss: 0.0034\n",
      "Epoch [7/50], Train Loss: 0.0067, Val Loss: 0.0066\n",
      "Epoch [8/50], Train Loss: 0.0039, Val Loss: 0.0072\n",
      "Epoch [9/50], Train Loss: 0.0028, Val Loss: 0.0019\n",
      "Epoch [10/50], Train Loss: 0.0034, Val Loss: 0.0048\n",
      "Epoch [11/50], Train Loss: 0.0030, Val Loss: 0.0100\n",
      "Epoch [12/50], Train Loss: 0.0025, Val Loss: 0.0047\n",
      "Epoch [13/50], Train Loss: 0.0029, Val Loss: 0.0029\n",
      "Epoch [14/50], Train Loss: 0.0031, Val Loss: 0.0097\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=32, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0372, Val Loss: 0.0154\n",
      "Epoch [2/50], Train Loss: 0.0604, Val Loss: 0.0708\n",
      "Epoch [3/50], Train Loss: 0.0295, Val Loss: 0.0280\n",
      "Epoch [4/50], Train Loss: 0.0275, Val Loss: 0.0092\n",
      "Epoch [5/50], Train Loss: 0.0195, Val Loss: 0.0043\n",
      "Epoch [6/50], Train Loss: 0.0141, Val Loss: 0.0059\n",
      "Epoch [7/50], Train Loss: 0.0102, Val Loss: 0.0158\n",
      "Epoch [8/50], Train Loss: 0.0084, Val Loss: 0.0023\n",
      "Epoch [9/50], Train Loss: 0.0093, Val Loss: 0.0022\n",
      "Epoch [10/50], Train Loss: 0.0071, Val Loss: 0.0051\n",
      "Epoch [11/50], Train Loss: 0.0078, Val Loss: 0.0135\n",
      "Epoch [12/50], Train Loss: 0.0071, Val Loss: 0.0026\n",
      "Epoch [13/50], Train Loss: 0.0085, Val Loss: 0.0022\n",
      "Epoch [14/50], Train Loss: 0.0074, Val Loss: 0.0119\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=32, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0730, Val Loss: 0.0852\n",
      "Epoch [2/50], Train Loss: 0.0462, Val Loss: 0.0589\n",
      "Epoch [3/50], Train Loss: 0.0395, Val Loss: 0.0461\n",
      "Epoch [4/50], Train Loss: 0.0306, Val Loss: 0.0223\n",
      "Epoch [5/50], Train Loss: 0.0219, Val Loss: 0.0080\n",
      "Epoch [6/50], Train Loss: 0.0156, Val Loss: 0.0105\n",
      "Epoch [7/50], Train Loss: 0.0139, Val Loss: 0.0133\n",
      "Epoch [8/50], Train Loss: 0.0132, Val Loss: 0.0070\n",
      "Epoch [9/50], Train Loss: 0.0138, Val Loss: 0.0023\n",
      "Epoch [10/50], Train Loss: 0.0126, Val Loss: 0.0147\n",
      "Epoch [11/50], Train Loss: 0.0107, Val Loss: 0.0035\n",
      "Epoch [12/50], Train Loss: 0.0117, Val Loss: 0.0024\n",
      "Epoch [13/50], Train Loss: 0.0100, Val Loss: 0.0074\n",
      "Epoch [14/50], Train Loss: 0.0107, Val Loss: 0.0140\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=32, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0333, Val Loss: 0.0373\n",
      "Epoch [2/50], Train Loss: 0.0656, Val Loss: 0.0923\n",
      "Epoch [3/50], Train Loss: 0.0317, Val Loss: 0.0479\n",
      "Epoch [4/50], Train Loss: 0.0230, Val Loss: 0.0172\n",
      "Epoch [5/50], Train Loss: 0.0120, Val Loss: 0.0159\n",
      "Epoch [6/50], Train Loss: 0.0048, Val Loss: 0.0052\n",
      "Epoch [7/50], Train Loss: 0.0038, Val Loss: 0.0041\n",
      "Epoch [8/50], Train Loss: 0.0040, Val Loss: 0.0105\n",
      "Epoch [9/50], Train Loss: 0.0031, Val Loss: 0.0049\n",
      "Epoch [10/50], Train Loss: 0.0030, Val Loss: 0.0032\n",
      "Epoch [11/50], Train Loss: 0.0031, Val Loss: 0.0124\n",
      "Epoch [12/50], Train Loss: 0.0023, Val Loss: 0.0050\n",
      "Epoch [13/50], Train Loss: 0.0027, Val Loss: 0.0030\n",
      "Epoch [14/50], Train Loss: 0.0030, Val Loss: 0.0082\n",
      "Epoch [15/50], Train Loss: 0.0025, Val Loss: 0.0066\n",
      "Epoch [16/50], Train Loss: 0.0025, Val Loss: 0.0037\n",
      "Epoch [17/50], Train Loss: 0.0023, Val Loss: 0.0059\n",
      "Epoch [18/50], Train Loss: 0.0022, Val Loss: 0.0074\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=32, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0417, Val Loss: 0.0515\n",
      "Epoch [2/50], Train Loss: 0.0627, Val Loss: 0.0978\n",
      "Epoch [3/50], Train Loss: 0.0362, Val Loss: 0.0505\n",
      "Epoch [4/50], Train Loss: 0.0357, Val Loss: 0.0307\n",
      "Epoch [5/50], Train Loss: 0.0255, Val Loss: 0.0095\n",
      "Epoch [6/50], Train Loss: 0.0169, Val Loss: 0.0069\n",
      "Epoch [7/50], Train Loss: 0.0149, Val Loss: 0.0163\n",
      "Epoch [8/50], Train Loss: 0.0093, Val Loss: 0.0052\n",
      "Epoch [9/50], Train Loss: 0.0089, Val Loss: 0.0044\n",
      "Epoch [10/50], Train Loss: 0.0071, Val Loss: 0.0142\n",
      "Epoch [11/50], Train Loss: 0.0065, Val Loss: 0.0035\n",
      "Epoch [12/50], Train Loss: 0.0065, Val Loss: 0.0017\n",
      "Epoch [13/50], Train Loss: 0.0082, Val Loss: 0.0241\n",
      "Epoch [14/50], Train Loss: 0.0085, Val Loss: 0.0022\n",
      "Epoch [15/50], Train Loss: 0.0103, Val Loss: 0.0076\n",
      "Epoch [16/50], Train Loss: 0.0100, Val Loss: 0.0249\n",
      "Epoch [17/50], Train Loss: 0.0106, Val Loss: 0.0028\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=32, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0750, Val Loss: 0.0577\n",
      "Epoch [2/50], Train Loss: 0.0751, Val Loss: 0.0902\n",
      "Epoch [3/50], Train Loss: 0.0515, Val Loss: 0.0587\n",
      "Epoch [4/50], Train Loss: 0.0424, Val Loss: 0.0288\n",
      "Epoch [5/50], Train Loss: 0.0344, Val Loss: 0.0166\n",
      "Epoch [6/50], Train Loss: 0.0287, Val Loss: 0.0157\n",
      "Epoch [7/50], Train Loss: 0.0216, Val Loss: 0.0108\n",
      "Epoch [8/50], Train Loss: 0.0200, Val Loss: 0.0172\n",
      "Epoch [9/50], Train Loss: 0.0173, Val Loss: 0.0075\n",
      "Epoch [10/50], Train Loss: 0.0183, Val Loss: 0.0039\n",
      "Epoch [11/50], Train Loss: 0.0183, Val Loss: 0.0239\n",
      "Epoch [12/50], Train Loss: 0.0181, Val Loss: 0.0024\n",
      "Epoch [13/50], Train Loss: 0.0151, Val Loss: 0.0027\n",
      "Epoch [14/50], Train Loss: 0.0151, Val Loss: 0.0219\n",
      "Epoch [15/50], Train Loss: 0.0145, Val Loss: 0.0053\n",
      "Epoch [16/50], Train Loss: 0.0151, Val Loss: 0.0039\n",
      "Epoch [17/50], Train Loss: 0.0146, Val Loss: 0.0286\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=64, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0434, Val Loss: 0.0179\n",
      "Epoch [2/50], Train Loss: 0.0486, Val Loss: 0.0474\n",
      "Epoch [3/50], Train Loss: 0.0254, Val Loss: 0.0310\n",
      "Epoch [4/50], Train Loss: 0.0193, Val Loss: 0.0164\n",
      "Epoch [5/50], Train Loss: 0.0139, Val Loss: 0.0079\n",
      "Epoch [6/50], Train Loss: 0.0074, Val Loss: 0.0026\n",
      "Epoch [7/50], Train Loss: 0.0045, Val Loss: 0.0048\n",
      "Epoch [8/50], Train Loss: 0.0024, Val Loss: 0.0017\n",
      "Epoch [9/50], Train Loss: 0.0023, Val Loss: 0.0036\n",
      "Epoch [10/50], Train Loss: 0.0027, Val Loss: 0.0038\n",
      "Epoch [11/50], Train Loss: 0.0027, Val Loss: 0.0044\n",
      "Epoch [12/50], Train Loss: 0.0027, Val Loss: 0.0048\n",
      "Epoch [13/50], Train Loss: 0.0022, Val Loss: 0.0035\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=64, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0460, Val Loss: 0.0183\n",
      "Epoch [2/50], Train Loss: 0.0451, Val Loss: 0.0494\n",
      "Epoch [3/50], Train Loss: 0.0240, Val Loss: 0.0217\n",
      "Epoch [4/50], Train Loss: 0.0193, Val Loss: 0.0082\n",
      "Epoch [5/50], Train Loss: 0.0123, Val Loss: 0.0028\n",
      "Epoch [6/50], Train Loss: 0.0074, Val Loss: 0.0028\n",
      "Epoch [7/50], Train Loss: 0.0070, Val Loss: 0.0075\n",
      "Epoch [8/50], Train Loss: 0.0050, Val Loss: 0.0018\n",
      "Epoch [9/50], Train Loss: 0.0061, Val Loss: 0.0018\n",
      "Epoch [10/50], Train Loss: 0.0055, Val Loss: 0.0040\n",
      "Epoch [11/50], Train Loss: 0.0053, Val Loss: 0.0051\n",
      "Epoch [12/50], Train Loss: 0.0050, Val Loss: 0.0022\n",
      "Epoch [13/50], Train Loss: 0.0055, Val Loss: 0.0013\n",
      "Epoch [14/50], Train Loss: 0.0041, Val Loss: 0.0057\n",
      "Epoch [15/50], Train Loss: 0.0037, Val Loss: 0.0020\n",
      "Epoch [16/50], Train Loss: 0.0046, Val Loss: 0.0015\n",
      "Epoch [17/50], Train Loss: 0.0036, Val Loss: 0.0019\n",
      "Epoch [18/50], Train Loss: 0.0038, Val Loss: 0.0054\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=64, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0643, Val Loss: 0.0136\n",
      "Epoch [2/50], Train Loss: 0.0648, Val Loss: 0.0698\n",
      "Epoch [3/50], Train Loss: 0.0327, Val Loss: 0.0288\n",
      "Epoch [4/50], Train Loss: 0.0284, Val Loss: 0.0184\n",
      "Epoch [5/50], Train Loss: 0.0221, Val Loss: 0.0119\n",
      "Epoch [6/50], Train Loss: 0.0163, Val Loss: 0.0047\n",
      "Epoch [7/50], Train Loss: 0.0112, Val Loss: 0.0032\n",
      "Epoch [8/50], Train Loss: 0.0110, Val Loss: 0.0104\n",
      "Epoch [9/50], Train Loss: 0.0083, Val Loss: 0.0023\n",
      "Epoch [10/50], Train Loss: 0.0088, Val Loss: 0.0019\n",
      "Epoch [11/50], Train Loss: 0.0081, Val Loss: 0.0061\n",
      "Epoch [12/50], Train Loss: 0.0083, Val Loss: 0.0041\n",
      "Epoch [13/50], Train Loss: 0.0076, Val Loss: 0.0018\n",
      "Epoch [14/50], Train Loss: 0.0073, Val Loss: 0.0070\n",
      "Epoch [15/50], Train Loss: 0.0071, Val Loss: 0.0036\n",
      "Epoch [16/50], Train Loss: 0.0077, Val Loss: 0.0019\n",
      "Epoch [17/50], Train Loss: 0.0073, Val Loss: 0.0014\n",
      "Epoch [18/50], Train Loss: 0.0064, Val Loss: 0.0066\n",
      "Epoch [19/50], Train Loss: 0.0061, Val Loss: 0.0021\n",
      "Epoch [20/50], Train Loss: 0.0066, Val Loss: 0.0018\n",
      "Epoch [21/50], Train Loss: 0.0056, Val Loss: 0.0018\n",
      "Epoch [22/50], Train Loss: 0.0061, Val Loss: 0.0016\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=64, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0520, Val Loss: 0.0592\n",
      "Epoch [2/50], Train Loss: 0.0626, Val Loss: 0.0331\n",
      "Epoch [3/50], Train Loss: 0.0364, Val Loss: 0.0184\n",
      "Epoch [4/50], Train Loss: 0.0204, Val Loss: 0.0090\n",
      "Epoch [5/50], Train Loss: 0.0130, Val Loss: 0.0064\n",
      "Epoch [6/50], Train Loss: 0.0073, Val Loss: 0.0066\n",
      "Epoch [7/50], Train Loss: 0.0059, Val Loss: 0.0176\n",
      "Epoch [8/50], Train Loss: 0.0053, Val Loss: 0.0101\n",
      "Epoch [9/50], Train Loss: 0.0030, Val Loss: 0.0023\n",
      "Epoch [10/50], Train Loss: 0.0030, Val Loss: 0.0069\n",
      "Epoch [11/50], Train Loss: 0.0025, Val Loss: 0.0059\n",
      "Epoch [12/50], Train Loss: 0.0031, Val Loss: 0.0039\n",
      "Epoch [13/50], Train Loss: 0.0023, Val Loss: 0.0021\n",
      "Epoch [14/50], Train Loss: 0.0029, Val Loss: 0.0103\n",
      "Epoch [15/50], Train Loss: 0.0026, Val Loss: 0.0039\n",
      "Epoch [16/50], Train Loss: 0.0035, Val Loss: 0.0055\n",
      "Epoch [17/50], Train Loss: 0.0029, Val Loss: 0.0053\n",
      "Epoch [18/50], Train Loss: 0.0026, Val Loss: 0.0087\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=64, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0402, Val Loss: 0.0592\n",
      "Epoch [2/50], Train Loss: 0.0628, Val Loss: 0.0378\n",
      "Epoch [3/50], Train Loss: 0.0373, Val Loss: 0.0206\n",
      "Epoch [4/50], Train Loss: 0.0254, Val Loss: 0.0100\n",
      "Epoch [5/50], Train Loss: 0.0158, Val Loss: 0.0076\n",
      "Epoch [6/50], Train Loss: 0.0073, Val Loss: 0.0080\n",
      "Epoch [7/50], Train Loss: 0.0066, Val Loss: 0.0065\n",
      "Epoch [8/50], Train Loss: 0.0073, Val Loss: 0.0032\n",
      "Epoch [9/50], Train Loss: 0.0061, Val Loss: 0.0031\n",
      "Epoch [10/50], Train Loss: 0.0074, Val Loss: 0.0101\n",
      "Epoch [11/50], Train Loss: 0.0074, Val Loss: 0.0021\n",
      "Epoch [12/50], Train Loss: 0.0061, Val Loss: 0.0016\n",
      "Epoch [13/50], Train Loss: 0.0061, Val Loss: 0.0088\n",
      "Epoch [14/50], Train Loss: 0.0053, Val Loss: 0.0026\n",
      "Epoch [15/50], Train Loss: 0.0054, Val Loss: 0.0022\n",
      "Epoch [16/50], Train Loss: 0.0055, Val Loss: 0.0072\n",
      "Epoch [17/50], Train Loss: 0.0044, Val Loss: 0.0037\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=64, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0377, Val Loss: 0.0625\n",
      "Epoch [2/50], Train Loss: 0.0660, Val Loss: 0.0636\n",
      "Epoch [3/50], Train Loss: 0.0341, Val Loss: 0.0133\n",
      "Epoch [4/50], Train Loss: 0.0233, Val Loss: 0.0053\n",
      "Epoch [5/50], Train Loss: 0.0148, Val Loss: 0.0099\n",
      "Epoch [6/50], Train Loss: 0.0107, Val Loss: 0.0114\n",
      "Epoch [7/50], Train Loss: 0.0122, Val Loss: 0.0104\n",
      "Epoch [8/50], Train Loss: 0.0087, Val Loss: 0.0019\n",
      "Epoch [9/50], Train Loss: 0.0084, Val Loss: 0.0074\n",
      "Epoch [10/50], Train Loss: 0.0087, Val Loss: 0.0146\n",
      "Epoch [11/50], Train Loss: 0.0090, Val Loss: 0.0052\n",
      "Epoch [12/50], Train Loss: 0.0080, Val Loss: 0.0024\n",
      "Epoch [13/50], Train Loss: 0.0083, Val Loss: 0.0138\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=64, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0260, Val Loss: 0.1024\n",
      "Epoch [2/50], Train Loss: 0.0681, Val Loss: 0.0286\n",
      "Epoch [3/50], Train Loss: 0.0557, Val Loss: 0.0333\n",
      "Epoch [4/50], Train Loss: 0.0298, Val Loss: 0.0039\n",
      "Epoch [5/50], Train Loss: 0.0184, Val Loss: 0.0135\n",
      "Epoch [6/50], Train Loss: 0.0104, Val Loss: 0.0070\n",
      "Epoch [7/50], Train Loss: 0.0096, Val Loss: 0.0115\n",
      "Epoch [8/50], Train Loss: 0.0109, Val Loss: 0.0163\n",
      "Epoch [9/50], Train Loss: 0.0146, Val Loss: 0.0089\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=64, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0300, Val Loss: 0.0911\n",
      "Epoch [2/50], Train Loss: 0.0709, Val Loss: 0.0429\n",
      "Epoch [3/50], Train Loss: 0.0441, Val Loss: 0.0120\n",
      "Epoch [4/50], Train Loss: 0.0239, Val Loss: 0.0077\n",
      "Epoch [5/50], Train Loss: 0.0212, Val Loss: 0.0165\n",
      "Epoch [6/50], Train Loss: 0.0252, Val Loss: 0.0147\n",
      "Epoch [7/50], Train Loss: 0.0179, Val Loss: 0.0454\n",
      "Epoch [8/50], Train Loss: 0.0132, Val Loss: 0.0029\n",
      "Epoch [9/50], Train Loss: 0.0088, Val Loss: 0.0045\n",
      "Epoch [10/50], Train Loss: 0.0069, Val Loss: 0.0140\n",
      "Epoch [11/50], Train Loss: 0.0064, Val Loss: 0.0038\n",
      "Epoch [12/50], Train Loss: 0.0070, Val Loss: 0.0046\n",
      "Epoch [13/50], Train Loss: 0.0058, Val Loss: 0.0143\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=64, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0385, Val Loss: 0.1105\n",
      "Epoch [2/50], Train Loss: 0.0723, Val Loss: 0.0440\n",
      "Epoch [3/50], Train Loss: 0.0617, Val Loss: 0.0336\n",
      "Epoch [4/50], Train Loss: 0.0366, Val Loss: 0.0080\n",
      "Epoch [5/50], Train Loss: 0.0244, Val Loss: 0.0097\n",
      "Epoch [6/50], Train Loss: 0.0145, Val Loss: 0.0254\n",
      "Epoch [7/50], Train Loss: 0.0144, Val Loss: 0.0048\n",
      "Epoch [8/50], Train Loss: 0.0144, Val Loss: 0.0039\n",
      "Epoch [9/50], Train Loss: 0.0122, Val Loss: 0.0197\n",
      "Epoch [10/50], Train Loss: 0.0108, Val Loss: 0.0107\n",
      "Epoch [11/50], Train Loss: 0.0164, Val Loss: 0.0020\n",
      "Epoch [12/50], Train Loss: 0.0121, Val Loss: 0.0137\n",
      "Epoch [13/50], Train Loss: 0.0102, Val Loss: 0.0114\n",
      "Epoch [14/50], Train Loss: 0.0137, Val Loss: 0.0025\n",
      "Epoch [15/50], Train Loss: 0.0121, Val Loss: 0.0267\n",
      "Epoch [16/50], Train Loss: 0.0098, Val Loss: 0.0159\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=128, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0279, Val Loss: 0.0576\n",
      "Epoch [2/50], Train Loss: 0.0540, Val Loss: 0.0175\n",
      "Epoch [3/50], Train Loss: 0.0263, Val Loss: 0.0068\n",
      "Epoch [4/50], Train Loss: 0.0140, Val Loss: 0.0018\n",
      "Epoch [5/50], Train Loss: 0.0075, Val Loss: 0.0020\n",
      "Epoch [6/50], Train Loss: 0.0043, Val Loss: 0.0066\n",
      "Epoch [7/50], Train Loss: 0.0040, Val Loss: 0.0047\n",
      "Epoch [8/50], Train Loss: 0.0037, Val Loss: 0.0021\n",
      "Epoch [9/50], Train Loss: 0.0031, Val Loss: 0.0018\n",
      "Epoch [10/50], Train Loss: 0.0039, Val Loss: 0.0110\n",
      "Epoch [11/50], Train Loss: 0.0028, Val Loss: 0.0033\n",
      "Epoch [12/50], Train Loss: 0.0051, Val Loss: 0.0020\n",
      "Epoch [13/50], Train Loss: 0.0026, Val Loss: 0.0044\n",
      "Epoch [14/50], Train Loss: 0.0021, Val Loss: 0.0033\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=128, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0281, Val Loss: 0.0302\n",
      "Epoch [2/50], Train Loss: 0.0540, Val Loss: 0.0199\n",
      "Epoch [3/50], Train Loss: 0.0267, Val Loss: 0.0107\n",
      "Epoch [4/50], Train Loss: 0.0144, Val Loss: 0.0013\n",
      "Epoch [5/50], Train Loss: 0.0069, Val Loss: 0.0068\n",
      "Epoch [6/50], Train Loss: 0.0060, Val Loss: 0.0039\n",
      "Epoch [7/50], Train Loss: 0.0042, Val Loss: 0.0028\n",
      "Epoch [8/50], Train Loss: 0.0036, Val Loss: 0.0031\n",
      "Epoch [9/50], Train Loss: 0.0031, Val Loss: 0.0045\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=128, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0376, Val Loss: 0.0678\n",
      "Epoch [2/50], Train Loss: 0.0535, Val Loss: 0.0203\n",
      "Epoch [3/50], Train Loss: 0.0287, Val Loss: 0.0105\n",
      "Epoch [4/50], Train Loss: 0.0159, Val Loss: 0.0018\n",
      "Epoch [5/50], Train Loss: 0.0096, Val Loss: 0.0025\n",
      "Epoch [6/50], Train Loss: 0.0078, Val Loss: 0.0116\n",
      "Epoch [7/50], Train Loss: 0.0058, Val Loss: 0.0017\n",
      "Epoch [8/50], Train Loss: 0.0062, Val Loss: 0.0033\n",
      "Epoch [9/50], Train Loss: 0.0055, Val Loss: 0.0057\n",
      "Epoch [10/50], Train Loss: 0.0055, Val Loss: 0.0022\n",
      "Epoch [11/50], Train Loss: 0.0060, Val Loss: 0.0049\n",
      "Epoch [12/50], Train Loss: 0.0049, Val Loss: 0.0054\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=128, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0211, Val Loss: 0.0988\n",
      "Epoch [2/50], Train Loss: 0.0593, Val Loss: 0.0133\n",
      "Epoch [3/50], Train Loss: 0.0465, Val Loss: 0.0111\n",
      "Epoch [4/50], Train Loss: 0.0113, Val Loss: 0.0097\n",
      "Epoch [5/50], Train Loss: 0.0131, Val Loss: 0.0188\n",
      "Epoch [6/50], Train Loss: 0.0075, Val Loss: 0.0026\n",
      "Epoch [7/50], Train Loss: 0.0034, Val Loss: 0.0019\n",
      "Epoch [8/50], Train Loss: 0.0036, Val Loss: 0.0134\n",
      "Epoch [9/50], Train Loss: 0.0029, Val Loss: 0.0021\n",
      "Epoch [10/50], Train Loss: 0.0034, Val Loss: 0.0044\n",
      "Epoch [11/50], Train Loss: 0.0025, Val Loss: 0.0040\n",
      "Epoch [12/50], Train Loss: 0.0026, Val Loss: 0.0046\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=128, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0228, Val Loss: 0.0734\n",
      "Epoch [2/50], Train Loss: 0.0556, Val Loss: 0.0032\n",
      "Epoch [3/50], Train Loss: 0.0315, Val Loss: 0.0101\n",
      "Epoch [4/50], Train Loss: 0.0145, Val Loss: 0.0162\n",
      "Epoch [5/50], Train Loss: 0.0086, Val Loss: 0.0080\n",
      "Epoch [6/50], Train Loss: 0.0096, Val Loss: 0.0032\n",
      "Epoch [7/50], Train Loss: 0.0060, Val Loss: 0.0015\n",
      "Epoch [8/50], Train Loss: 0.0070, Val Loss: 0.0118\n",
      "Epoch [9/50], Train Loss: 0.0071, Val Loss: 0.0023\n",
      "Epoch [10/50], Train Loss: 0.0059, Val Loss: 0.0043\n",
      "Epoch [11/50], Train Loss: 0.0060, Val Loss: 0.0094\n",
      "Epoch [12/50], Train Loss: 0.0059, Val Loss: 0.0086\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=128, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0275, Val Loss: 0.0882\n",
      "Epoch [2/50], Train Loss: 0.0647, Val Loss: 0.0108\n",
      "Epoch [3/50], Train Loss: 0.0501, Val Loss: 0.0030\n",
      "Epoch [4/50], Train Loss: 0.0238, Val Loss: 0.0078\n",
      "Epoch [5/50], Train Loss: 0.0154, Val Loss: 0.0148\n",
      "Epoch [6/50], Train Loss: 0.0129, Val Loss: 0.0158\n",
      "Epoch [7/50], Train Loss: 0.0274, Val Loss: 0.0268\n",
      "Epoch [8/50], Train Loss: 0.0180, Val Loss: 0.0124\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=128, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0184, Val Loss: 0.0847\n",
      "Epoch [2/50], Train Loss: 0.0545, Val Loss: 0.0848\n",
      "Epoch [3/50], Train Loss: 0.0469, Val Loss: 0.0122\n",
      "Epoch [4/50], Train Loss: 0.0398, Val Loss: 0.0208\n",
      "Epoch [5/50], Train Loss: 0.0171, Val Loss: 0.0145\n",
      "Epoch [6/50], Train Loss: 0.0107, Val Loss: 0.0147\n",
      "Epoch [7/50], Train Loss: 0.0237, Val Loss: 0.0307\n",
      "Epoch [8/50], Train Loss: 0.0278, Val Loss: 0.0038\n",
      "Epoch [9/50], Train Loss: 0.0283, Val Loss: 0.0370\n",
      "Epoch [10/50], Train Loss: 0.0067, Val Loss: 0.0130\n",
      "Epoch [11/50], Train Loss: 0.0042, Val Loss: 0.0042\n",
      "Epoch [12/50], Train Loss: 0.0075, Val Loss: 0.0091\n",
      "Epoch [13/50], Train Loss: 0.0137, Val Loss: 0.0095\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=128, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0201, Val Loss: 0.0723\n",
      "Epoch [2/50], Train Loss: 0.0606, Val Loss: 0.0728\n",
      "Epoch [3/50], Train Loss: 0.0485, Val Loss: 0.0126\n",
      "Epoch [4/50], Train Loss: 0.0327, Val Loss: 0.0120\n",
      "Epoch [5/50], Train Loss: 0.0171, Val Loss: 0.0063\n",
      "Epoch [6/50], Train Loss: 0.0102, Val Loss: 0.0131\n",
      "Epoch [7/50], Train Loss: 0.0187, Val Loss: 0.0183\n",
      "Epoch [8/50], Train Loss: 0.0276, Val Loss: 0.0239\n",
      "Epoch [9/50], Train Loss: 0.0081, Val Loss: 0.0176\n",
      "Epoch [10/50], Train Loss: 0.0061, Val Loss: 0.0062\n",
      "Epoch [11/50], Train Loss: 0.0099, Val Loss: 0.0058\n",
      "Epoch [12/50], Train Loss: 0.0107, Val Loss: 0.0137\n",
      "Epoch [13/50], Train Loss: 0.0061, Val Loss: 0.0177\n",
      "Epoch [14/50], Train Loss: 0.0121, Val Loss: 0.0176\n",
      "Epoch [15/50], Train Loss: 0.0103, Val Loss: 0.0044\n",
      "Epoch [16/50], Train Loss: 0.0154, Val Loss: 0.0569\n",
      "Epoch [17/50], Train Loss: 0.0133, Val Loss: 0.0048\n",
      "Epoch [18/50], Train Loss: 0.0068, Val Loss: 0.0038\n",
      "Epoch [19/50], Train Loss: 0.0104, Val Loss: 0.0073\n",
      "Epoch [20/50], Train Loss: 0.0074, Val Loss: 0.0027\n",
      "Epoch [21/50], Train Loss: 0.0074, Val Loss: 0.0201\n",
      "Epoch [22/50], Train Loss: 0.0209, Val Loss: 0.0111\n",
      "Epoch [23/50], Train Loss: 0.0065, Val Loss: 0.0066\n",
      "Epoch [24/50], Train Loss: 0.0087, Val Loss: 0.0112\n",
      "Epoch [25/50], Train Loss: 0.0200, Val Loss: 0.0054\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.001, optimizer=adamw, hidden_size=128, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0269, Val Loss: 0.0639\n",
      "Epoch [2/50], Train Loss: 0.0595, Val Loss: 0.0630\n",
      "Epoch [3/50], Train Loss: 0.0462, Val Loss: 0.0085\n",
      "Epoch [4/50], Train Loss: 0.0275, Val Loss: 0.0044\n",
      "Epoch [5/50], Train Loss: 0.0164, Val Loss: 0.0034\n",
      "Epoch [6/50], Train Loss: 0.0405, Val Loss: 0.0302\n",
      "Epoch [7/50], Train Loss: 0.0342, Val Loss: 0.0387\n",
      "Epoch [8/50], Train Loss: 0.0241, Val Loss: 0.0823\n",
      "Epoch [9/50], Train Loss: 0.0186, Val Loss: 0.0048\n",
      "Epoch [10/50], Train Loss: 0.0119, Val Loss: 0.0102\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=16, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.2052, Val Loss: 0.4236\n",
      "Epoch [2/50], Train Loss: 0.1552, Val Loss: 0.3166\n",
      "Epoch [3/50], Train Loss: 0.1092, Val Loss: 0.2063\n",
      "Epoch [4/50], Train Loss: 0.0700, Val Loss: 0.1110\n",
      "Epoch [5/50], Train Loss: 0.0459, Val Loss: 0.0550\n",
      "Epoch [6/50], Train Loss: 0.0361, Val Loss: 0.0356\n",
      "Epoch [7/50], Train Loss: 0.0314, Val Loss: 0.0312\n",
      "Epoch [8/50], Train Loss: 0.0283, Val Loss: 0.0295\n",
      "Epoch [9/50], Train Loss: 0.0263, Val Loss: 0.0280\n",
      "Epoch [10/50], Train Loss: 0.0248, Val Loss: 0.0265\n",
      "Epoch [11/50], Train Loss: 0.0237, Val Loss: 0.0249\n",
      "Epoch [12/50], Train Loss: 0.0226, Val Loss: 0.0232\n",
      "Epoch [13/50], Train Loss: 0.0217, Val Loss: 0.0215\n",
      "Epoch [14/50], Train Loss: 0.0208, Val Loss: 0.0199\n",
      "Epoch [15/50], Train Loss: 0.0200, Val Loss: 0.0184\n",
      "Epoch [16/50], Train Loss: 0.0192, Val Loss: 0.0171\n",
      "Epoch [17/50], Train Loss: 0.0185, Val Loss: 0.0158\n",
      "Epoch [18/50], Train Loss: 0.0179, Val Loss: 0.0147\n",
      "Epoch [19/50], Train Loss: 0.0172, Val Loss: 0.0137\n",
      "Epoch [20/50], Train Loss: 0.0166, Val Loss: 0.0128\n",
      "Epoch [21/50], Train Loss: 0.0160, Val Loss: 0.0120\n",
      "Epoch [22/50], Train Loss: 0.0154, Val Loss: 0.0113\n",
      "Epoch [23/50], Train Loss: 0.0148, Val Loss: 0.0106\n",
      "Epoch [24/50], Train Loss: 0.0141, Val Loss: 0.0100\n",
      "Epoch [25/50], Train Loss: 0.0134, Val Loss: 0.0095\n",
      "Epoch [26/50], Train Loss: 0.0127, Val Loss: 0.0090\n",
      "Epoch [27/50], Train Loss: 0.0119, Val Loss: 0.0085\n",
      "Epoch [28/50], Train Loss: 0.0110, Val Loss: 0.0080\n",
      "Epoch [29/50], Train Loss: 0.0101, Val Loss: 0.0076\n",
      "Epoch [30/50], Train Loss: 0.0091, Val Loss: 0.0072\n",
      "Epoch [31/50], Train Loss: 0.0080, Val Loss: 0.0068\n",
      "Epoch [32/50], Train Loss: 0.0069, Val Loss: 0.0065\n",
      "Epoch [33/50], Train Loss: 0.0058, Val Loss: 0.0062\n",
      "Epoch [34/50], Train Loss: 0.0048, Val Loss: 0.0061\n",
      "Epoch [35/50], Train Loss: 0.0040, Val Loss: 0.0061\n",
      "Epoch [36/50], Train Loss: 0.0035, Val Loss: 0.0061\n",
      "Epoch [37/50], Train Loss: 0.0031, Val Loss: 0.0060\n",
      "Epoch [38/50], Train Loss: 0.0028, Val Loss: 0.0058\n",
      "Epoch [39/50], Train Loss: 0.0026, Val Loss: 0.0056\n",
      "Epoch [40/50], Train Loss: 0.0025, Val Loss: 0.0054\n",
      "Epoch [41/50], Train Loss: 0.0024, Val Loss: 0.0052\n",
      "Epoch [42/50], Train Loss: 0.0023, Val Loss: 0.0050\n",
      "Epoch [43/50], Train Loss: 0.0022, Val Loss: 0.0048\n",
      "Epoch [44/50], Train Loss: 0.0021, Val Loss: 0.0047\n",
      "Epoch [45/50], Train Loss: 0.0021, Val Loss: 0.0046\n",
      "Epoch [46/50], Train Loss: 0.0021, Val Loss: 0.0045\n",
      "Epoch [47/50], Train Loss: 0.0020, Val Loss: 0.0044\n",
      "Epoch [48/50], Train Loss: 0.0020, Val Loss: 0.0043\n",
      "Epoch [49/50], Train Loss: 0.0020, Val Loss: 0.0043\n",
      "Epoch [50/50], Train Loss: 0.0020, Val Loss: 0.0042\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=16, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.2099, Val Loss: 0.4634\n",
      "Epoch [2/50], Train Loss: 0.1594, Val Loss: 0.3649\n",
      "Epoch [3/50], Train Loss: 0.1227, Val Loss: 0.2841\n",
      "Epoch [4/50], Train Loss: 0.0915, Val Loss: 0.2134\n",
      "Epoch [5/50], Train Loss: 0.0664, Val Loss: 0.1516\n",
      "Epoch [6/50], Train Loss: 0.0464, Val Loss: 0.1042\n",
      "Epoch [7/50], Train Loss: 0.0371, Val Loss: 0.0764\n",
      "Epoch [8/50], Train Loss: 0.0331, Val Loss: 0.0623\n",
      "Epoch [9/50], Train Loss: 0.0311, Val Loss: 0.0524\n",
      "Epoch [10/50], Train Loss: 0.0291, Val Loss: 0.0462\n",
      "Epoch [11/50], Train Loss: 0.0271, Val Loss: 0.0397\n",
      "Epoch [12/50], Train Loss: 0.0243, Val Loss: 0.0354\n",
      "Epoch [13/50], Train Loss: 0.0226, Val Loss: 0.0305\n",
      "Epoch [14/50], Train Loss: 0.0208, Val Loss: 0.0262\n",
      "Epoch [15/50], Train Loss: 0.0193, Val Loss: 0.0223\n",
      "Epoch [16/50], Train Loss: 0.0179, Val Loss: 0.0193\n",
      "Epoch [17/50], Train Loss: 0.0170, Val Loss: 0.0157\n",
      "Epoch [18/50], Train Loss: 0.0158, Val Loss: 0.0140\n",
      "Epoch [19/50], Train Loss: 0.0138, Val Loss: 0.0121\n",
      "Epoch [20/50], Train Loss: 0.0137, Val Loss: 0.0116\n",
      "Epoch [21/50], Train Loss: 0.0126, Val Loss: 0.0101\n",
      "Epoch [22/50], Train Loss: 0.0130, Val Loss: 0.0090\n",
      "Epoch [23/50], Train Loss: 0.0118, Val Loss: 0.0093\n",
      "Epoch [24/50], Train Loss: 0.0111, Val Loss: 0.0077\n",
      "Epoch [25/50], Train Loss: 0.0109, Val Loss: 0.0071\n",
      "Epoch [26/50], Train Loss: 0.0113, Val Loss: 0.0065\n",
      "Epoch [27/50], Train Loss: 0.0107, Val Loss: 0.0053\n",
      "Epoch [28/50], Train Loss: 0.0098, Val Loss: 0.0060\n",
      "Epoch [29/50], Train Loss: 0.0094, Val Loss: 0.0058\n",
      "Epoch [30/50], Train Loss: 0.0100, Val Loss: 0.0051\n",
      "Epoch [31/50], Train Loss: 0.0093, Val Loss: 0.0053\n",
      "Epoch [32/50], Train Loss: 0.0091, Val Loss: 0.0047\n",
      "Epoch [33/50], Train Loss: 0.0087, Val Loss: 0.0051\n",
      "Epoch [34/50], Train Loss: 0.0089, Val Loss: 0.0043\n",
      "Epoch [35/50], Train Loss: 0.0087, Val Loss: 0.0049\n",
      "Epoch [36/50], Train Loss: 0.0081, Val Loss: 0.0046\n",
      "Epoch [37/50], Train Loss: 0.0084, Val Loss: 0.0046\n",
      "Epoch [38/50], Train Loss: 0.0080, Val Loss: 0.0045\n",
      "Epoch [39/50], Train Loss: 0.0079, Val Loss: 0.0034\n",
      "Epoch [40/50], Train Loss: 0.0080, Val Loss: 0.0039\n",
      "Epoch [41/50], Train Loss: 0.0077, Val Loss: 0.0031\n",
      "Epoch [42/50], Train Loss: 0.0072, Val Loss: 0.0040\n",
      "Epoch [43/50], Train Loss: 0.0076, Val Loss: 0.0041\n",
      "Epoch [44/50], Train Loss: 0.0070, Val Loss: 0.0038\n",
      "Epoch [45/50], Train Loss: 0.0071, Val Loss: 0.0036\n",
      "Epoch [46/50], Train Loss: 0.0076, Val Loss: 0.0040\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=16, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1012, Val Loss: 0.1995\n",
      "Epoch [2/50], Train Loss: 0.0677, Val Loss: 0.1360\n",
      "Epoch [3/50], Train Loss: 0.0514, Val Loss: 0.0967\n",
      "Epoch [4/50], Train Loss: 0.0454, Val Loss: 0.0741\n",
      "Epoch [5/50], Train Loss: 0.0407, Val Loss: 0.0619\n",
      "Epoch [6/50], Train Loss: 0.0380, Val Loss: 0.0548\n",
      "Epoch [7/50], Train Loss: 0.0374, Val Loss: 0.0499\n",
      "Epoch [8/50], Train Loss: 0.0347, Val Loss: 0.0451\n",
      "Epoch [9/50], Train Loss: 0.0333, Val Loss: 0.0395\n",
      "Epoch [10/50], Train Loss: 0.0319, Val Loss: 0.0355\n",
      "Epoch [11/50], Train Loss: 0.0306, Val Loss: 0.0327\n",
      "Epoch [12/50], Train Loss: 0.0286, Val Loss: 0.0284\n",
      "Epoch [13/50], Train Loss: 0.0283, Val Loss: 0.0252\n",
      "Epoch [14/50], Train Loss: 0.0265, Val Loss: 0.0217\n",
      "Epoch [15/50], Train Loss: 0.0248, Val Loss: 0.0204\n",
      "Epoch [16/50], Train Loss: 0.0231, Val Loss: 0.0184\n",
      "Epoch [17/50], Train Loss: 0.0235, Val Loss: 0.0159\n",
      "Epoch [18/50], Train Loss: 0.0221, Val Loss: 0.0151\n",
      "Epoch [19/50], Train Loss: 0.0207, Val Loss: 0.0126\n",
      "Epoch [20/50], Train Loss: 0.0206, Val Loss: 0.0117\n",
      "Epoch [21/50], Train Loss: 0.0205, Val Loss: 0.0112\n",
      "Epoch [22/50], Train Loss: 0.0188, Val Loss: 0.0098\n",
      "Epoch [23/50], Train Loss: 0.0173, Val Loss: 0.0087\n",
      "Epoch [24/50], Train Loss: 0.0167, Val Loss: 0.0071\n",
      "Epoch [25/50], Train Loss: 0.0175, Val Loss: 0.0058\n",
      "Epoch [26/50], Train Loss: 0.0159, Val Loss: 0.0054\n",
      "Epoch [27/50], Train Loss: 0.0150, Val Loss: 0.0060\n",
      "Epoch [28/50], Train Loss: 0.0147, Val Loss: 0.0059\n",
      "Epoch [29/50], Train Loss: 0.0140, Val Loss: 0.0052\n",
      "Epoch [30/50], Train Loss: 0.0140, Val Loss: 0.0039\n",
      "Epoch [31/50], Train Loss: 0.0125, Val Loss: 0.0038\n",
      "Epoch [32/50], Train Loss: 0.0121, Val Loss: 0.0051\n",
      "Epoch [33/50], Train Loss: 0.0125, Val Loss: 0.0040\n",
      "Epoch [34/50], Train Loss: 0.0115, Val Loss: 0.0049\n",
      "Epoch [35/50], Train Loss: 0.0105, Val Loss: 0.0047\n",
      "Epoch [36/50], Train Loss: 0.0123, Val Loss: 0.0038\n",
      "Epoch [37/50], Train Loss: 0.0108, Val Loss: 0.0048\n",
      "Epoch [38/50], Train Loss: 0.0105, Val Loss: 0.0047\n",
      "Epoch [39/50], Train Loss: 0.0099, Val Loss: 0.0043\n",
      "Epoch [40/50], Train Loss: 0.0109, Val Loss: 0.0036\n",
      "Epoch [41/50], Train Loss: 0.0106, Val Loss: 0.0037\n",
      "Epoch [42/50], Train Loss: 0.0105, Val Loss: 0.0038\n",
      "Epoch [43/50], Train Loss: 0.0106, Val Loss: 0.0031\n",
      "Epoch [44/50], Train Loss: 0.0097, Val Loss: 0.0045\n",
      "Epoch [45/50], Train Loss: 0.0099, Val Loss: 0.0038\n",
      "Epoch [46/50], Train Loss: 0.0095, Val Loss: 0.0041\n",
      "Epoch [47/50], Train Loss: 0.0093, Val Loss: 0.0033\n",
      "Epoch [48/50], Train Loss: 0.0095, Val Loss: 0.0037\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=16, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0847, Val Loss: 0.2254\n",
      "Epoch [2/50], Train Loss: 0.0542, Val Loss: 0.1555\n",
      "Epoch [3/50], Train Loss: 0.0401, Val Loss: 0.1074\n",
      "Epoch [4/50], Train Loss: 0.0367, Val Loss: 0.0860\n",
      "Epoch [5/50], Train Loss: 0.0353, Val Loss: 0.0767\n",
      "Epoch [6/50], Train Loss: 0.0331, Val Loss: 0.0694\n",
      "Epoch [7/50], Train Loss: 0.0307, Val Loss: 0.0622\n",
      "Epoch [8/50], Train Loss: 0.0282, Val Loss: 0.0548\n",
      "Epoch [9/50], Train Loss: 0.0257, Val Loss: 0.0475\n",
      "Epoch [10/50], Train Loss: 0.0231, Val Loss: 0.0403\n",
      "Epoch [11/50], Train Loss: 0.0205, Val Loss: 0.0335\n",
      "Epoch [12/50], Train Loss: 0.0179, Val Loss: 0.0274\n",
      "Epoch [13/50], Train Loss: 0.0155, Val Loss: 0.0224\n",
      "Epoch [14/50], Train Loss: 0.0134, Val Loss: 0.0184\n",
      "Epoch [15/50], Train Loss: 0.0115, Val Loss: 0.0155\n",
      "Epoch [16/50], Train Loss: 0.0096, Val Loss: 0.0133\n",
      "Epoch [17/50], Train Loss: 0.0077, Val Loss: 0.0114\n",
      "Epoch [18/50], Train Loss: 0.0059, Val Loss: 0.0097\n",
      "Epoch [19/50], Train Loss: 0.0043, Val Loss: 0.0082\n",
      "Epoch [20/50], Train Loss: 0.0032, Val Loss: 0.0069\n",
      "Epoch [21/50], Train Loss: 0.0027, Val Loss: 0.0062\n",
      "Epoch [22/50], Train Loss: 0.0024, Val Loss: 0.0058\n",
      "Epoch [23/50], Train Loss: 0.0023, Val Loss: 0.0056\n",
      "Epoch [24/50], Train Loss: 0.0022, Val Loss: 0.0055\n",
      "Epoch [25/50], Train Loss: 0.0022, Val Loss: 0.0055\n",
      "Epoch [26/50], Train Loss: 0.0021, Val Loss: 0.0054\n",
      "Epoch [27/50], Train Loss: 0.0021, Val Loss: 0.0054\n",
      "Epoch [28/50], Train Loss: 0.0021, Val Loss: 0.0053\n",
      "Epoch [29/50], Train Loss: 0.0021, Val Loss: 0.0053\n",
      "Epoch [30/50], Train Loss: 0.0021, Val Loss: 0.0052\n",
      "Epoch [31/50], Train Loss: 0.0021, Val Loss: 0.0052\n",
      "Epoch [32/50], Train Loss: 0.0020, Val Loss: 0.0052\n",
      "Epoch [33/50], Train Loss: 0.0020, Val Loss: 0.0051\n",
      "Epoch [34/50], Train Loss: 0.0020, Val Loss: 0.0051\n",
      "Epoch [35/50], Train Loss: 0.0020, Val Loss: 0.0050\n",
      "Epoch [36/50], Train Loss: 0.0020, Val Loss: 0.0050\n",
      "Epoch [37/50], Train Loss: 0.0020, Val Loss: 0.0050\n",
      "Epoch [38/50], Train Loss: 0.0019, Val Loss: 0.0049\n",
      "Epoch [39/50], Train Loss: 0.0019, Val Loss: 0.0049\n",
      "Epoch [40/50], Train Loss: 0.0019, Val Loss: 0.0048\n",
      "Epoch [41/50], Train Loss: 0.0019, Val Loss: 0.0048\n",
      "Epoch [42/50], Train Loss: 0.0019, Val Loss: 0.0047\n",
      "Epoch [43/50], Train Loss: 0.0019, Val Loss: 0.0047\n",
      "Epoch [44/50], Train Loss: 0.0019, Val Loss: 0.0047\n",
      "Epoch [45/50], Train Loss: 0.0019, Val Loss: 0.0046\n",
      "Epoch [46/50], Train Loss: 0.0019, Val Loss: 0.0046\n",
      "Epoch [47/50], Train Loss: 0.0018, Val Loss: 0.0045\n",
      "Epoch [48/50], Train Loss: 0.0018, Val Loss: 0.0045\n",
      "Epoch [49/50], Train Loss: 0.0018, Val Loss: 0.0044\n",
      "Epoch [50/50], Train Loss: 0.0018, Val Loss: 0.0044\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=16, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1703, Val Loss: 0.3789\n",
      "Epoch [2/50], Train Loss: 0.1003, Val Loss: 0.2398\n",
      "Epoch [3/50], Train Loss: 0.0564, Val Loss: 0.1221\n",
      "Epoch [4/50], Train Loss: 0.0432, Val Loss: 0.0744\n",
      "Epoch [5/50], Train Loss: 0.0424, Val Loss: 0.0647\n",
      "Epoch [6/50], Train Loss: 0.0384, Val Loss: 0.0598\n",
      "Epoch [7/50], Train Loss: 0.0369, Val Loss: 0.0501\n",
      "Epoch [8/50], Train Loss: 0.0340, Val Loss: 0.0409\n",
      "Epoch [9/50], Train Loss: 0.0314, Val Loss: 0.0336\n",
      "Epoch [10/50], Train Loss: 0.0283, Val Loss: 0.0286\n",
      "Epoch [11/50], Train Loss: 0.0262, Val Loss: 0.0221\n",
      "Epoch [12/50], Train Loss: 0.0249, Val Loss: 0.0171\n",
      "Epoch [13/50], Train Loss: 0.0220, Val Loss: 0.0152\n",
      "Epoch [14/50], Train Loss: 0.0205, Val Loss: 0.0126\n",
      "Epoch [15/50], Train Loss: 0.0198, Val Loss: 0.0109\n",
      "Epoch [16/50], Train Loss: 0.0172, Val Loss: 0.0097\n",
      "Epoch [17/50], Train Loss: 0.0155, Val Loss: 0.0079\n",
      "Epoch [18/50], Train Loss: 0.0135, Val Loss: 0.0071\n",
      "Epoch [19/50], Train Loss: 0.0130, Val Loss: 0.0080\n",
      "Epoch [20/50], Train Loss: 0.0131, Val Loss: 0.0062\n",
      "Epoch [21/50], Train Loss: 0.0115, Val Loss: 0.0059\n",
      "Epoch [22/50], Train Loss: 0.0121, Val Loss: 0.0055\n",
      "Epoch [23/50], Train Loss: 0.0115, Val Loss: 0.0051\n",
      "Epoch [24/50], Train Loss: 0.0120, Val Loss: 0.0059\n",
      "Epoch [25/50], Train Loss: 0.0107, Val Loss: 0.0048\n",
      "Epoch [26/50], Train Loss: 0.0108, Val Loss: 0.0042\n",
      "Epoch [27/50], Train Loss: 0.0108, Val Loss: 0.0055\n",
      "Epoch [28/50], Train Loss: 0.0106, Val Loss: 0.0046\n",
      "Epoch [29/50], Train Loss: 0.0101, Val Loss: 0.0042\n",
      "Epoch [30/50], Train Loss: 0.0096, Val Loss: 0.0040\n",
      "Epoch [31/50], Train Loss: 0.0098, Val Loss: 0.0043\n",
      "Epoch [32/50], Train Loss: 0.0104, Val Loss: 0.0052\n",
      "Epoch [33/50], Train Loss: 0.0094, Val Loss: 0.0047\n",
      "Epoch [34/50], Train Loss: 0.0095, Val Loss: 0.0032\n",
      "Epoch [35/50], Train Loss: 0.0093, Val Loss: 0.0058\n",
      "Epoch [36/50], Train Loss: 0.0092, Val Loss: 0.0048\n",
      "Epoch [37/50], Train Loss: 0.0093, Val Loss: 0.0026\n",
      "Epoch [38/50], Train Loss: 0.0093, Val Loss: 0.0039\n",
      "Epoch [39/50], Train Loss: 0.0087, Val Loss: 0.0060\n",
      "Epoch [40/50], Train Loss: 0.0087, Val Loss: 0.0031\n",
      "Epoch [41/50], Train Loss: 0.0088, Val Loss: 0.0024\n",
      "Epoch [42/50], Train Loss: 0.0086, Val Loss: 0.0058\n",
      "Epoch [43/50], Train Loss: 0.0084, Val Loss: 0.0056\n",
      "Epoch [44/50], Train Loss: 0.0083, Val Loss: 0.0025\n",
      "Epoch [45/50], Train Loss: 0.0083, Val Loss: 0.0036\n",
      "Epoch [46/50], Train Loss: 0.0087, Val Loss: 0.0043\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=16, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1776, Val Loss: 0.3704\n",
      "Epoch [2/50], Train Loss: 0.1204, Val Loss: 0.2680\n",
      "Epoch [3/50], Train Loss: 0.0884, Val Loss: 0.1811\n",
      "Epoch [4/50], Train Loss: 0.0681, Val Loss: 0.1222\n",
      "Epoch [5/50], Train Loss: 0.0580, Val Loss: 0.0915\n",
      "Epoch [6/50], Train Loss: 0.0523, Val Loss: 0.0700\n",
      "Epoch [7/50], Train Loss: 0.0461, Val Loss: 0.0551\n",
      "Epoch [8/50], Train Loss: 0.0414, Val Loss: 0.0440\n",
      "Epoch [9/50], Train Loss: 0.0356, Val Loss: 0.0312\n",
      "Epoch [10/50], Train Loss: 0.0343, Val Loss: 0.0275\n",
      "Epoch [11/50], Train Loss: 0.0315, Val Loss: 0.0210\n",
      "Epoch [12/50], Train Loss: 0.0284, Val Loss: 0.0166\n",
      "Epoch [13/50], Train Loss: 0.0280, Val Loss: 0.0132\n",
      "Epoch [14/50], Train Loss: 0.0276, Val Loss: 0.0139\n",
      "Epoch [15/50], Train Loss: 0.0257, Val Loss: 0.0120\n",
      "Epoch [16/50], Train Loss: 0.0240, Val Loss: 0.0101\n",
      "Epoch [17/50], Train Loss: 0.0233, Val Loss: 0.0122\n",
      "Epoch [18/50], Train Loss: 0.0234, Val Loss: 0.0095\n",
      "Epoch [19/50], Train Loss: 0.0227, Val Loss: 0.0086\n",
      "Epoch [20/50], Train Loss: 0.0233, Val Loss: 0.0122\n",
      "Epoch [21/50], Train Loss: 0.0217, Val Loss: 0.0085\n",
      "Epoch [22/50], Train Loss: 0.0206, Val Loss: 0.0088\n",
      "Epoch [23/50], Train Loss: 0.0200, Val Loss: 0.0112\n",
      "Epoch [24/50], Train Loss: 0.0208, Val Loss: 0.0058\n",
      "Epoch [25/50], Train Loss: 0.0194, Val Loss: 0.0067\n",
      "Epoch [26/50], Train Loss: 0.0190, Val Loss: 0.0084\n",
      "Epoch [27/50], Train Loss: 0.0197, Val Loss: 0.0095\n",
      "Epoch [28/50], Train Loss: 0.0186, Val Loss: 0.0064\n",
      "Epoch [29/50], Train Loss: 0.0195, Val Loss: 0.0069\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=16, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1091, Val Loss: 0.1495\n",
      "Epoch [2/50], Train Loss: 0.0517, Val Loss: 0.0979\n",
      "Epoch [3/50], Train Loss: 0.0405, Val Loss: 0.0850\n",
      "Epoch [4/50], Train Loss: 0.0392, Val Loss: 0.0816\n",
      "Epoch [5/50], Train Loss: 0.0373, Val Loss: 0.0761\n",
      "Epoch [6/50], Train Loss: 0.0351, Val Loss: 0.0675\n",
      "Epoch [7/50], Train Loss: 0.0323, Val Loss: 0.0552\n",
      "Epoch [8/50], Train Loss: 0.0284, Val Loss: 0.0393\n",
      "Epoch [9/50], Train Loss: 0.0232, Val Loss: 0.0234\n",
      "Epoch [10/50], Train Loss: 0.0175, Val Loss: 0.0141\n",
      "Epoch [11/50], Train Loss: 0.0143, Val Loss: 0.0119\n",
      "Epoch [12/50], Train Loss: 0.0130, Val Loss: 0.0117\n",
      "Epoch [13/50], Train Loss: 0.0121, Val Loss: 0.0117\n",
      "Epoch [14/50], Train Loss: 0.0113, Val Loss: 0.0119\n",
      "Epoch [15/50], Train Loss: 0.0105, Val Loss: 0.0122\n",
      "Epoch [16/50], Train Loss: 0.0095, Val Loss: 0.0123\n",
      "Epoch [17/50], Train Loss: 0.0085, Val Loss: 0.0123\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=16, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0966, Val Loss: 0.2556\n",
      "Epoch [2/50], Train Loss: 0.0581, Val Loss: 0.1604\n",
      "Epoch [3/50], Train Loss: 0.0435, Val Loss: 0.1047\n",
      "Epoch [4/50], Train Loss: 0.0415, Val Loss: 0.0845\n",
      "Epoch [5/50], Train Loss: 0.0412, Val Loss: 0.0758\n",
      "Epoch [6/50], Train Loss: 0.0363, Val Loss: 0.0638\n",
      "Epoch [7/50], Train Loss: 0.0330, Val Loss: 0.0492\n",
      "Epoch [8/50], Train Loss: 0.0297, Val Loss: 0.0346\n",
      "Epoch [9/50], Train Loss: 0.0246, Val Loss: 0.0265\n",
      "Epoch [10/50], Train Loss: 0.0225, Val Loss: 0.0233\n",
      "Epoch [11/50], Train Loss: 0.0210, Val Loss: 0.0199\n",
      "Epoch [12/50], Train Loss: 0.0197, Val Loss: 0.0187\n",
      "Epoch [13/50], Train Loss: 0.0179, Val Loss: 0.0177\n",
      "Epoch [14/50], Train Loss: 0.0163, Val Loss: 0.0142\n",
      "Epoch [15/50], Train Loss: 0.0147, Val Loss: 0.0149\n",
      "Epoch [16/50], Train Loss: 0.0133, Val Loss: 0.0152\n",
      "Epoch [17/50], Train Loss: 0.0110, Val Loss: 0.0128\n",
      "Epoch [18/50], Train Loss: 0.0104, Val Loss: 0.0121\n",
      "Epoch [19/50], Train Loss: 0.0099, Val Loss: 0.0124\n",
      "Epoch [20/50], Train Loss: 0.0102, Val Loss: 0.0133\n",
      "Epoch [21/50], Train Loss: 0.0093, Val Loss: 0.0078\n",
      "Epoch [22/50], Train Loss: 0.0094, Val Loss: 0.0086\n",
      "Epoch [23/50], Train Loss: 0.0097, Val Loss: 0.0173\n",
      "Epoch [24/50], Train Loss: 0.0095, Val Loss: 0.0083\n",
      "Epoch [25/50], Train Loss: 0.0102, Val Loss: 0.0036\n",
      "Epoch [26/50], Train Loss: 0.0101, Val Loss: 0.0197\n",
      "Epoch [27/50], Train Loss: 0.0089, Val Loss: 0.0115\n",
      "Epoch [28/50], Train Loss: 0.0098, Val Loss: 0.0053\n",
      "Epoch [29/50], Train Loss: 0.0091, Val Loss: 0.0133\n",
      "Epoch [30/50], Train Loss: 0.0092, Val Loss: 0.0142\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=16, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1701, Val Loss: 0.2956\n",
      "Epoch [2/50], Train Loss: 0.1095, Val Loss: 0.1877\n",
      "Epoch [3/50], Train Loss: 0.0896, Val Loss: 0.1424\n",
      "Epoch [4/50], Train Loss: 0.0781, Val Loss: 0.1200\n",
      "Epoch [5/50], Train Loss: 0.0726, Val Loss: 0.1086\n",
      "Epoch [6/50], Train Loss: 0.0646, Val Loss: 0.0924\n",
      "Epoch [7/50], Train Loss: 0.0622, Val Loss: 0.0835\n",
      "Epoch [8/50], Train Loss: 0.0567, Val Loss: 0.0711\n",
      "Epoch [9/50], Train Loss: 0.0532, Val Loss: 0.0639\n",
      "Epoch [10/50], Train Loss: 0.0475, Val Loss: 0.0555\n",
      "Epoch [11/50], Train Loss: 0.0469, Val Loss: 0.0453\n",
      "Epoch [12/50], Train Loss: 0.0435, Val Loss: 0.0403\n",
      "Epoch [13/50], Train Loss: 0.0412, Val Loss: 0.0350\n",
      "Epoch [14/50], Train Loss: 0.0403, Val Loss: 0.0361\n",
      "Epoch [15/50], Train Loss: 0.0378, Val Loss: 0.0307\n",
      "Epoch [16/50], Train Loss: 0.0361, Val Loss: 0.0307\n",
      "Epoch [17/50], Train Loss: 0.0372, Val Loss: 0.0266\n",
      "Epoch [18/50], Train Loss: 0.0363, Val Loss: 0.0262\n",
      "Epoch [19/50], Train Loss: 0.0328, Val Loss: 0.0282\n",
      "Epoch [20/50], Train Loss: 0.0329, Val Loss: 0.0256\n",
      "Epoch [21/50], Train Loss: 0.0306, Val Loss: 0.0234\n",
      "Epoch [22/50], Train Loss: 0.0309, Val Loss: 0.0221\n",
      "Epoch [23/50], Train Loss: 0.0311, Val Loss: 0.0222\n",
      "Epoch [24/50], Train Loss: 0.0290, Val Loss: 0.0244\n",
      "Epoch [25/50], Train Loss: 0.0268, Val Loss: 0.0185\n",
      "Epoch [26/50], Train Loss: 0.0261, Val Loss: 0.0199\n",
      "Epoch [27/50], Train Loss: 0.0241, Val Loss: 0.0219\n",
      "Epoch [28/50], Train Loss: 0.0221, Val Loss: 0.0191\n",
      "Epoch [29/50], Train Loss: 0.0221, Val Loss: 0.0135\n",
      "Epoch [30/50], Train Loss: 0.0220, Val Loss: 0.0152\n",
      "Epoch [31/50], Train Loss: 0.0189, Val Loss: 0.0150\n",
      "Epoch [32/50], Train Loss: 0.0181, Val Loss: 0.0121\n",
      "Epoch [33/50], Train Loss: 0.0193, Val Loss: 0.0149\n",
      "Epoch [34/50], Train Loss: 0.0192, Val Loss: 0.0148\n",
      "Epoch [35/50], Train Loss: 0.0185, Val Loss: 0.0093\n",
      "Epoch [36/50], Train Loss: 0.0187, Val Loss: 0.0096\n",
      "Epoch [37/50], Train Loss: 0.0192, Val Loss: 0.0180\n",
      "Epoch [38/50], Train Loss: 0.0169, Val Loss: 0.0170\n",
      "Epoch [39/50], Train Loss: 0.0179, Val Loss: 0.0094\n",
      "Epoch [40/50], Train Loss: 0.0190, Val Loss: 0.0127\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=32, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1362, Val Loss: 0.2928\n",
      "Epoch [2/50], Train Loss: 0.0644, Val Loss: 0.1335\n",
      "Epoch [3/50], Train Loss: 0.0270, Val Loss: 0.0458\n",
      "Epoch [4/50], Train Loss: 0.0266, Val Loss: 0.0405\n",
      "Epoch [5/50], Train Loss: 0.0230, Val Loss: 0.0332\n",
      "Epoch [6/50], Train Loss: 0.0206, Val Loss: 0.0276\n",
      "Epoch [7/50], Train Loss: 0.0183, Val Loss: 0.0224\n",
      "Epoch [8/50], Train Loss: 0.0161, Val Loss: 0.0178\n",
      "Epoch [9/50], Train Loss: 0.0140, Val Loss: 0.0137\n",
      "Epoch [10/50], Train Loss: 0.0120, Val Loss: 0.0101\n",
      "Epoch [11/50], Train Loss: 0.0101, Val Loss: 0.0072\n",
      "Epoch [12/50], Train Loss: 0.0083, Val Loss: 0.0050\n",
      "Epoch [13/50], Train Loss: 0.0065, Val Loss: 0.0034\n",
      "Epoch [14/50], Train Loss: 0.0050, Val Loss: 0.0027\n",
      "Epoch [15/50], Train Loss: 0.0037, Val Loss: 0.0027\n",
      "Epoch [16/50], Train Loss: 0.0029, Val Loss: 0.0027\n",
      "Epoch [17/50], Train Loss: 0.0024, Val Loss: 0.0026\n",
      "Epoch [18/50], Train Loss: 0.0022, Val Loss: 0.0024\n",
      "Epoch [19/50], Train Loss: 0.0020, Val Loss: 0.0022\n",
      "Epoch [20/50], Train Loss: 0.0019, Val Loss: 0.0021\n",
      "Epoch [21/50], Train Loss: 0.0018, Val Loss: 0.0020\n",
      "Epoch [22/50], Train Loss: 0.0018, Val Loss: 0.0019\n",
      "Epoch [23/50], Train Loss: 0.0018, Val Loss: 0.0019\n",
      "Epoch [24/50], Train Loss: 0.0017, Val Loss: 0.0019\n",
      "Epoch [25/50], Train Loss: 0.0017, Val Loss: 0.0019\n",
      "Epoch [26/50], Train Loss: 0.0017, Val Loss: 0.0019\n",
      "Epoch [27/50], Train Loss: 0.0017, Val Loss: 0.0019\n",
      "Epoch [28/50], Train Loss: 0.0017, Val Loss: 0.0019\n",
      "Epoch [29/50], Train Loss: 0.0017, Val Loss: 0.0019\n",
      "Epoch [30/50], Train Loss: 0.0017, Val Loss: 0.0019\n",
      "Epoch [31/50], Train Loss: 0.0017, Val Loss: 0.0019\n",
      "Epoch [32/50], Train Loss: 0.0017, Val Loss: 0.0019\n",
      "Epoch [33/50], Train Loss: 0.0016, Val Loss: 0.0019\n",
      "Epoch [34/50], Train Loss: 0.0016, Val Loss: 0.0019\n",
      "Epoch [35/50], Train Loss: 0.0016, Val Loss: 0.0018\n",
      "Epoch [36/50], Train Loss: 0.0016, Val Loss: 0.0018\n",
      "Epoch [37/50], Train Loss: 0.0016, Val Loss: 0.0018\n",
      "Epoch [38/50], Train Loss: 0.0016, Val Loss: 0.0018\n",
      "Epoch [39/50], Train Loss: 0.0016, Val Loss: 0.0018\n",
      "Epoch [40/50], Train Loss: 0.0016, Val Loss: 0.0018\n",
      "Epoch [41/50], Train Loss: 0.0016, Val Loss: 0.0018\n",
      "Epoch [42/50], Train Loss: 0.0016, Val Loss: 0.0018\n",
      "Epoch [43/50], Train Loss: 0.0016, Val Loss: 0.0018\n",
      "Epoch [44/50], Train Loss: 0.0016, Val Loss: 0.0018\n",
      "Epoch [45/50], Train Loss: 0.0016, Val Loss: 0.0018\n",
      "Epoch [46/50], Train Loss: 0.0016, Val Loss: 0.0018\n",
      "Epoch [47/50], Train Loss: 0.0016, Val Loss: 0.0018\n",
      "Epoch [48/50], Train Loss: 0.0015, Val Loss: 0.0018\n",
      "Epoch [49/50], Train Loss: 0.0015, Val Loss: 0.0018\n",
      "Epoch [50/50], Train Loss: 0.0015, Val Loss: 0.0018\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=32, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0766, Val Loss: 0.1967\n",
      "Epoch [2/50], Train Loss: 0.0475, Val Loss: 0.1240\n",
      "Epoch [3/50], Train Loss: 0.0371, Val Loss: 0.0838\n",
      "Epoch [4/50], Train Loss: 0.0339, Val Loss: 0.0641\n",
      "Epoch [5/50], Train Loss: 0.0312, Val Loss: 0.0532\n",
      "Epoch [6/50], Train Loss: 0.0280, Val Loss: 0.0443\n",
      "Epoch [7/50], Train Loss: 0.0252, Val Loss: 0.0335\n",
      "Epoch [8/50], Train Loss: 0.0209, Val Loss: 0.0250\n",
      "Epoch [9/50], Train Loss: 0.0180, Val Loss: 0.0155\n",
      "Epoch [10/50], Train Loss: 0.0145, Val Loss: 0.0094\n",
      "Epoch [11/50], Train Loss: 0.0111, Val Loss: 0.0058\n",
      "Epoch [12/50], Train Loss: 0.0082, Val Loss: 0.0030\n",
      "Epoch [13/50], Train Loss: 0.0062, Val Loss: 0.0023\n",
      "Epoch [14/50], Train Loss: 0.0057, Val Loss: 0.0022\n",
      "Epoch [15/50], Train Loss: 0.0056, Val Loss: 0.0024\n",
      "Epoch [16/50], Train Loss: 0.0054, Val Loss: 0.0020\n",
      "Epoch [17/50], Train Loss: 0.0053, Val Loss: 0.0025\n",
      "Epoch [18/50], Train Loss: 0.0053, Val Loss: 0.0022\n",
      "Epoch [19/50], Train Loss: 0.0053, Val Loss: 0.0018\n",
      "Epoch [20/50], Train Loss: 0.0052, Val Loss: 0.0022\n",
      "Epoch [21/50], Train Loss: 0.0051, Val Loss: 0.0025\n",
      "Epoch [22/50], Train Loss: 0.0048, Val Loss: 0.0018\n",
      "Epoch [23/50], Train Loss: 0.0047, Val Loss: 0.0019\n",
      "Epoch [24/50], Train Loss: 0.0048, Val Loss: 0.0027\n",
      "Epoch [25/50], Train Loss: 0.0046, Val Loss: 0.0017\n",
      "Epoch [26/50], Train Loss: 0.0045, Val Loss: 0.0016\n",
      "Epoch [27/50], Train Loss: 0.0042, Val Loss: 0.0029\n",
      "Epoch [28/50], Train Loss: 0.0043, Val Loss: 0.0020\n",
      "Epoch [29/50], Train Loss: 0.0042, Val Loss: 0.0016\n",
      "Epoch [30/50], Train Loss: 0.0040, Val Loss: 0.0019\n",
      "Epoch [31/50], Train Loss: 0.0042, Val Loss: 0.0025\n",
      "Epoch [32/50], Train Loss: 0.0040, Val Loss: 0.0018\n",
      "Epoch [33/50], Train Loss: 0.0043, Val Loss: 0.0018\n",
      "Epoch [34/50], Train Loss: 0.0038, Val Loss: 0.0024\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=32, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1217, Val Loss: 0.2767\n",
      "Epoch [2/50], Train Loss: 0.0703, Val Loss: 0.1530\n",
      "Epoch [3/50], Train Loss: 0.0481, Val Loss: 0.0747\n",
      "Epoch [4/50], Train Loss: 0.0442, Val Loss: 0.0598\n",
      "Epoch [5/50], Train Loss: 0.0390, Val Loss: 0.0468\n",
      "Epoch [6/50], Train Loss: 0.0325, Val Loss: 0.0340\n",
      "Epoch [7/50], Train Loss: 0.0303, Val Loss: 0.0254\n",
      "Epoch [8/50], Train Loss: 0.0271, Val Loss: 0.0166\n",
      "Epoch [9/50], Train Loss: 0.0231, Val Loss: 0.0104\n",
      "Epoch [10/50], Train Loss: 0.0211, Val Loss: 0.0094\n",
      "Epoch [11/50], Train Loss: 0.0191, Val Loss: 0.0056\n",
      "Epoch [12/50], Train Loss: 0.0180, Val Loss: 0.0049\n",
      "Epoch [13/50], Train Loss: 0.0177, Val Loss: 0.0056\n",
      "Epoch [14/50], Train Loss: 0.0159, Val Loss: 0.0054\n",
      "Epoch [15/50], Train Loss: 0.0153, Val Loss: 0.0046\n",
      "Epoch [16/50], Train Loss: 0.0135, Val Loss: 0.0044\n",
      "Epoch [17/50], Train Loss: 0.0140, Val Loss: 0.0051\n",
      "Epoch [18/50], Train Loss: 0.0126, Val Loss: 0.0052\n",
      "Epoch [19/50], Train Loss: 0.0126, Val Loss: 0.0041\n",
      "Epoch [20/50], Train Loss: 0.0114, Val Loss: 0.0035\n",
      "Epoch [21/50], Train Loss: 0.0118, Val Loss: 0.0044\n",
      "Epoch [22/50], Train Loss: 0.0105, Val Loss: 0.0026\n",
      "Epoch [23/50], Train Loss: 0.0112, Val Loss: 0.0026\n",
      "Epoch [24/50], Train Loss: 0.0104, Val Loss: 0.0026\n",
      "Epoch [25/50], Train Loss: 0.0113, Val Loss: 0.0038\n",
      "Epoch [26/50], Train Loss: 0.0102, Val Loss: 0.0031\n",
      "Epoch [27/50], Train Loss: 0.0098, Val Loss: 0.0024\n",
      "Epoch [28/50], Train Loss: 0.0099, Val Loss: 0.0028\n",
      "Epoch [29/50], Train Loss: 0.0094, Val Loss: 0.0026\n",
      "Epoch [30/50], Train Loss: 0.0090, Val Loss: 0.0022\n",
      "Epoch [31/50], Train Loss: 0.0090, Val Loss: 0.0045\n",
      "Epoch [32/50], Train Loss: 0.0089, Val Loss: 0.0018\n",
      "Epoch [33/50], Train Loss: 0.0097, Val Loss: 0.0029\n",
      "Epoch [34/50], Train Loss: 0.0081, Val Loss: 0.0032\n",
      "Epoch [35/50], Train Loss: 0.0088, Val Loss: 0.0018\n",
      "Epoch [36/50], Train Loss: 0.0084, Val Loss: 0.0027\n",
      "Epoch [37/50], Train Loss: 0.0079, Val Loss: 0.0040\n",
      "Epoch [38/50], Train Loss: 0.0084, Val Loss: 0.0024\n",
      "Epoch [39/50], Train Loss: 0.0076, Val Loss: 0.0024\n",
      "Epoch [40/50], Train Loss: 0.0078, Val Loss: 0.0026\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=32, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1224, Val Loss: 0.2535\n",
      "Epoch [2/50], Train Loss: 0.0460, Val Loss: 0.0830\n",
      "Epoch [3/50], Train Loss: 0.0301, Val Loss: 0.0404\n",
      "Epoch [4/50], Train Loss: 0.0303, Val Loss: 0.0355\n",
      "Epoch [5/50], Train Loss: 0.0244, Val Loss: 0.0239\n",
      "Epoch [6/50], Train Loss: 0.0199, Val Loss: 0.0148\n",
      "Epoch [7/50], Train Loss: 0.0149, Val Loss: 0.0100\n",
      "Epoch [8/50], Train Loss: 0.0105, Val Loss: 0.0106\n",
      "Epoch [9/50], Train Loss: 0.0082, Val Loss: 0.0106\n",
      "Epoch [10/50], Train Loss: 0.0063, Val Loss: 0.0091\n",
      "Epoch [11/50], Train Loss: 0.0046, Val Loss: 0.0071\n",
      "Epoch [12/50], Train Loss: 0.0032, Val Loss: 0.0051\n",
      "Epoch [13/50], Train Loss: 0.0024, Val Loss: 0.0044\n",
      "Epoch [14/50], Train Loss: 0.0021, Val Loss: 0.0052\n",
      "Epoch [15/50], Train Loss: 0.0022, Val Loss: 0.0067\n",
      "Epoch [16/50], Train Loss: 0.0021, Val Loss: 0.0066\n",
      "Epoch [17/50], Train Loss: 0.0020, Val Loss: 0.0055\n",
      "Epoch [18/50], Train Loss: 0.0020, Val Loss: 0.0052\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=32, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0907, Val Loss: 0.1598\n",
      "Epoch [2/50], Train Loss: 0.0371, Val Loss: 0.0528\n",
      "Epoch [3/50], Train Loss: 0.0375, Val Loss: 0.0473\n",
      "Epoch [4/50], Train Loss: 0.0312, Val Loss: 0.0334\n",
      "Epoch [5/50], Train Loss: 0.0252, Val Loss: 0.0208\n",
      "Epoch [6/50], Train Loss: 0.0193, Val Loss: 0.0114\n",
      "Epoch [7/50], Train Loss: 0.0144, Val Loss: 0.0058\n",
      "Epoch [8/50], Train Loss: 0.0113, Val Loss: 0.0039\n",
      "Epoch [9/50], Train Loss: 0.0089, Val Loss: 0.0048\n",
      "Epoch [10/50], Train Loss: 0.0085, Val Loss: 0.0041\n",
      "Epoch [11/50], Train Loss: 0.0078, Val Loss: 0.0040\n",
      "Epoch [12/50], Train Loss: 0.0083, Val Loss: 0.0064\n",
      "Epoch [13/50], Train Loss: 0.0075, Val Loss: 0.0030\n",
      "Epoch [14/50], Train Loss: 0.0080, Val Loss: 0.0036\n",
      "Epoch [15/50], Train Loss: 0.0074, Val Loss: 0.0047\n",
      "Epoch [16/50], Train Loss: 0.0077, Val Loss: 0.0076\n",
      "Epoch [17/50], Train Loss: 0.0072, Val Loss: 0.0026\n",
      "Epoch [18/50], Train Loss: 0.0077, Val Loss: 0.0026\n",
      "Epoch [19/50], Train Loss: 0.0072, Val Loss: 0.0052\n",
      "Epoch [20/50], Train Loss: 0.0072, Val Loss: 0.0105\n",
      "Epoch [21/50], Train Loss: 0.0069, Val Loss: 0.0018\n",
      "Epoch [22/50], Train Loss: 0.0083, Val Loss: 0.0044\n",
      "Epoch [23/50], Train Loss: 0.0082, Val Loss: 0.0177\n",
      "Epoch [24/50], Train Loss: 0.0087, Val Loss: 0.0047\n",
      "Epoch [25/50], Train Loss: 0.0103, Val Loss: 0.0051\n",
      "Epoch [26/50], Train Loss: 0.0077, Val Loss: 0.0042\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=32, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1052, Val Loss: 0.1997\n",
      "Epoch [2/50], Train Loss: 0.0570, Val Loss: 0.1002\n",
      "Epoch [3/50], Train Loss: 0.0496, Val Loss: 0.0836\n",
      "Epoch [4/50], Train Loss: 0.0472, Val Loss: 0.0742\n",
      "Epoch [5/50], Train Loss: 0.0437, Val Loss: 0.0620\n",
      "Epoch [6/50], Train Loss: 0.0382, Val Loss: 0.0472\n",
      "Epoch [7/50], Train Loss: 0.0348, Val Loss: 0.0395\n",
      "Epoch [8/50], Train Loss: 0.0301, Val Loss: 0.0262\n",
      "Epoch [9/50], Train Loss: 0.0277, Val Loss: 0.0195\n",
      "Epoch [10/50], Train Loss: 0.0253, Val Loss: 0.0154\n",
      "Epoch [11/50], Train Loss: 0.0223, Val Loss: 0.0153\n",
      "Epoch [12/50], Train Loss: 0.0195, Val Loss: 0.0088\n",
      "Epoch [13/50], Train Loss: 0.0184, Val Loss: 0.0072\n",
      "Epoch [14/50], Train Loss: 0.0169, Val Loss: 0.0090\n",
      "Epoch [15/50], Train Loss: 0.0155, Val Loss: 0.0060\n",
      "Epoch [16/50], Train Loss: 0.0159, Val Loss: 0.0085\n",
      "Epoch [17/50], Train Loss: 0.0144, Val Loss: 0.0075\n",
      "Epoch [18/50], Train Loss: 0.0143, Val Loss: 0.0053\n",
      "Epoch [19/50], Train Loss: 0.0138, Val Loss: 0.0049\n",
      "Epoch [20/50], Train Loss: 0.0132, Val Loss: 0.0090\n",
      "Epoch [21/50], Train Loss: 0.0127, Val Loss: 0.0038\n",
      "Epoch [22/50], Train Loss: 0.0133, Val Loss: 0.0050\n",
      "Epoch [23/50], Train Loss: 0.0121, Val Loss: 0.0100\n",
      "Epoch [24/50], Train Loss: 0.0126, Val Loss: 0.0051\n",
      "Epoch [25/50], Train Loss: 0.0124, Val Loss: 0.0042\n",
      "Epoch [26/50], Train Loss: 0.0116, Val Loss: 0.0066\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=32, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0701, Val Loss: 0.1263\n",
      "Epoch [2/50], Train Loss: 0.0375, Val Loss: 0.0685\n",
      "Epoch [3/50], Train Loss: 0.0370, Val Loss: 0.0583\n",
      "Epoch [4/50], Train Loss: 0.0304, Val Loss: 0.0331\n",
      "Epoch [5/50], Train Loss: 0.0231, Val Loss: 0.0122\n",
      "Epoch [6/50], Train Loss: 0.0161, Val Loss: 0.0095\n",
      "Epoch [7/50], Train Loss: 0.0156, Val Loss: 0.0146\n",
      "Epoch [8/50], Train Loss: 0.0127, Val Loss: 0.0110\n",
      "Epoch [9/50], Train Loss: 0.0105, Val Loss: 0.0087\n",
      "Epoch [10/50], Train Loss: 0.0075, Val Loss: 0.0091\n",
      "Epoch [11/50], Train Loss: 0.0041, Val Loss: 0.0051\n",
      "Epoch [12/50], Train Loss: 0.0031, Val Loss: 0.0020\n",
      "Epoch [13/50], Train Loss: 0.0031, Val Loss: 0.0089\n",
      "Epoch [14/50], Train Loss: 0.0030, Val Loss: 0.0104\n",
      "Epoch [15/50], Train Loss: 0.0033, Val Loss: 0.0018\n",
      "Epoch [16/50], Train Loss: 0.0029, Val Loss: 0.0024\n",
      "Epoch [17/50], Train Loss: 0.0045, Val Loss: 0.0187\n",
      "Epoch [18/50], Train Loss: 0.0043, Val Loss: 0.0042\n",
      "Epoch [19/50], Train Loss: 0.0050, Val Loss: 0.0020\n",
      "Epoch [20/50], Train Loss: 0.0053, Val Loss: 0.0203\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=32, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1010, Val Loss: 0.1511\n",
      "Epoch [2/50], Train Loss: 0.0420, Val Loss: 0.0699\n",
      "Epoch [3/50], Train Loss: 0.0466, Val Loss: 0.0745\n",
      "Epoch [4/50], Train Loss: 0.0403, Val Loss: 0.0571\n",
      "Epoch [5/50], Train Loss: 0.0367, Val Loss: 0.0399\n",
      "Epoch [6/50], Train Loss: 0.0301, Val Loss: 0.0201\n",
      "Epoch [7/50], Train Loss: 0.0225, Val Loss: 0.0099\n",
      "Epoch [8/50], Train Loss: 0.0176, Val Loss: 0.0098\n",
      "Epoch [9/50], Train Loss: 0.0143, Val Loss: 0.0155\n",
      "Epoch [10/50], Train Loss: 0.0110, Val Loss: 0.0075\n",
      "Epoch [11/50], Train Loss: 0.0110, Val Loss: 0.0024\n",
      "Epoch [12/50], Train Loss: 0.0095, Val Loss: 0.0034\n",
      "Epoch [13/50], Train Loss: 0.0096, Val Loss: 0.0156\n",
      "Epoch [14/50], Train Loss: 0.0090, Val Loss: 0.0057\n",
      "Epoch [15/50], Train Loss: 0.0088, Val Loss: 0.0031\n",
      "Epoch [16/50], Train Loss: 0.0091, Val Loss: 0.0109\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=32, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0974, Val Loss: 0.1634\n",
      "Epoch [2/50], Train Loss: 0.0560, Val Loss: 0.0833\n",
      "Epoch [3/50], Train Loss: 0.0582, Val Loss: 0.0875\n",
      "Epoch [4/50], Train Loss: 0.0471, Val Loss: 0.0654\n",
      "Epoch [5/50], Train Loss: 0.0441, Val Loss: 0.0529\n",
      "Epoch [6/50], Train Loss: 0.0397, Val Loss: 0.0342\n",
      "Epoch [7/50], Train Loss: 0.0339, Val Loss: 0.0251\n",
      "Epoch [8/50], Train Loss: 0.0272, Val Loss: 0.0131\n",
      "Epoch [9/50], Train Loss: 0.0219, Val Loss: 0.0111\n",
      "Epoch [10/50], Train Loss: 0.0199, Val Loss: 0.0106\n",
      "Epoch [11/50], Train Loss: 0.0168, Val Loss: 0.0060\n",
      "Epoch [12/50], Train Loss: 0.0175, Val Loss: 0.0046\n",
      "Epoch [13/50], Train Loss: 0.0167, Val Loss: 0.0240\n",
      "Epoch [14/50], Train Loss: 0.0160, Val Loss: 0.0099\n",
      "Epoch [15/50], Train Loss: 0.0190, Val Loss: 0.0046\n",
      "Epoch [16/50], Train Loss: 0.0152, Val Loss: 0.0066\n",
      "Epoch [17/50], Train Loss: 0.0133, Val Loss: 0.0090\n",
      "Epoch [18/50], Train Loss: 0.0140, Val Loss: 0.0103\n",
      "Epoch [19/50], Train Loss: 0.0130, Val Loss: 0.0049\n",
      "Epoch [20/50], Train Loss: 0.0130, Val Loss: 0.0062\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=64, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0726, Val Loss: 0.1378\n",
      "Epoch [2/50], Train Loss: 0.0251, Val Loss: 0.0386\n",
      "Epoch [3/50], Train Loss: 0.0282, Val Loss: 0.0336\n",
      "Epoch [4/50], Train Loss: 0.0207, Val Loss: 0.0188\n",
      "Epoch [5/50], Train Loss: 0.0164, Val Loss: 0.0088\n",
      "Epoch [6/50], Train Loss: 0.0118, Val Loss: 0.0039\n",
      "Epoch [7/50], Train Loss: 0.0084, Val Loss: 0.0033\n",
      "Epoch [8/50], Train Loss: 0.0067, Val Loss: 0.0030\n",
      "Epoch [9/50], Train Loss: 0.0048, Val Loss: 0.0025\n",
      "Epoch [10/50], Train Loss: 0.0035, Val Loss: 0.0013\n",
      "Epoch [11/50], Train Loss: 0.0026, Val Loss: 0.0014\n",
      "Epoch [12/50], Train Loss: 0.0022, Val Loss: 0.0035\n",
      "Epoch [13/50], Train Loss: 0.0024, Val Loss: 0.0031\n",
      "Epoch [14/50], Train Loss: 0.0022, Val Loss: 0.0015\n",
      "Epoch [15/50], Train Loss: 0.0022, Val Loss: 0.0018\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=64, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0825, Val Loss: 0.1737\n",
      "Epoch [2/50], Train Loss: 0.0314, Val Loss: 0.0492\n",
      "Epoch [3/50], Train Loss: 0.0322, Val Loss: 0.0420\n",
      "Epoch [4/50], Train Loss: 0.0251, Val Loss: 0.0255\n",
      "Epoch [5/50], Train Loss: 0.0202, Val Loss: 0.0133\n",
      "Epoch [6/50], Train Loss: 0.0144, Val Loss: 0.0044\n",
      "Epoch [7/50], Train Loss: 0.0091, Val Loss: 0.0038\n",
      "Epoch [8/50], Train Loss: 0.0063, Val Loss: 0.0023\n",
      "Epoch [9/50], Train Loss: 0.0057, Val Loss: 0.0017\n",
      "Epoch [10/50], Train Loss: 0.0045, Val Loss: 0.0036\n",
      "Epoch [11/50], Train Loss: 0.0049, Val Loss: 0.0043\n",
      "Epoch [12/50], Train Loss: 0.0046, Val Loss: 0.0019\n",
      "Epoch [13/50], Train Loss: 0.0043, Val Loss: 0.0023\n",
      "Epoch [14/50], Train Loss: 0.0043, Val Loss: 0.0034\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=64, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0686, Val Loss: 0.1254\n",
      "Epoch [2/50], Train Loss: 0.0351, Val Loss: 0.0467\n",
      "Epoch [3/50], Train Loss: 0.0319, Val Loss: 0.0328\n",
      "Epoch [4/50], Train Loss: 0.0259, Val Loss: 0.0174\n",
      "Epoch [5/50], Train Loss: 0.0209, Val Loss: 0.0072\n",
      "Epoch [6/50], Train Loss: 0.0170, Val Loss: 0.0031\n",
      "Epoch [7/50], Train Loss: 0.0139, Val Loss: 0.0045\n",
      "Epoch [8/50], Train Loss: 0.0125, Val Loss: 0.0043\n",
      "Epoch [9/50], Train Loss: 0.0103, Val Loss: 0.0033\n",
      "Epoch [10/50], Train Loss: 0.0089, Val Loss: 0.0019\n",
      "Epoch [11/50], Train Loss: 0.0081, Val Loss: 0.0033\n",
      "Epoch [12/50], Train Loss: 0.0090, Val Loss: 0.0117\n",
      "Epoch [13/50], Train Loss: 0.0081, Val Loss: 0.0021\n",
      "Epoch [14/50], Train Loss: 0.0091, Val Loss: 0.0032\n",
      "Epoch [15/50], Train Loss: 0.0076, Val Loss: 0.0084\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=64, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0589, Val Loss: 0.0654\n",
      "Epoch [2/50], Train Loss: 0.0467, Val Loss: 0.0628\n",
      "Epoch [3/50], Train Loss: 0.0319, Val Loss: 0.0403\n",
      "Epoch [4/50], Train Loss: 0.0258, Val Loss: 0.0209\n",
      "Epoch [5/50], Train Loss: 0.0179, Val Loss: 0.0108\n",
      "Epoch [6/50], Train Loss: 0.0113, Val Loss: 0.0090\n",
      "Epoch [7/50], Train Loss: 0.0089, Val Loss: 0.0065\n",
      "Epoch [8/50], Train Loss: 0.0053, Val Loss: 0.0020\n",
      "Epoch [9/50], Train Loss: 0.0040, Val Loss: 0.0017\n",
      "Epoch [10/50], Train Loss: 0.0026, Val Loss: 0.0032\n",
      "Epoch [11/50], Train Loss: 0.0027, Val Loss: 0.0093\n",
      "Epoch [12/50], Train Loss: 0.0023, Val Loss: 0.0047\n",
      "Epoch [13/50], Train Loss: 0.0024, Val Loss: 0.0020\n",
      "Epoch [14/50], Train Loss: 0.0023, Val Loss: 0.0039\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=64, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0491, Val Loss: 0.0546\n",
      "Epoch [2/50], Train Loss: 0.0450, Val Loss: 0.0490\n",
      "Epoch [3/50], Train Loss: 0.0294, Val Loss: 0.0280\n",
      "Epoch [4/50], Train Loss: 0.0221, Val Loss: 0.0107\n",
      "Epoch [5/50], Train Loss: 0.0153, Val Loss: 0.0045\n",
      "Epoch [6/50], Train Loss: 0.0089, Val Loss: 0.0036\n",
      "Epoch [7/50], Train Loss: 0.0075, Val Loss: 0.0130\n",
      "Epoch [8/50], Train Loss: 0.0062, Val Loss: 0.0021\n",
      "Epoch [9/50], Train Loss: 0.0075, Val Loss: 0.0032\n",
      "Epoch [10/50], Train Loss: 0.0064, Val Loss: 0.0102\n",
      "Epoch [11/50], Train Loss: 0.0057, Val Loss: 0.0026\n",
      "Epoch [12/50], Train Loss: 0.0069, Val Loss: 0.0023\n",
      "Epoch [13/50], Train Loss: 0.0053, Val Loss: 0.0035\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=64, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0727, Val Loss: 0.0814\n",
      "Epoch [2/50], Train Loss: 0.0482, Val Loss: 0.0548\n",
      "Epoch [3/50], Train Loss: 0.0369, Val Loss: 0.0291\n",
      "Epoch [4/50], Train Loss: 0.0283, Val Loss: 0.0100\n",
      "Epoch [5/50], Train Loss: 0.0202, Val Loss: 0.0025\n",
      "Epoch [6/50], Train Loss: 0.0150, Val Loss: 0.0082\n",
      "Epoch [7/50], Train Loss: 0.0140, Val Loss: 0.0126\n",
      "Epoch [8/50], Train Loss: 0.0150, Val Loss: 0.0026\n",
      "Epoch [9/50], Train Loss: 0.0138, Val Loss: 0.0028\n",
      "Epoch [10/50], Train Loss: 0.0150, Val Loss: 0.0289\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=64, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0511, Val Loss: 0.0150\n",
      "Epoch [2/50], Train Loss: 0.0723, Val Loss: 0.0918\n",
      "Epoch [3/50], Train Loss: 0.0316, Val Loss: 0.0306\n",
      "Epoch [4/50], Train Loss: 0.0336, Val Loss: 0.0129\n",
      "Epoch [5/50], Train Loss: 0.0224, Val Loss: 0.0050\n",
      "Epoch [6/50], Train Loss: 0.0159, Val Loss: 0.0043\n",
      "Epoch [7/50], Train Loss: 0.0099, Val Loss: 0.0113\n",
      "Epoch [8/50], Train Loss: 0.0065, Val Loss: 0.0099\n",
      "Epoch [9/50], Train Loss: 0.0069, Val Loss: 0.0098\n",
      "Epoch [10/50], Train Loss: 0.0035, Val Loss: 0.0035\n",
      "Epoch [11/50], Train Loss: 0.0033, Val Loss: 0.0108\n",
      "Epoch [12/50], Train Loss: 0.0026, Val Loss: 0.0032\n",
      "Epoch [13/50], Train Loss: 0.0035, Val Loss: 0.0036\n",
      "Epoch [14/50], Train Loss: 0.0033, Val Loss: 0.0081\n",
      "Epoch [15/50], Train Loss: 0.0027, Val Loss: 0.0080\n",
      "Epoch [16/50], Train Loss: 0.0030, Val Loss: 0.0031\n",
      "Epoch [17/50], Train Loss: 0.0026, Val Loss: 0.0029\n",
      "Epoch [18/50], Train Loss: 0.0033, Val Loss: 0.0119\n",
      "Epoch [19/50], Train Loss: 0.0028, Val Loss: 0.0044\n",
      "Epoch [20/50], Train Loss: 0.0035, Val Loss: 0.0054\n",
      "Epoch [21/50], Train Loss: 0.0031, Val Loss: 0.0067\n",
      "Epoch [22/50], Train Loss: 0.0028, Val Loss: 0.0099\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=64, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0568, Val Loss: 0.0304\n",
      "Epoch [2/50], Train Loss: 0.0679, Val Loss: 0.0731\n",
      "Epoch [3/50], Train Loss: 0.0360, Val Loss: 0.0345\n",
      "Epoch [4/50], Train Loss: 0.0323, Val Loss: 0.0118\n",
      "Epoch [5/50], Train Loss: 0.0225, Val Loss: 0.0029\n",
      "Epoch [6/50], Train Loss: 0.0164, Val Loss: 0.0053\n",
      "Epoch [7/50], Train Loss: 0.0108, Val Loss: 0.0113\n",
      "Epoch [8/50], Train Loss: 0.0080, Val Loss: 0.0062\n",
      "Epoch [9/50], Train Loss: 0.0081, Val Loss: 0.0028\n",
      "Epoch [10/50], Train Loss: 0.0074, Val Loss: 0.0100\n",
      "Epoch [11/50], Train Loss: 0.0071, Val Loss: 0.0079\n",
      "Epoch [12/50], Train Loss: 0.0073, Val Loss: 0.0036\n",
      "Epoch [13/50], Train Loss: 0.0071, Val Loss: 0.0053\n",
      "Epoch [14/50], Train Loss: 0.0070, Val Loss: 0.0161\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=64, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0710, Val Loss: 0.0738\n",
      "Epoch [2/50], Train Loss: 0.0677, Val Loss: 0.1020\n",
      "Epoch [3/50], Train Loss: 0.0459, Val Loss: 0.0572\n",
      "Epoch [4/50], Train Loss: 0.0400, Val Loss: 0.0232\n",
      "Epoch [5/50], Train Loss: 0.0294, Val Loss: 0.0050\n",
      "Epoch [6/50], Train Loss: 0.0238, Val Loss: 0.0083\n",
      "Epoch [7/50], Train Loss: 0.0185, Val Loss: 0.0076\n",
      "Epoch [8/50], Train Loss: 0.0159, Val Loss: 0.0211\n",
      "Epoch [9/50], Train Loss: 0.0145, Val Loss: 0.0039\n",
      "Epoch [10/50], Train Loss: 0.0146, Val Loss: 0.0024\n",
      "Epoch [11/50], Train Loss: 0.0125, Val Loss: 0.0069\n",
      "Epoch [12/50], Train Loss: 0.0129, Val Loss: 0.0179\n",
      "Epoch [13/50], Train Loss: 0.0137, Val Loss: 0.0058\n",
      "Epoch [14/50], Train Loss: 0.0134, Val Loss: 0.0051\n",
      "Epoch [15/50], Train Loss: 0.0119, Val Loss: 0.0078\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=128, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0590, Val Loss: 0.0231\n",
      "Epoch [2/50], Train Loss: 0.0451, Val Loss: 0.0461\n",
      "Epoch [3/50], Train Loss: 0.0238, Val Loss: 0.0202\n",
      "Epoch [4/50], Train Loss: 0.0209, Val Loss: 0.0113\n",
      "Epoch [5/50], Train Loss: 0.0153, Val Loss: 0.0048\n",
      "Epoch [6/50], Train Loss: 0.0096, Val Loss: 0.0027\n",
      "Epoch [7/50], Train Loss: 0.0043, Val Loss: 0.0026\n",
      "Epoch [8/50], Train Loss: 0.0048, Val Loss: 0.0065\n",
      "Epoch [9/50], Train Loss: 0.0027, Val Loss: 0.0051\n",
      "Epoch [10/50], Train Loss: 0.0037, Val Loss: 0.0023\n",
      "Epoch [11/50], Train Loss: 0.0029, Val Loss: 0.0037\n",
      "Epoch [12/50], Train Loss: 0.0022, Val Loss: 0.0024\n",
      "Epoch [13/50], Train Loss: 0.0026, Val Loss: 0.0040\n",
      "Epoch [14/50], Train Loss: 0.0020, Val Loss: 0.0015\n",
      "Epoch [15/50], Train Loss: 0.0024, Val Loss: 0.0030\n",
      "Epoch [16/50], Train Loss: 0.0019, Val Loss: 0.0029\n",
      "Epoch [17/50], Train Loss: 0.0023, Val Loss: 0.0033\n",
      "Epoch [18/50], Train Loss: 0.0020, Val Loss: 0.0021\n",
      "Epoch [19/50], Train Loss: 0.0020, Val Loss: 0.0020\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=128, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0420, Val Loss: 0.0275\n",
      "Epoch [2/50], Train Loss: 0.0386, Val Loss: 0.0417\n",
      "Epoch [3/50], Train Loss: 0.0238, Val Loss: 0.0218\n",
      "Epoch [4/50], Train Loss: 0.0196, Val Loss: 0.0111\n",
      "Epoch [5/50], Train Loss: 0.0137, Val Loss: 0.0029\n",
      "Epoch [6/50], Train Loss: 0.0064, Val Loss: 0.0046\n",
      "Epoch [7/50], Train Loss: 0.0050, Val Loss: 0.0054\n",
      "Epoch [8/50], Train Loss: 0.0035, Val Loss: 0.0023\n",
      "Epoch [9/50], Train Loss: 0.0045, Val Loss: 0.0048\n",
      "Epoch [10/50], Train Loss: 0.0040, Val Loss: 0.0027\n",
      "Epoch [11/50], Train Loss: 0.0045, Val Loss: 0.0042\n",
      "Epoch [12/50], Train Loss: 0.0035, Val Loss: 0.0060\n",
      "Epoch [13/50], Train Loss: 0.0042, Val Loss: 0.0031\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=128, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0486, Val Loss: 0.0293\n",
      "Epoch [2/50], Train Loss: 0.0406, Val Loss: 0.0376\n",
      "Epoch [3/50], Train Loss: 0.0252, Val Loss: 0.0208\n",
      "Epoch [4/50], Train Loss: 0.0207, Val Loss: 0.0073\n",
      "Epoch [5/50], Train Loss: 0.0153, Val Loss: 0.0030\n",
      "Epoch [6/50], Train Loss: 0.0105, Val Loss: 0.0034\n",
      "Epoch [7/50], Train Loss: 0.0088, Val Loss: 0.0045\n",
      "Epoch [8/50], Train Loss: 0.0076, Val Loss: 0.0041\n",
      "Epoch [9/50], Train Loss: 0.0071, Val Loss: 0.0028\n",
      "Epoch [10/50], Train Loss: 0.0069, Val Loss: 0.0025\n",
      "Epoch [11/50], Train Loss: 0.0054, Val Loss: 0.0034\n",
      "Epoch [12/50], Train Loss: 0.0057, Val Loss: 0.0039\n",
      "Epoch [13/50], Train Loss: 0.0056, Val Loss: 0.0019\n",
      "Epoch [14/50], Train Loss: 0.0054, Val Loss: 0.0021\n",
      "Epoch [15/50], Train Loss: 0.0047, Val Loss: 0.0026\n",
      "Epoch [16/50], Train Loss: 0.0050, Val Loss: 0.0055\n",
      "Epoch [17/50], Train Loss: 0.0048, Val Loss: 0.0022\n",
      "Epoch [18/50], Train Loss: 0.0060, Val Loss: 0.0025\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=128, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0297, Val Loss: 0.0741\n",
      "Epoch [2/50], Train Loss: 0.0618, Val Loss: 0.0327\n",
      "Epoch [3/50], Train Loss: 0.0298, Val Loss: 0.0037\n",
      "Epoch [4/50], Train Loss: 0.0171, Val Loss: 0.0069\n",
      "Epoch [5/50], Train Loss: 0.0115, Val Loss: 0.0039\n",
      "Epoch [6/50], Train Loss: 0.0052, Val Loss: 0.0035\n",
      "Epoch [7/50], Train Loss: 0.0054, Val Loss: 0.0071\n",
      "Epoch [8/50], Train Loss: 0.0039, Val Loss: 0.0058\n",
      "Epoch [9/50], Train Loss: 0.0041, Val Loss: 0.0027\n",
      "Epoch [10/50], Train Loss: 0.0050, Val Loss: 0.0109\n",
      "Epoch [11/50], Train Loss: 0.0033, Val Loss: 0.0053\n",
      "Epoch [12/50], Train Loss: 0.0052, Val Loss: 0.0021\n",
      "Epoch [13/50], Train Loss: 0.0040, Val Loss: 0.0073\n",
      "Epoch [14/50], Train Loss: 0.0024, Val Loss: 0.0031\n",
      "Epoch [15/50], Train Loss: 0.0047, Val Loss: 0.0022\n",
      "Epoch [16/50], Train Loss: 0.0030, Val Loss: 0.0047\n",
      "Epoch [17/50], Train Loss: 0.0022, Val Loss: 0.0024\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=128, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0374, Val Loss: 0.0499\n",
      "Epoch [2/50], Train Loss: 0.0597, Val Loss: 0.0380\n",
      "Epoch [3/50], Train Loss: 0.0309, Val Loss: 0.0077\n",
      "Epoch [4/50], Train Loss: 0.0200, Val Loss: 0.0033\n",
      "Epoch [5/50], Train Loss: 0.0114, Val Loss: 0.0033\n",
      "Epoch [6/50], Train Loss: 0.0076, Val Loss: 0.0103\n",
      "Epoch [7/50], Train Loss: 0.0067, Val Loss: 0.0096\n",
      "Epoch [8/50], Train Loss: 0.0067, Val Loss: 0.0032\n",
      "Epoch [9/50], Train Loss: 0.0054, Val Loss: 0.0030\n",
      "Epoch [10/50], Train Loss: 0.0064, Val Loss: 0.0146\n",
      "Epoch [11/50], Train Loss: 0.0065, Val Loss: 0.0028\n",
      "Epoch [12/50], Train Loss: 0.0084, Val Loss: 0.0093\n",
      "Epoch [13/50], Train Loss: 0.0060, Val Loss: 0.0187\n",
      "Epoch [14/50], Train Loss: 0.0045, Val Loss: 0.0029\n",
      "Epoch [15/50], Train Loss: 0.0070, Val Loss: 0.0032\n",
      "Epoch [16/50], Train Loss: 0.0047, Val Loss: 0.0107\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=128, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0383, Val Loss: 0.0527\n",
      "Epoch [2/50], Train Loss: 0.0657, Val Loss: 0.0634\n",
      "Epoch [3/50], Train Loss: 0.0285, Val Loss: 0.0094\n",
      "Epoch [4/50], Train Loss: 0.0181, Val Loss: 0.0056\n",
      "Epoch [5/50], Train Loss: 0.0115, Val Loss: 0.0065\n",
      "Epoch [6/50], Train Loss: 0.0090, Val Loss: 0.0053\n",
      "Epoch [7/50], Train Loss: 0.0093, Val Loss: 0.0024\n",
      "Epoch [8/50], Train Loss: 0.0084, Val Loss: 0.0049\n",
      "Epoch [9/50], Train Loss: 0.0096, Val Loss: 0.0189\n",
      "Epoch [10/50], Train Loss: 0.0092, Val Loss: 0.0023\n",
      "Epoch [11/50], Train Loss: 0.0091, Val Loss: 0.0025\n",
      "Epoch [12/50], Train Loss: 0.0086, Val Loss: 0.0175\n",
      "Epoch [13/50], Train Loss: 0.0085, Val Loss: 0.0017\n",
      "Epoch [14/50], Train Loss: 0.0086, Val Loss: 0.0019\n",
      "Epoch [15/50], Train Loss: 0.0076, Val Loss: 0.0127\n",
      "Epoch [16/50], Train Loss: 0.0072, Val Loss: 0.0089\n",
      "Epoch [17/50], Train Loss: 0.0065, Val Loss: 0.0021\n",
      "Epoch [18/50], Train Loss: 0.0062, Val Loss: 0.0025\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=128, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0238, Val Loss: 0.0720\n",
      "Epoch [2/50], Train Loss: 0.0667, Val Loss: 0.0124\n",
      "Epoch [3/50], Train Loss: 0.0414, Val Loss: 0.0062\n",
      "Epoch [4/50], Train Loss: 0.0200, Val Loss: 0.0209\n",
      "Epoch [5/50], Train Loss: 0.0200, Val Loss: 0.0104\n",
      "Epoch [6/50], Train Loss: 0.0215, Val Loss: 0.0160\n",
      "Epoch [7/50], Train Loss: 0.0071, Val Loss: 0.0087\n",
      "Epoch [8/50], Train Loss: 0.0060, Val Loss: 0.0075\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=128, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0338, Val Loss: 0.0850\n",
      "Epoch [2/50], Train Loss: 0.0677, Val Loss: 0.0377\n",
      "Epoch [3/50], Train Loss: 0.0437, Val Loss: 0.0132\n",
      "Epoch [4/50], Train Loss: 0.0300, Val Loss: 0.0123\n",
      "Epoch [5/50], Train Loss: 0.0159, Val Loss: 0.0030\n",
      "Epoch [6/50], Train Loss: 0.0104, Val Loss: 0.0145\n",
      "Epoch [7/50], Train Loss: 0.0102, Val Loss: 0.0029\n",
      "Epoch [8/50], Train Loss: 0.0080, Val Loss: 0.0088\n",
      "Epoch [9/50], Train Loss: 0.0079, Val Loss: 0.0091\n",
      "Epoch [10/50], Train Loss: 0.0081, Val Loss: 0.0036\n",
      "Epoch [11/50], Train Loss: 0.0070, Val Loss: 0.0109\n",
      "Epoch [12/50], Train Loss: 0.0076, Val Loss: 0.0058\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adam, hidden_size=128, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0317, Val Loss: 0.0927\n",
      "Epoch [2/50], Train Loss: 0.0659, Val Loss: 0.0190\n",
      "Epoch [3/50], Train Loss: 0.0445, Val Loss: 0.0046\n",
      "Epoch [4/50], Train Loss: 0.0236, Val Loss: 0.0127\n",
      "Epoch [5/50], Train Loss: 0.0206, Val Loss: 0.0138\n",
      "Epoch [6/50], Train Loss: 0.0206, Val Loss: 0.0157\n",
      "Epoch [7/50], Train Loss: 0.0104, Val Loss: 0.0191\n",
      "Epoch [8/50], Train Loss: 0.0091, Val Loss: 0.0040\n",
      "Epoch [9/50], Train Loss: 0.0116, Val Loss: 0.0119\n",
      "Epoch [10/50], Train Loss: 0.0115, Val Loss: 0.0331\n",
      "Epoch [11/50], Train Loss: 0.0101, Val Loss: 0.0029\n",
      "Epoch [12/50], Train Loss: 0.0136, Val Loss: 0.0092\n",
      "Epoch [13/50], Train Loss: 0.0171, Val Loss: 0.0326\n",
      "Epoch [14/50], Train Loss: 0.0182, Val Loss: 0.0035\n",
      "Epoch [15/50], Train Loss: 0.0091, Val Loss: 0.0048\n",
      "Epoch [16/50], Train Loss: 0.0076, Val Loss: 0.0080\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=16, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1810, Val Loss: 0.4952\n",
      "Epoch [2/50], Train Loss: 0.1772, Val Loss: 0.4876\n",
      "Epoch [3/50], Train Loss: 0.1735, Val Loss: 0.4802\n",
      "Epoch [4/50], Train Loss: 0.1699, Val Loss: 0.4730\n",
      "Epoch [5/50], Train Loss: 0.1665, Val Loss: 0.4660\n",
      "Epoch [6/50], Train Loss: 0.1631, Val Loss: 0.4592\n",
      "Epoch [7/50], Train Loss: 0.1598, Val Loss: 0.4525\n",
      "Epoch [8/50], Train Loss: 0.1566, Val Loss: 0.4459\n",
      "Epoch [9/50], Train Loss: 0.1536, Val Loss: 0.4396\n",
      "Epoch [10/50], Train Loss: 0.1506, Val Loss: 0.4333\n",
      "Epoch [11/50], Train Loss: 0.1476, Val Loss: 0.4272\n",
      "Epoch [12/50], Train Loss: 0.1448, Val Loss: 0.4212\n",
      "Epoch [13/50], Train Loss: 0.1420, Val Loss: 0.4153\n",
      "Epoch [14/50], Train Loss: 0.1393, Val Loss: 0.4096\n",
      "Epoch [15/50], Train Loss: 0.1367, Val Loss: 0.4040\n",
      "Epoch [16/50], Train Loss: 0.1342, Val Loss: 0.3985\n",
      "Epoch [17/50], Train Loss: 0.1317, Val Loss: 0.3932\n",
      "Epoch [18/50], Train Loss: 0.1293, Val Loss: 0.3879\n",
      "Epoch [19/50], Train Loss: 0.1269, Val Loss: 0.3828\n",
      "Epoch [20/50], Train Loss: 0.1247, Val Loss: 0.3777\n",
      "Epoch [21/50], Train Loss: 0.1224, Val Loss: 0.3728\n",
      "Epoch [22/50], Train Loss: 0.1203, Val Loss: 0.3679\n",
      "Epoch [23/50], Train Loss: 0.1182, Val Loss: 0.3632\n",
      "Epoch [24/50], Train Loss: 0.1161, Val Loss: 0.3586\n",
      "Epoch [25/50], Train Loss: 0.1141, Val Loss: 0.3540\n",
      "Epoch [26/50], Train Loss: 0.1122, Val Loss: 0.3496\n",
      "Epoch [27/50], Train Loss: 0.1103, Val Loss: 0.3452\n",
      "Epoch [28/50], Train Loss: 0.1084, Val Loss: 0.3409\n",
      "Epoch [29/50], Train Loss: 0.1066, Val Loss: 0.3367\n",
      "Epoch [30/50], Train Loss: 0.1048, Val Loss: 0.3326\n",
      "Epoch [31/50], Train Loss: 0.1031, Val Loss: 0.3285\n",
      "Epoch [32/50], Train Loss: 0.1015, Val Loss: 0.3246\n",
      "Epoch [33/50], Train Loss: 0.0998, Val Loss: 0.3207\n",
      "Epoch [34/50], Train Loss: 0.0983, Val Loss: 0.3168\n",
      "Epoch [35/50], Train Loss: 0.0967, Val Loss: 0.3131\n",
      "Epoch [36/50], Train Loss: 0.0952, Val Loss: 0.3094\n",
      "Epoch [37/50], Train Loss: 0.0937, Val Loss: 0.3058\n",
      "Epoch [38/50], Train Loss: 0.0923, Val Loss: 0.3023\n",
      "Epoch [39/50], Train Loss: 0.0909, Val Loss: 0.2988\n",
      "Epoch [40/50], Train Loss: 0.0895, Val Loss: 0.2954\n",
      "Epoch [41/50], Train Loss: 0.0882, Val Loss: 0.2921\n",
      "Epoch [42/50], Train Loss: 0.0869, Val Loss: 0.2888\n",
      "Epoch [43/50], Train Loss: 0.0856, Val Loss: 0.2856\n",
      "Epoch [44/50], Train Loss: 0.0844, Val Loss: 0.2824\n",
      "Epoch [45/50], Train Loss: 0.0832, Val Loss: 0.2793\n",
      "Epoch [46/50], Train Loss: 0.0820, Val Loss: 0.2763\n",
      "Epoch [47/50], Train Loss: 0.0809, Val Loss: 0.2732\n",
      "Epoch [48/50], Train Loss: 0.0798, Val Loss: 0.2703\n",
      "Epoch [49/50], Train Loss: 0.0787, Val Loss: 0.2674\n",
      "Epoch [50/50], Train Loss: 0.0776, Val Loss: 0.2646\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=16, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0917, Val Loss: 0.2913\n",
      "Epoch [2/50], Train Loss: 0.0905, Val Loss: 0.2886\n",
      "Epoch [3/50], Train Loss: 0.0904, Val Loss: 0.2859\n",
      "Epoch [4/50], Train Loss: 0.0890, Val Loss: 0.2833\n",
      "Epoch [5/50], Train Loss: 0.0880, Val Loss: 0.2807\n",
      "Epoch [6/50], Train Loss: 0.0856, Val Loss: 0.2782\n",
      "Epoch [7/50], Train Loss: 0.0849, Val Loss: 0.2757\n",
      "Epoch [8/50], Train Loss: 0.0855, Val Loss: 0.2733\n",
      "Epoch [9/50], Train Loss: 0.0843, Val Loss: 0.2709\n",
      "Epoch [10/50], Train Loss: 0.0819, Val Loss: 0.2685\n",
      "Epoch [11/50], Train Loss: 0.0815, Val Loss: 0.2662\n",
      "Epoch [12/50], Train Loss: 0.0811, Val Loss: 0.2639\n",
      "Epoch [13/50], Train Loss: 0.0800, Val Loss: 0.2616\n",
      "Epoch [14/50], Train Loss: 0.0791, Val Loss: 0.2594\n",
      "Epoch [15/50], Train Loss: 0.0785, Val Loss: 0.2572\n",
      "Epoch [16/50], Train Loss: 0.0763, Val Loss: 0.2551\n",
      "Epoch [17/50], Train Loss: 0.0772, Val Loss: 0.2530\n",
      "Epoch [18/50], Train Loss: 0.0762, Val Loss: 0.2509\n",
      "Epoch [19/50], Train Loss: 0.0746, Val Loss: 0.2488\n",
      "Epoch [20/50], Train Loss: 0.0747, Val Loss: 0.2468\n",
      "Epoch [21/50], Train Loss: 0.0745, Val Loss: 0.2449\n",
      "Epoch [22/50], Train Loss: 0.0732, Val Loss: 0.2429\n",
      "Epoch [23/50], Train Loss: 0.0726, Val Loss: 0.2410\n",
      "Epoch [24/50], Train Loss: 0.0723, Val Loss: 0.2391\n",
      "Epoch [25/50], Train Loss: 0.0712, Val Loss: 0.2372\n",
      "Epoch [26/50], Train Loss: 0.0704, Val Loss: 0.2354\n",
      "Epoch [27/50], Train Loss: 0.0706, Val Loss: 0.2336\n",
      "Epoch [28/50], Train Loss: 0.0693, Val Loss: 0.2318\n",
      "Epoch [29/50], Train Loss: 0.0689, Val Loss: 0.2300\n",
      "Epoch [30/50], Train Loss: 0.0679, Val Loss: 0.2283\n",
      "Epoch [31/50], Train Loss: 0.0681, Val Loss: 0.2266\n",
      "Epoch [32/50], Train Loss: 0.0669, Val Loss: 0.2249\n",
      "Epoch [33/50], Train Loss: 0.0661, Val Loss: 0.2233\n",
      "Epoch [34/50], Train Loss: 0.0664, Val Loss: 0.2216\n",
      "Epoch [35/50], Train Loss: 0.0655, Val Loss: 0.2201\n",
      "Epoch [36/50], Train Loss: 0.0648, Val Loss: 0.2185\n",
      "Epoch [37/50], Train Loss: 0.0645, Val Loss: 0.2169\n",
      "Epoch [38/50], Train Loss: 0.0637, Val Loss: 0.2154\n",
      "Epoch [39/50], Train Loss: 0.0629, Val Loss: 0.2139\n",
      "Epoch [40/50], Train Loss: 0.0630, Val Loss: 0.2124\n",
      "Epoch [41/50], Train Loss: 0.0627, Val Loss: 0.2110\n",
      "Epoch [42/50], Train Loss: 0.0618, Val Loss: 0.2095\n",
      "Epoch [43/50], Train Loss: 0.0607, Val Loss: 0.2081\n",
      "Epoch [44/50], Train Loss: 0.0612, Val Loss: 0.2067\n",
      "Epoch [45/50], Train Loss: 0.0613, Val Loss: 0.2053\n",
      "Epoch [46/50], Train Loss: 0.0601, Val Loss: 0.2040\n",
      "Epoch [47/50], Train Loss: 0.0594, Val Loss: 0.2026\n",
      "Epoch [48/50], Train Loss: 0.0602, Val Loss: 0.2013\n",
      "Epoch [49/50], Train Loss: 0.0594, Val Loss: 0.2000\n",
      "Epoch [50/50], Train Loss: 0.0584, Val Loss: 0.1987\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=16, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.2103, Val Loss: 0.4041\n",
      "Epoch [2/50], Train Loss: 0.2079, Val Loss: 0.3979\n",
      "Epoch [3/50], Train Loss: 0.2033, Val Loss: 0.3919\n",
      "Epoch [4/50], Train Loss: 0.1982, Val Loss: 0.3861\n",
      "Epoch [5/50], Train Loss: 0.1956, Val Loss: 0.3804\n",
      "Epoch [6/50], Train Loss: 0.1898, Val Loss: 0.3750\n",
      "Epoch [7/50], Train Loss: 0.1837, Val Loss: 0.3697\n",
      "Epoch [8/50], Train Loss: 0.1820, Val Loss: 0.3646\n",
      "Epoch [9/50], Train Loss: 0.1772, Val Loss: 0.3596\n",
      "Epoch [10/50], Train Loss: 0.1752, Val Loss: 0.3547\n",
      "Epoch [11/50], Train Loss: 0.1728, Val Loss: 0.3498\n",
      "Epoch [12/50], Train Loss: 0.1670, Val Loss: 0.3453\n",
      "Epoch [13/50], Train Loss: 0.1653, Val Loss: 0.3408\n",
      "Epoch [14/50], Train Loss: 0.1619, Val Loss: 0.3363\n",
      "Epoch [15/50], Train Loss: 0.1605, Val Loss: 0.3320\n",
      "Epoch [16/50], Train Loss: 0.1548, Val Loss: 0.3279\n",
      "Epoch [17/50], Train Loss: 0.1531, Val Loss: 0.3238\n",
      "Epoch [18/50], Train Loss: 0.1500, Val Loss: 0.3199\n",
      "Epoch [19/50], Train Loss: 0.1462, Val Loss: 0.3162\n",
      "Epoch [20/50], Train Loss: 0.1438, Val Loss: 0.3125\n",
      "Epoch [21/50], Train Loss: 0.1412, Val Loss: 0.3088\n",
      "Epoch [22/50], Train Loss: 0.1400, Val Loss: 0.3052\n",
      "Epoch [23/50], Train Loss: 0.1359, Val Loss: 0.3017\n",
      "Epoch [24/50], Train Loss: 0.1323, Val Loss: 0.2983\n",
      "Epoch [25/50], Train Loss: 0.1332, Val Loss: 0.2950\n",
      "Epoch [26/50], Train Loss: 0.1311, Val Loss: 0.2917\n",
      "Epoch [27/50], Train Loss: 0.1287, Val Loss: 0.2886\n",
      "Epoch [28/50], Train Loss: 0.1263, Val Loss: 0.2855\n",
      "Epoch [29/50], Train Loss: 0.1248, Val Loss: 0.2824\n",
      "Epoch [30/50], Train Loss: 0.1226, Val Loss: 0.2794\n",
      "Epoch [31/50], Train Loss: 0.1191, Val Loss: 0.2765\n",
      "Epoch [32/50], Train Loss: 0.1182, Val Loss: 0.2736\n",
      "Epoch [33/50], Train Loss: 0.1144, Val Loss: 0.2709\n",
      "Epoch [34/50], Train Loss: 0.1160, Val Loss: 0.2682\n",
      "Epoch [35/50], Train Loss: 0.1102, Val Loss: 0.2655\n",
      "Epoch [36/50], Train Loss: 0.1144, Val Loss: 0.2629\n",
      "Epoch [37/50], Train Loss: 0.1106, Val Loss: 0.2604\n",
      "Epoch [38/50], Train Loss: 0.1097, Val Loss: 0.2579\n",
      "Epoch [39/50], Train Loss: 0.1094, Val Loss: 0.2555\n",
      "Epoch [40/50], Train Loss: 0.1055, Val Loss: 0.2531\n",
      "Epoch [41/50], Train Loss: 0.1037, Val Loss: 0.2508\n",
      "Epoch [42/50], Train Loss: 0.1031, Val Loss: 0.2485\n",
      "Epoch [43/50], Train Loss: 0.1025, Val Loss: 0.2463\n",
      "Epoch [44/50], Train Loss: 0.1012, Val Loss: 0.2441\n",
      "Epoch [45/50], Train Loss: 0.0983, Val Loss: 0.2419\n",
      "Epoch [46/50], Train Loss: 0.0971, Val Loss: 0.2398\n",
      "Epoch [47/50], Train Loss: 0.0971, Val Loss: 0.2377\n",
      "Epoch [48/50], Train Loss: 0.0939, Val Loss: 0.2357\n",
      "Epoch [49/50], Train Loss: 0.0936, Val Loss: 0.2337\n",
      "Epoch [50/50], Train Loss: 0.0917, Val Loss: 0.2317\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=16, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1046, Val Loss: 0.2854\n",
      "Epoch [2/50], Train Loss: 0.1031, Val Loss: 0.2826\n",
      "Epoch [3/50], Train Loss: 0.1016, Val Loss: 0.2798\n",
      "Epoch [4/50], Train Loss: 0.1001, Val Loss: 0.2771\n",
      "Epoch [5/50], Train Loss: 0.0987, Val Loss: 0.2744\n",
      "Epoch [6/50], Train Loss: 0.0973, Val Loss: 0.2718\n",
      "Epoch [7/50], Train Loss: 0.0959, Val Loss: 0.2692\n",
      "Epoch [8/50], Train Loss: 0.0946, Val Loss: 0.2667\n",
      "Epoch [9/50], Train Loss: 0.0933, Val Loss: 0.2643\n",
      "Epoch [10/50], Train Loss: 0.0920, Val Loss: 0.2619\n",
      "Epoch [11/50], Train Loss: 0.0908, Val Loss: 0.2595\n",
      "Epoch [12/50], Train Loss: 0.0896, Val Loss: 0.2572\n",
      "Epoch [13/50], Train Loss: 0.0884, Val Loss: 0.2549\n",
      "Epoch [14/50], Train Loss: 0.0872, Val Loss: 0.2527\n",
      "Epoch [15/50], Train Loss: 0.0861, Val Loss: 0.2505\n",
      "Epoch [16/50], Train Loss: 0.0850, Val Loss: 0.2483\n",
      "Epoch [17/50], Train Loss: 0.0840, Val Loss: 0.2462\n",
      "Epoch [18/50], Train Loss: 0.0829, Val Loss: 0.2441\n",
      "Epoch [19/50], Train Loss: 0.0819, Val Loss: 0.2421\n",
      "Epoch [20/50], Train Loss: 0.0809, Val Loss: 0.2401\n",
      "Epoch [21/50], Train Loss: 0.0799, Val Loss: 0.2381\n",
      "Epoch [22/50], Train Loss: 0.0790, Val Loss: 0.2362\n",
      "Epoch [23/50], Train Loss: 0.0780, Val Loss: 0.2343\n",
      "Epoch [24/50], Train Loss: 0.0771, Val Loss: 0.2324\n",
      "Epoch [25/50], Train Loss: 0.0763, Val Loss: 0.2306\n",
      "Epoch [26/50], Train Loss: 0.0754, Val Loss: 0.2288\n",
      "Epoch [27/50], Train Loss: 0.0745, Val Loss: 0.2270\n",
      "Epoch [28/50], Train Loss: 0.0737, Val Loss: 0.2253\n",
      "Epoch [29/50], Train Loss: 0.0729, Val Loss: 0.2236\n",
      "Epoch [30/50], Train Loss: 0.0721, Val Loss: 0.2219\n",
      "Epoch [31/50], Train Loss: 0.0714, Val Loss: 0.2202\n",
      "Epoch [32/50], Train Loss: 0.0706, Val Loss: 0.2186\n",
      "Epoch [33/50], Train Loss: 0.0699, Val Loss: 0.2170\n",
      "Epoch [34/50], Train Loss: 0.0691, Val Loss: 0.2155\n",
      "Epoch [35/50], Train Loss: 0.0684, Val Loss: 0.2139\n",
      "Epoch [36/50], Train Loss: 0.0678, Val Loss: 0.2124\n",
      "Epoch [37/50], Train Loss: 0.0671, Val Loss: 0.2109\n",
      "Epoch [38/50], Train Loss: 0.0664, Val Loss: 0.2094\n",
      "Epoch [39/50], Train Loss: 0.0658, Val Loss: 0.2080\n",
      "Epoch [40/50], Train Loss: 0.0652, Val Loss: 0.2066\n",
      "Epoch [41/50], Train Loss: 0.0646, Val Loss: 0.2052\n",
      "Epoch [42/50], Train Loss: 0.0640, Val Loss: 0.2038\n",
      "Epoch [43/50], Train Loss: 0.0634, Val Loss: 0.2025\n",
      "Epoch [44/50], Train Loss: 0.0628, Val Loss: 0.2011\n",
      "Epoch [45/50], Train Loss: 0.0622, Val Loss: 0.1999\n",
      "Epoch [46/50], Train Loss: 0.0617, Val Loss: 0.1986\n",
      "Epoch [47/50], Train Loss: 0.0612, Val Loss: 0.1973\n",
      "Epoch [48/50], Train Loss: 0.0606, Val Loss: 0.1961\n",
      "Epoch [49/50], Train Loss: 0.0601, Val Loss: 0.1948\n",
      "Epoch [50/50], Train Loss: 0.0596, Val Loss: 0.1936\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=16, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1025, Val Loss: 0.3173\n",
      "Epoch [2/50], Train Loss: 0.1004, Val Loss: 0.3142\n",
      "Epoch [3/50], Train Loss: 0.0998, Val Loss: 0.3111\n",
      "Epoch [4/50], Train Loss: 0.0985, Val Loss: 0.3081\n",
      "Epoch [5/50], Train Loss: 0.0964, Val Loss: 0.3051\n",
      "Epoch [6/50], Train Loss: 0.0950, Val Loss: 0.3022\n",
      "Epoch [7/50], Train Loss: 0.0946, Val Loss: 0.2993\n",
      "Epoch [8/50], Train Loss: 0.0923, Val Loss: 0.2965\n",
      "Epoch [9/50], Train Loss: 0.0903, Val Loss: 0.2937\n",
      "Epoch [10/50], Train Loss: 0.0906, Val Loss: 0.2909\n",
      "Epoch [11/50], Train Loss: 0.0891, Val Loss: 0.2883\n",
      "Epoch [12/50], Train Loss: 0.0873, Val Loss: 0.2856\n",
      "Epoch [13/50], Train Loss: 0.0868, Val Loss: 0.2830\n",
      "Epoch [14/50], Train Loss: 0.0850, Val Loss: 0.2804\n",
      "Epoch [15/50], Train Loss: 0.0857, Val Loss: 0.2779\n",
      "Epoch [16/50], Train Loss: 0.0836, Val Loss: 0.2754\n",
      "Epoch [17/50], Train Loss: 0.0835, Val Loss: 0.2729\n",
      "Epoch [18/50], Train Loss: 0.0815, Val Loss: 0.2706\n",
      "Epoch [19/50], Train Loss: 0.0804, Val Loss: 0.2682\n",
      "Epoch [20/50], Train Loss: 0.0803, Val Loss: 0.2658\n",
      "Epoch [21/50], Train Loss: 0.0794, Val Loss: 0.2636\n",
      "Epoch [22/50], Train Loss: 0.0791, Val Loss: 0.2613\n",
      "Epoch [23/50], Train Loss: 0.0778, Val Loss: 0.2590\n",
      "Epoch [24/50], Train Loss: 0.0761, Val Loss: 0.2569\n",
      "Epoch [25/50], Train Loss: 0.0758, Val Loss: 0.2547\n",
      "Epoch [26/50], Train Loss: 0.0743, Val Loss: 0.2526\n",
      "Epoch [27/50], Train Loss: 0.0743, Val Loss: 0.2505\n",
      "Epoch [28/50], Train Loss: 0.0727, Val Loss: 0.2485\n",
      "Epoch [29/50], Train Loss: 0.0714, Val Loss: 0.2465\n",
      "Epoch [30/50], Train Loss: 0.0714, Val Loss: 0.2445\n",
      "Epoch [31/50], Train Loss: 0.0706, Val Loss: 0.2425\n",
      "Epoch [32/50], Train Loss: 0.0699, Val Loss: 0.2406\n",
      "Epoch [33/50], Train Loss: 0.0694, Val Loss: 0.2387\n",
      "Epoch [34/50], Train Loss: 0.0691, Val Loss: 0.2368\n",
      "Epoch [35/50], Train Loss: 0.0680, Val Loss: 0.2350\n",
      "Epoch [36/50], Train Loss: 0.0675, Val Loss: 0.2332\n",
      "Epoch [37/50], Train Loss: 0.0668, Val Loss: 0.2314\n",
      "Epoch [38/50], Train Loss: 0.0659, Val Loss: 0.2296\n",
      "Epoch [39/50], Train Loss: 0.0663, Val Loss: 0.2279\n",
      "Epoch [40/50], Train Loss: 0.0653, Val Loss: 0.2261\n",
      "Epoch [41/50], Train Loss: 0.0638, Val Loss: 0.2245\n",
      "Epoch [42/50], Train Loss: 0.0632, Val Loss: 0.2228\n",
      "Epoch [43/50], Train Loss: 0.0636, Val Loss: 0.2212\n",
      "Epoch [44/50], Train Loss: 0.0623, Val Loss: 0.2196\n",
      "Epoch [45/50], Train Loss: 0.0623, Val Loss: 0.2180\n",
      "Epoch [46/50], Train Loss: 0.0620, Val Loss: 0.2165\n",
      "Epoch [47/50], Train Loss: 0.0614, Val Loss: 0.2149\n",
      "Epoch [48/50], Train Loss: 0.0611, Val Loss: 0.2134\n",
      "Epoch [49/50], Train Loss: 0.0603, Val Loss: 0.2120\n",
      "Epoch [50/50], Train Loss: 0.0606, Val Loss: 0.2105\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=16, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.2520, Val Loss: 0.4681\n",
      "Epoch [2/50], Train Loss: 0.2496, Val Loss: 0.4596\n",
      "Epoch [3/50], Train Loss: 0.2452, Val Loss: 0.4514\n",
      "Epoch [4/50], Train Loss: 0.2345, Val Loss: 0.4437\n",
      "Epoch [5/50], Train Loss: 0.2273, Val Loss: 0.4363\n",
      "Epoch [6/50], Train Loss: 0.2281, Val Loss: 0.4290\n",
      "Epoch [7/50], Train Loss: 0.2173, Val Loss: 0.4223\n",
      "Epoch [8/50], Train Loss: 0.2140, Val Loss: 0.4157\n",
      "Epoch [9/50], Train Loss: 0.2109, Val Loss: 0.4094\n",
      "Epoch [10/50], Train Loss: 0.2032, Val Loss: 0.4033\n",
      "Epoch [11/50], Train Loss: 0.1959, Val Loss: 0.3975\n",
      "Epoch [12/50], Train Loss: 0.1958, Val Loss: 0.3918\n",
      "Epoch [13/50], Train Loss: 0.1898, Val Loss: 0.3864\n",
      "Epoch [14/50], Train Loss: 0.1883, Val Loss: 0.3810\n",
      "Epoch [15/50], Train Loss: 0.1844, Val Loss: 0.3759\n",
      "Epoch [16/50], Train Loss: 0.1802, Val Loss: 0.3709\n",
      "Epoch [17/50], Train Loss: 0.1779, Val Loss: 0.3660\n",
      "Epoch [18/50], Train Loss: 0.1733, Val Loss: 0.3614\n",
      "Epoch [19/50], Train Loss: 0.1712, Val Loss: 0.3568\n",
      "Epoch [20/50], Train Loss: 0.1678, Val Loss: 0.3524\n",
      "Epoch [21/50], Train Loss: 0.1626, Val Loss: 0.3482\n",
      "Epoch [22/50], Train Loss: 0.1615, Val Loss: 0.3441\n",
      "Epoch [23/50], Train Loss: 0.1586, Val Loss: 0.3400\n",
      "Epoch [24/50], Train Loss: 0.1552, Val Loss: 0.3361\n",
      "Epoch [25/50], Train Loss: 0.1530, Val Loss: 0.3322\n",
      "Epoch [26/50], Train Loss: 0.1535, Val Loss: 0.3285\n",
      "Epoch [27/50], Train Loss: 0.1481, Val Loss: 0.3248\n",
      "Epoch [28/50], Train Loss: 0.1452, Val Loss: 0.3213\n",
      "Epoch [29/50], Train Loss: 0.1438, Val Loss: 0.3177\n",
      "Epoch [30/50], Train Loss: 0.1421, Val Loss: 0.3143\n",
      "Epoch [31/50], Train Loss: 0.1386, Val Loss: 0.3110\n",
      "Epoch [32/50], Train Loss: 0.1349, Val Loss: 0.3077\n",
      "Epoch [33/50], Train Loss: 0.1360, Val Loss: 0.3045\n",
      "Epoch [34/50], Train Loss: 0.1319, Val Loss: 0.3014\n",
      "Epoch [35/50], Train Loss: 0.1313, Val Loss: 0.2983\n",
      "Epoch [36/50], Train Loss: 0.1305, Val Loss: 0.2954\n",
      "Epoch [37/50], Train Loss: 0.1271, Val Loss: 0.2924\n",
      "Epoch [38/50], Train Loss: 0.1254, Val Loss: 0.2896\n",
      "Epoch [39/50], Train Loss: 0.1250, Val Loss: 0.2868\n",
      "Epoch [40/50], Train Loss: 0.1229, Val Loss: 0.2841\n",
      "Epoch [41/50], Train Loss: 0.1206, Val Loss: 0.2814\n",
      "Epoch [42/50], Train Loss: 0.1205, Val Loss: 0.2787\n",
      "Epoch [43/50], Train Loss: 0.1188, Val Loss: 0.2762\n",
      "Epoch [44/50], Train Loss: 0.1148, Val Loss: 0.2737\n",
      "Epoch [45/50], Train Loss: 0.1140, Val Loss: 0.2712\n",
      "Epoch [46/50], Train Loss: 0.1132, Val Loss: 0.2688\n",
      "Epoch [47/50], Train Loss: 0.1114, Val Loss: 0.2664\n",
      "Epoch [48/50], Train Loss: 0.1087, Val Loss: 0.2641\n",
      "Epoch [49/50], Train Loss: 0.1084, Val Loss: 0.2618\n",
      "Epoch [50/50], Train Loss: 0.1071, Val Loss: 0.2596\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=16, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.2164, Val Loss: 0.4275\n",
      "Epoch [2/50], Train Loss: 0.2086, Val Loss: 0.4167\n",
      "Epoch [3/50], Train Loss: 0.2011, Val Loss: 0.4063\n",
      "Epoch [4/50], Train Loss: 0.1939, Val Loss: 0.3961\n",
      "Epoch [5/50], Train Loss: 0.1870, Val Loss: 0.3864\n",
      "Epoch [6/50], Train Loss: 0.1804, Val Loss: 0.3770\n",
      "Epoch [7/50], Train Loss: 0.1741, Val Loss: 0.3678\n",
      "Epoch [8/50], Train Loss: 0.1680, Val Loss: 0.3590\n",
      "Epoch [9/50], Train Loss: 0.1622, Val Loss: 0.3505\n",
      "Epoch [10/50], Train Loss: 0.1566, Val Loss: 0.3423\n",
      "Epoch [11/50], Train Loss: 0.1513, Val Loss: 0.3343\n",
      "Epoch [12/50], Train Loss: 0.1461, Val Loss: 0.3265\n",
      "Epoch [13/50], Train Loss: 0.1412, Val Loss: 0.3191\n",
      "Epoch [14/50], Train Loss: 0.1365, Val Loss: 0.3118\n",
      "Epoch [15/50], Train Loss: 0.1319, Val Loss: 0.3048\n",
      "Epoch [16/50], Train Loss: 0.1276, Val Loss: 0.2980\n",
      "Epoch [17/50], Train Loss: 0.1234, Val Loss: 0.2915\n",
      "Epoch [18/50], Train Loss: 0.1194, Val Loss: 0.2851\n",
      "Epoch [19/50], Train Loss: 0.1155, Val Loss: 0.2789\n",
      "Epoch [20/50], Train Loss: 0.1118, Val Loss: 0.2729\n",
      "Epoch [21/50], Train Loss: 0.1083, Val Loss: 0.2671\n",
      "Epoch [22/50], Train Loss: 0.1049, Val Loss: 0.2615\n",
      "Epoch [23/50], Train Loss: 0.1016, Val Loss: 0.2561\n",
      "Epoch [24/50], Train Loss: 0.0985, Val Loss: 0.2508\n",
      "Epoch [25/50], Train Loss: 0.0955, Val Loss: 0.2457\n",
      "Epoch [26/50], Train Loss: 0.0926, Val Loss: 0.2408\n",
      "Epoch [27/50], Train Loss: 0.0899, Val Loss: 0.2360\n",
      "Epoch [28/50], Train Loss: 0.0872, Val Loss: 0.2314\n",
      "Epoch [29/50], Train Loss: 0.0847, Val Loss: 0.2269\n",
      "Epoch [30/50], Train Loss: 0.0823, Val Loss: 0.2225\n",
      "Epoch [31/50], Train Loss: 0.0800, Val Loss: 0.2183\n",
      "Epoch [32/50], Train Loss: 0.0778, Val Loss: 0.2142\n",
      "Epoch [33/50], Train Loss: 0.0756, Val Loss: 0.2103\n",
      "Epoch [34/50], Train Loss: 0.0736, Val Loss: 0.2064\n",
      "Epoch [35/50], Train Loss: 0.0716, Val Loss: 0.2028\n",
      "Epoch [36/50], Train Loss: 0.0698, Val Loss: 0.1991\n",
      "Epoch [37/50], Train Loss: 0.0680, Val Loss: 0.1957\n",
      "Epoch [38/50], Train Loss: 0.0663, Val Loss: 0.1923\n",
      "Epoch [39/50], Train Loss: 0.0647, Val Loss: 0.1891\n",
      "Epoch [40/50], Train Loss: 0.0631, Val Loss: 0.1859\n",
      "Epoch [41/50], Train Loss: 0.0616, Val Loss: 0.1829\n",
      "Epoch [42/50], Train Loss: 0.0602, Val Loss: 0.1799\n",
      "Epoch [43/50], Train Loss: 0.0588, Val Loss: 0.1770\n",
      "Epoch [44/50], Train Loss: 0.0575, Val Loss: 0.1742\n",
      "Epoch [45/50], Train Loss: 0.0563, Val Loss: 0.1716\n",
      "Epoch [46/50], Train Loss: 0.0551, Val Loss: 0.1690\n",
      "Epoch [47/50], Train Loss: 0.0540, Val Loss: 0.1665\n",
      "Epoch [48/50], Train Loss: 0.0529, Val Loss: 0.1640\n",
      "Epoch [49/50], Train Loss: 0.0519, Val Loss: 0.1617\n",
      "Epoch [50/50], Train Loss: 0.0509, Val Loss: 0.1594\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=16, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1475, Val Loss: 0.3719\n",
      "Epoch [2/50], Train Loss: 0.1447, Val Loss: 0.3671\n",
      "Epoch [3/50], Train Loss: 0.1428, Val Loss: 0.3623\n",
      "Epoch [4/50], Train Loss: 0.1408, Val Loss: 0.3576\n",
      "Epoch [5/50], Train Loss: 0.1373, Val Loss: 0.3531\n",
      "Epoch [6/50], Train Loss: 0.1353, Val Loss: 0.3486\n",
      "Epoch [7/50], Train Loss: 0.1310, Val Loss: 0.3442\n",
      "Epoch [8/50], Train Loss: 0.1283, Val Loss: 0.3399\n",
      "Epoch [9/50], Train Loss: 0.1274, Val Loss: 0.3356\n",
      "Epoch [10/50], Train Loss: 0.1241, Val Loss: 0.3315\n",
      "Epoch [11/50], Train Loss: 0.1225, Val Loss: 0.3274\n",
      "Epoch [12/50], Train Loss: 0.1197, Val Loss: 0.3234\n",
      "Epoch [13/50], Train Loss: 0.1178, Val Loss: 0.3195\n",
      "Epoch [14/50], Train Loss: 0.1171, Val Loss: 0.3156\n",
      "Epoch [15/50], Train Loss: 0.1139, Val Loss: 0.3118\n",
      "Epoch [16/50], Train Loss: 0.1115, Val Loss: 0.3081\n",
      "Epoch [17/50], Train Loss: 0.1113, Val Loss: 0.3045\n",
      "Epoch [18/50], Train Loss: 0.1094, Val Loss: 0.3009\n",
      "Epoch [19/50], Train Loss: 0.1078, Val Loss: 0.2974\n",
      "Epoch [20/50], Train Loss: 0.1051, Val Loss: 0.2939\n",
      "Epoch [21/50], Train Loss: 0.1030, Val Loss: 0.2906\n",
      "Epoch [22/50], Train Loss: 0.1012, Val Loss: 0.2872\n",
      "Epoch [23/50], Train Loss: 0.0991, Val Loss: 0.2840\n",
      "Epoch [24/50], Train Loss: 0.0973, Val Loss: 0.2807\n",
      "Epoch [25/50], Train Loss: 0.0945, Val Loss: 0.2776\n",
      "Epoch [26/50], Train Loss: 0.0953, Val Loss: 0.2745\n",
      "Epoch [27/50], Train Loss: 0.0930, Val Loss: 0.2714\n",
      "Epoch [28/50], Train Loss: 0.0927, Val Loss: 0.2684\n",
      "Epoch [29/50], Train Loss: 0.0915, Val Loss: 0.2655\n",
      "Epoch [30/50], Train Loss: 0.0886, Val Loss: 0.2626\n",
      "Epoch [31/50], Train Loss: 0.0870, Val Loss: 0.2598\n",
      "Epoch [32/50], Train Loss: 0.0868, Val Loss: 0.2570\n",
      "Epoch [33/50], Train Loss: 0.0852, Val Loss: 0.2542\n",
      "Epoch [34/50], Train Loss: 0.0834, Val Loss: 0.2515\n",
      "Epoch [35/50], Train Loss: 0.0817, Val Loss: 0.2489\n",
      "Epoch [36/50], Train Loss: 0.0818, Val Loss: 0.2463\n",
      "Epoch [37/50], Train Loss: 0.0791, Val Loss: 0.2437\n",
      "Epoch [38/50], Train Loss: 0.0798, Val Loss: 0.2412\n",
      "Epoch [39/50], Train Loss: 0.0771, Val Loss: 0.2387\n",
      "Epoch [40/50], Train Loss: 0.0757, Val Loss: 0.2363\n",
      "Epoch [41/50], Train Loss: 0.0758, Val Loss: 0.2339\n",
      "Epoch [42/50], Train Loss: 0.0755, Val Loss: 0.2316\n",
      "Epoch [43/50], Train Loss: 0.0735, Val Loss: 0.2293\n",
      "Epoch [44/50], Train Loss: 0.0730, Val Loss: 0.2271\n",
      "Epoch [45/50], Train Loss: 0.0716, Val Loss: 0.2248\n",
      "Epoch [46/50], Train Loss: 0.0715, Val Loss: 0.2226\n",
      "Epoch [47/50], Train Loss: 0.0712, Val Loss: 0.2205\n",
      "Epoch [48/50], Train Loss: 0.0691, Val Loss: 0.2184\n",
      "Epoch [49/50], Train Loss: 0.0684, Val Loss: 0.2164\n",
      "Epoch [50/50], Train Loss: 0.0663, Val Loss: 0.2143\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=16, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.2106, Val Loss: 0.4720\n",
      "Epoch [2/50], Train Loss: 0.2061, Val Loss: 0.4630\n",
      "Epoch [3/50], Train Loss: 0.2015, Val Loss: 0.4546\n",
      "Epoch [4/50], Train Loss: 0.1994, Val Loss: 0.4462\n",
      "Epoch [5/50], Train Loss: 0.1915, Val Loss: 0.4383\n",
      "Epoch [6/50], Train Loss: 0.1893, Val Loss: 0.4304\n",
      "Epoch [7/50], Train Loss: 0.1836, Val Loss: 0.4228\n",
      "Epoch [8/50], Train Loss: 0.1793, Val Loss: 0.4155\n",
      "Epoch [9/50], Train Loss: 0.1773, Val Loss: 0.4081\n",
      "Epoch [10/50], Train Loss: 0.1731, Val Loss: 0.4010\n",
      "Epoch [11/50], Train Loss: 0.1696, Val Loss: 0.3940\n",
      "Epoch [12/50], Train Loss: 0.1642, Val Loss: 0.3873\n",
      "Epoch [13/50], Train Loss: 0.1638, Val Loss: 0.3808\n",
      "Epoch [14/50], Train Loss: 0.1557, Val Loss: 0.3745\n",
      "Epoch [15/50], Train Loss: 0.1542, Val Loss: 0.3684\n",
      "Epoch [16/50], Train Loss: 0.1501, Val Loss: 0.3624\n",
      "Epoch [17/50], Train Loss: 0.1523, Val Loss: 0.3565\n",
      "Epoch [18/50], Train Loss: 0.1480, Val Loss: 0.3508\n",
      "Epoch [19/50], Train Loss: 0.1422, Val Loss: 0.3452\n",
      "Epoch [20/50], Train Loss: 0.1428, Val Loss: 0.3397\n",
      "Epoch [21/50], Train Loss: 0.1420, Val Loss: 0.3344\n",
      "Epoch [22/50], Train Loss: 0.1372, Val Loss: 0.3293\n",
      "Epoch [23/50], Train Loss: 0.1317, Val Loss: 0.3244\n",
      "Epoch [24/50], Train Loss: 0.1330, Val Loss: 0.3194\n",
      "Epoch [25/50], Train Loss: 0.1298, Val Loss: 0.3147\n",
      "Epoch [26/50], Train Loss: 0.1288, Val Loss: 0.3100\n",
      "Epoch [27/50], Train Loss: 0.1269, Val Loss: 0.3055\n",
      "Epoch [28/50], Train Loss: 0.1238, Val Loss: 0.3011\n",
      "Epoch [29/50], Train Loss: 0.1187, Val Loss: 0.2968\n",
      "Epoch [30/50], Train Loss: 0.1187, Val Loss: 0.2926\n",
      "Epoch [31/50], Train Loss: 0.1155, Val Loss: 0.2886\n",
      "Epoch [32/50], Train Loss: 0.1163, Val Loss: 0.2845\n",
      "Epoch [33/50], Train Loss: 0.1156, Val Loss: 0.2805\n",
      "Epoch [34/50], Train Loss: 0.1119, Val Loss: 0.2767\n",
      "Epoch [35/50], Train Loss: 0.1108, Val Loss: 0.2730\n",
      "Epoch [36/50], Train Loss: 0.1105, Val Loss: 0.2693\n",
      "Epoch [37/50], Train Loss: 0.1077, Val Loss: 0.2657\n",
      "Epoch [38/50], Train Loss: 0.1093, Val Loss: 0.2622\n",
      "Epoch [39/50], Train Loss: 0.1060, Val Loss: 0.2589\n",
      "Epoch [40/50], Train Loss: 0.1040, Val Loss: 0.2557\n",
      "Epoch [41/50], Train Loss: 0.1035, Val Loss: 0.2526\n",
      "Epoch [42/50], Train Loss: 0.1021, Val Loss: 0.2494\n",
      "Epoch [43/50], Train Loss: 0.0999, Val Loss: 0.2463\n",
      "Epoch [44/50], Train Loss: 0.0996, Val Loss: 0.2434\n",
      "Epoch [45/50], Train Loss: 0.0969, Val Loss: 0.2405\n",
      "Epoch [46/50], Train Loss: 0.0958, Val Loss: 0.2376\n",
      "Epoch [47/50], Train Loss: 0.0977, Val Loss: 0.2349\n",
      "Epoch [48/50], Train Loss: 0.0967, Val Loss: 0.2323\n",
      "Epoch [49/50], Train Loss: 0.0895, Val Loss: 0.2297\n",
      "Epoch [50/50], Train Loss: 0.0943, Val Loss: 0.2272\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=32, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0637, Val Loss: 0.1917\n",
      "Epoch [2/50], Train Loss: 0.0629, Val Loss: 0.1899\n",
      "Epoch [3/50], Train Loss: 0.0622, Val Loss: 0.1881\n",
      "Epoch [4/50], Train Loss: 0.0614, Val Loss: 0.1863\n",
      "Epoch [5/50], Train Loss: 0.0607, Val Loss: 0.1845\n",
      "Epoch [6/50], Train Loss: 0.0600, Val Loss: 0.1828\n",
      "Epoch [7/50], Train Loss: 0.0593, Val Loss: 0.1811\n",
      "Epoch [8/50], Train Loss: 0.0586, Val Loss: 0.1795\n",
      "Epoch [9/50], Train Loss: 0.0580, Val Loss: 0.1778\n",
      "Epoch [10/50], Train Loss: 0.0573, Val Loss: 0.1762\n",
      "Epoch [11/50], Train Loss: 0.0567, Val Loss: 0.1747\n",
      "Epoch [12/50], Train Loss: 0.0561, Val Loss: 0.1731\n",
      "Epoch [13/50], Train Loss: 0.0555, Val Loss: 0.1716\n",
      "Epoch [14/50], Train Loss: 0.0549, Val Loss: 0.1701\n",
      "Epoch [15/50], Train Loss: 0.0544, Val Loss: 0.1686\n",
      "Epoch [16/50], Train Loss: 0.0538, Val Loss: 0.1672\n",
      "Epoch [17/50], Train Loss: 0.0533, Val Loss: 0.1657\n",
      "Epoch [18/50], Train Loss: 0.0527, Val Loss: 0.1644\n",
      "Epoch [19/50], Train Loss: 0.0522, Val Loss: 0.1630\n",
      "Epoch [20/50], Train Loss: 0.0517, Val Loss: 0.1616\n",
      "Epoch [21/50], Train Loss: 0.0512, Val Loss: 0.1603\n",
      "Epoch [22/50], Train Loss: 0.0507, Val Loss: 0.1590\n",
      "Epoch [23/50], Train Loss: 0.0502, Val Loss: 0.1577\n",
      "Epoch [24/50], Train Loss: 0.0498, Val Loss: 0.1564\n",
      "Epoch [25/50], Train Loss: 0.0493, Val Loss: 0.1552\n",
      "Epoch [26/50], Train Loss: 0.0489, Val Loss: 0.1540\n",
      "Epoch [27/50], Train Loss: 0.0485, Val Loss: 0.1528\n",
      "Epoch [28/50], Train Loss: 0.0480, Val Loss: 0.1516\n",
      "Epoch [29/50], Train Loss: 0.0476, Val Loss: 0.1504\n",
      "Epoch [30/50], Train Loss: 0.0472, Val Loss: 0.1493\n",
      "Epoch [31/50], Train Loss: 0.0468, Val Loss: 0.1482\n",
      "Epoch [32/50], Train Loss: 0.0465, Val Loss: 0.1471\n",
      "Epoch [33/50], Train Loss: 0.0461, Val Loss: 0.1460\n",
      "Epoch [34/50], Train Loss: 0.0457, Val Loss: 0.1449\n",
      "Epoch [35/50], Train Loss: 0.0454, Val Loss: 0.1439\n",
      "Epoch [36/50], Train Loss: 0.0450, Val Loss: 0.1428\n",
      "Epoch [37/50], Train Loss: 0.0447, Val Loss: 0.1418\n",
      "Epoch [38/50], Train Loss: 0.0443, Val Loss: 0.1408\n",
      "Epoch [39/50], Train Loss: 0.0440, Val Loss: 0.1398\n",
      "Epoch [40/50], Train Loss: 0.0437, Val Loss: 0.1388\n",
      "Epoch [41/50], Train Loss: 0.0434, Val Loss: 0.1379\n",
      "Epoch [42/50], Train Loss: 0.0431, Val Loss: 0.1370\n",
      "Epoch [43/50], Train Loss: 0.0428, Val Loss: 0.1360\n",
      "Epoch [44/50], Train Loss: 0.0425, Val Loss: 0.1351\n",
      "Epoch [45/50], Train Loss: 0.0422, Val Loss: 0.1342\n",
      "Epoch [46/50], Train Loss: 0.0419, Val Loss: 0.1334\n",
      "Epoch [47/50], Train Loss: 0.0417, Val Loss: 0.1325\n",
      "Epoch [48/50], Train Loss: 0.0414, Val Loss: 0.1316\n",
      "Epoch [49/50], Train Loss: 0.0412, Val Loss: 0.1308\n",
      "Epoch [50/50], Train Loss: 0.0409, Val Loss: 0.1300\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=32, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1626, Val Loss: 0.4225\n",
      "Epoch [2/50], Train Loss: 0.1592, Val Loss: 0.4167\n",
      "Epoch [3/50], Train Loss: 0.1552, Val Loss: 0.4111\n",
      "Epoch [4/50], Train Loss: 0.1535, Val Loss: 0.4055\n",
      "Epoch [5/50], Train Loss: 0.1504, Val Loss: 0.4001\n",
      "Epoch [6/50], Train Loss: 0.1479, Val Loss: 0.3948\n",
      "Epoch [7/50], Train Loss: 0.1446, Val Loss: 0.3896\n",
      "Epoch [8/50], Train Loss: 0.1418, Val Loss: 0.3845\n",
      "Epoch [9/50], Train Loss: 0.1404, Val Loss: 0.3795\n",
      "Epoch [10/50], Train Loss: 0.1385, Val Loss: 0.3746\n",
      "Epoch [11/50], Train Loss: 0.1346, Val Loss: 0.3699\n",
      "Epoch [12/50], Train Loss: 0.1329, Val Loss: 0.3652\n",
      "Epoch [13/50], Train Loss: 0.1311, Val Loss: 0.3606\n",
      "Epoch [14/50], Train Loss: 0.1279, Val Loss: 0.3561\n",
      "Epoch [15/50], Train Loss: 0.1264, Val Loss: 0.3517\n",
      "Epoch [16/50], Train Loss: 0.1235, Val Loss: 0.3474\n",
      "Epoch [17/50], Train Loss: 0.1222, Val Loss: 0.3432\n",
      "Epoch [18/50], Train Loss: 0.1196, Val Loss: 0.3391\n",
      "Epoch [19/50], Train Loss: 0.1175, Val Loss: 0.3350\n",
      "Epoch [20/50], Train Loss: 0.1152, Val Loss: 0.3310\n",
      "Epoch [21/50], Train Loss: 0.1142, Val Loss: 0.3272\n",
      "Epoch [22/50], Train Loss: 0.1122, Val Loss: 0.3233\n",
      "Epoch [23/50], Train Loss: 0.1104, Val Loss: 0.3196\n",
      "Epoch [24/50], Train Loss: 0.1089, Val Loss: 0.3159\n",
      "Epoch [25/50], Train Loss: 0.1079, Val Loss: 0.3123\n",
      "Epoch [26/50], Train Loss: 0.1061, Val Loss: 0.3087\n",
      "Epoch [27/50], Train Loss: 0.1035, Val Loss: 0.3052\n",
      "Epoch [28/50], Train Loss: 0.1019, Val Loss: 0.3018\n",
      "Epoch [29/50], Train Loss: 0.1002, Val Loss: 0.2985\n",
      "Epoch [30/50], Train Loss: 0.0989, Val Loss: 0.2951\n",
      "Epoch [31/50], Train Loss: 0.0971, Val Loss: 0.2919\n",
      "Epoch [32/50], Train Loss: 0.0962, Val Loss: 0.2887\n",
      "Epoch [33/50], Train Loss: 0.0958, Val Loss: 0.2856\n",
      "Epoch [34/50], Train Loss: 0.0936, Val Loss: 0.2825\n",
      "Epoch [35/50], Train Loss: 0.0921, Val Loss: 0.2795\n",
      "Epoch [36/50], Train Loss: 0.0912, Val Loss: 0.2766\n",
      "Epoch [37/50], Train Loss: 0.0897, Val Loss: 0.2736\n",
      "Epoch [38/50], Train Loss: 0.0884, Val Loss: 0.2708\n",
      "Epoch [39/50], Train Loss: 0.0876, Val Loss: 0.2680\n",
      "Epoch [40/50], Train Loss: 0.0861, Val Loss: 0.2652\n",
      "Epoch [41/50], Train Loss: 0.0848, Val Loss: 0.2625\n",
      "Epoch [42/50], Train Loss: 0.0841, Val Loss: 0.2598\n",
      "Epoch [43/50], Train Loss: 0.0821, Val Loss: 0.2572\n",
      "Epoch [44/50], Train Loss: 0.0824, Val Loss: 0.2546\n",
      "Epoch [45/50], Train Loss: 0.0809, Val Loss: 0.2520\n",
      "Epoch [46/50], Train Loss: 0.0795, Val Loss: 0.2496\n",
      "Epoch [47/50], Train Loss: 0.0776, Val Loss: 0.2471\n",
      "Epoch [48/50], Train Loss: 0.0781, Val Loss: 0.2447\n",
      "Epoch [49/50], Train Loss: 0.0764, Val Loss: 0.2423\n",
      "Epoch [50/50], Train Loss: 0.0749, Val Loss: 0.2399\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=32, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1538, Val Loss: 0.3827\n",
      "Epoch [2/50], Train Loss: 0.1524, Val Loss: 0.3772\n",
      "Epoch [3/50], Train Loss: 0.1490, Val Loss: 0.3717\n",
      "Epoch [4/50], Train Loss: 0.1446, Val Loss: 0.3663\n",
      "Epoch [5/50], Train Loss: 0.1428, Val Loss: 0.3612\n",
      "Epoch [6/50], Train Loss: 0.1426, Val Loss: 0.3561\n",
      "Epoch [7/50], Train Loss: 0.1377, Val Loss: 0.3512\n",
      "Epoch [8/50], Train Loss: 0.1358, Val Loss: 0.3463\n",
      "Epoch [9/50], Train Loss: 0.1327, Val Loss: 0.3416\n",
      "Epoch [10/50], Train Loss: 0.1305, Val Loss: 0.3370\n",
      "Epoch [11/50], Train Loss: 0.1270, Val Loss: 0.3324\n",
      "Epoch [12/50], Train Loss: 0.1257, Val Loss: 0.3280\n",
      "Epoch [13/50], Train Loss: 0.1223, Val Loss: 0.3236\n",
      "Epoch [14/50], Train Loss: 0.1204, Val Loss: 0.3194\n",
      "Epoch [15/50], Train Loss: 0.1200, Val Loss: 0.3152\n",
      "Epoch [16/50], Train Loss: 0.1167, Val Loss: 0.3111\n",
      "Epoch [17/50], Train Loss: 0.1150, Val Loss: 0.3071\n",
      "Epoch [18/50], Train Loss: 0.1136, Val Loss: 0.3032\n",
      "Epoch [19/50], Train Loss: 0.1108, Val Loss: 0.2994\n",
      "Epoch [20/50], Train Loss: 0.1093, Val Loss: 0.2956\n",
      "Epoch [21/50], Train Loss: 0.1078, Val Loss: 0.2920\n",
      "Epoch [22/50], Train Loss: 0.1040, Val Loss: 0.2884\n",
      "Epoch [23/50], Train Loss: 0.1038, Val Loss: 0.2848\n",
      "Epoch [24/50], Train Loss: 0.1016, Val Loss: 0.2813\n",
      "Epoch [25/50], Train Loss: 0.0999, Val Loss: 0.2779\n",
      "Epoch [26/50], Train Loss: 0.0993, Val Loss: 0.2745\n",
      "Epoch [27/50], Train Loss: 0.0983, Val Loss: 0.2713\n",
      "Epoch [28/50], Train Loss: 0.0967, Val Loss: 0.2681\n",
      "Epoch [29/50], Train Loss: 0.0945, Val Loss: 0.2649\n",
      "Epoch [30/50], Train Loss: 0.0945, Val Loss: 0.2618\n",
      "Epoch [31/50], Train Loss: 0.0929, Val Loss: 0.2588\n",
      "Epoch [32/50], Train Loss: 0.0901, Val Loss: 0.2559\n",
      "Epoch [33/50], Train Loss: 0.0891, Val Loss: 0.2529\n",
      "Epoch [34/50], Train Loss: 0.0876, Val Loss: 0.2501\n",
      "Epoch [35/50], Train Loss: 0.0871, Val Loss: 0.2473\n",
      "Epoch [36/50], Train Loss: 0.0863, Val Loss: 0.2445\n",
      "Epoch [37/50], Train Loss: 0.0853, Val Loss: 0.2418\n",
      "Epoch [38/50], Train Loss: 0.0830, Val Loss: 0.2391\n",
      "Epoch [39/50], Train Loss: 0.0815, Val Loss: 0.2365\n",
      "Epoch [40/50], Train Loss: 0.0816, Val Loss: 0.2340\n",
      "Epoch [41/50], Train Loss: 0.0799, Val Loss: 0.2314\n",
      "Epoch [42/50], Train Loss: 0.0788, Val Loss: 0.2290\n",
      "Epoch [43/50], Train Loss: 0.0784, Val Loss: 0.2265\n",
      "Epoch [44/50], Train Loss: 0.0768, Val Loss: 0.2242\n",
      "Epoch [45/50], Train Loss: 0.0748, Val Loss: 0.2219\n",
      "Epoch [46/50], Train Loss: 0.0755, Val Loss: 0.2196\n",
      "Epoch [47/50], Train Loss: 0.0739, Val Loss: 0.2173\n",
      "Epoch [48/50], Train Loss: 0.0743, Val Loss: 0.2151\n",
      "Epoch [49/50], Train Loss: 0.0732, Val Loss: 0.2130\n",
      "Epoch [50/50], Train Loss: 0.0706, Val Loss: 0.2108\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=32, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1605, Val Loss: 0.3700\n",
      "Epoch [2/50], Train Loss: 0.1574, Val Loss: 0.3651\n",
      "Epoch [3/50], Train Loss: 0.1544, Val Loss: 0.3603\n",
      "Epoch [4/50], Train Loss: 0.1515, Val Loss: 0.3556\n",
      "Epoch [5/50], Train Loss: 0.1486, Val Loss: 0.3509\n",
      "Epoch [6/50], Train Loss: 0.1458, Val Loss: 0.3464\n",
      "Epoch [7/50], Train Loss: 0.1431, Val Loss: 0.3419\n",
      "Epoch [8/50], Train Loss: 0.1405, Val Loss: 0.3376\n",
      "Epoch [9/50], Train Loss: 0.1379, Val Loss: 0.3333\n",
      "Epoch [10/50], Train Loss: 0.1354, Val Loss: 0.3291\n",
      "Epoch [11/50], Train Loss: 0.1329, Val Loss: 0.3250\n",
      "Epoch [12/50], Train Loss: 0.1305, Val Loss: 0.3209\n",
      "Epoch [13/50], Train Loss: 0.1281, Val Loss: 0.3170\n",
      "Epoch [14/50], Train Loss: 0.1259, Val Loss: 0.3131\n",
      "Epoch [15/50], Train Loss: 0.1236, Val Loss: 0.3093\n",
      "Epoch [16/50], Train Loss: 0.1214, Val Loss: 0.3056\n",
      "Epoch [17/50], Train Loss: 0.1193, Val Loss: 0.3019\n",
      "Epoch [18/50], Train Loss: 0.1172, Val Loss: 0.2983\n",
      "Epoch [19/50], Train Loss: 0.1152, Val Loss: 0.2947\n",
      "Epoch [20/50], Train Loss: 0.1132, Val Loss: 0.2913\n",
      "Epoch [21/50], Train Loss: 0.1113, Val Loss: 0.2879\n",
      "Epoch [22/50], Train Loss: 0.1094, Val Loss: 0.2845\n",
      "Epoch [23/50], Train Loss: 0.1075, Val Loss: 0.2812\n",
      "Epoch [24/50], Train Loss: 0.1057, Val Loss: 0.2780\n",
      "Epoch [25/50], Train Loss: 0.1039, Val Loss: 0.2748\n",
      "Epoch [26/50], Train Loss: 0.1022, Val Loss: 0.2717\n",
      "Epoch [27/50], Train Loss: 0.1005, Val Loss: 0.2687\n",
      "Epoch [28/50], Train Loss: 0.0989, Val Loss: 0.2656\n",
      "Epoch [29/50], Train Loss: 0.0973, Val Loss: 0.2627\n",
      "Epoch [30/50], Train Loss: 0.0957, Val Loss: 0.2598\n",
      "Epoch [31/50], Train Loss: 0.0941, Val Loss: 0.2569\n",
      "Epoch [32/50], Train Loss: 0.0926, Val Loss: 0.2541\n",
      "Epoch [33/50], Train Loss: 0.0912, Val Loss: 0.2513\n",
      "Epoch [34/50], Train Loss: 0.0897, Val Loss: 0.2486\n",
      "Epoch [35/50], Train Loss: 0.0883, Val Loss: 0.2460\n",
      "Epoch [36/50], Train Loss: 0.0870, Val Loss: 0.2434\n",
      "Epoch [37/50], Train Loss: 0.0856, Val Loss: 0.2408\n",
      "Epoch [38/50], Train Loss: 0.0843, Val Loss: 0.2383\n",
      "Epoch [39/50], Train Loss: 0.0830, Val Loss: 0.2358\n",
      "Epoch [40/50], Train Loss: 0.0818, Val Loss: 0.2333\n",
      "Epoch [41/50], Train Loss: 0.0806, Val Loss: 0.2309\n",
      "Epoch [42/50], Train Loss: 0.0794, Val Loss: 0.2286\n",
      "Epoch [43/50], Train Loss: 0.0782, Val Loss: 0.2263\n",
      "Epoch [44/50], Train Loss: 0.0771, Val Loss: 0.2240\n",
      "Epoch [45/50], Train Loss: 0.0760, Val Loss: 0.2217\n",
      "Epoch [46/50], Train Loss: 0.0749, Val Loss: 0.2195\n",
      "Epoch [47/50], Train Loss: 0.0738, Val Loss: 0.2173\n",
      "Epoch [48/50], Train Loss: 0.0728, Val Loss: 0.2152\n",
      "Epoch [49/50], Train Loss: 0.0718, Val Loss: 0.2131\n",
      "Epoch [50/50], Train Loss: 0.0708, Val Loss: 0.2110\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=32, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.2221, Val Loss: 0.4817\n",
      "Epoch [2/50], Train Loss: 0.2172, Val Loss: 0.4727\n",
      "Epoch [3/50], Train Loss: 0.2106, Val Loss: 0.4640\n",
      "Epoch [4/50], Train Loss: 0.2036, Val Loss: 0.4556\n",
      "Epoch [5/50], Train Loss: 0.1997, Val Loss: 0.4474\n",
      "Epoch [6/50], Train Loss: 0.1949, Val Loss: 0.4393\n",
      "Epoch [7/50], Train Loss: 0.1915, Val Loss: 0.4315\n",
      "Epoch [8/50], Train Loss: 0.1854, Val Loss: 0.4238\n",
      "Epoch [9/50], Train Loss: 0.1798, Val Loss: 0.4165\n",
      "Epoch [10/50], Train Loss: 0.1786, Val Loss: 0.4092\n",
      "Epoch [11/50], Train Loss: 0.1730, Val Loss: 0.4021\n",
      "Epoch [12/50], Train Loss: 0.1675, Val Loss: 0.3953\n",
      "Epoch [13/50], Train Loss: 0.1643, Val Loss: 0.3886\n",
      "Epoch [14/50], Train Loss: 0.1602, Val Loss: 0.3820\n",
      "Epoch [15/50], Train Loss: 0.1581, Val Loss: 0.3756\n",
      "Epoch [16/50], Train Loss: 0.1534, Val Loss: 0.3694\n",
      "Epoch [17/50], Train Loss: 0.1488, Val Loss: 0.3633\n",
      "Epoch [18/50], Train Loss: 0.1449, Val Loss: 0.3573\n",
      "Epoch [19/50], Train Loss: 0.1432, Val Loss: 0.3515\n",
      "Epoch [20/50], Train Loss: 0.1389, Val Loss: 0.3459\n",
      "Epoch [21/50], Train Loss: 0.1350, Val Loss: 0.3404\n",
      "Epoch [22/50], Train Loss: 0.1334, Val Loss: 0.3350\n",
      "Epoch [23/50], Train Loss: 0.1302, Val Loss: 0.3297\n",
      "Epoch [24/50], Train Loss: 0.1285, Val Loss: 0.3245\n",
      "Epoch [25/50], Train Loss: 0.1239, Val Loss: 0.3195\n",
      "Epoch [26/50], Train Loss: 0.1222, Val Loss: 0.3146\n",
      "Epoch [27/50], Train Loss: 0.1200, Val Loss: 0.3097\n",
      "Epoch [28/50], Train Loss: 0.1162, Val Loss: 0.3051\n",
      "Epoch [29/50], Train Loss: 0.1150, Val Loss: 0.3004\n",
      "Epoch [30/50], Train Loss: 0.1112, Val Loss: 0.2959\n",
      "Epoch [31/50], Train Loss: 0.1094, Val Loss: 0.2916\n",
      "Epoch [32/50], Train Loss: 0.1073, Val Loss: 0.2872\n",
      "Epoch [33/50], Train Loss: 0.1056, Val Loss: 0.2830\n",
      "Epoch [34/50], Train Loss: 0.1038, Val Loss: 0.2789\n",
      "Epoch [35/50], Train Loss: 0.1011, Val Loss: 0.2749\n",
      "Epoch [36/50], Train Loss: 0.1001, Val Loss: 0.2710\n",
      "Epoch [37/50], Train Loss: 0.0978, Val Loss: 0.2671\n",
      "Epoch [38/50], Train Loss: 0.0951, Val Loss: 0.2634\n",
      "Epoch [39/50], Train Loss: 0.0942, Val Loss: 0.2597\n",
      "Epoch [40/50], Train Loss: 0.0920, Val Loss: 0.2561\n",
      "Epoch [41/50], Train Loss: 0.0904, Val Loss: 0.2526\n",
      "Epoch [42/50], Train Loss: 0.0885, Val Loss: 0.2492\n",
      "Epoch [43/50], Train Loss: 0.0870, Val Loss: 0.2459\n",
      "Epoch [44/50], Train Loss: 0.0854, Val Loss: 0.2426\n",
      "Epoch [45/50], Train Loss: 0.0837, Val Loss: 0.2394\n",
      "Epoch [46/50], Train Loss: 0.0824, Val Loss: 0.2362\n",
      "Epoch [47/50], Train Loss: 0.0799, Val Loss: 0.2332\n",
      "Epoch [48/50], Train Loss: 0.0803, Val Loss: 0.2302\n",
      "Epoch [49/50], Train Loss: 0.0788, Val Loss: 0.2272\n",
      "Epoch [50/50], Train Loss: 0.0767, Val Loss: 0.2244\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=32, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.2023, Val Loss: 0.4315\n",
      "Epoch [2/50], Train Loss: 0.1979, Val Loss: 0.4256\n",
      "Epoch [3/50], Train Loss: 0.1929, Val Loss: 0.4199\n",
      "Epoch [4/50], Train Loss: 0.1890, Val Loss: 0.4142\n",
      "Epoch [5/50], Train Loss: 0.1850, Val Loss: 0.4088\n",
      "Epoch [6/50], Train Loss: 0.1820, Val Loss: 0.4034\n",
      "Epoch [7/50], Train Loss: 0.1780, Val Loss: 0.3982\n",
      "Epoch [8/50], Train Loss: 0.1754, Val Loss: 0.3930\n",
      "Epoch [9/50], Train Loss: 0.1715, Val Loss: 0.3880\n",
      "Epoch [10/50], Train Loss: 0.1674, Val Loss: 0.3830\n",
      "Epoch [11/50], Train Loss: 0.1653, Val Loss: 0.3782\n",
      "Epoch [12/50], Train Loss: 0.1626, Val Loss: 0.3735\n",
      "Epoch [13/50], Train Loss: 0.1572, Val Loss: 0.3689\n",
      "Epoch [14/50], Train Loss: 0.1565, Val Loss: 0.3643\n",
      "Epoch [15/50], Train Loss: 0.1528, Val Loss: 0.3599\n",
      "Epoch [16/50], Train Loss: 0.1502, Val Loss: 0.3555\n",
      "Epoch [17/50], Train Loss: 0.1462, Val Loss: 0.3512\n",
      "Epoch [18/50], Train Loss: 0.1452, Val Loss: 0.3470\n",
      "Epoch [19/50], Train Loss: 0.1428, Val Loss: 0.3429\n",
      "Epoch [20/50], Train Loss: 0.1396, Val Loss: 0.3389\n",
      "Epoch [21/50], Train Loss: 0.1381, Val Loss: 0.3349\n",
      "Epoch [22/50], Train Loss: 0.1353, Val Loss: 0.3310\n",
      "Epoch [23/50], Train Loss: 0.1338, Val Loss: 0.3272\n",
      "Epoch [24/50], Train Loss: 0.1298, Val Loss: 0.3235\n",
      "Epoch [25/50], Train Loss: 0.1281, Val Loss: 0.3198\n",
      "Epoch [26/50], Train Loss: 0.1256, Val Loss: 0.3163\n",
      "Epoch [27/50], Train Loss: 0.1243, Val Loss: 0.3127\n",
      "Epoch [28/50], Train Loss: 0.1215, Val Loss: 0.3093\n",
      "Epoch [29/50], Train Loss: 0.1204, Val Loss: 0.3059\n",
      "Epoch [30/50], Train Loss: 0.1180, Val Loss: 0.3025\n",
      "Epoch [31/50], Train Loss: 0.1179, Val Loss: 0.2992\n",
      "Epoch [32/50], Train Loss: 0.1151, Val Loss: 0.2961\n",
      "Epoch [33/50], Train Loss: 0.1131, Val Loss: 0.2929\n",
      "Epoch [34/50], Train Loss: 0.1110, Val Loss: 0.2898\n",
      "Epoch [35/50], Train Loss: 0.1089, Val Loss: 0.2868\n",
      "Epoch [36/50], Train Loss: 0.1076, Val Loss: 0.2838\n",
      "Epoch [37/50], Train Loss: 0.1061, Val Loss: 0.2809\n",
      "Epoch [38/50], Train Loss: 0.1046, Val Loss: 0.2780\n",
      "Epoch [39/50], Train Loss: 0.1027, Val Loss: 0.2752\n",
      "Epoch [40/50], Train Loss: 0.1014, Val Loss: 0.2724\n",
      "Epoch [41/50], Train Loss: 0.1021, Val Loss: 0.2697\n",
      "Epoch [42/50], Train Loss: 0.0995, Val Loss: 0.2670\n",
      "Epoch [43/50], Train Loss: 0.0980, Val Loss: 0.2644\n",
      "Epoch [44/50], Train Loss: 0.0960, Val Loss: 0.2618\n",
      "Epoch [45/50], Train Loss: 0.0954, Val Loss: 0.2592\n",
      "Epoch [46/50], Train Loss: 0.0934, Val Loss: 0.2567\n",
      "Epoch [47/50], Train Loss: 0.0924, Val Loss: 0.2542\n",
      "Epoch [48/50], Train Loss: 0.0916, Val Loss: 0.2518\n",
      "Epoch [49/50], Train Loss: 0.0900, Val Loss: 0.2494\n",
      "Epoch [50/50], Train Loss: 0.0893, Val Loss: 0.2471\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=32, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1814, Val Loss: 0.4443\n",
      "Epoch [2/50], Train Loss: 0.1765, Val Loss: 0.4363\n",
      "Epoch [3/50], Train Loss: 0.1719, Val Loss: 0.4285\n",
      "Epoch [4/50], Train Loss: 0.1674, Val Loss: 0.4209\n",
      "Epoch [5/50], Train Loss: 0.1631, Val Loss: 0.4136\n",
      "Epoch [6/50], Train Loss: 0.1589, Val Loss: 0.4064\n",
      "Epoch [7/50], Train Loss: 0.1549, Val Loss: 0.3995\n",
      "Epoch [8/50], Train Loss: 0.1510, Val Loss: 0.3928\n",
      "Epoch [9/50], Train Loss: 0.1473, Val Loss: 0.3863\n",
      "Epoch [10/50], Train Loss: 0.1437, Val Loss: 0.3799\n",
      "Epoch [11/50], Train Loss: 0.1402, Val Loss: 0.3738\n",
      "Epoch [12/50], Train Loss: 0.1369, Val Loss: 0.3677\n",
      "Epoch [13/50], Train Loss: 0.1336, Val Loss: 0.3619\n",
      "Epoch [14/50], Train Loss: 0.1305, Val Loss: 0.3562\n",
      "Epoch [15/50], Train Loss: 0.1275, Val Loss: 0.3506\n",
      "Epoch [16/50], Train Loss: 0.1246, Val Loss: 0.3452\n",
      "Epoch [17/50], Train Loss: 0.1218, Val Loss: 0.3399\n",
      "Epoch [18/50], Train Loss: 0.1190, Val Loss: 0.3348\n",
      "Epoch [19/50], Train Loss: 0.1164, Val Loss: 0.3298\n",
      "Epoch [20/50], Train Loss: 0.1138, Val Loss: 0.3249\n",
      "Epoch [21/50], Train Loss: 0.1114, Val Loss: 0.3201\n",
      "Epoch [22/50], Train Loss: 0.1090, Val Loss: 0.3155\n",
      "Epoch [23/50], Train Loss: 0.1067, Val Loss: 0.3110\n",
      "Epoch [24/50], Train Loss: 0.1044, Val Loss: 0.3065\n",
      "Epoch [25/50], Train Loss: 0.1023, Val Loss: 0.3022\n",
      "Epoch [26/50], Train Loss: 0.1002, Val Loss: 0.2980\n",
      "Epoch [27/50], Train Loss: 0.0981, Val Loss: 0.2938\n",
      "Epoch [28/50], Train Loss: 0.0962, Val Loss: 0.2898\n",
      "Epoch [29/50], Train Loss: 0.0942, Val Loss: 0.2858\n",
      "Epoch [30/50], Train Loss: 0.0924, Val Loss: 0.2820\n",
      "Epoch [31/50], Train Loss: 0.0906, Val Loss: 0.2782\n",
      "Epoch [32/50], Train Loss: 0.0889, Val Loss: 0.2745\n",
      "Epoch [33/50], Train Loss: 0.0872, Val Loss: 0.2709\n",
      "Epoch [34/50], Train Loss: 0.0856, Val Loss: 0.2674\n",
      "Epoch [35/50], Train Loss: 0.0840, Val Loss: 0.2639\n",
      "Epoch [36/50], Train Loss: 0.0824, Val Loss: 0.2606\n",
      "Epoch [37/50], Train Loss: 0.0810, Val Loss: 0.2573\n",
      "Epoch [38/50], Train Loss: 0.0795, Val Loss: 0.2540\n",
      "Epoch [39/50], Train Loss: 0.0781, Val Loss: 0.2509\n",
      "Epoch [40/50], Train Loss: 0.0768, Val Loss: 0.2478\n",
      "Epoch [41/50], Train Loss: 0.0755, Val Loss: 0.2448\n",
      "Epoch [42/50], Train Loss: 0.0742, Val Loss: 0.2418\n",
      "Epoch [43/50], Train Loss: 0.0730, Val Loss: 0.2389\n",
      "Epoch [44/50], Train Loss: 0.0718, Val Loss: 0.2361\n",
      "Epoch [45/50], Train Loss: 0.0706, Val Loss: 0.2333\n",
      "Epoch [46/50], Train Loss: 0.0695, Val Loss: 0.2306\n",
      "Epoch [47/50], Train Loss: 0.0684, Val Loss: 0.2280\n",
      "Epoch [48/50], Train Loss: 0.0674, Val Loss: 0.2254\n",
      "Epoch [49/50], Train Loss: 0.0663, Val Loss: 0.2228\n",
      "Epoch [50/50], Train Loss: 0.0653, Val Loss: 0.2203\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=32, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1659, Val Loss: 0.4133\n",
      "Epoch [2/50], Train Loss: 0.1618, Val Loss: 0.4065\n",
      "Epoch [3/50], Train Loss: 0.1584, Val Loss: 0.4000\n",
      "Epoch [4/50], Train Loss: 0.1537, Val Loss: 0.3936\n",
      "Epoch [5/50], Train Loss: 0.1498, Val Loss: 0.3873\n",
      "Epoch [6/50], Train Loss: 0.1477, Val Loss: 0.3812\n",
      "Epoch [7/50], Train Loss: 0.1438, Val Loss: 0.3754\n",
      "Epoch [8/50], Train Loss: 0.1413, Val Loss: 0.3696\n",
      "Epoch [9/50], Train Loss: 0.1381, Val Loss: 0.3641\n",
      "Epoch [10/50], Train Loss: 0.1342, Val Loss: 0.3586\n",
      "Epoch [11/50], Train Loss: 0.1323, Val Loss: 0.3533\n",
      "Epoch [12/50], Train Loss: 0.1284, Val Loss: 0.3482\n",
      "Epoch [13/50], Train Loss: 0.1254, Val Loss: 0.3431\n",
      "Epoch [14/50], Train Loss: 0.1229, Val Loss: 0.3382\n",
      "Epoch [15/50], Train Loss: 0.1211, Val Loss: 0.3334\n",
      "Epoch [16/50], Train Loss: 0.1178, Val Loss: 0.3287\n",
      "Epoch [17/50], Train Loss: 0.1163, Val Loss: 0.3242\n",
      "Epoch [18/50], Train Loss: 0.1136, Val Loss: 0.3197\n",
      "Epoch [19/50], Train Loss: 0.1119, Val Loss: 0.3154\n",
      "Epoch [20/50], Train Loss: 0.1091, Val Loss: 0.3111\n",
      "Epoch [21/50], Train Loss: 0.1071, Val Loss: 0.3069\n",
      "Epoch [22/50], Train Loss: 0.1057, Val Loss: 0.3029\n",
      "Epoch [23/50], Train Loss: 0.1036, Val Loss: 0.2989\n",
      "Epoch [24/50], Train Loss: 0.1006, Val Loss: 0.2951\n",
      "Epoch [25/50], Train Loss: 0.0995, Val Loss: 0.2913\n",
      "Epoch [26/50], Train Loss: 0.0972, Val Loss: 0.2877\n",
      "Epoch [27/50], Train Loss: 0.0955, Val Loss: 0.2840\n",
      "Epoch [28/50], Train Loss: 0.0945, Val Loss: 0.2805\n",
      "Epoch [29/50], Train Loss: 0.0923, Val Loss: 0.2770\n",
      "Epoch [30/50], Train Loss: 0.0911, Val Loss: 0.2737\n",
      "Epoch [31/50], Train Loss: 0.0885, Val Loss: 0.2703\n",
      "Epoch [32/50], Train Loss: 0.0873, Val Loss: 0.2671\n",
      "Epoch [33/50], Train Loss: 0.0871, Val Loss: 0.2639\n",
      "Epoch [34/50], Train Loss: 0.0842, Val Loss: 0.2608\n",
      "Epoch [35/50], Train Loss: 0.0838, Val Loss: 0.2577\n",
      "Epoch [36/50], Train Loss: 0.0817, Val Loss: 0.2547\n",
      "Epoch [37/50], Train Loss: 0.0805, Val Loss: 0.2518\n",
      "Epoch [38/50], Train Loss: 0.0796, Val Loss: 0.2490\n",
      "Epoch [39/50], Train Loss: 0.0786, Val Loss: 0.2462\n",
      "Epoch [40/50], Train Loss: 0.0770, Val Loss: 0.2434\n",
      "Epoch [41/50], Train Loss: 0.0758, Val Loss: 0.2407\n",
      "Epoch [42/50], Train Loss: 0.0751, Val Loss: 0.2381\n",
      "Epoch [43/50], Train Loss: 0.0739, Val Loss: 0.2355\n",
      "Epoch [44/50], Train Loss: 0.0735, Val Loss: 0.2330\n",
      "Epoch [45/50], Train Loss: 0.0719, Val Loss: 0.2305\n",
      "Epoch [46/50], Train Loss: 0.0711, Val Loss: 0.2281\n",
      "Epoch [47/50], Train Loss: 0.0693, Val Loss: 0.2257\n",
      "Epoch [48/50], Train Loss: 0.0694, Val Loss: 0.2234\n",
      "Epoch [49/50], Train Loss: 0.0677, Val Loss: 0.2211\n",
      "Epoch [50/50], Train Loss: 0.0687, Val Loss: 0.2189\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=32, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1209, Val Loss: 0.3338\n",
      "Epoch [2/50], Train Loss: 0.1217, Val Loss: 0.3291\n",
      "Epoch [3/50], Train Loss: 0.1188, Val Loss: 0.3244\n",
      "Epoch [4/50], Train Loss: 0.1147, Val Loss: 0.3200\n",
      "Epoch [5/50], Train Loss: 0.1159, Val Loss: 0.3155\n",
      "Epoch [6/50], Train Loss: 0.1127, Val Loss: 0.3112\n",
      "Epoch [7/50], Train Loss: 0.1091, Val Loss: 0.3070\n",
      "Epoch [8/50], Train Loss: 0.1099, Val Loss: 0.3029\n",
      "Epoch [9/50], Train Loss: 0.1066, Val Loss: 0.2989\n",
      "Epoch [10/50], Train Loss: 0.1048, Val Loss: 0.2950\n",
      "Epoch [11/50], Train Loss: 0.1044, Val Loss: 0.2912\n",
      "Epoch [12/50], Train Loss: 0.1006, Val Loss: 0.2876\n",
      "Epoch [13/50], Train Loss: 0.0982, Val Loss: 0.2840\n",
      "Epoch [14/50], Train Loss: 0.0989, Val Loss: 0.2804\n",
      "Epoch [15/50], Train Loss: 0.0964, Val Loss: 0.2769\n",
      "Epoch [16/50], Train Loss: 0.0941, Val Loss: 0.2735\n",
      "Epoch [17/50], Train Loss: 0.0939, Val Loss: 0.2701\n",
      "Epoch [18/50], Train Loss: 0.0920, Val Loss: 0.2669\n",
      "Epoch [19/50], Train Loss: 0.0909, Val Loss: 0.2637\n",
      "Epoch [20/50], Train Loss: 0.0891, Val Loss: 0.2606\n",
      "Epoch [21/50], Train Loss: 0.0898, Val Loss: 0.2576\n",
      "Epoch [22/50], Train Loss: 0.0873, Val Loss: 0.2546\n",
      "Epoch [23/50], Train Loss: 0.0864, Val Loss: 0.2516\n",
      "Epoch [24/50], Train Loss: 0.0841, Val Loss: 0.2488\n",
      "Epoch [25/50], Train Loss: 0.0832, Val Loss: 0.2460\n",
      "Epoch [26/50], Train Loss: 0.0817, Val Loss: 0.2433\n",
      "Epoch [27/50], Train Loss: 0.0820, Val Loss: 0.2406\n",
      "Epoch [28/50], Train Loss: 0.0794, Val Loss: 0.2380\n",
      "Epoch [29/50], Train Loss: 0.0800, Val Loss: 0.2355\n",
      "Epoch [30/50], Train Loss: 0.0806, Val Loss: 0.2329\n",
      "Epoch [31/50], Train Loss: 0.0763, Val Loss: 0.2306\n",
      "Epoch [32/50], Train Loss: 0.0762, Val Loss: 0.2282\n",
      "Epoch [33/50], Train Loss: 0.0762, Val Loss: 0.2258\n",
      "Epoch [34/50], Train Loss: 0.0742, Val Loss: 0.2236\n",
      "Epoch [35/50], Train Loss: 0.0749, Val Loss: 0.2213\n",
      "Epoch [36/50], Train Loss: 0.0735, Val Loss: 0.2192\n",
      "Epoch [37/50], Train Loss: 0.0734, Val Loss: 0.2170\n",
      "Epoch [38/50], Train Loss: 0.0726, Val Loss: 0.2149\n",
      "Epoch [39/50], Train Loss: 0.0717, Val Loss: 0.2128\n",
      "Epoch [40/50], Train Loss: 0.0695, Val Loss: 0.2108\n",
      "Epoch [41/50], Train Loss: 0.0693, Val Loss: 0.2088\n",
      "Epoch [42/50], Train Loss: 0.0684, Val Loss: 0.2069\n",
      "Epoch [43/50], Train Loss: 0.0681, Val Loss: 0.2050\n",
      "Epoch [44/50], Train Loss: 0.0676, Val Loss: 0.2032\n",
      "Epoch [45/50], Train Loss: 0.0671, Val Loss: 0.2014\n",
      "Epoch [46/50], Train Loss: 0.0667, Val Loss: 0.1996\n",
      "Epoch [47/50], Train Loss: 0.0655, Val Loss: 0.1978\n",
      "Epoch [48/50], Train Loss: 0.0649, Val Loss: 0.1961\n",
      "Epoch [49/50], Train Loss: 0.0667, Val Loss: 0.1944\n",
      "Epoch [50/50], Train Loss: 0.0636, Val Loss: 0.1927\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=64, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1880, Val Loss: 0.5220\n",
      "Epoch [2/50], Train Loss: 0.1841, Val Loss: 0.5140\n",
      "Epoch [3/50], Train Loss: 0.1803, Val Loss: 0.5063\n",
      "Epoch [4/50], Train Loss: 0.1766, Val Loss: 0.4987\n",
      "Epoch [5/50], Train Loss: 0.1730, Val Loss: 0.4913\n",
      "Epoch [6/50], Train Loss: 0.1696, Val Loss: 0.4840\n",
      "Epoch [7/50], Train Loss: 0.1662, Val Loss: 0.4769\n",
      "Epoch [8/50], Train Loss: 0.1629, Val Loss: 0.4700\n",
      "Epoch [9/50], Train Loss: 0.1597, Val Loss: 0.4632\n",
      "Epoch [10/50], Train Loss: 0.1565, Val Loss: 0.4566\n",
      "Epoch [11/50], Train Loss: 0.1535, Val Loss: 0.4500\n",
      "Epoch [12/50], Train Loss: 0.1505, Val Loss: 0.4437\n",
      "Epoch [13/50], Train Loss: 0.1476, Val Loss: 0.4375\n",
      "Epoch [14/50], Train Loss: 0.1448, Val Loss: 0.4314\n",
      "Epoch [15/50], Train Loss: 0.1421, Val Loss: 0.4254\n",
      "Epoch [16/50], Train Loss: 0.1394, Val Loss: 0.4195\n",
      "Epoch [17/50], Train Loss: 0.1368, Val Loss: 0.4138\n",
      "Epoch [18/50], Train Loss: 0.1342, Val Loss: 0.4082\n",
      "Epoch [19/50], Train Loss: 0.1318, Val Loss: 0.4027\n",
      "Epoch [20/50], Train Loss: 0.1293, Val Loss: 0.3973\n",
      "Epoch [21/50], Train Loss: 0.1270, Val Loss: 0.3920\n",
      "Epoch [22/50], Train Loss: 0.1247, Val Loss: 0.3868\n",
      "Epoch [23/50], Train Loss: 0.1224, Val Loss: 0.3817\n",
      "Epoch [24/50], Train Loss: 0.1203, Val Loss: 0.3767\n",
      "Epoch [25/50], Train Loss: 0.1181, Val Loss: 0.3718\n",
      "Epoch [26/50], Train Loss: 0.1160, Val Loss: 0.3670\n",
      "Epoch [27/50], Train Loss: 0.1140, Val Loss: 0.3623\n",
      "Epoch [28/50], Train Loss: 0.1120, Val Loss: 0.3577\n",
      "Epoch [29/50], Train Loss: 0.1101, Val Loss: 0.3531\n",
      "Epoch [30/50], Train Loss: 0.1082, Val Loss: 0.3487\n",
      "Epoch [31/50], Train Loss: 0.1064, Val Loss: 0.3443\n",
      "Epoch [32/50], Train Loss: 0.1046, Val Loss: 0.3400\n",
      "Epoch [33/50], Train Loss: 0.1028, Val Loss: 0.3358\n",
      "Epoch [34/50], Train Loss: 0.1011, Val Loss: 0.3317\n",
      "Epoch [35/50], Train Loss: 0.0994, Val Loss: 0.3276\n",
      "Epoch [36/50], Train Loss: 0.0978, Val Loss: 0.3236\n",
      "Epoch [37/50], Train Loss: 0.0962, Val Loss: 0.3197\n",
      "Epoch [38/50], Train Loss: 0.0946, Val Loss: 0.3158\n",
      "Epoch [39/50], Train Loss: 0.0931, Val Loss: 0.3121\n",
      "Epoch [40/50], Train Loss: 0.0916, Val Loss: 0.3083\n",
      "Epoch [41/50], Train Loss: 0.0902, Val Loss: 0.3047\n",
      "Epoch [42/50], Train Loss: 0.0887, Val Loss: 0.3011\n",
      "Epoch [43/50], Train Loss: 0.0874, Val Loss: 0.2976\n",
      "Epoch [44/50], Train Loss: 0.0860, Val Loss: 0.2941\n",
      "Epoch [45/50], Train Loss: 0.0847, Val Loss: 0.2907\n",
      "Epoch [46/50], Train Loss: 0.0834, Val Loss: 0.2874\n",
      "Epoch [47/50], Train Loss: 0.0822, Val Loss: 0.2841\n",
      "Epoch [48/50], Train Loss: 0.0809, Val Loss: 0.2808\n",
      "Epoch [49/50], Train Loss: 0.0797, Val Loss: 0.2777\n",
      "Epoch [50/50], Train Loss: 0.0786, Val Loss: 0.2745\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=64, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1787, Val Loss: 0.4539\n",
      "Epoch [2/50], Train Loss: 0.1760, Val Loss: 0.4473\n",
      "Epoch [3/50], Train Loss: 0.1719, Val Loss: 0.4407\n",
      "Epoch [4/50], Train Loss: 0.1683, Val Loss: 0.4344\n",
      "Epoch [5/50], Train Loss: 0.1654, Val Loss: 0.4282\n",
      "Epoch [6/50], Train Loss: 0.1626, Val Loss: 0.4221\n",
      "Epoch [7/50], Train Loss: 0.1593, Val Loss: 0.4161\n",
      "Epoch [8/50], Train Loss: 0.1559, Val Loss: 0.4103\n",
      "Epoch [9/50], Train Loss: 0.1531, Val Loss: 0.4045\n",
      "Epoch [10/50], Train Loss: 0.1505, Val Loss: 0.3989\n",
      "Epoch [11/50], Train Loss: 0.1471, Val Loss: 0.3934\n",
      "Epoch [12/50], Train Loss: 0.1440, Val Loss: 0.3880\n",
      "Epoch [13/50], Train Loss: 0.1411, Val Loss: 0.3827\n",
      "Epoch [14/50], Train Loss: 0.1392, Val Loss: 0.3775\n",
      "Epoch [15/50], Train Loss: 0.1363, Val Loss: 0.3724\n",
      "Epoch [16/50], Train Loss: 0.1341, Val Loss: 0.3674\n",
      "Epoch [17/50], Train Loss: 0.1316, Val Loss: 0.3625\n",
      "Epoch [18/50], Train Loss: 0.1289, Val Loss: 0.3578\n",
      "Epoch [19/50], Train Loss: 0.1270, Val Loss: 0.3531\n",
      "Epoch [20/50], Train Loss: 0.1243, Val Loss: 0.3484\n",
      "Epoch [21/50], Train Loss: 0.1216, Val Loss: 0.3439\n",
      "Epoch [22/50], Train Loss: 0.1195, Val Loss: 0.3394\n",
      "Epoch [23/50], Train Loss: 0.1176, Val Loss: 0.3351\n",
      "Epoch [24/50], Train Loss: 0.1160, Val Loss: 0.3308\n",
      "Epoch [25/50], Train Loss: 0.1136, Val Loss: 0.3266\n",
      "Epoch [26/50], Train Loss: 0.1116, Val Loss: 0.3224\n",
      "Epoch [27/50], Train Loss: 0.1097, Val Loss: 0.3184\n",
      "Epoch [28/50], Train Loss: 0.1076, Val Loss: 0.3144\n",
      "Epoch [29/50], Train Loss: 0.1059, Val Loss: 0.3105\n",
      "Epoch [30/50], Train Loss: 0.1041, Val Loss: 0.3067\n",
      "Epoch [31/50], Train Loss: 0.1024, Val Loss: 0.3029\n",
      "Epoch [32/50], Train Loss: 0.1005, Val Loss: 0.2992\n",
      "Epoch [33/50], Train Loss: 0.0990, Val Loss: 0.2956\n",
      "Epoch [34/50], Train Loss: 0.0977, Val Loss: 0.2920\n",
      "Epoch [35/50], Train Loss: 0.0955, Val Loss: 0.2885\n",
      "Epoch [36/50], Train Loss: 0.0941, Val Loss: 0.2850\n",
      "Epoch [37/50], Train Loss: 0.0921, Val Loss: 0.2816\n",
      "Epoch [38/50], Train Loss: 0.0907, Val Loss: 0.2783\n",
      "Epoch [39/50], Train Loss: 0.0894, Val Loss: 0.2750\n",
      "Epoch [40/50], Train Loss: 0.0884, Val Loss: 0.2718\n",
      "Epoch [41/50], Train Loss: 0.0869, Val Loss: 0.2687\n",
      "Epoch [42/50], Train Loss: 0.0853, Val Loss: 0.2656\n",
      "Epoch [43/50], Train Loss: 0.0838, Val Loss: 0.2625\n",
      "Epoch [44/50], Train Loss: 0.0824, Val Loss: 0.2595\n",
      "Epoch [45/50], Train Loss: 0.0815, Val Loss: 0.2566\n",
      "Epoch [46/50], Train Loss: 0.0804, Val Loss: 0.2537\n",
      "Epoch [47/50], Train Loss: 0.0787, Val Loss: 0.2508\n",
      "Epoch [48/50], Train Loss: 0.0778, Val Loss: 0.2480\n",
      "Epoch [49/50], Train Loss: 0.0765, Val Loss: 0.2453\n",
      "Epoch [50/50], Train Loss: 0.0754, Val Loss: 0.2426\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=64, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1352, Val Loss: 0.3913\n",
      "Epoch [2/50], Train Loss: 0.1296, Val Loss: 0.3855\n",
      "Epoch [3/50], Train Loss: 0.1276, Val Loss: 0.3799\n",
      "Epoch [4/50], Train Loss: 0.1254, Val Loss: 0.3743\n",
      "Epoch [5/50], Train Loss: 0.1241, Val Loss: 0.3689\n",
      "Epoch [6/50], Train Loss: 0.1200, Val Loss: 0.3636\n",
      "Epoch [7/50], Train Loss: 0.1186, Val Loss: 0.3585\n",
      "Epoch [8/50], Train Loss: 0.1164, Val Loss: 0.3534\n",
      "Epoch [9/50], Train Loss: 0.1144, Val Loss: 0.3485\n",
      "Epoch [10/50], Train Loss: 0.1125, Val Loss: 0.3436\n",
      "Epoch [11/50], Train Loss: 0.1108, Val Loss: 0.3388\n",
      "Epoch [12/50], Train Loss: 0.1081, Val Loss: 0.3342\n",
      "Epoch [13/50], Train Loss: 0.1056, Val Loss: 0.3297\n",
      "Epoch [14/50], Train Loss: 0.1045, Val Loss: 0.3252\n",
      "Epoch [15/50], Train Loss: 0.1029, Val Loss: 0.3208\n",
      "Epoch [16/50], Train Loss: 0.1003, Val Loss: 0.3165\n",
      "Epoch [17/50], Train Loss: 0.0984, Val Loss: 0.3123\n",
      "Epoch [18/50], Train Loss: 0.0989, Val Loss: 0.3082\n",
      "Epoch [19/50], Train Loss: 0.0959, Val Loss: 0.3041\n",
      "Epoch [20/50], Train Loss: 0.0937, Val Loss: 0.3002\n",
      "Epoch [21/50], Train Loss: 0.0925, Val Loss: 0.2963\n",
      "Epoch [22/50], Train Loss: 0.0914, Val Loss: 0.2924\n",
      "Epoch [23/50], Train Loss: 0.0888, Val Loss: 0.2887\n",
      "Epoch [24/50], Train Loss: 0.0878, Val Loss: 0.2850\n",
      "Epoch [25/50], Train Loss: 0.0853, Val Loss: 0.2814\n",
      "Epoch [26/50], Train Loss: 0.0844, Val Loss: 0.2779\n",
      "Epoch [27/50], Train Loss: 0.0832, Val Loss: 0.2745\n",
      "Epoch [28/50], Train Loss: 0.0812, Val Loss: 0.2711\n",
      "Epoch [29/50], Train Loss: 0.0808, Val Loss: 0.2678\n",
      "Epoch [30/50], Train Loss: 0.0797, Val Loss: 0.2645\n",
      "Epoch [31/50], Train Loss: 0.0786, Val Loss: 0.2614\n",
      "Epoch [32/50], Train Loss: 0.0780, Val Loss: 0.2582\n",
      "Epoch [33/50], Train Loss: 0.0771, Val Loss: 0.2551\n",
      "Epoch [34/50], Train Loss: 0.0755, Val Loss: 0.2521\n",
      "Epoch [35/50], Train Loss: 0.0744, Val Loss: 0.2491\n",
      "Epoch [36/50], Train Loss: 0.0726, Val Loss: 0.2462\n",
      "Epoch [37/50], Train Loss: 0.0724, Val Loss: 0.2434\n",
      "Epoch [38/50], Train Loss: 0.0710, Val Loss: 0.2406\n",
      "Epoch [39/50], Train Loss: 0.0706, Val Loss: 0.2379\n",
      "Epoch [40/50], Train Loss: 0.0698, Val Loss: 0.2352\n",
      "Epoch [41/50], Train Loss: 0.0673, Val Loss: 0.2325\n",
      "Epoch [42/50], Train Loss: 0.0669, Val Loss: 0.2300\n",
      "Epoch [43/50], Train Loss: 0.0658, Val Loss: 0.2274\n",
      "Epoch [44/50], Train Loss: 0.0654, Val Loss: 0.2249\n",
      "Epoch [45/50], Train Loss: 0.0637, Val Loss: 0.2224\n",
      "Epoch [46/50], Train Loss: 0.0651, Val Loss: 0.2201\n",
      "Epoch [47/50], Train Loss: 0.0638, Val Loss: 0.2177\n",
      "Epoch [48/50], Train Loss: 0.0624, Val Loss: 0.2154\n",
      "Epoch [49/50], Train Loss: 0.0614, Val Loss: 0.2131\n",
      "Epoch [50/50], Train Loss: 0.0619, Val Loss: 0.2109\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=64, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1693, Val Loss: 0.4409\n",
      "Epoch [2/50], Train Loss: 0.1651, Val Loss: 0.4332\n",
      "Epoch [3/50], Train Loss: 0.1610, Val Loss: 0.4258\n",
      "Epoch [4/50], Train Loss: 0.1570, Val Loss: 0.4186\n",
      "Epoch [5/50], Train Loss: 0.1531, Val Loss: 0.4115\n",
      "Epoch [6/50], Train Loss: 0.1494, Val Loss: 0.4046\n",
      "Epoch [7/50], Train Loss: 0.1458, Val Loss: 0.3979\n",
      "Epoch [8/50], Train Loss: 0.1423, Val Loss: 0.3914\n",
      "Epoch [9/50], Train Loss: 0.1390, Val Loss: 0.3850\n",
      "Epoch [10/50], Train Loss: 0.1357, Val Loss: 0.3788\n",
      "Epoch [11/50], Train Loss: 0.1325, Val Loss: 0.3728\n",
      "Epoch [12/50], Train Loss: 0.1294, Val Loss: 0.3668\n",
      "Epoch [13/50], Train Loss: 0.1265, Val Loss: 0.3611\n",
      "Epoch [14/50], Train Loss: 0.1236, Val Loss: 0.3554\n",
      "Epoch [15/50], Train Loss: 0.1208, Val Loss: 0.3499\n",
      "Epoch [16/50], Train Loss: 0.1181, Val Loss: 0.3446\n",
      "Epoch [17/50], Train Loss: 0.1155, Val Loss: 0.3393\n",
      "Epoch [18/50], Train Loss: 0.1129, Val Loss: 0.3342\n",
      "Epoch [19/50], Train Loss: 0.1105, Val Loss: 0.3292\n",
      "Epoch [20/50], Train Loss: 0.1081, Val Loss: 0.3243\n",
      "Epoch [21/50], Train Loss: 0.1058, Val Loss: 0.3195\n",
      "Epoch [22/50], Train Loss: 0.1035, Val Loss: 0.3149\n",
      "Epoch [23/50], Train Loss: 0.1013, Val Loss: 0.3103\n",
      "Epoch [24/50], Train Loss: 0.0992, Val Loss: 0.3058\n",
      "Epoch [25/50], Train Loss: 0.0972, Val Loss: 0.3014\n",
      "Epoch [26/50], Train Loss: 0.0952, Val Loss: 0.2972\n",
      "Epoch [27/50], Train Loss: 0.0933, Val Loss: 0.2930\n",
      "Epoch [28/50], Train Loss: 0.0914, Val Loss: 0.2889\n",
      "Epoch [29/50], Train Loss: 0.0896, Val Loss: 0.2849\n",
      "Epoch [30/50], Train Loss: 0.0878, Val Loss: 0.2810\n",
      "Epoch [31/50], Train Loss: 0.0861, Val Loss: 0.2772\n",
      "Epoch [32/50], Train Loss: 0.0845, Val Loss: 0.2734\n",
      "Epoch [33/50], Train Loss: 0.0829, Val Loss: 0.2698\n",
      "Epoch [34/50], Train Loss: 0.0813, Val Loss: 0.2662\n",
      "Epoch [35/50], Train Loss: 0.0798, Val Loss: 0.2627\n",
      "Epoch [36/50], Train Loss: 0.0784, Val Loss: 0.2592\n",
      "Epoch [37/50], Train Loss: 0.0769, Val Loss: 0.2559\n",
      "Epoch [38/50], Train Loss: 0.0756, Val Loss: 0.2526\n",
      "Epoch [39/50], Train Loss: 0.0742, Val Loss: 0.2494\n",
      "Epoch [40/50], Train Loss: 0.0729, Val Loss: 0.2462\n",
      "Epoch [41/50], Train Loss: 0.0717, Val Loss: 0.2431\n",
      "Epoch [42/50], Train Loss: 0.0705, Val Loss: 0.2401\n",
      "Epoch [43/50], Train Loss: 0.0693, Val Loss: 0.2371\n",
      "Epoch [44/50], Train Loss: 0.0682, Val Loss: 0.2342\n",
      "Epoch [45/50], Train Loss: 0.0670, Val Loss: 0.2314\n",
      "Epoch [46/50], Train Loss: 0.0660, Val Loss: 0.2286\n",
      "Epoch [47/50], Train Loss: 0.0649, Val Loss: 0.2259\n",
      "Epoch [48/50], Train Loss: 0.0639, Val Loss: 0.2233\n",
      "Epoch [49/50], Train Loss: 0.0629, Val Loss: 0.2207\n",
      "Epoch [50/50], Train Loss: 0.0620, Val Loss: 0.2181\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=64, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1793, Val Loss: 0.4661\n",
      "Epoch [2/50], Train Loss: 0.1738, Val Loss: 0.4580\n",
      "Epoch [3/50], Train Loss: 0.1702, Val Loss: 0.4501\n",
      "Epoch [4/50], Train Loss: 0.1658, Val Loss: 0.4424\n",
      "Epoch [5/50], Train Loss: 0.1629, Val Loss: 0.4349\n",
      "Epoch [6/50], Train Loss: 0.1578, Val Loss: 0.4277\n",
      "Epoch [7/50], Train Loss: 0.1553, Val Loss: 0.4206\n",
      "Epoch [8/50], Train Loss: 0.1511, Val Loss: 0.4137\n",
      "Epoch [9/50], Train Loss: 0.1481, Val Loss: 0.4070\n",
      "Epoch [10/50], Train Loss: 0.1447, Val Loss: 0.4005\n",
      "Epoch [11/50], Train Loss: 0.1404, Val Loss: 0.3942\n",
      "Epoch [12/50], Train Loss: 0.1391, Val Loss: 0.3879\n",
      "Epoch [13/50], Train Loss: 0.1350, Val Loss: 0.3818\n",
      "Epoch [14/50], Train Loss: 0.1323, Val Loss: 0.3759\n",
      "Epoch [15/50], Train Loss: 0.1292, Val Loss: 0.3702\n",
      "Epoch [16/50], Train Loss: 0.1274, Val Loss: 0.3645\n",
      "Epoch [17/50], Train Loss: 0.1237, Val Loss: 0.3590\n",
      "Epoch [18/50], Train Loss: 0.1207, Val Loss: 0.3536\n",
      "Epoch [19/50], Train Loss: 0.1181, Val Loss: 0.3484\n",
      "Epoch [20/50], Train Loss: 0.1162, Val Loss: 0.3433\n",
      "Epoch [21/50], Train Loss: 0.1136, Val Loss: 0.3383\n",
      "Epoch [22/50], Train Loss: 0.1120, Val Loss: 0.3334\n",
      "Epoch [23/50], Train Loss: 0.1098, Val Loss: 0.3286\n",
      "Epoch [24/50], Train Loss: 0.1073, Val Loss: 0.3240\n",
      "Epoch [25/50], Train Loss: 0.1052, Val Loss: 0.3194\n",
      "Epoch [26/50], Train Loss: 0.1029, Val Loss: 0.3149\n",
      "Epoch [27/50], Train Loss: 0.1012, Val Loss: 0.3105\n",
      "Epoch [28/50], Train Loss: 0.1002, Val Loss: 0.3062\n",
      "Epoch [29/50], Train Loss: 0.0972, Val Loss: 0.3021\n",
      "Epoch [30/50], Train Loss: 0.0951, Val Loss: 0.2980\n",
      "Epoch [31/50], Train Loss: 0.0935, Val Loss: 0.2940\n",
      "Epoch [32/50], Train Loss: 0.0924, Val Loss: 0.2901\n",
      "Epoch [33/50], Train Loss: 0.0912, Val Loss: 0.2862\n",
      "Epoch [34/50], Train Loss: 0.0890, Val Loss: 0.2825\n",
      "Epoch [35/50], Train Loss: 0.0868, Val Loss: 0.2788\n",
      "Epoch [36/50], Train Loss: 0.0857, Val Loss: 0.2752\n",
      "Epoch [37/50], Train Loss: 0.0849, Val Loss: 0.2717\n",
      "Epoch [38/50], Train Loss: 0.0832, Val Loss: 0.2682\n",
      "Epoch [39/50], Train Loss: 0.0824, Val Loss: 0.2648\n",
      "Epoch [40/50], Train Loss: 0.0804, Val Loss: 0.2615\n",
      "Epoch [41/50], Train Loss: 0.0792, Val Loss: 0.2582\n",
      "Epoch [42/50], Train Loss: 0.0776, Val Loss: 0.2551\n",
      "Epoch [43/50], Train Loss: 0.0771, Val Loss: 0.2520\n",
      "Epoch [44/50], Train Loss: 0.0749, Val Loss: 0.2489\n",
      "Epoch [45/50], Train Loss: 0.0742, Val Loss: 0.2459\n",
      "Epoch [46/50], Train Loss: 0.0731, Val Loss: 0.2430\n",
      "Epoch [47/50], Train Loss: 0.0721, Val Loss: 0.2401\n",
      "Epoch [48/50], Train Loss: 0.0713, Val Loss: 0.2373\n",
      "Epoch [49/50], Train Loss: 0.0700, Val Loss: 0.2345\n",
      "Epoch [50/50], Train Loss: 0.0685, Val Loss: 0.2318\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=64, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1388, Val Loss: 0.3655\n",
      "Epoch [2/50], Train Loss: 0.1364, Val Loss: 0.3596\n",
      "Epoch [3/50], Train Loss: 0.1315, Val Loss: 0.3540\n",
      "Epoch [4/50], Train Loss: 0.1296, Val Loss: 0.3485\n",
      "Epoch [5/50], Train Loss: 0.1263, Val Loss: 0.3432\n",
      "Epoch [6/50], Train Loss: 0.1239, Val Loss: 0.3380\n",
      "Epoch [7/50], Train Loss: 0.1213, Val Loss: 0.3329\n",
      "Epoch [8/50], Train Loss: 0.1194, Val Loss: 0.3279\n",
      "Epoch [9/50], Train Loss: 0.1154, Val Loss: 0.3231\n",
      "Epoch [10/50], Train Loss: 0.1135, Val Loss: 0.3183\n",
      "Epoch [11/50], Train Loss: 0.1113, Val Loss: 0.3138\n",
      "Epoch [12/50], Train Loss: 0.1096, Val Loss: 0.3093\n",
      "Epoch [13/50], Train Loss: 0.1064, Val Loss: 0.3049\n",
      "Epoch [14/50], Train Loss: 0.1053, Val Loss: 0.3006\n",
      "Epoch [15/50], Train Loss: 0.1039, Val Loss: 0.2964\n",
      "Epoch [16/50], Train Loss: 0.1013, Val Loss: 0.2924\n",
      "Epoch [17/50], Train Loss: 0.1006, Val Loss: 0.2883\n",
      "Epoch [18/50], Train Loss: 0.0981, Val Loss: 0.2844\n",
      "Epoch [19/50], Train Loss: 0.0953, Val Loss: 0.2806\n",
      "Epoch [20/50], Train Loss: 0.0941, Val Loss: 0.2768\n",
      "Epoch [21/50], Train Loss: 0.0934, Val Loss: 0.2732\n",
      "Epoch [22/50], Train Loss: 0.0917, Val Loss: 0.2696\n",
      "Epoch [23/50], Train Loss: 0.0898, Val Loss: 0.2661\n",
      "Epoch [24/50], Train Loss: 0.0879, Val Loss: 0.2627\n",
      "Epoch [25/50], Train Loss: 0.0864, Val Loss: 0.2594\n",
      "Epoch [26/50], Train Loss: 0.0842, Val Loss: 0.2561\n",
      "Epoch [27/50], Train Loss: 0.0841, Val Loss: 0.2529\n",
      "Epoch [28/50], Train Loss: 0.0825, Val Loss: 0.2498\n",
      "Epoch [29/50], Train Loss: 0.0814, Val Loss: 0.2467\n",
      "Epoch [30/50], Train Loss: 0.0796, Val Loss: 0.2437\n",
      "Epoch [31/50], Train Loss: 0.0775, Val Loss: 0.2407\n",
      "Epoch [32/50], Train Loss: 0.0767, Val Loss: 0.2379\n",
      "Epoch [33/50], Train Loss: 0.0766, Val Loss: 0.2350\n",
      "Epoch [34/50], Train Loss: 0.0751, Val Loss: 0.2322\n",
      "Epoch [35/50], Train Loss: 0.0748, Val Loss: 0.2295\n",
      "Epoch [36/50], Train Loss: 0.0737, Val Loss: 0.2269\n",
      "Epoch [37/50], Train Loss: 0.0720, Val Loss: 0.2243\n",
      "Epoch [38/50], Train Loss: 0.0711, Val Loss: 0.2218\n",
      "Epoch [39/50], Train Loss: 0.0701, Val Loss: 0.2193\n",
      "Epoch [40/50], Train Loss: 0.0700, Val Loss: 0.2168\n",
      "Epoch [41/50], Train Loss: 0.0681, Val Loss: 0.2144\n",
      "Epoch [42/50], Train Loss: 0.0664, Val Loss: 0.2120\n",
      "Epoch [43/50], Train Loss: 0.0656, Val Loss: 0.2097\n",
      "Epoch [44/50], Train Loss: 0.0657, Val Loss: 0.2075\n",
      "Epoch [45/50], Train Loss: 0.0652, Val Loss: 0.2052\n",
      "Epoch [46/50], Train Loss: 0.0648, Val Loss: 0.2031\n",
      "Epoch [47/50], Train Loss: 0.0635, Val Loss: 0.2010\n",
      "Epoch [48/50], Train Loss: 0.0620, Val Loss: 0.1989\n",
      "Epoch [49/50], Train Loss: 0.0612, Val Loss: 0.1969\n",
      "Epoch [50/50], Train Loss: 0.0608, Val Loss: 0.1949\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=64, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1292, Val Loss: 0.3439\n",
      "Epoch [2/50], Train Loss: 0.1261, Val Loss: 0.3383\n",
      "Epoch [3/50], Train Loss: 0.1232, Val Loss: 0.3328\n",
      "Epoch [4/50], Train Loss: 0.1203, Val Loss: 0.3275\n",
      "Epoch [5/50], Train Loss: 0.1175, Val Loss: 0.3223\n",
      "Epoch [6/50], Train Loss: 0.1148, Val Loss: 0.3172\n",
      "Epoch [7/50], Train Loss: 0.1122, Val Loss: 0.3123\n",
      "Epoch [8/50], Train Loss: 0.1097, Val Loss: 0.3074\n",
      "Epoch [9/50], Train Loss: 0.1073, Val Loss: 0.3027\n",
      "Epoch [10/50], Train Loss: 0.1049, Val Loss: 0.2981\n",
      "Epoch [11/50], Train Loss: 0.1026, Val Loss: 0.2935\n",
      "Epoch [12/50], Train Loss: 0.1004, Val Loss: 0.2891\n",
      "Epoch [13/50], Train Loss: 0.0982, Val Loss: 0.2848\n",
      "Epoch [14/50], Train Loss: 0.0961, Val Loss: 0.2806\n",
      "Epoch [15/50], Train Loss: 0.0941, Val Loss: 0.2765\n",
      "Epoch [16/50], Train Loss: 0.0922, Val Loss: 0.2725\n",
      "Epoch [17/50], Train Loss: 0.0903, Val Loss: 0.2685\n",
      "Epoch [18/50], Train Loss: 0.0884, Val Loss: 0.2647\n",
      "Epoch [19/50], Train Loss: 0.0866, Val Loss: 0.2609\n",
      "Epoch [20/50], Train Loss: 0.0849, Val Loss: 0.2573\n",
      "Epoch [21/50], Train Loss: 0.0832, Val Loss: 0.2537\n",
      "Epoch [22/50], Train Loss: 0.0816, Val Loss: 0.2502\n",
      "Epoch [23/50], Train Loss: 0.0800, Val Loss: 0.2467\n",
      "Epoch [24/50], Train Loss: 0.0785, Val Loss: 0.2434\n",
      "Epoch [25/50], Train Loss: 0.0770, Val Loss: 0.2401\n",
      "Epoch [26/50], Train Loss: 0.0755, Val Loss: 0.2369\n",
      "Epoch [27/50], Train Loss: 0.0742, Val Loss: 0.2337\n",
      "Epoch [28/50], Train Loss: 0.0728, Val Loss: 0.2307\n",
      "Epoch [29/50], Train Loss: 0.0715, Val Loss: 0.2276\n",
      "Epoch [30/50], Train Loss: 0.0702, Val Loss: 0.2247\n",
      "Epoch [31/50], Train Loss: 0.0690, Val Loss: 0.2218\n",
      "Epoch [32/50], Train Loss: 0.0678, Val Loss: 0.2190\n",
      "Epoch [33/50], Train Loss: 0.0667, Val Loss: 0.2163\n",
      "Epoch [34/50], Train Loss: 0.0655, Val Loss: 0.2136\n",
      "Epoch [35/50], Train Loss: 0.0645, Val Loss: 0.2109\n",
      "Epoch [36/50], Train Loss: 0.0634, Val Loss: 0.2084\n",
      "Epoch [37/50], Train Loss: 0.0624, Val Loss: 0.2059\n",
      "Epoch [38/50], Train Loss: 0.0614, Val Loss: 0.2034\n",
      "Epoch [39/50], Train Loss: 0.0605, Val Loss: 0.2010\n",
      "Epoch [40/50], Train Loss: 0.0596, Val Loss: 0.1986\n",
      "Epoch [41/50], Train Loss: 0.0587, Val Loss: 0.1963\n",
      "Epoch [42/50], Train Loss: 0.0578, Val Loss: 0.1941\n",
      "Epoch [43/50], Train Loss: 0.0570, Val Loss: 0.1919\n",
      "Epoch [44/50], Train Loss: 0.0562, Val Loss: 0.1897\n",
      "Epoch [45/50], Train Loss: 0.0554, Val Loss: 0.1876\n",
      "Epoch [46/50], Train Loss: 0.0546, Val Loss: 0.1856\n",
      "Epoch [47/50], Train Loss: 0.0539, Val Loss: 0.1836\n",
      "Epoch [48/50], Train Loss: 0.0532, Val Loss: 0.1816\n",
      "Epoch [49/50], Train Loss: 0.0525, Val Loss: 0.1797\n",
      "Epoch [50/50], Train Loss: 0.0519, Val Loss: 0.1778\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=64, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1786, Val Loss: 0.4532\n",
      "Epoch [2/50], Train Loss: 0.1746, Val Loss: 0.4464\n",
      "Epoch [3/50], Train Loss: 0.1706, Val Loss: 0.4398\n",
      "Epoch [4/50], Train Loss: 0.1670, Val Loss: 0.4333\n",
      "Epoch [5/50], Train Loss: 0.1637, Val Loss: 0.4269\n",
      "Epoch [6/50], Train Loss: 0.1604, Val Loss: 0.4207\n",
      "Epoch [7/50], Train Loss: 0.1566, Val Loss: 0.4147\n",
      "Epoch [8/50], Train Loss: 0.1537, Val Loss: 0.4087\n",
      "Epoch [9/50], Train Loss: 0.1504, Val Loss: 0.4029\n",
      "Epoch [10/50], Train Loss: 0.1469, Val Loss: 0.3972\n",
      "Epoch [11/50], Train Loss: 0.1449, Val Loss: 0.3916\n",
      "Epoch [12/50], Train Loss: 0.1409, Val Loss: 0.3862\n",
      "Epoch [13/50], Train Loss: 0.1379, Val Loss: 0.3808\n",
      "Epoch [14/50], Train Loss: 0.1356, Val Loss: 0.3756\n",
      "Epoch [15/50], Train Loss: 0.1332, Val Loss: 0.3704\n",
      "Epoch [16/50], Train Loss: 0.1301, Val Loss: 0.3654\n",
      "Epoch [17/50], Train Loss: 0.1276, Val Loss: 0.3605\n",
      "Epoch [18/50], Train Loss: 0.1255, Val Loss: 0.3556\n",
      "Epoch [19/50], Train Loss: 0.1230, Val Loss: 0.3509\n",
      "Epoch [20/50], Train Loss: 0.1206, Val Loss: 0.3462\n",
      "Epoch [21/50], Train Loss: 0.1184, Val Loss: 0.3417\n",
      "Epoch [22/50], Train Loss: 0.1159, Val Loss: 0.3372\n",
      "Epoch [23/50], Train Loss: 0.1137, Val Loss: 0.3328\n",
      "Epoch [24/50], Train Loss: 0.1117, Val Loss: 0.3285\n",
      "Epoch [25/50], Train Loss: 0.1101, Val Loss: 0.3243\n",
      "Epoch [26/50], Train Loss: 0.1080, Val Loss: 0.3201\n",
      "Epoch [27/50], Train Loss: 0.1056, Val Loss: 0.3160\n",
      "Epoch [28/50], Train Loss: 0.1041, Val Loss: 0.3120\n",
      "Epoch [29/50], Train Loss: 0.1021, Val Loss: 0.3080\n",
      "Epoch [30/50], Train Loss: 0.1008, Val Loss: 0.3042\n",
      "Epoch [31/50], Train Loss: 0.0987, Val Loss: 0.3004\n",
      "Epoch [32/50], Train Loss: 0.0967, Val Loss: 0.2967\n",
      "Epoch [33/50], Train Loss: 0.0954, Val Loss: 0.2930\n",
      "Epoch [34/50], Train Loss: 0.0935, Val Loss: 0.2894\n",
      "Epoch [35/50], Train Loss: 0.0920, Val Loss: 0.2858\n",
      "Epoch [36/50], Train Loss: 0.0902, Val Loss: 0.2824\n",
      "Epoch [37/50], Train Loss: 0.0887, Val Loss: 0.2790\n",
      "Epoch [38/50], Train Loss: 0.0873, Val Loss: 0.2756\n",
      "Epoch [39/50], Train Loss: 0.0855, Val Loss: 0.2723\n",
      "Epoch [40/50], Train Loss: 0.0849, Val Loss: 0.2691\n",
      "Epoch [41/50], Train Loss: 0.0831, Val Loss: 0.2660\n",
      "Epoch [42/50], Train Loss: 0.0822, Val Loss: 0.2628\n",
      "Epoch [43/50], Train Loss: 0.0808, Val Loss: 0.2597\n",
      "Epoch [44/50], Train Loss: 0.0796, Val Loss: 0.2567\n",
      "Epoch [45/50], Train Loss: 0.0779, Val Loss: 0.2538\n",
      "Epoch [46/50], Train Loss: 0.0764, Val Loss: 0.2509\n",
      "Epoch [47/50], Train Loss: 0.0756, Val Loss: 0.2480\n",
      "Epoch [48/50], Train Loss: 0.0746, Val Loss: 0.2452\n",
      "Epoch [49/50], Train Loss: 0.0737, Val Loss: 0.2425\n",
      "Epoch [50/50], Train Loss: 0.0728, Val Loss: 0.2398\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=64, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1533, Val Loss: 0.3914\n",
      "Epoch [2/50], Train Loss: 0.1503, Val Loss: 0.3850\n",
      "Epoch [3/50], Train Loss: 0.1482, Val Loss: 0.3786\n",
      "Epoch [4/50], Train Loss: 0.1439, Val Loss: 0.3726\n",
      "Epoch [5/50], Train Loss: 0.1397, Val Loss: 0.3667\n",
      "Epoch [6/50], Train Loss: 0.1361, Val Loss: 0.3610\n",
      "Epoch [7/50], Train Loss: 0.1328, Val Loss: 0.3554\n",
      "Epoch [8/50], Train Loss: 0.1283, Val Loss: 0.3501\n",
      "Epoch [9/50], Train Loss: 0.1266, Val Loss: 0.3448\n",
      "Epoch [10/50], Train Loss: 0.1239, Val Loss: 0.3397\n",
      "Epoch [11/50], Train Loss: 0.1220, Val Loss: 0.3346\n",
      "Epoch [12/50], Train Loss: 0.1177, Val Loss: 0.3298\n",
      "Epoch [13/50], Train Loss: 0.1160, Val Loss: 0.3251\n",
      "Epoch [14/50], Train Loss: 0.1136, Val Loss: 0.3204\n",
      "Epoch [15/50], Train Loss: 0.1109, Val Loss: 0.3159\n",
      "Epoch [16/50], Train Loss: 0.1089, Val Loss: 0.3115\n",
      "Epoch [17/50], Train Loss: 0.1073, Val Loss: 0.3073\n",
      "Epoch [18/50], Train Loss: 0.1052, Val Loss: 0.3030\n",
      "Epoch [19/50], Train Loss: 0.1034, Val Loss: 0.2990\n",
      "Epoch [20/50], Train Loss: 0.1002, Val Loss: 0.2950\n",
      "Epoch [21/50], Train Loss: 0.0985, Val Loss: 0.2911\n",
      "Epoch [22/50], Train Loss: 0.0975, Val Loss: 0.2873\n",
      "Epoch [23/50], Train Loss: 0.0959, Val Loss: 0.2836\n",
      "Epoch [24/50], Train Loss: 0.0942, Val Loss: 0.2800\n",
      "Epoch [25/50], Train Loss: 0.0910, Val Loss: 0.2765\n",
      "Epoch [26/50], Train Loss: 0.0900, Val Loss: 0.2731\n",
      "Epoch [27/50], Train Loss: 0.0892, Val Loss: 0.2697\n",
      "Epoch [28/50], Train Loss: 0.0866, Val Loss: 0.2664\n",
      "Epoch [29/50], Train Loss: 0.0856, Val Loss: 0.2631\n",
      "Epoch [30/50], Train Loss: 0.0830, Val Loss: 0.2601\n",
      "Epoch [31/50], Train Loss: 0.0830, Val Loss: 0.2569\n",
      "Epoch [32/50], Train Loss: 0.0812, Val Loss: 0.2539\n",
      "Epoch [33/50], Train Loss: 0.0788, Val Loss: 0.2510\n",
      "Epoch [34/50], Train Loss: 0.0790, Val Loss: 0.2482\n",
      "Epoch [35/50], Train Loss: 0.0775, Val Loss: 0.2453\n",
      "Epoch [36/50], Train Loss: 0.0770, Val Loss: 0.2425\n",
      "Epoch [37/50], Train Loss: 0.0758, Val Loss: 0.2398\n",
      "Epoch [38/50], Train Loss: 0.0740, Val Loss: 0.2372\n",
      "Epoch [39/50], Train Loss: 0.0732, Val Loss: 0.2346\n",
      "Epoch [40/50], Train Loss: 0.0717, Val Loss: 0.2321\n",
      "Epoch [41/50], Train Loss: 0.0704, Val Loss: 0.2297\n",
      "Epoch [42/50], Train Loss: 0.0697, Val Loss: 0.2272\n",
      "Epoch [43/50], Train Loss: 0.0685, Val Loss: 0.2249\n",
      "Epoch [44/50], Train Loss: 0.0683, Val Loss: 0.2226\n",
      "Epoch [45/50], Train Loss: 0.0681, Val Loss: 0.2203\n",
      "Epoch [46/50], Train Loss: 0.0665, Val Loss: 0.2181\n",
      "Epoch [47/50], Train Loss: 0.0657, Val Loss: 0.2159\n",
      "Epoch [48/50], Train Loss: 0.0655, Val Loss: 0.2138\n",
      "Epoch [49/50], Train Loss: 0.0649, Val Loss: 0.2117\n",
      "Epoch [50/50], Train Loss: 0.0635, Val Loss: 0.2097\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=128, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1428, Val Loss: 0.3943\n",
      "Epoch [2/50], Train Loss: 0.1400, Val Loss: 0.3887\n",
      "Epoch [3/50], Train Loss: 0.1373, Val Loss: 0.3832\n",
      "Epoch [4/50], Train Loss: 0.1346, Val Loss: 0.3778\n",
      "Epoch [5/50], Train Loss: 0.1321, Val Loss: 0.3725\n",
      "Epoch [6/50], Train Loss: 0.1296, Val Loss: 0.3673\n",
      "Epoch [7/50], Train Loss: 0.1271, Val Loss: 0.3622\n",
      "Epoch [8/50], Train Loss: 0.1247, Val Loss: 0.3573\n",
      "Epoch [9/50], Train Loss: 0.1224, Val Loss: 0.3524\n",
      "Epoch [10/50], Train Loss: 0.1201, Val Loss: 0.3476\n",
      "Epoch [11/50], Train Loss: 0.1179, Val Loss: 0.3429\n",
      "Epoch [12/50], Train Loss: 0.1158, Val Loss: 0.3382\n",
      "Epoch [13/50], Train Loss: 0.1136, Val Loss: 0.3337\n",
      "Epoch [14/50], Train Loss: 0.1116, Val Loss: 0.3293\n",
      "Epoch [15/50], Train Loss: 0.1096, Val Loss: 0.3249\n",
      "Epoch [16/50], Train Loss: 0.1076, Val Loss: 0.3207\n",
      "Epoch [17/50], Train Loss: 0.1057, Val Loss: 0.3165\n",
      "Epoch [18/50], Train Loss: 0.1038, Val Loss: 0.3123\n",
      "Epoch [19/50], Train Loss: 0.1020, Val Loss: 0.3083\n",
      "Epoch [20/50], Train Loss: 0.1002, Val Loss: 0.3043\n",
      "Epoch [21/50], Train Loss: 0.0985, Val Loss: 0.3004\n",
      "Epoch [22/50], Train Loss: 0.0968, Val Loss: 0.2966\n",
      "Epoch [23/50], Train Loss: 0.0951, Val Loss: 0.2928\n",
      "Epoch [24/50], Train Loss: 0.0935, Val Loss: 0.2891\n",
      "Epoch [25/50], Train Loss: 0.0919, Val Loss: 0.2855\n",
      "Epoch [26/50], Train Loss: 0.0904, Val Loss: 0.2819\n",
      "Epoch [27/50], Train Loss: 0.0889, Val Loss: 0.2784\n",
      "Epoch [28/50], Train Loss: 0.0874, Val Loss: 0.2750\n",
      "Epoch [29/50], Train Loss: 0.0860, Val Loss: 0.2716\n",
      "Epoch [30/50], Train Loss: 0.0846, Val Loss: 0.2683\n",
      "Epoch [31/50], Train Loss: 0.0832, Val Loss: 0.2650\n",
      "Epoch [32/50], Train Loss: 0.0819, Val Loss: 0.2618\n",
      "Epoch [33/50], Train Loss: 0.0805, Val Loss: 0.2587\n",
      "Epoch [34/50], Train Loss: 0.0793, Val Loss: 0.2556\n",
      "Epoch [35/50], Train Loss: 0.0780, Val Loss: 0.2526\n",
      "Epoch [36/50], Train Loss: 0.0768, Val Loss: 0.2496\n",
      "Epoch [37/50], Train Loss: 0.0756, Val Loss: 0.2466\n",
      "Epoch [38/50], Train Loss: 0.0745, Val Loss: 0.2437\n",
      "Epoch [39/50], Train Loss: 0.0733, Val Loss: 0.2409\n",
      "Epoch [40/50], Train Loss: 0.0722, Val Loss: 0.2381\n",
      "Epoch [41/50], Train Loss: 0.0712, Val Loss: 0.2354\n",
      "Epoch [42/50], Train Loss: 0.0701, Val Loss: 0.2327\n",
      "Epoch [43/50], Train Loss: 0.0691, Val Loss: 0.2300\n",
      "Epoch [44/50], Train Loss: 0.0681, Val Loss: 0.2274\n",
      "Epoch [45/50], Train Loss: 0.0671, Val Loss: 0.2249\n",
      "Epoch [46/50], Train Loss: 0.0661, Val Loss: 0.2224\n",
      "Epoch [47/50], Train Loss: 0.0652, Val Loss: 0.2199\n",
      "Epoch [48/50], Train Loss: 0.0643, Val Loss: 0.2175\n",
      "Epoch [49/50], Train Loss: 0.0634, Val Loss: 0.2151\n",
      "Epoch [50/50], Train Loss: 0.0626, Val Loss: 0.2127\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=128, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1895, Val Loss: 0.4770\n",
      "Epoch [2/50], Train Loss: 0.1849, Val Loss: 0.4695\n",
      "Epoch [3/50], Train Loss: 0.1819, Val Loss: 0.4622\n",
      "Epoch [4/50], Train Loss: 0.1777, Val Loss: 0.4550\n",
      "Epoch [5/50], Train Loss: 0.1741, Val Loss: 0.4480\n",
      "Epoch [6/50], Train Loss: 0.1701, Val Loss: 0.4412\n",
      "Epoch [7/50], Train Loss: 0.1673, Val Loss: 0.4345\n",
      "Epoch [8/50], Train Loss: 0.1632, Val Loss: 0.4280\n",
      "Epoch [9/50], Train Loss: 0.1596, Val Loss: 0.4216\n",
      "Epoch [10/50], Train Loss: 0.1570, Val Loss: 0.4154\n",
      "Epoch [11/50], Train Loss: 0.1538, Val Loss: 0.4092\n",
      "Epoch [12/50], Train Loss: 0.1502, Val Loss: 0.4033\n",
      "Epoch [13/50], Train Loss: 0.1474, Val Loss: 0.3975\n",
      "Epoch [14/50], Train Loss: 0.1449, Val Loss: 0.3917\n",
      "Epoch [15/50], Train Loss: 0.1413, Val Loss: 0.3862\n",
      "Epoch [16/50], Train Loss: 0.1392, Val Loss: 0.3807\n",
      "Epoch [17/50], Train Loss: 0.1364, Val Loss: 0.3753\n",
      "Epoch [18/50], Train Loss: 0.1335, Val Loss: 0.3701\n",
      "Epoch [19/50], Train Loss: 0.1316, Val Loss: 0.3649\n",
      "Epoch [20/50], Train Loss: 0.1288, Val Loss: 0.3599\n",
      "Epoch [21/50], Train Loss: 0.1261, Val Loss: 0.3550\n",
      "Epoch [22/50], Train Loss: 0.1243, Val Loss: 0.3501\n",
      "Epoch [23/50], Train Loss: 0.1220, Val Loss: 0.3454\n",
      "Epoch [24/50], Train Loss: 0.1191, Val Loss: 0.3407\n",
      "Epoch [25/50], Train Loss: 0.1170, Val Loss: 0.3362\n",
      "Epoch [26/50], Train Loss: 0.1149, Val Loss: 0.3317\n",
      "Epoch [27/50], Train Loss: 0.1132, Val Loss: 0.3273\n",
      "Epoch [28/50], Train Loss: 0.1108, Val Loss: 0.3230\n",
      "Epoch [29/50], Train Loss: 0.1092, Val Loss: 0.3188\n",
      "Epoch [30/50], Train Loss: 0.1069, Val Loss: 0.3147\n",
      "Epoch [31/50], Train Loss: 0.1049, Val Loss: 0.3106\n",
      "Epoch [32/50], Train Loss: 0.1033, Val Loss: 0.3066\n",
      "Epoch [33/50], Train Loss: 0.1019, Val Loss: 0.3027\n",
      "Epoch [34/50], Train Loss: 0.1000, Val Loss: 0.2989\n",
      "Epoch [35/50], Train Loss: 0.0978, Val Loss: 0.2951\n",
      "Epoch [36/50], Train Loss: 0.0963, Val Loss: 0.2914\n",
      "Epoch [37/50], Train Loss: 0.0947, Val Loss: 0.2878\n",
      "Epoch [38/50], Train Loss: 0.0935, Val Loss: 0.2842\n",
      "Epoch [39/50], Train Loss: 0.0914, Val Loss: 0.2807\n",
      "Epoch [40/50], Train Loss: 0.0901, Val Loss: 0.2773\n",
      "Epoch [41/50], Train Loss: 0.0882, Val Loss: 0.2739\n",
      "Epoch [42/50], Train Loss: 0.0867, Val Loss: 0.2706\n",
      "Epoch [43/50], Train Loss: 0.0854, Val Loss: 0.2674\n",
      "Epoch [44/50], Train Loss: 0.0842, Val Loss: 0.2642\n",
      "Epoch [45/50], Train Loss: 0.0830, Val Loss: 0.2610\n",
      "Epoch [46/50], Train Loss: 0.0819, Val Loss: 0.2579\n",
      "Epoch [47/50], Train Loss: 0.0803, Val Loss: 0.2549\n",
      "Epoch [48/50], Train Loss: 0.0790, Val Loss: 0.2519\n",
      "Epoch [49/50], Train Loss: 0.0778, Val Loss: 0.2490\n",
      "Epoch [50/50], Train Loss: 0.0766, Val Loss: 0.2462\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=128, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1457, Val Loss: 0.3666\n",
      "Epoch [2/50], Train Loss: 0.1416, Val Loss: 0.3607\n",
      "Epoch [3/50], Train Loss: 0.1390, Val Loss: 0.3548\n",
      "Epoch [4/50], Train Loss: 0.1359, Val Loss: 0.3491\n",
      "Epoch [5/50], Train Loss: 0.1339, Val Loss: 0.3435\n",
      "Epoch [6/50], Train Loss: 0.1308, Val Loss: 0.3380\n",
      "Epoch [7/50], Train Loss: 0.1273, Val Loss: 0.3326\n",
      "Epoch [8/50], Train Loss: 0.1258, Val Loss: 0.3274\n",
      "Epoch [9/50], Train Loss: 0.1234, Val Loss: 0.3223\n",
      "Epoch [10/50], Train Loss: 0.1206, Val Loss: 0.3172\n",
      "Epoch [11/50], Train Loss: 0.1169, Val Loss: 0.3123\n",
      "Epoch [12/50], Train Loss: 0.1159, Val Loss: 0.3075\n",
      "Epoch [13/50], Train Loss: 0.1135, Val Loss: 0.3028\n",
      "Epoch [14/50], Train Loss: 0.1105, Val Loss: 0.2981\n",
      "Epoch [15/50], Train Loss: 0.1088, Val Loss: 0.2936\n",
      "Epoch [16/50], Train Loss: 0.1065, Val Loss: 0.2891\n",
      "Epoch [17/50], Train Loss: 0.1043, Val Loss: 0.2847\n",
      "Epoch [18/50], Train Loss: 0.1023, Val Loss: 0.2804\n",
      "Epoch [19/50], Train Loss: 0.1005, Val Loss: 0.2762\n",
      "Epoch [20/50], Train Loss: 0.0987, Val Loss: 0.2721\n",
      "Epoch [21/50], Train Loss: 0.0965, Val Loss: 0.2681\n",
      "Epoch [22/50], Train Loss: 0.0959, Val Loss: 0.2642\n",
      "Epoch [23/50], Train Loss: 0.0932, Val Loss: 0.2603\n",
      "Epoch [24/50], Train Loss: 0.0907, Val Loss: 0.2565\n",
      "Epoch [25/50], Train Loss: 0.0908, Val Loss: 0.2528\n",
      "Epoch [26/50], Train Loss: 0.0878, Val Loss: 0.2491\n",
      "Epoch [27/50], Train Loss: 0.0874, Val Loss: 0.2455\n",
      "Epoch [28/50], Train Loss: 0.0849, Val Loss: 0.2420\n",
      "Epoch [29/50], Train Loss: 0.0831, Val Loss: 0.2386\n",
      "Epoch [30/50], Train Loss: 0.0822, Val Loss: 0.2353\n",
      "Epoch [31/50], Train Loss: 0.0804, Val Loss: 0.2320\n",
      "Epoch [32/50], Train Loss: 0.0799, Val Loss: 0.2287\n",
      "Epoch [33/50], Train Loss: 0.0782, Val Loss: 0.2255\n",
      "Epoch [34/50], Train Loss: 0.0772, Val Loss: 0.2224\n",
      "Epoch [35/50], Train Loss: 0.0749, Val Loss: 0.2194\n",
      "Epoch [36/50], Train Loss: 0.0742, Val Loss: 0.2164\n",
      "Epoch [37/50], Train Loss: 0.0731, Val Loss: 0.2134\n",
      "Epoch [38/50], Train Loss: 0.0711, Val Loss: 0.2106\n",
      "Epoch [39/50], Train Loss: 0.0705, Val Loss: 0.2078\n",
      "Epoch [40/50], Train Loss: 0.0686, Val Loss: 0.2050\n",
      "Epoch [41/50], Train Loss: 0.0688, Val Loss: 0.2023\n",
      "Epoch [42/50], Train Loss: 0.0673, Val Loss: 0.1996\n",
      "Epoch [43/50], Train Loss: 0.0663, Val Loss: 0.1970\n",
      "Epoch [44/50], Train Loss: 0.0649, Val Loss: 0.1944\n",
      "Epoch [45/50], Train Loss: 0.0649, Val Loss: 0.1920\n",
      "Epoch [46/50], Train Loss: 0.0629, Val Loss: 0.1895\n",
      "Epoch [47/50], Train Loss: 0.0626, Val Loss: 0.1871\n",
      "Epoch [48/50], Train Loss: 0.0613, Val Loss: 0.1847\n",
      "Epoch [49/50], Train Loss: 0.0618, Val Loss: 0.1824\n",
      "Epoch [50/50], Train Loss: 0.0591, Val Loss: 0.1801\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=128, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1404, Val Loss: 0.3776\n",
      "Epoch [2/50], Train Loss: 0.1374, Val Loss: 0.3720\n",
      "Epoch [3/50], Train Loss: 0.1344, Val Loss: 0.3664\n",
      "Epoch [4/50], Train Loss: 0.1315, Val Loss: 0.3610\n",
      "Epoch [5/50], Train Loss: 0.1287, Val Loss: 0.3557\n",
      "Epoch [6/50], Train Loss: 0.1260, Val Loss: 0.3505\n",
      "Epoch [7/50], Train Loss: 0.1233, Val Loss: 0.3454\n",
      "Epoch [8/50], Train Loss: 0.1208, Val Loss: 0.3404\n",
      "Epoch [9/50], Train Loss: 0.1182, Val Loss: 0.3355\n",
      "Epoch [10/50], Train Loss: 0.1158, Val Loss: 0.3308\n",
      "Epoch [11/50], Train Loss: 0.1134, Val Loss: 0.3261\n",
      "Epoch [12/50], Train Loss: 0.1111, Val Loss: 0.3215\n",
      "Epoch [13/50], Train Loss: 0.1089, Val Loss: 0.3170\n",
      "Epoch [14/50], Train Loss: 0.1067, Val Loss: 0.3126\n",
      "Epoch [15/50], Train Loss: 0.1046, Val Loss: 0.3083\n",
      "Epoch [16/50], Train Loss: 0.1025, Val Loss: 0.3041\n",
      "Epoch [17/50], Train Loss: 0.1005, Val Loss: 0.3000\n",
      "Epoch [18/50], Train Loss: 0.0986, Val Loss: 0.2959\n",
      "Epoch [19/50], Train Loss: 0.0967, Val Loss: 0.2920\n",
      "Epoch [20/50], Train Loss: 0.0949, Val Loss: 0.2881\n",
      "Epoch [21/50], Train Loss: 0.0931, Val Loss: 0.2843\n",
      "Epoch [22/50], Train Loss: 0.0913, Val Loss: 0.2805\n",
      "Epoch [23/50], Train Loss: 0.0896, Val Loss: 0.2769\n",
      "Epoch [24/50], Train Loss: 0.0880, Val Loss: 0.2733\n",
      "Epoch [25/50], Train Loss: 0.0864, Val Loss: 0.2697\n",
      "Epoch [26/50], Train Loss: 0.0848, Val Loss: 0.2663\n",
      "Epoch [27/50], Train Loss: 0.0833, Val Loss: 0.2629\n",
      "Epoch [28/50], Train Loss: 0.0818, Val Loss: 0.2596\n",
      "Epoch [29/50], Train Loss: 0.0804, Val Loss: 0.2563\n",
      "Epoch [30/50], Train Loss: 0.0790, Val Loss: 0.2531\n",
      "Epoch [31/50], Train Loss: 0.0776, Val Loss: 0.2500\n",
      "Epoch [32/50], Train Loss: 0.0763, Val Loss: 0.2469\n",
      "Epoch [33/50], Train Loss: 0.0750, Val Loss: 0.2439\n",
      "Epoch [34/50], Train Loss: 0.0738, Val Loss: 0.2409\n",
      "Epoch [35/50], Train Loss: 0.0726, Val Loss: 0.2380\n",
      "Epoch [36/50], Train Loss: 0.0714, Val Loss: 0.2352\n",
      "Epoch [37/50], Train Loss: 0.0702, Val Loss: 0.2324\n",
      "Epoch [38/50], Train Loss: 0.0691, Val Loss: 0.2296\n",
      "Epoch [39/50], Train Loss: 0.0680, Val Loss: 0.2270\n",
      "Epoch [40/50], Train Loss: 0.0670, Val Loss: 0.2243\n",
      "Epoch [41/50], Train Loss: 0.0660, Val Loss: 0.2217\n",
      "Epoch [42/50], Train Loss: 0.0650, Val Loss: 0.2192\n",
      "Epoch [43/50], Train Loss: 0.0640, Val Loss: 0.2167\n",
      "Epoch [44/50], Train Loss: 0.0631, Val Loss: 0.2143\n",
      "Epoch [45/50], Train Loss: 0.0621, Val Loss: 0.2119\n",
      "Epoch [46/50], Train Loss: 0.0613, Val Loss: 0.2095\n",
      "Epoch [47/50], Train Loss: 0.0604, Val Loss: 0.2072\n",
      "Epoch [48/50], Train Loss: 0.0596, Val Loss: 0.2050\n",
      "Epoch [49/50], Train Loss: 0.0588, Val Loss: 0.2028\n",
      "Epoch [50/50], Train Loss: 0.0580, Val Loss: 0.2006\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=128, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1197, Val Loss: 0.3448\n",
      "Epoch [2/50], Train Loss: 0.1160, Val Loss: 0.3397\n",
      "Epoch [3/50], Train Loss: 0.1143, Val Loss: 0.3346\n",
      "Epoch [4/50], Train Loss: 0.1118, Val Loss: 0.3297\n",
      "Epoch [5/50], Train Loss: 0.1094, Val Loss: 0.3249\n",
      "Epoch [6/50], Train Loss: 0.1072, Val Loss: 0.3202\n",
      "Epoch [7/50], Train Loss: 0.1049, Val Loss: 0.3155\n",
      "Epoch [8/50], Train Loss: 0.1031, Val Loss: 0.3110\n",
      "Epoch [9/50], Train Loss: 0.1010, Val Loss: 0.3066\n",
      "Epoch [10/50], Train Loss: 0.0988, Val Loss: 0.3023\n",
      "Epoch [11/50], Train Loss: 0.0973, Val Loss: 0.2981\n",
      "Epoch [12/50], Train Loss: 0.0952, Val Loss: 0.2940\n",
      "Epoch [13/50], Train Loss: 0.0932, Val Loss: 0.2900\n",
      "Epoch [14/50], Train Loss: 0.0915, Val Loss: 0.2860\n",
      "Epoch [15/50], Train Loss: 0.0899, Val Loss: 0.2821\n",
      "Epoch [16/50], Train Loss: 0.0888, Val Loss: 0.2783\n",
      "Epoch [17/50], Train Loss: 0.0866, Val Loss: 0.2746\n",
      "Epoch [18/50], Train Loss: 0.0844, Val Loss: 0.2710\n",
      "Epoch [19/50], Train Loss: 0.0829, Val Loss: 0.2674\n",
      "Epoch [20/50], Train Loss: 0.0820, Val Loss: 0.2639\n",
      "Epoch [21/50], Train Loss: 0.0808, Val Loss: 0.2605\n",
      "Epoch [22/50], Train Loss: 0.0791, Val Loss: 0.2572\n",
      "Epoch [23/50], Train Loss: 0.0784, Val Loss: 0.2539\n",
      "Epoch [24/50], Train Loss: 0.0767, Val Loss: 0.2507\n",
      "Epoch [25/50], Train Loss: 0.0752, Val Loss: 0.2476\n",
      "Epoch [26/50], Train Loss: 0.0737, Val Loss: 0.2445\n",
      "Epoch [27/50], Train Loss: 0.0729, Val Loss: 0.2414\n",
      "Epoch [28/50], Train Loss: 0.0716, Val Loss: 0.2385\n",
      "Epoch [29/50], Train Loss: 0.0704, Val Loss: 0.2356\n",
      "Epoch [30/50], Train Loss: 0.0691, Val Loss: 0.2327\n",
      "Epoch [31/50], Train Loss: 0.0678, Val Loss: 0.2300\n",
      "Epoch [32/50], Train Loss: 0.0673, Val Loss: 0.2272\n",
      "Epoch [33/50], Train Loss: 0.0664, Val Loss: 0.2246\n",
      "Epoch [34/50], Train Loss: 0.0654, Val Loss: 0.2219\n",
      "Epoch [35/50], Train Loss: 0.0643, Val Loss: 0.2194\n",
      "Epoch [36/50], Train Loss: 0.0634, Val Loss: 0.2169\n",
      "Epoch [37/50], Train Loss: 0.0623, Val Loss: 0.2144\n",
      "Epoch [38/50], Train Loss: 0.0621, Val Loss: 0.2120\n",
      "Epoch [39/50], Train Loss: 0.0612, Val Loss: 0.2096\n",
      "Epoch [40/50], Train Loss: 0.0601, Val Loss: 0.2073\n",
      "Epoch [41/50], Train Loss: 0.0594, Val Loss: 0.2050\n",
      "Epoch [42/50], Train Loss: 0.0581, Val Loss: 0.2028\n",
      "Epoch [43/50], Train Loss: 0.0571, Val Loss: 0.2006\n",
      "Epoch [44/50], Train Loss: 0.0566, Val Loss: 0.1984\n",
      "Epoch [45/50], Train Loss: 0.0563, Val Loss: 0.1963\n",
      "Epoch [46/50], Train Loss: 0.0553, Val Loss: 0.1943\n",
      "Epoch [47/50], Train Loss: 0.0546, Val Loss: 0.1923\n",
      "Epoch [48/50], Train Loss: 0.0544, Val Loss: 0.1903\n",
      "Epoch [49/50], Train Loss: 0.0535, Val Loss: 0.1883\n",
      "Epoch [50/50], Train Loss: 0.0530, Val Loss: 0.1864\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=128, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1154, Val Loss: 0.3204\n",
      "Epoch [2/50], Train Loss: 0.1132, Val Loss: 0.3159\n",
      "Epoch [3/50], Train Loss: 0.1109, Val Loss: 0.3115\n",
      "Epoch [4/50], Train Loss: 0.1084, Val Loss: 0.3071\n",
      "Epoch [5/50], Train Loss: 0.1071, Val Loss: 0.3029\n",
      "Epoch [6/50], Train Loss: 0.1048, Val Loss: 0.2987\n",
      "Epoch [7/50], Train Loss: 0.1042, Val Loss: 0.2947\n",
      "Epoch [8/50], Train Loss: 0.1006, Val Loss: 0.2907\n",
      "Epoch [9/50], Train Loss: 0.0987, Val Loss: 0.2868\n",
      "Epoch [10/50], Train Loss: 0.0975, Val Loss: 0.2829\n",
      "Epoch [11/50], Train Loss: 0.0963, Val Loss: 0.2792\n",
      "Epoch [12/50], Train Loss: 0.0947, Val Loss: 0.2755\n",
      "Epoch [13/50], Train Loss: 0.0918, Val Loss: 0.2720\n",
      "Epoch [14/50], Train Loss: 0.0899, Val Loss: 0.2684\n",
      "Epoch [15/50], Train Loss: 0.0888, Val Loss: 0.2650\n",
      "Epoch [16/50], Train Loss: 0.0876, Val Loss: 0.2616\n",
      "Epoch [17/50], Train Loss: 0.0860, Val Loss: 0.2583\n",
      "Epoch [18/50], Train Loss: 0.0850, Val Loss: 0.2551\n",
      "Epoch [19/50], Train Loss: 0.0829, Val Loss: 0.2519\n",
      "Epoch [20/50], Train Loss: 0.0817, Val Loss: 0.2488\n",
      "Epoch [21/50], Train Loss: 0.0818, Val Loss: 0.2457\n",
      "Epoch [22/50], Train Loss: 0.0789, Val Loss: 0.2427\n",
      "Epoch [23/50], Train Loss: 0.0783, Val Loss: 0.2398\n",
      "Epoch [24/50], Train Loss: 0.0771, Val Loss: 0.2369\n",
      "Epoch [25/50], Train Loss: 0.0757, Val Loss: 0.2341\n",
      "Epoch [26/50], Train Loss: 0.0746, Val Loss: 0.2313\n",
      "Epoch [27/50], Train Loss: 0.0731, Val Loss: 0.2286\n",
      "Epoch [28/50], Train Loss: 0.0727, Val Loss: 0.2259\n",
      "Epoch [29/50], Train Loss: 0.0713, Val Loss: 0.2233\n",
      "Epoch [30/50], Train Loss: 0.0713, Val Loss: 0.2207\n",
      "Epoch [31/50], Train Loss: 0.0701, Val Loss: 0.2182\n",
      "Epoch [32/50], Train Loss: 0.0689, Val Loss: 0.2158\n",
      "Epoch [33/50], Train Loss: 0.0670, Val Loss: 0.2134\n",
      "Epoch [34/50], Train Loss: 0.0669, Val Loss: 0.2110\n",
      "Epoch [35/50], Train Loss: 0.0657, Val Loss: 0.2087\n",
      "Epoch [36/50], Train Loss: 0.0646, Val Loss: 0.2064\n",
      "Epoch [37/50], Train Loss: 0.0639, Val Loss: 0.2042\n",
      "Epoch [38/50], Train Loss: 0.0629, Val Loss: 0.2020\n",
      "Epoch [39/50], Train Loss: 0.0627, Val Loss: 0.1999\n",
      "Epoch [40/50], Train Loss: 0.0616, Val Loss: 0.1978\n",
      "Epoch [41/50], Train Loss: 0.0609, Val Loss: 0.1958\n",
      "Epoch [42/50], Train Loss: 0.0604, Val Loss: 0.1937\n",
      "Epoch [43/50], Train Loss: 0.0593, Val Loss: 0.1917\n",
      "Epoch [44/50], Train Loss: 0.0593, Val Loss: 0.1898\n",
      "Epoch [45/50], Train Loss: 0.0571, Val Loss: 0.1879\n",
      "Epoch [46/50], Train Loss: 0.0577, Val Loss: 0.1860\n",
      "Epoch [47/50], Train Loss: 0.0569, Val Loss: 0.1842\n",
      "Epoch [48/50], Train Loss: 0.0566, Val Loss: 0.1824\n",
      "Epoch [49/50], Train Loss: 0.0561, Val Loss: 0.1806\n",
      "Epoch [50/50], Train Loss: 0.0554, Val Loss: 0.1789\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=128, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1326, Val Loss: 0.3636\n",
      "Epoch [2/50], Train Loss: 0.1294, Val Loss: 0.3577\n",
      "Epoch [3/50], Train Loss: 0.1263, Val Loss: 0.3519\n",
      "Epoch [4/50], Train Loss: 0.1233, Val Loss: 0.3463\n",
      "Epoch [5/50], Train Loss: 0.1204, Val Loss: 0.3409\n",
      "Epoch [6/50], Train Loss: 0.1176, Val Loss: 0.3355\n",
      "Epoch [7/50], Train Loss: 0.1149, Val Loss: 0.3303\n",
      "Epoch [8/50], Train Loss: 0.1123, Val Loss: 0.3252\n",
      "Epoch [9/50], Train Loss: 0.1097, Val Loss: 0.3203\n",
      "Epoch [10/50], Train Loss: 0.1073, Val Loss: 0.3154\n",
      "Epoch [11/50], Train Loss: 0.1049, Val Loss: 0.3107\n",
      "Epoch [12/50], Train Loss: 0.1026, Val Loss: 0.3060\n",
      "Epoch [13/50], Train Loss: 0.1003, Val Loss: 0.3015\n",
      "Epoch [14/50], Train Loss: 0.0982, Val Loss: 0.2970\n",
      "Epoch [15/50], Train Loss: 0.0961, Val Loss: 0.2927\n",
      "Epoch [16/50], Train Loss: 0.0940, Val Loss: 0.2885\n",
      "Epoch [17/50], Train Loss: 0.0920, Val Loss: 0.2843\n",
      "Epoch [18/50], Train Loss: 0.0901, Val Loss: 0.2803\n",
      "Epoch [19/50], Train Loss: 0.0883, Val Loss: 0.2763\n",
      "Epoch [20/50], Train Loss: 0.0865, Val Loss: 0.2724\n",
      "Epoch [21/50], Train Loss: 0.0847, Val Loss: 0.2686\n",
      "Epoch [22/50], Train Loss: 0.0830, Val Loss: 0.2649\n",
      "Epoch [23/50], Train Loss: 0.0814, Val Loss: 0.2613\n",
      "Epoch [24/50], Train Loss: 0.0798, Val Loss: 0.2577\n",
      "Epoch [25/50], Train Loss: 0.0782, Val Loss: 0.2542\n",
      "Epoch [26/50], Train Loss: 0.0768, Val Loss: 0.2508\n",
      "Epoch [27/50], Train Loss: 0.0753, Val Loss: 0.2475\n",
      "Epoch [28/50], Train Loss: 0.0739, Val Loss: 0.2442\n",
      "Epoch [29/50], Train Loss: 0.0726, Val Loss: 0.2411\n",
      "Epoch [30/50], Train Loss: 0.0712, Val Loss: 0.2379\n",
      "Epoch [31/50], Train Loss: 0.0700, Val Loss: 0.2349\n",
      "Epoch [32/50], Train Loss: 0.0687, Val Loss: 0.2319\n",
      "Epoch [33/50], Train Loss: 0.0675, Val Loss: 0.2289\n",
      "Epoch [34/50], Train Loss: 0.0664, Val Loss: 0.2261\n",
      "Epoch [35/50], Train Loss: 0.0653, Val Loss: 0.2233\n",
      "Epoch [36/50], Train Loss: 0.0642, Val Loss: 0.2205\n",
      "Epoch [37/50], Train Loss: 0.0631, Val Loss: 0.2178\n",
      "Epoch [38/50], Train Loss: 0.0621, Val Loss: 0.2152\n",
      "Epoch [39/50], Train Loss: 0.0611, Val Loss: 0.2126\n",
      "Epoch [40/50], Train Loss: 0.0602, Val Loss: 0.2101\n",
      "Epoch [41/50], Train Loss: 0.0593, Val Loss: 0.2077\n",
      "Epoch [42/50], Train Loss: 0.0584, Val Loss: 0.2052\n",
      "Epoch [43/50], Train Loss: 0.0575, Val Loss: 0.2029\n",
      "Epoch [44/50], Train Loss: 0.0567, Val Loss: 0.2006\n",
      "Epoch [45/50], Train Loss: 0.0559, Val Loss: 0.1983\n",
      "Epoch [46/50], Train Loss: 0.0551, Val Loss: 0.1961\n",
      "Epoch [47/50], Train Loss: 0.0544, Val Loss: 0.1939\n",
      "Epoch [48/50], Train Loss: 0.0536, Val Loss: 0.1918\n",
      "Epoch [49/50], Train Loss: 0.0529, Val Loss: 0.1897\n",
      "Epoch [50/50], Train Loss: 0.0523, Val Loss: 0.1877\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=128, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1224, Val Loss: 0.3407\n",
      "Epoch [2/50], Train Loss: 0.1199, Val Loss: 0.3357\n",
      "Epoch [3/50], Train Loss: 0.1172, Val Loss: 0.3309\n",
      "Epoch [4/50], Train Loss: 0.1140, Val Loss: 0.3261\n",
      "Epoch [5/50], Train Loss: 0.1125, Val Loss: 0.3214\n",
      "Epoch [6/50], Train Loss: 0.1100, Val Loss: 0.3169\n",
      "Epoch [7/50], Train Loss: 0.1079, Val Loss: 0.3124\n",
      "Epoch [8/50], Train Loss: 0.1056, Val Loss: 0.3080\n",
      "Epoch [9/50], Train Loss: 0.1034, Val Loss: 0.3038\n",
      "Epoch [10/50], Train Loss: 0.1008, Val Loss: 0.2996\n",
      "Epoch [11/50], Train Loss: 0.0992, Val Loss: 0.2955\n",
      "Epoch [12/50], Train Loss: 0.0974, Val Loss: 0.2915\n",
      "Epoch [13/50], Train Loss: 0.0952, Val Loss: 0.2876\n",
      "Epoch [14/50], Train Loss: 0.0936, Val Loss: 0.2838\n",
      "Epoch [15/50], Train Loss: 0.0921, Val Loss: 0.2800\n",
      "Epoch [16/50], Train Loss: 0.0899, Val Loss: 0.2764\n",
      "Epoch [17/50], Train Loss: 0.0887, Val Loss: 0.2728\n",
      "Epoch [18/50], Train Loss: 0.0867, Val Loss: 0.2692\n",
      "Epoch [19/50], Train Loss: 0.0852, Val Loss: 0.2658\n",
      "Epoch [20/50], Train Loss: 0.0840, Val Loss: 0.2624\n",
      "Epoch [21/50], Train Loss: 0.0826, Val Loss: 0.2591\n",
      "Epoch [22/50], Train Loss: 0.0807, Val Loss: 0.2559\n",
      "Epoch [23/50], Train Loss: 0.0796, Val Loss: 0.2527\n",
      "Epoch [24/50], Train Loss: 0.0777, Val Loss: 0.2496\n",
      "Epoch [25/50], Train Loss: 0.0769, Val Loss: 0.2465\n",
      "Epoch [26/50], Train Loss: 0.0755, Val Loss: 0.2435\n",
      "Epoch [27/50], Train Loss: 0.0745, Val Loss: 0.2406\n",
      "Epoch [28/50], Train Loss: 0.0732, Val Loss: 0.2377\n",
      "Epoch [29/50], Train Loss: 0.0718, Val Loss: 0.2349\n",
      "Epoch [30/50], Train Loss: 0.0710, Val Loss: 0.2322\n",
      "Epoch [31/50], Train Loss: 0.0696, Val Loss: 0.2294\n",
      "Epoch [32/50], Train Loss: 0.0686, Val Loss: 0.2268\n",
      "Epoch [33/50], Train Loss: 0.0675, Val Loss: 0.2242\n",
      "Epoch [34/50], Train Loss: 0.0666, Val Loss: 0.2216\n",
      "Epoch [35/50], Train Loss: 0.0657, Val Loss: 0.2191\n",
      "Epoch [36/50], Train Loss: 0.0648, Val Loss: 0.2167\n",
      "Epoch [37/50], Train Loss: 0.0632, Val Loss: 0.2143\n",
      "Epoch [38/50], Train Loss: 0.0630, Val Loss: 0.2119\n",
      "Epoch [39/50], Train Loss: 0.0619, Val Loss: 0.2096\n",
      "Epoch [40/50], Train Loss: 0.0611, Val Loss: 0.2074\n",
      "Epoch [41/50], Train Loss: 0.0605, Val Loss: 0.2051\n",
      "Epoch [42/50], Train Loss: 0.0595, Val Loss: 0.2030\n",
      "Epoch [43/50], Train Loss: 0.0590, Val Loss: 0.2008\n",
      "Epoch [44/50], Train Loss: 0.0582, Val Loss: 0.1987\n",
      "Epoch [45/50], Train Loss: 0.0575, Val Loss: 0.1967\n",
      "Epoch [46/50], Train Loss: 0.0565, Val Loss: 0.1947\n",
      "Epoch [47/50], Train Loss: 0.0555, Val Loss: 0.1927\n",
      "Epoch [48/50], Train Loss: 0.0549, Val Loss: 0.1908\n",
      "Epoch [49/50], Train Loss: 0.0545, Val Loss: 0.1889\n",
      "Epoch [50/50], Train Loss: 0.0541, Val Loss: 0.1870\n",
      "Testing parameters: lr=0.0005, optimizer=sgd, hidden_size=128, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1118, Val Loss: 0.3118\n",
      "Epoch [2/50], Train Loss: 0.1098, Val Loss: 0.3072\n",
      "Epoch [3/50], Train Loss: 0.1071, Val Loss: 0.3028\n",
      "Epoch [4/50], Train Loss: 0.1050, Val Loss: 0.2984\n",
      "Epoch [5/50], Train Loss: 0.1039, Val Loss: 0.2941\n",
      "Epoch [6/50], Train Loss: 0.1012, Val Loss: 0.2899\n",
      "Epoch [7/50], Train Loss: 0.0991, Val Loss: 0.2858\n",
      "Epoch [8/50], Train Loss: 0.0976, Val Loss: 0.2818\n",
      "Epoch [9/50], Train Loss: 0.0948, Val Loss: 0.2779\n",
      "Epoch [10/50], Train Loss: 0.0941, Val Loss: 0.2741\n",
      "Epoch [11/50], Train Loss: 0.0918, Val Loss: 0.2703\n",
      "Epoch [12/50], Train Loss: 0.0895, Val Loss: 0.2667\n",
      "Epoch [13/50], Train Loss: 0.0887, Val Loss: 0.2631\n",
      "Epoch [14/50], Train Loss: 0.0862, Val Loss: 0.2596\n",
      "Epoch [15/50], Train Loss: 0.0847, Val Loss: 0.2561\n",
      "Epoch [16/50], Train Loss: 0.0836, Val Loss: 0.2528\n",
      "Epoch [17/50], Train Loss: 0.0825, Val Loss: 0.2495\n",
      "Epoch [18/50], Train Loss: 0.0807, Val Loss: 0.2463\n",
      "Epoch [19/50], Train Loss: 0.0799, Val Loss: 0.2431\n",
      "Epoch [20/50], Train Loss: 0.0782, Val Loss: 0.2400\n",
      "Epoch [21/50], Train Loss: 0.0772, Val Loss: 0.2370\n",
      "Epoch [22/50], Train Loss: 0.0764, Val Loss: 0.2340\n",
      "Epoch [23/50], Train Loss: 0.0738, Val Loss: 0.2311\n",
      "Epoch [24/50], Train Loss: 0.0735, Val Loss: 0.2283\n",
      "Epoch [25/50], Train Loss: 0.0714, Val Loss: 0.2255\n",
      "Epoch [26/50], Train Loss: 0.0713, Val Loss: 0.2227\n",
      "Epoch [27/50], Train Loss: 0.0703, Val Loss: 0.2201\n",
      "Epoch [28/50], Train Loss: 0.0695, Val Loss: 0.2174\n",
      "Epoch [29/50], Train Loss: 0.0680, Val Loss: 0.2149\n",
      "Epoch [30/50], Train Loss: 0.0675, Val Loss: 0.2124\n",
      "Epoch [31/50], Train Loss: 0.0655, Val Loss: 0.2099\n",
      "Epoch [32/50], Train Loss: 0.0649, Val Loss: 0.2075\n",
      "Epoch [33/50], Train Loss: 0.0637, Val Loss: 0.2052\n",
      "Epoch [34/50], Train Loss: 0.0627, Val Loss: 0.2029\n",
      "Epoch [35/50], Train Loss: 0.0618, Val Loss: 0.2006\n",
      "Epoch [36/50], Train Loss: 0.0620, Val Loss: 0.1984\n",
      "Epoch [37/50], Train Loss: 0.0609, Val Loss: 0.1963\n",
      "Epoch [38/50], Train Loss: 0.0596, Val Loss: 0.1941\n",
      "Epoch [39/50], Train Loss: 0.0585, Val Loss: 0.1921\n",
      "Epoch [40/50], Train Loss: 0.0582, Val Loss: 0.1901\n",
      "Epoch [41/50], Train Loss: 0.0577, Val Loss: 0.1880\n",
      "Epoch [42/50], Train Loss: 0.0571, Val Loss: 0.1861\n",
      "Epoch [43/50], Train Loss: 0.0568, Val Loss: 0.1842\n",
      "Epoch [44/50], Train Loss: 0.0563, Val Loss: 0.1824\n",
      "Epoch [45/50], Train Loss: 0.0552, Val Loss: 0.1805\n",
      "Epoch [46/50], Train Loss: 0.0542, Val Loss: 0.1788\n",
      "Epoch [47/50], Train Loss: 0.0549, Val Loss: 0.1770\n",
      "Epoch [48/50], Train Loss: 0.0533, Val Loss: 0.1753\n",
      "Epoch [49/50], Train Loss: 0.0534, Val Loss: 0.1736\n",
      "Epoch [50/50], Train Loss: 0.0518, Val Loss: 0.1720\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=16, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1405, Val Loss: 0.2757\n",
      "Epoch [2/50], Train Loss: 0.0883, Val Loss: 0.1853\n",
      "Epoch [3/50], Train Loss: 0.0571, Val Loss: 0.1268\n",
      "Epoch [4/50], Train Loss: 0.0421, Val Loss: 0.0960\n",
      "Epoch [5/50], Train Loss: 0.0367, Val Loss: 0.0818\n",
      "Epoch [6/50], Train Loss: 0.0344, Val Loss: 0.0740\n",
      "Epoch [7/50], Train Loss: 0.0328, Val Loss: 0.0683\n",
      "Epoch [8/50], Train Loss: 0.0314, Val Loss: 0.0637\n",
      "Epoch [9/50], Train Loss: 0.0300, Val Loss: 0.0596\n",
      "Epoch [10/50], Train Loss: 0.0286, Val Loss: 0.0557\n",
      "Epoch [11/50], Train Loss: 0.0272, Val Loss: 0.0520\n",
      "Epoch [12/50], Train Loss: 0.0257, Val Loss: 0.0483\n",
      "Epoch [13/50], Train Loss: 0.0242, Val Loss: 0.0445\n",
      "Epoch [14/50], Train Loss: 0.0227, Val Loss: 0.0407\n",
      "Epoch [15/50], Train Loss: 0.0211, Val Loss: 0.0369\n",
      "Epoch [16/50], Train Loss: 0.0195, Val Loss: 0.0330\n",
      "Epoch [17/50], Train Loss: 0.0179, Val Loss: 0.0291\n",
      "Epoch [18/50], Train Loss: 0.0163, Val Loss: 0.0253\n",
      "Epoch [19/50], Train Loss: 0.0147, Val Loss: 0.0216\n",
      "Epoch [20/50], Train Loss: 0.0131, Val Loss: 0.0181\n",
      "Epoch [21/50], Train Loss: 0.0115, Val Loss: 0.0149\n",
      "Epoch [22/50], Train Loss: 0.0101, Val Loss: 0.0121\n",
      "Epoch [23/50], Train Loss: 0.0088, Val Loss: 0.0099\n",
      "Epoch [24/50], Train Loss: 0.0076, Val Loss: 0.0081\n",
      "Epoch [25/50], Train Loss: 0.0066, Val Loss: 0.0068\n",
      "Epoch [26/50], Train Loss: 0.0058, Val Loss: 0.0059\n",
      "Epoch [27/50], Train Loss: 0.0051, Val Loss: 0.0052\n",
      "Epoch [28/50], Train Loss: 0.0046, Val Loss: 0.0047\n",
      "Epoch [29/50], Train Loss: 0.0041, Val Loss: 0.0043\n",
      "Epoch [30/50], Train Loss: 0.0036, Val Loss: 0.0039\n",
      "Epoch [31/50], Train Loss: 0.0033, Val Loss: 0.0035\n",
      "Epoch [32/50], Train Loss: 0.0030, Val Loss: 0.0032\n",
      "Epoch [33/50], Train Loss: 0.0027, Val Loss: 0.0030\n",
      "Epoch [34/50], Train Loss: 0.0025, Val Loss: 0.0027\n",
      "Epoch [35/50], Train Loss: 0.0024, Val Loss: 0.0026\n",
      "Epoch [36/50], Train Loss: 0.0023, Val Loss: 0.0025\n",
      "Epoch [37/50], Train Loss: 0.0022, Val Loss: 0.0024\n",
      "Epoch [38/50], Train Loss: 0.0021, Val Loss: 0.0024\n",
      "Epoch [39/50], Train Loss: 0.0021, Val Loss: 0.0023\n",
      "Epoch [40/50], Train Loss: 0.0020, Val Loss: 0.0023\n",
      "Epoch [41/50], Train Loss: 0.0020, Val Loss: 0.0023\n",
      "Epoch [42/50], Train Loss: 0.0020, Val Loss: 0.0023\n",
      "Epoch [43/50], Train Loss: 0.0019, Val Loss: 0.0023\n",
      "Epoch [44/50], Train Loss: 0.0019, Val Loss: 0.0023\n",
      "Epoch [45/50], Train Loss: 0.0019, Val Loss: 0.0024\n",
      "Epoch [46/50], Train Loss: 0.0019, Val Loss: 0.0024\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=16, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1760, Val Loss: 0.3958\n",
      "Epoch [2/50], Train Loss: 0.1183, Val Loss: 0.2849\n",
      "Epoch [3/50], Train Loss: 0.0773, Val Loss: 0.1952\n",
      "Epoch [4/50], Train Loss: 0.0539, Val Loss: 0.1320\n",
      "Epoch [5/50], Train Loss: 0.0424, Val Loss: 0.0959\n",
      "Epoch [6/50], Train Loss: 0.0389, Val Loss: 0.0790\n",
      "Epoch [7/50], Train Loss: 0.0392, Val Loss: 0.0705\n",
      "Epoch [8/50], Train Loss: 0.0374, Val Loss: 0.0647\n",
      "Epoch [9/50], Train Loss: 0.0334, Val Loss: 0.0603\n",
      "Epoch [10/50], Train Loss: 0.0334, Val Loss: 0.0549\n",
      "Epoch [11/50], Train Loss: 0.0321, Val Loss: 0.0508\n",
      "Epoch [12/50], Train Loss: 0.0306, Val Loss: 0.0461\n",
      "Epoch [13/50], Train Loss: 0.0278, Val Loss: 0.0421\n",
      "Epoch [14/50], Train Loss: 0.0264, Val Loss: 0.0374\n",
      "Epoch [15/50], Train Loss: 0.0254, Val Loss: 0.0339\n",
      "Epoch [16/50], Train Loss: 0.0231, Val Loss: 0.0317\n",
      "Epoch [17/50], Train Loss: 0.0222, Val Loss: 0.0278\n",
      "Epoch [18/50], Train Loss: 0.0206, Val Loss: 0.0236\n",
      "Epoch [19/50], Train Loss: 0.0200, Val Loss: 0.0214\n",
      "Epoch [20/50], Train Loss: 0.0181, Val Loss: 0.0189\n",
      "Epoch [21/50], Train Loss: 0.0171, Val Loss: 0.0161\n",
      "Epoch [22/50], Train Loss: 0.0157, Val Loss: 0.0145\n",
      "Epoch [23/50], Train Loss: 0.0130, Val Loss: 0.0128\n",
      "Epoch [24/50], Train Loss: 0.0127, Val Loss: 0.0116\n",
      "Epoch [25/50], Train Loss: 0.0123, Val Loss: 0.0095\n",
      "Epoch [26/50], Train Loss: 0.0108, Val Loss: 0.0096\n",
      "Epoch [27/50], Train Loss: 0.0121, Val Loss: 0.0076\n",
      "Epoch [28/50], Train Loss: 0.0106, Val Loss: 0.0072\n",
      "Epoch [29/50], Train Loss: 0.0101, Val Loss: 0.0073\n",
      "Epoch [30/50], Train Loss: 0.0096, Val Loss: 0.0066\n",
      "Epoch [31/50], Train Loss: 0.0103, Val Loss: 0.0065\n",
      "Epoch [32/50], Train Loss: 0.0098, Val Loss: 0.0057\n",
      "Epoch [33/50], Train Loss: 0.0096, Val Loss: 0.0053\n",
      "Epoch [34/50], Train Loss: 0.0084, Val Loss: 0.0060\n",
      "Epoch [35/50], Train Loss: 0.0092, Val Loss: 0.0049\n",
      "Epoch [36/50], Train Loss: 0.0090, Val Loss: 0.0043\n",
      "Epoch [37/50], Train Loss: 0.0090, Val Loss: 0.0048\n",
      "Epoch [38/50], Train Loss: 0.0088, Val Loss: 0.0049\n",
      "Epoch [39/50], Train Loss: 0.0086, Val Loss: 0.0043\n",
      "Epoch [40/50], Train Loss: 0.0091, Val Loss: 0.0041\n",
      "Epoch [41/50], Train Loss: 0.0085, Val Loss: 0.0046\n",
      "Epoch [42/50], Train Loss: 0.0089, Val Loss: 0.0040\n",
      "Epoch [43/50], Train Loss: 0.0082, Val Loss: 0.0047\n",
      "Epoch [44/50], Train Loss: 0.0082, Val Loss: 0.0041\n",
      "Epoch [45/50], Train Loss: 0.0080, Val Loss: 0.0039\n",
      "Epoch [46/50], Train Loss: 0.0078, Val Loss: 0.0045\n",
      "Epoch [47/50], Train Loss: 0.0078, Val Loss: 0.0048\n",
      "Epoch [48/50], Train Loss: 0.0078, Val Loss: 0.0045\n",
      "Epoch [49/50], Train Loss: 0.0076, Val Loss: 0.0039\n",
      "Epoch [50/50], Train Loss: 0.0073, Val Loss: 0.0041\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=16, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1333, Val Loss: 0.1641\n",
      "Epoch [2/50], Train Loss: 0.1025, Val Loss: 0.1288\n",
      "Epoch [3/50], Train Loss: 0.0823, Val Loss: 0.0979\n",
      "Epoch [4/50], Train Loss: 0.0665, Val Loss: 0.0719\n",
      "Epoch [5/50], Train Loss: 0.0554, Val Loss: 0.0533\n",
      "Epoch [6/50], Train Loss: 0.0491, Val Loss: 0.0416\n",
      "Epoch [7/50], Train Loss: 0.0443, Val Loss: 0.0338\n",
      "Epoch [8/50], Train Loss: 0.0403, Val Loss: 0.0264\n",
      "Epoch [9/50], Train Loss: 0.0358, Val Loss: 0.0233\n",
      "Epoch [10/50], Train Loss: 0.0331, Val Loss: 0.0199\n",
      "Epoch [11/50], Train Loss: 0.0312, Val Loss: 0.0163\n",
      "Epoch [12/50], Train Loss: 0.0278, Val Loss: 0.0138\n",
      "Epoch [13/50], Train Loss: 0.0270, Val Loss: 0.0135\n",
      "Epoch [14/50], Train Loss: 0.0262, Val Loss: 0.0145\n",
      "Epoch [15/50], Train Loss: 0.0240, Val Loss: 0.0130\n",
      "Epoch [16/50], Train Loss: 0.0242, Val Loss: 0.0141\n",
      "Epoch [17/50], Train Loss: 0.0220, Val Loss: 0.0149\n",
      "Epoch [18/50], Train Loss: 0.0216, Val Loss: 0.0146\n",
      "Epoch [19/50], Train Loss: 0.0204, Val Loss: 0.0152\n",
      "Epoch [20/50], Train Loss: 0.0201, Val Loss: 0.0151\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=16, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.2145, Val Loss: 0.3883\n",
      "Epoch [2/50], Train Loss: 0.1326, Val Loss: 0.2653\n",
      "Epoch [3/50], Train Loss: 0.0690, Val Loss: 0.1416\n",
      "Epoch [4/50], Train Loss: 0.0335, Val Loss: 0.0793\n",
      "Epoch [5/50], Train Loss: 0.0294, Val Loss: 0.0681\n",
      "Epoch [6/50], Train Loss: 0.0287, Val Loss: 0.0644\n",
      "Epoch [7/50], Train Loss: 0.0276, Val Loss: 0.0615\n",
      "Epoch [8/50], Train Loss: 0.0263, Val Loss: 0.0584\n",
      "Epoch [9/50], Train Loss: 0.0250, Val Loss: 0.0552\n",
      "Epoch [10/50], Train Loss: 0.0235, Val Loss: 0.0518\n",
      "Epoch [11/50], Train Loss: 0.0220, Val Loss: 0.0484\n",
      "Epoch [12/50], Train Loss: 0.0204, Val Loss: 0.0448\n",
      "Epoch [13/50], Train Loss: 0.0187, Val Loss: 0.0412\n",
      "Epoch [14/50], Train Loss: 0.0170, Val Loss: 0.0375\n",
      "Epoch [15/50], Train Loss: 0.0152, Val Loss: 0.0338\n",
      "Epoch [16/50], Train Loss: 0.0133, Val Loss: 0.0301\n",
      "Epoch [17/50], Train Loss: 0.0113, Val Loss: 0.0264\n",
      "Epoch [18/50], Train Loss: 0.0094, Val Loss: 0.0225\n",
      "Epoch [19/50], Train Loss: 0.0074, Val Loss: 0.0185\n",
      "Epoch [20/50], Train Loss: 0.0055, Val Loss: 0.0147\n",
      "Epoch [21/50], Train Loss: 0.0040, Val Loss: 0.0115\n",
      "Epoch [22/50], Train Loss: 0.0030, Val Loss: 0.0096\n",
      "Epoch [23/50], Train Loss: 0.0025, Val Loss: 0.0090\n",
      "Epoch [24/50], Train Loss: 0.0024, Val Loss: 0.0088\n",
      "Epoch [25/50], Train Loss: 0.0023, Val Loss: 0.0088\n",
      "Epoch [26/50], Train Loss: 0.0023, Val Loss: 0.0088\n",
      "Epoch [27/50], Train Loss: 0.0022, Val Loss: 0.0087\n",
      "Epoch [28/50], Train Loss: 0.0022, Val Loss: 0.0086\n",
      "Epoch [29/50], Train Loss: 0.0022, Val Loss: 0.0085\n",
      "Epoch [30/50], Train Loss: 0.0022, Val Loss: 0.0084\n",
      "Epoch [31/50], Train Loss: 0.0021, Val Loss: 0.0083\n",
      "Epoch [32/50], Train Loss: 0.0021, Val Loss: 0.0082\n",
      "Epoch [33/50], Train Loss: 0.0021, Val Loss: 0.0081\n",
      "Epoch [34/50], Train Loss: 0.0021, Val Loss: 0.0081\n",
      "Epoch [35/50], Train Loss: 0.0021, Val Loss: 0.0080\n",
      "Epoch [36/50], Train Loss: 0.0021, Val Loss: 0.0079\n",
      "Epoch [37/50], Train Loss: 0.0021, Val Loss: 0.0078\n",
      "Epoch [38/50], Train Loss: 0.0021, Val Loss: 0.0077\n",
      "Epoch [39/50], Train Loss: 0.0020, Val Loss: 0.0076\n",
      "Epoch [40/50], Train Loss: 0.0020, Val Loss: 0.0076\n",
      "Epoch [41/50], Train Loss: 0.0020, Val Loss: 0.0075\n",
      "Epoch [42/50], Train Loss: 0.0020, Val Loss: 0.0074\n",
      "Epoch [43/50], Train Loss: 0.0020, Val Loss: 0.0073\n",
      "Epoch [44/50], Train Loss: 0.0020, Val Loss: 0.0073\n",
      "Epoch [45/50], Train Loss: 0.0020, Val Loss: 0.0072\n",
      "Epoch [46/50], Train Loss: 0.0020, Val Loss: 0.0071\n",
      "Epoch [47/50], Train Loss: 0.0020, Val Loss: 0.0071\n",
      "Epoch [48/50], Train Loss: 0.0020, Val Loss: 0.0070\n",
      "Epoch [49/50], Train Loss: 0.0019, Val Loss: 0.0069\n",
      "Epoch [50/50], Train Loss: 0.0019, Val Loss: 0.0069\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=16, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0782, Val Loss: 0.1964\n",
      "Epoch [2/50], Train Loss: 0.0513, Val Loss: 0.1378\n",
      "Epoch [3/50], Train Loss: 0.0388, Val Loss: 0.0972\n",
      "Epoch [4/50], Train Loss: 0.0319, Val Loss: 0.0730\n",
      "Epoch [5/50], Train Loss: 0.0291, Val Loss: 0.0589\n",
      "Epoch [6/50], Train Loss: 0.0248, Val Loss: 0.0477\n",
      "Epoch [7/50], Train Loss: 0.0213, Val Loss: 0.0387\n",
      "Epoch [8/50], Train Loss: 0.0188, Val Loss: 0.0314\n",
      "Epoch [9/50], Train Loss: 0.0172, Val Loss: 0.0260\n",
      "Epoch [10/50], Train Loss: 0.0162, Val Loss: 0.0224\n",
      "Epoch [11/50], Train Loss: 0.0146, Val Loss: 0.0193\n",
      "Epoch [12/50], Train Loss: 0.0121, Val Loss: 0.0158\n",
      "Epoch [13/50], Train Loss: 0.0113, Val Loss: 0.0130\n",
      "Epoch [14/50], Train Loss: 0.0091, Val Loss: 0.0104\n",
      "Epoch [15/50], Train Loss: 0.0080, Val Loss: 0.0076\n",
      "Epoch [16/50], Train Loss: 0.0079, Val Loss: 0.0060\n",
      "Epoch [17/50], Train Loss: 0.0074, Val Loss: 0.0058\n",
      "Epoch [18/50], Train Loss: 0.0071, Val Loss: 0.0055\n",
      "Epoch [19/50], Train Loss: 0.0074, Val Loss: 0.0054\n",
      "Epoch [20/50], Train Loss: 0.0070, Val Loss: 0.0042\n",
      "Epoch [21/50], Train Loss: 0.0068, Val Loss: 0.0045\n",
      "Epoch [22/50], Train Loss: 0.0071, Val Loss: 0.0044\n",
      "Epoch [23/50], Train Loss: 0.0065, Val Loss: 0.0050\n",
      "Epoch [24/50], Train Loss: 0.0063, Val Loss: 0.0043\n",
      "Epoch [25/50], Train Loss: 0.0067, Val Loss: 0.0045\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=16, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1845, Val Loss: 0.3844\n",
      "Epoch [2/50], Train Loss: 0.1152, Val Loss: 0.2781\n",
      "Epoch [3/50], Train Loss: 0.0809, Val Loss: 0.2006\n",
      "Epoch [4/50], Train Loss: 0.0658, Val Loss: 0.1496\n",
      "Epoch [5/50], Train Loss: 0.0602, Val Loss: 0.1180\n",
      "Epoch [6/50], Train Loss: 0.0514, Val Loss: 0.0969\n",
      "Epoch [7/50], Train Loss: 0.0502, Val Loss: 0.0846\n",
      "Epoch [8/50], Train Loss: 0.0449, Val Loss: 0.0665\n",
      "Epoch [9/50], Train Loss: 0.0415, Val Loss: 0.0566\n",
      "Epoch [10/50], Train Loss: 0.0386, Val Loss: 0.0457\n",
      "Epoch [11/50], Train Loss: 0.0358, Val Loss: 0.0337\n",
      "Epoch [12/50], Train Loss: 0.0330, Val Loss: 0.0242\n",
      "Epoch [13/50], Train Loss: 0.0305, Val Loss: 0.0211\n",
      "Epoch [14/50], Train Loss: 0.0288, Val Loss: 0.0139\n",
      "Epoch [15/50], Train Loss: 0.0280, Val Loss: 0.0168\n",
      "Epoch [16/50], Train Loss: 0.0269, Val Loss: 0.0133\n",
      "Epoch [17/50], Train Loss: 0.0241, Val Loss: 0.0112\n",
      "Epoch [18/50], Train Loss: 0.0238, Val Loss: 0.0116\n",
      "Epoch [19/50], Train Loss: 0.0205, Val Loss: 0.0095\n",
      "Epoch [20/50], Train Loss: 0.0209, Val Loss: 0.0103\n",
      "Epoch [21/50], Train Loss: 0.0199, Val Loss: 0.0075\n",
      "Epoch [22/50], Train Loss: 0.0184, Val Loss: 0.0097\n",
      "Epoch [23/50], Train Loss: 0.0192, Val Loss: 0.0092\n",
      "Epoch [24/50], Train Loss: 0.0174, Val Loss: 0.0075\n",
      "Epoch [25/50], Train Loss: 0.0181, Val Loss: 0.0072\n",
      "Epoch [26/50], Train Loss: 0.0172, Val Loss: 0.0067\n",
      "Epoch [27/50], Train Loss: 0.0160, Val Loss: 0.0078\n",
      "Epoch [28/50], Train Loss: 0.0165, Val Loss: 0.0065\n",
      "Epoch [29/50], Train Loss: 0.0170, Val Loss: 0.0097\n",
      "Epoch [30/50], Train Loss: 0.0159, Val Loss: 0.0066\n",
      "Epoch [31/50], Train Loss: 0.0155, Val Loss: 0.0065\n",
      "Epoch [32/50], Train Loss: 0.0155, Val Loss: 0.0082\n",
      "Epoch [33/50], Train Loss: 0.0148, Val Loss: 0.0070\n",
      "Epoch [34/50], Train Loss: 0.0142, Val Loss: 0.0066\n",
      "Epoch [35/50], Train Loss: 0.0155, Val Loss: 0.0077\n",
      "Epoch [36/50], Train Loss: 0.0139, Val Loss: 0.0068\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=16, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0599, Val Loss: 0.1578\n",
      "Epoch [2/50], Train Loss: 0.0394, Val Loss: 0.1169\n",
      "Epoch [3/50], Train Loss: 0.0363, Val Loss: 0.0969\n",
      "Epoch [4/50], Train Loss: 0.0339, Val Loss: 0.0816\n",
      "Epoch [5/50], Train Loss: 0.0305, Val Loss: 0.0650\n",
      "Epoch [6/50], Train Loss: 0.0254, Val Loss: 0.0456\n",
      "Epoch [7/50], Train Loss: 0.0189, Val Loss: 0.0277\n",
      "Epoch [8/50], Train Loss: 0.0129, Val Loss: 0.0168\n",
      "Epoch [9/50], Train Loss: 0.0089, Val Loss: 0.0125\n",
      "Epoch [10/50], Train Loss: 0.0058, Val Loss: 0.0092\n",
      "Epoch [11/50], Train Loss: 0.0039, Val Loss: 0.0070\n",
      "Epoch [12/50], Train Loss: 0.0031, Val Loss: 0.0066\n",
      "Epoch [13/50], Train Loss: 0.0028, Val Loss: 0.0067\n",
      "Epoch [14/50], Train Loss: 0.0026, Val Loss: 0.0062\n",
      "Epoch [15/50], Train Loss: 0.0025, Val Loss: 0.0061\n",
      "Epoch [16/50], Train Loss: 0.0024, Val Loss: 0.0059\n",
      "Epoch [17/50], Train Loss: 0.0024, Val Loss: 0.0059\n",
      "Epoch [18/50], Train Loss: 0.0024, Val Loss: 0.0059\n",
      "Epoch [19/50], Train Loss: 0.0023, Val Loss: 0.0056\n",
      "Epoch [20/50], Train Loss: 0.0023, Val Loss: 0.0054\n",
      "Epoch [21/50], Train Loss: 0.0023, Val Loss: 0.0057\n",
      "Epoch [22/50], Train Loss: 0.0024, Val Loss: 0.0065\n",
      "Epoch [23/50], Train Loss: 0.0023, Val Loss: 0.0047\n",
      "Epoch [24/50], Train Loss: 0.0024, Val Loss: 0.0048\n",
      "Epoch [25/50], Train Loss: 0.0025, Val Loss: 0.0077\n",
      "Epoch [26/50], Train Loss: 0.0024, Val Loss: 0.0057\n",
      "Epoch [27/50], Train Loss: 0.0026, Val Loss: 0.0038\n",
      "Epoch [28/50], Train Loss: 0.0026, Val Loss: 0.0050\n",
      "Epoch [29/50], Train Loss: 0.0032, Val Loss: 0.0119\n",
      "Epoch [30/50], Train Loss: 0.0030, Val Loss: 0.0042\n",
      "Epoch [31/50], Train Loss: 0.0037, Val Loss: 0.0028\n",
      "Epoch [32/50], Train Loss: 0.0036, Val Loss: 0.0126\n",
      "Epoch [33/50], Train Loss: 0.0032, Val Loss: 0.0070\n",
      "Epoch [34/50], Train Loss: 0.0040, Val Loss: 0.0034\n",
      "Epoch [35/50], Train Loss: 0.0029, Val Loss: 0.0070\n",
      "Epoch [36/50], Train Loss: 0.0030, Val Loss: 0.0093\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=16, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0863, Val Loss: 0.1847\n",
      "Epoch [2/50], Train Loss: 0.0574, Val Loss: 0.1216\n",
      "Epoch [3/50], Train Loss: 0.0433, Val Loss: 0.0774\n",
      "Epoch [4/50], Train Loss: 0.0369, Val Loss: 0.0557\n",
      "Epoch [5/50], Train Loss: 0.0344, Val Loss: 0.0439\n",
      "Epoch [6/50], Train Loss: 0.0298, Val Loss: 0.0330\n",
      "Epoch [7/50], Train Loss: 0.0271, Val Loss: 0.0223\n",
      "Epoch [8/50], Train Loss: 0.0234, Val Loss: 0.0153\n",
      "Epoch [9/50], Train Loss: 0.0209, Val Loss: 0.0129\n",
      "Epoch [10/50], Train Loss: 0.0202, Val Loss: 0.0130\n",
      "Epoch [11/50], Train Loss: 0.0190, Val Loss: 0.0126\n",
      "Epoch [12/50], Train Loss: 0.0177, Val Loss: 0.0114\n",
      "Epoch [13/50], Train Loss: 0.0167, Val Loss: 0.0128\n",
      "Epoch [14/50], Train Loss: 0.0157, Val Loss: 0.0109\n",
      "Epoch [15/50], Train Loss: 0.0134, Val Loss: 0.0116\n",
      "Epoch [16/50], Train Loss: 0.0125, Val Loss: 0.0108\n",
      "Epoch [17/50], Train Loss: 0.0118, Val Loss: 0.0084\n",
      "Epoch [18/50], Train Loss: 0.0101, Val Loss: 0.0080\n",
      "Epoch [19/50], Train Loss: 0.0096, Val Loss: 0.0080\n",
      "Epoch [20/50], Train Loss: 0.0079, Val Loss: 0.0068\n",
      "Epoch [21/50], Train Loss: 0.0083, Val Loss: 0.0048\n",
      "Epoch [22/50], Train Loss: 0.0078, Val Loss: 0.0072\n",
      "Epoch [23/50], Train Loss: 0.0082, Val Loss: 0.0060\n",
      "Epoch [24/50], Train Loss: 0.0080, Val Loss: 0.0035\n",
      "Epoch [25/50], Train Loss: 0.0081, Val Loss: 0.0103\n",
      "Epoch [26/50], Train Loss: 0.0077, Val Loss: 0.0058\n",
      "Epoch [27/50], Train Loss: 0.0080, Val Loss: 0.0030\n",
      "Epoch [28/50], Train Loss: 0.0076, Val Loss: 0.0103\n",
      "Epoch [29/50], Train Loss: 0.0072, Val Loss: 0.0074\n",
      "Epoch [30/50], Train Loss: 0.0073, Val Loss: 0.0042\n",
      "Epoch [31/50], Train Loss: 0.0076, Val Loss: 0.0086\n",
      "Epoch [32/50], Train Loss: 0.0074, Val Loss: 0.0082\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=16, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1528, Val Loss: 0.2643\n",
      "Epoch [2/50], Train Loss: 0.0957, Val Loss: 0.1824\n",
      "Epoch [3/50], Train Loss: 0.0728, Val Loss: 0.1337\n",
      "Epoch [4/50], Train Loss: 0.0636, Val Loss: 0.1105\n",
      "Epoch [5/50], Train Loss: 0.0583, Val Loss: 0.0999\n",
      "Epoch [6/50], Train Loss: 0.0535, Val Loss: 0.0861\n",
      "Epoch [7/50], Train Loss: 0.0480, Val Loss: 0.0765\n",
      "Epoch [8/50], Train Loss: 0.0455, Val Loss: 0.0634\n",
      "Epoch [9/50], Train Loss: 0.0424, Val Loss: 0.0495\n",
      "Epoch [10/50], Train Loss: 0.0375, Val Loss: 0.0344\n",
      "Epoch [11/50], Train Loss: 0.0333, Val Loss: 0.0307\n",
      "Epoch [12/50], Train Loss: 0.0309, Val Loss: 0.0256\n",
      "Epoch [13/50], Train Loss: 0.0294, Val Loss: 0.0188\n",
      "Epoch [14/50], Train Loss: 0.0280, Val Loss: 0.0222\n",
      "Epoch [15/50], Train Loss: 0.0252, Val Loss: 0.0191\n",
      "Epoch [16/50], Train Loss: 0.0244, Val Loss: 0.0188\n",
      "Epoch [17/50], Train Loss: 0.0235, Val Loss: 0.0151\n",
      "Epoch [18/50], Train Loss: 0.0219, Val Loss: 0.0171\n",
      "Epoch [19/50], Train Loss: 0.0215, Val Loss: 0.0141\n",
      "Epoch [20/50], Train Loss: 0.0220, Val Loss: 0.0161\n",
      "Epoch [21/50], Train Loss: 0.0213, Val Loss: 0.0127\n",
      "Epoch [22/50], Train Loss: 0.0208, Val Loss: 0.0138\n",
      "Epoch [23/50], Train Loss: 0.0197, Val Loss: 0.0149\n",
      "Epoch [24/50], Train Loss: 0.0180, Val Loss: 0.0156\n",
      "Epoch [25/50], Train Loss: 0.0179, Val Loss: 0.0123\n",
      "Epoch [26/50], Train Loss: 0.0182, Val Loss: 0.0139\n",
      "Epoch [27/50], Train Loss: 0.0180, Val Loss: 0.0139\n",
      "Epoch [28/50], Train Loss: 0.0173, Val Loss: 0.0135\n",
      "Epoch [29/50], Train Loss: 0.0171, Val Loss: 0.0113\n",
      "Epoch [30/50], Train Loss: 0.0163, Val Loss: 0.0144\n",
      "Epoch [31/50], Train Loss: 0.0170, Val Loss: 0.0123\n",
      "Epoch [32/50], Train Loss: 0.0161, Val Loss: 0.0135\n",
      "Epoch [33/50], Train Loss: 0.0163, Val Loss: 0.0119\n",
      "Epoch [34/50], Train Loss: 0.0170, Val Loss: 0.0098\n",
      "Epoch [35/50], Train Loss: 0.0166, Val Loss: 0.0138\n",
      "Epoch [36/50], Train Loss: 0.0153, Val Loss: 0.0132\n",
      "Epoch [37/50], Train Loss: 0.0146, Val Loss: 0.0081\n",
      "Epoch [38/50], Train Loss: 0.0151, Val Loss: 0.0093\n",
      "Epoch [39/50], Train Loss: 0.0150, Val Loss: 0.0111\n",
      "Epoch [40/50], Train Loss: 0.0153, Val Loss: 0.0065\n",
      "Epoch [41/50], Train Loss: 0.0150, Val Loss: 0.0097\n",
      "Epoch [42/50], Train Loss: 0.0153, Val Loss: 0.0147\n",
      "Epoch [43/50], Train Loss: 0.0149, Val Loss: 0.0094\n",
      "Epoch [44/50], Train Loss: 0.0143, Val Loss: 0.0095\n",
      "Epoch [45/50], Train Loss: 0.0138, Val Loss: 0.0142\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=32, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0968, Val Loss: 0.1855\n",
      "Epoch [2/50], Train Loss: 0.0437, Val Loss: 0.0709\n",
      "Epoch [3/50], Train Loss: 0.0313, Val Loss: 0.0482\n",
      "Epoch [4/50], Train Loss: 0.0301, Val Loss: 0.0478\n",
      "Epoch [5/50], Train Loss: 0.0274, Val Loss: 0.0428\n",
      "Epoch [6/50], Train Loss: 0.0253, Val Loss: 0.0377\n",
      "Epoch [7/50], Train Loss: 0.0233, Val Loss: 0.0325\n",
      "Epoch [8/50], Train Loss: 0.0213, Val Loss: 0.0274\n",
      "Epoch [9/50], Train Loss: 0.0192, Val Loss: 0.0223\n",
      "Epoch [10/50], Train Loss: 0.0171, Val Loss: 0.0175\n",
      "Epoch [11/50], Train Loss: 0.0150, Val Loss: 0.0132\n",
      "Epoch [12/50], Train Loss: 0.0130, Val Loss: 0.0096\n",
      "Epoch [13/50], Train Loss: 0.0110, Val Loss: 0.0069\n",
      "Epoch [14/50], Train Loss: 0.0094, Val Loss: 0.0051\n",
      "Epoch [15/50], Train Loss: 0.0080, Val Loss: 0.0039\n",
      "Epoch [16/50], Train Loss: 0.0066, Val Loss: 0.0032\n",
      "Epoch [17/50], Train Loss: 0.0054, Val Loss: 0.0029\n",
      "Epoch [18/50], Train Loss: 0.0042, Val Loss: 0.0030\n",
      "Epoch [19/50], Train Loss: 0.0034, Val Loss: 0.0033\n",
      "Epoch [20/50], Train Loss: 0.0028, Val Loss: 0.0033\n",
      "Epoch [21/50], Train Loss: 0.0024, Val Loss: 0.0031\n",
      "Epoch [22/50], Train Loss: 0.0021, Val Loss: 0.0029\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=32, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1159, Val Loss: 0.2922\n",
      "Epoch [2/50], Train Loss: 0.0708, Val Loss: 0.1879\n",
      "Epoch [3/50], Train Loss: 0.0451, Val Loss: 0.1109\n",
      "Epoch [4/50], Train Loss: 0.0331, Val Loss: 0.0674\n",
      "Epoch [5/50], Train Loss: 0.0300, Val Loss: 0.0507\n",
      "Epoch [6/50], Train Loss: 0.0266, Val Loss: 0.0419\n",
      "Epoch [7/50], Train Loss: 0.0242, Val Loss: 0.0352\n",
      "Epoch [8/50], Train Loss: 0.0220, Val Loss: 0.0271\n",
      "Epoch [9/50], Train Loss: 0.0192, Val Loss: 0.0209\n",
      "Epoch [10/50], Train Loss: 0.0170, Val Loss: 0.0143\n",
      "Epoch [11/50], Train Loss: 0.0148, Val Loss: 0.0101\n",
      "Epoch [12/50], Train Loss: 0.0120, Val Loss: 0.0068\n",
      "Epoch [13/50], Train Loss: 0.0102, Val Loss: 0.0044\n",
      "Epoch [14/50], Train Loss: 0.0095, Val Loss: 0.0036\n",
      "Epoch [15/50], Train Loss: 0.0077, Val Loss: 0.0035\n",
      "Epoch [16/50], Train Loss: 0.0066, Val Loss: 0.0033\n",
      "Epoch [17/50], Train Loss: 0.0061, Val Loss: 0.0029\n",
      "Epoch [18/50], Train Loss: 0.0055, Val Loss: 0.0034\n",
      "Epoch [19/50], Train Loss: 0.0057, Val Loss: 0.0027\n",
      "Epoch [20/50], Train Loss: 0.0055, Val Loss: 0.0025\n",
      "Epoch [21/50], Train Loss: 0.0056, Val Loss: 0.0024\n",
      "Epoch [22/50], Train Loss: 0.0054, Val Loss: 0.0025\n",
      "Epoch [23/50], Train Loss: 0.0052, Val Loss: 0.0026\n",
      "Epoch [24/50], Train Loss: 0.0050, Val Loss: 0.0023\n",
      "Epoch [25/50], Train Loss: 0.0047, Val Loss: 0.0021\n",
      "Epoch [26/50], Train Loss: 0.0048, Val Loss: 0.0024\n",
      "Epoch [27/50], Train Loss: 0.0048, Val Loss: 0.0022\n",
      "Epoch [28/50], Train Loss: 0.0048, Val Loss: 0.0022\n",
      "Epoch [29/50], Train Loss: 0.0048, Val Loss: 0.0023\n",
      "Epoch [30/50], Train Loss: 0.0046, Val Loss: 0.0021\n",
      "Epoch [31/50], Train Loss: 0.0045, Val Loss: 0.0020\n",
      "Epoch [32/50], Train Loss: 0.0046, Val Loss: 0.0022\n",
      "Epoch [33/50], Train Loss: 0.0046, Val Loss: 0.0024\n",
      "Epoch [34/50], Train Loss: 0.0041, Val Loss: 0.0021\n",
      "Epoch [35/50], Train Loss: 0.0042, Val Loss: 0.0020\n",
      "Epoch [36/50], Train Loss: 0.0044, Val Loss: 0.0024\n",
      "Epoch [37/50], Train Loss: 0.0042, Val Loss: 0.0026\n",
      "Epoch [38/50], Train Loss: 0.0042, Val Loss: 0.0019\n",
      "Epoch [39/50], Train Loss: 0.0044, Val Loss: 0.0021\n",
      "Epoch [40/50], Train Loss: 0.0041, Val Loss: 0.0024\n",
      "Epoch [41/50], Train Loss: 0.0041, Val Loss: 0.0018\n",
      "Epoch [42/50], Train Loss: 0.0041, Val Loss: 0.0021\n",
      "Epoch [43/50], Train Loss: 0.0040, Val Loss: 0.0023\n",
      "Epoch [44/50], Train Loss: 0.0040, Val Loss: 0.0020\n",
      "Epoch [45/50], Train Loss: 0.0037, Val Loss: 0.0020\n",
      "Epoch [46/50], Train Loss: 0.0036, Val Loss: 0.0020\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=32, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0920, Val Loss: 0.1720\n",
      "Epoch [2/50], Train Loss: 0.0509, Val Loss: 0.0835\n",
      "Epoch [3/50], Train Loss: 0.0441, Val Loss: 0.0569\n",
      "Epoch [4/50], Train Loss: 0.0394, Val Loss: 0.0490\n",
      "Epoch [5/50], Train Loss: 0.0374, Val Loss: 0.0416\n",
      "Epoch [6/50], Train Loss: 0.0329, Val Loss: 0.0331\n",
      "Epoch [7/50], Train Loss: 0.0306, Val Loss: 0.0288\n",
      "Epoch [8/50], Train Loss: 0.0268, Val Loss: 0.0247\n",
      "Epoch [9/50], Train Loss: 0.0246, Val Loss: 0.0204\n",
      "Epoch [10/50], Train Loss: 0.0230, Val Loss: 0.0164\n",
      "Epoch [11/50], Train Loss: 0.0206, Val Loss: 0.0127\n",
      "Epoch [12/50], Train Loss: 0.0182, Val Loss: 0.0113\n",
      "Epoch [13/50], Train Loss: 0.0175, Val Loss: 0.0101\n",
      "Epoch [14/50], Train Loss: 0.0153, Val Loss: 0.0067\n",
      "Epoch [15/50], Train Loss: 0.0147, Val Loss: 0.0092\n",
      "Epoch [16/50], Train Loss: 0.0138, Val Loss: 0.0055\n",
      "Epoch [17/50], Train Loss: 0.0126, Val Loss: 0.0069\n",
      "Epoch [18/50], Train Loss: 0.0117, Val Loss: 0.0057\n",
      "Epoch [19/50], Train Loss: 0.0121, Val Loss: 0.0040\n",
      "Epoch [20/50], Train Loss: 0.0127, Val Loss: 0.0067\n",
      "Epoch [21/50], Train Loss: 0.0113, Val Loss: 0.0035\n",
      "Epoch [22/50], Train Loss: 0.0105, Val Loss: 0.0055\n",
      "Epoch [23/50], Train Loss: 0.0109, Val Loss: 0.0052\n",
      "Epoch [24/50], Train Loss: 0.0104, Val Loss: 0.0042\n",
      "Epoch [25/50], Train Loss: 0.0103, Val Loss: 0.0054\n",
      "Epoch [26/50], Train Loss: 0.0107, Val Loss: 0.0035\n",
      "Epoch [27/50], Train Loss: 0.0102, Val Loss: 0.0032\n",
      "Epoch [28/50], Train Loss: 0.0099, Val Loss: 0.0036\n",
      "Epoch [29/50], Train Loss: 0.0089, Val Loss: 0.0047\n",
      "Epoch [30/50], Train Loss: 0.0094, Val Loss: 0.0044\n",
      "Epoch [31/50], Train Loss: 0.0093, Val Loss: 0.0038\n",
      "Epoch [32/50], Train Loss: 0.0093, Val Loss: 0.0029\n",
      "Epoch [33/50], Train Loss: 0.0092, Val Loss: 0.0027\n",
      "Epoch [34/50], Train Loss: 0.0090, Val Loss: 0.0031\n",
      "Epoch [35/50], Train Loss: 0.0081, Val Loss: 0.0037\n",
      "Epoch [36/50], Train Loss: 0.0087, Val Loss: 0.0044\n",
      "Epoch [37/50], Train Loss: 0.0092, Val Loss: 0.0039\n",
      "Epoch [38/50], Train Loss: 0.0083, Val Loss: 0.0029\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=32, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0630, Val Loss: 0.1309\n",
      "Epoch [2/50], Train Loss: 0.0314, Val Loss: 0.0582\n",
      "Epoch [3/50], Train Loss: 0.0324, Val Loss: 0.0497\n",
      "Epoch [4/50], Train Loss: 0.0275, Val Loss: 0.0365\n",
      "Epoch [5/50], Train Loss: 0.0226, Val Loss: 0.0219\n",
      "Epoch [6/50], Train Loss: 0.0173, Val Loss: 0.0108\n",
      "Epoch [7/50], Train Loss: 0.0125, Val Loss: 0.0065\n",
      "Epoch [8/50], Train Loss: 0.0095, Val Loss: 0.0068\n",
      "Epoch [9/50], Train Loss: 0.0070, Val Loss: 0.0054\n",
      "Epoch [10/50], Train Loss: 0.0046, Val Loss: 0.0035\n",
      "Epoch [11/50], Train Loss: 0.0030, Val Loss: 0.0028\n",
      "Epoch [12/50], Train Loss: 0.0026, Val Loss: 0.0034\n",
      "Epoch [13/50], Train Loss: 0.0024, Val Loss: 0.0043\n",
      "Epoch [14/50], Train Loss: 0.0023, Val Loss: 0.0025\n",
      "Epoch [15/50], Train Loss: 0.0023, Val Loss: 0.0022\n",
      "Epoch [16/50], Train Loss: 0.0023, Val Loss: 0.0042\n",
      "Epoch [17/50], Train Loss: 0.0024, Val Loss: 0.0044\n",
      "Epoch [18/50], Train Loss: 0.0022, Val Loss: 0.0019\n",
      "Epoch [19/50], Train Loss: 0.0022, Val Loss: 0.0017\n",
      "Epoch [20/50], Train Loss: 0.0024, Val Loss: 0.0062\n",
      "Epoch [21/50], Train Loss: 0.0025, Val Loss: 0.0046\n",
      "Epoch [22/50], Train Loss: 0.0025, Val Loss: 0.0019\n",
      "Epoch [23/50], Train Loss: 0.0024, Val Loss: 0.0015\n",
      "Epoch [24/50], Train Loss: 0.0028, Val Loss: 0.0087\n",
      "Epoch [25/50], Train Loss: 0.0029, Val Loss: 0.0049\n",
      "Epoch [26/50], Train Loss: 0.0031, Val Loss: 0.0023\n",
      "Epoch [27/50], Train Loss: 0.0028, Val Loss: 0.0018\n",
      "Epoch [28/50], Train Loss: 0.0034, Val Loss: 0.0112\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=32, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1091, Val Loss: 0.2074\n",
      "Epoch [2/50], Train Loss: 0.0427, Val Loss: 0.0745\n",
      "Epoch [3/50], Train Loss: 0.0387, Val Loss: 0.0570\n",
      "Epoch [4/50], Train Loss: 0.0349, Val Loss: 0.0435\n",
      "Epoch [5/50], Train Loss: 0.0302, Val Loss: 0.0290\n",
      "Epoch [6/50], Train Loss: 0.0260, Val Loss: 0.0165\n",
      "Epoch [7/50], Train Loss: 0.0215, Val Loss: 0.0078\n",
      "Epoch [8/50], Train Loss: 0.0181, Val Loss: 0.0045\n",
      "Epoch [9/50], Train Loss: 0.0164, Val Loss: 0.0052\n",
      "Epoch [10/50], Train Loss: 0.0149, Val Loss: 0.0045\n",
      "Epoch [11/50], Train Loss: 0.0133, Val Loss: 0.0035\n",
      "Epoch [12/50], Train Loss: 0.0117, Val Loss: 0.0035\n",
      "Epoch [13/50], Train Loss: 0.0095, Val Loss: 0.0032\n",
      "Epoch [14/50], Train Loss: 0.0084, Val Loss: 0.0028\n",
      "Epoch [15/50], Train Loss: 0.0071, Val Loss: 0.0027\n",
      "Epoch [16/50], Train Loss: 0.0075, Val Loss: 0.0025\n",
      "Epoch [17/50], Train Loss: 0.0071, Val Loss: 0.0031\n",
      "Epoch [18/50], Train Loss: 0.0064, Val Loss: 0.0044\n",
      "Epoch [19/50], Train Loss: 0.0066, Val Loss: 0.0021\n",
      "Epoch [20/50], Train Loss: 0.0067, Val Loss: 0.0018\n",
      "Epoch [21/50], Train Loss: 0.0064, Val Loss: 0.0024\n",
      "Epoch [22/50], Train Loss: 0.0067, Val Loss: 0.0055\n",
      "Epoch [23/50], Train Loss: 0.0066, Val Loss: 0.0043\n",
      "Epoch [24/50], Train Loss: 0.0063, Val Loss: 0.0018\n",
      "Epoch [25/50], Train Loss: 0.0065, Val Loss: 0.0016\n",
      "Epoch [26/50], Train Loss: 0.0064, Val Loss: 0.0061\n",
      "Epoch [27/50], Train Loss: 0.0062, Val Loss: 0.0054\n",
      "Epoch [28/50], Train Loss: 0.0061, Val Loss: 0.0019\n",
      "Epoch [29/50], Train Loss: 0.0061, Val Loss: 0.0015\n",
      "Epoch [30/50], Train Loss: 0.0059, Val Loss: 0.0042\n",
      "Epoch [31/50], Train Loss: 0.0058, Val Loss: 0.0054\n",
      "Epoch [32/50], Train Loss: 0.0059, Val Loss: 0.0020\n",
      "Epoch [33/50], Train Loss: 0.0061, Val Loss: 0.0015\n",
      "Epoch [34/50], Train Loss: 0.0060, Val Loss: 0.0041\n",
      "Epoch [35/50], Train Loss: 0.0051, Val Loss: 0.0042\n",
      "Epoch [36/50], Train Loss: 0.0058, Val Loss: 0.0043\n",
      "Epoch [37/50], Train Loss: 0.0058, Val Loss: 0.0015\n",
      "Epoch [38/50], Train Loss: 0.0056, Val Loss: 0.0033\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=32, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1168, Val Loss: 0.1766\n",
      "Epoch [2/50], Train Loss: 0.0561, Val Loss: 0.0905\n",
      "Epoch [3/50], Train Loss: 0.0505, Val Loss: 0.0776\n",
      "Epoch [4/50], Train Loss: 0.0439, Val Loss: 0.0702\n",
      "Epoch [5/50], Train Loss: 0.0430, Val Loss: 0.0635\n",
      "Epoch [6/50], Train Loss: 0.0399, Val Loss: 0.0554\n",
      "Epoch [7/50], Train Loss: 0.0366, Val Loss: 0.0459\n",
      "Epoch [8/50], Train Loss: 0.0326, Val Loss: 0.0374\n",
      "Epoch [9/50], Train Loss: 0.0285, Val Loss: 0.0288\n",
      "Epoch [10/50], Train Loss: 0.0245, Val Loss: 0.0208\n",
      "Epoch [11/50], Train Loss: 0.0219, Val Loss: 0.0129\n",
      "Epoch [12/50], Train Loss: 0.0192, Val Loss: 0.0098\n",
      "Epoch [13/50], Train Loss: 0.0174, Val Loss: 0.0070\n",
      "Epoch [14/50], Train Loss: 0.0172, Val Loss: 0.0050\n",
      "Epoch [15/50], Train Loss: 0.0170, Val Loss: 0.0081\n",
      "Epoch [16/50], Train Loss: 0.0164, Val Loss: 0.0057\n",
      "Epoch [17/50], Train Loss: 0.0158, Val Loss: 0.0050\n",
      "Epoch [18/50], Train Loss: 0.0149, Val Loss: 0.0054\n",
      "Epoch [19/50], Train Loss: 0.0151, Val Loss: 0.0089\n",
      "Epoch [20/50], Train Loss: 0.0156, Val Loss: 0.0098\n",
      "Epoch [21/50], Train Loss: 0.0146, Val Loss: 0.0034\n",
      "Epoch [22/50], Train Loss: 0.0137, Val Loss: 0.0045\n",
      "Epoch [23/50], Train Loss: 0.0144, Val Loss: 0.0093\n",
      "Epoch [24/50], Train Loss: 0.0138, Val Loss: 0.0124\n",
      "Epoch [25/50], Train Loss: 0.0132, Val Loss: 0.0036\n",
      "Epoch [26/50], Train Loss: 0.0139, Val Loss: 0.0027\n",
      "Epoch [27/50], Train Loss: 0.0127, Val Loss: 0.0105\n",
      "Epoch [28/50], Train Loss: 0.0130, Val Loss: 0.0112\n",
      "Epoch [29/50], Train Loss: 0.0128, Val Loss: 0.0058\n",
      "Epoch [30/50], Train Loss: 0.0133, Val Loss: 0.0027\n",
      "Epoch [31/50], Train Loss: 0.0116, Val Loss: 0.0067\n",
      "Epoch [32/50], Train Loss: 0.0124, Val Loss: 0.0126\n",
      "Epoch [33/50], Train Loss: 0.0124, Val Loss: 0.0054\n",
      "Epoch [34/50], Train Loss: 0.0126, Val Loss: 0.0028\n",
      "Epoch [35/50], Train Loss: 0.0115, Val Loss: 0.0085\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=32, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0929, Val Loss: 0.1701\n",
      "Epoch [2/50], Train Loss: 0.0318, Val Loss: 0.0523\n",
      "Epoch [3/50], Train Loss: 0.0428, Val Loss: 0.0667\n",
      "Epoch [4/50], Train Loss: 0.0334, Val Loss: 0.0494\n",
      "Epoch [5/50], Train Loss: 0.0313, Val Loss: 0.0400\n",
      "Epoch [6/50], Train Loss: 0.0273, Val Loss: 0.0284\n",
      "Epoch [7/50], Train Loss: 0.0229, Val Loss: 0.0178\n",
      "Epoch [8/50], Train Loss: 0.0178, Val Loss: 0.0103\n",
      "Epoch [9/50], Train Loss: 0.0124, Val Loss: 0.0075\n",
      "Epoch [10/50], Train Loss: 0.0078, Val Loss: 0.0072\n",
      "Epoch [11/50], Train Loss: 0.0051, Val Loss: 0.0035\n",
      "Epoch [12/50], Train Loss: 0.0036, Val Loss: 0.0026\n",
      "Epoch [13/50], Train Loss: 0.0025, Val Loss: 0.0030\n",
      "Epoch [14/50], Train Loss: 0.0026, Val Loss: 0.0053\n",
      "Epoch [15/50], Train Loss: 0.0025, Val Loss: 0.0061\n",
      "Epoch [16/50], Train Loss: 0.0023, Val Loss: 0.0048\n",
      "Epoch [17/50], Train Loss: 0.0023, Val Loss: 0.0039\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=32, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0622, Val Loss: 0.1333\n",
      "Epoch [2/50], Train Loss: 0.0387, Val Loss: 0.0806\n",
      "Epoch [3/50], Train Loss: 0.0413, Val Loss: 0.0787\n",
      "Epoch [4/50], Train Loss: 0.0360, Val Loss: 0.0607\n",
      "Epoch [5/50], Train Loss: 0.0307, Val Loss: 0.0406\n",
      "Epoch [6/50], Train Loss: 0.0240, Val Loss: 0.0172\n",
      "Epoch [7/50], Train Loss: 0.0156, Val Loss: 0.0045\n",
      "Epoch [8/50], Train Loss: 0.0097, Val Loss: 0.0074\n",
      "Epoch [9/50], Train Loss: 0.0094, Val Loss: 0.0129\n",
      "Epoch [10/50], Train Loss: 0.0094, Val Loss: 0.0042\n",
      "Epoch [11/50], Train Loss: 0.0083, Val Loss: 0.0046\n",
      "Epoch [12/50], Train Loss: 0.0091, Val Loss: 0.0185\n",
      "Epoch [13/50], Train Loss: 0.0084, Val Loss: 0.0039\n",
      "Epoch [14/50], Train Loss: 0.0081, Val Loss: 0.0040\n",
      "Epoch [15/50], Train Loss: 0.0084, Val Loss: 0.0165\n",
      "Epoch [16/50], Train Loss: 0.0071, Val Loss: 0.0042\n",
      "Epoch [17/50], Train Loss: 0.0078, Val Loss: 0.0055\n",
      "Epoch [18/50], Train Loss: 0.0081, Val Loss: 0.0181\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=32, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1610, Val Loss: 0.2460\n",
      "Epoch [2/50], Train Loss: 0.0650, Val Loss: 0.0709\n",
      "Epoch [3/50], Train Loss: 0.0632, Val Loss: 0.0797\n",
      "Epoch [4/50], Train Loss: 0.0542, Val Loss: 0.0595\n",
      "Epoch [5/50], Train Loss: 0.0492, Val Loss: 0.0467\n",
      "Epoch [6/50], Train Loss: 0.0464, Val Loss: 0.0337\n",
      "Epoch [7/50], Train Loss: 0.0398, Val Loss: 0.0263\n",
      "Epoch [8/50], Train Loss: 0.0376, Val Loss: 0.0248\n",
      "Epoch [9/50], Train Loss: 0.0362, Val Loss: 0.0262\n",
      "Epoch [10/50], Train Loss: 0.0331, Val Loss: 0.0179\n",
      "Epoch [11/50], Train Loss: 0.0302, Val Loss: 0.0152\n",
      "Epoch [12/50], Train Loss: 0.0275, Val Loss: 0.0201\n",
      "Epoch [13/50], Train Loss: 0.0261, Val Loss: 0.0143\n",
      "Epoch [14/50], Train Loss: 0.0228, Val Loss: 0.0045\n",
      "Epoch [15/50], Train Loss: 0.0214, Val Loss: 0.0093\n",
      "Epoch [16/50], Train Loss: 0.0200, Val Loss: 0.0202\n",
      "Epoch [17/50], Train Loss: 0.0199, Val Loss: 0.0052\n",
      "Epoch [18/50], Train Loss: 0.0186, Val Loss: 0.0030\n",
      "Epoch [19/50], Train Loss: 0.0178, Val Loss: 0.0062\n",
      "Epoch [20/50], Train Loss: 0.0215, Val Loss: 0.0377\n",
      "Epoch [21/50], Train Loss: 0.0173, Val Loss: 0.0072\n",
      "Epoch [22/50], Train Loss: 0.0236, Val Loss: 0.0102\n",
      "Epoch [23/50], Train Loss: 0.0174, Val Loss: 0.0134\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=64, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0665, Val Loss: 0.1349\n",
      "Epoch [2/50], Train Loss: 0.0268, Val Loss: 0.0403\n",
      "Epoch [3/50], Train Loss: 0.0294, Val Loss: 0.0366\n",
      "Epoch [4/50], Train Loss: 0.0228, Val Loss: 0.0226\n",
      "Epoch [5/50], Train Loss: 0.0182, Val Loss: 0.0116\n",
      "Epoch [6/50], Train Loss: 0.0134, Val Loss: 0.0043\n",
      "Epoch [7/50], Train Loss: 0.0088, Val Loss: 0.0032\n",
      "Epoch [8/50], Train Loss: 0.0060, Val Loss: 0.0031\n",
      "Epoch [9/50], Train Loss: 0.0044, Val Loss: 0.0029\n",
      "Epoch [10/50], Train Loss: 0.0031, Val Loss: 0.0028\n",
      "Epoch [11/50], Train Loss: 0.0028, Val Loss: 0.0045\n",
      "Epoch [12/50], Train Loss: 0.0027, Val Loss: 0.0038\n",
      "Epoch [13/50], Train Loss: 0.0025, Val Loss: 0.0019\n",
      "Epoch [14/50], Train Loss: 0.0027, Val Loss: 0.0040\n",
      "Epoch [15/50], Train Loss: 0.0024, Val Loss: 0.0032\n",
      "Epoch [16/50], Train Loss: 0.0027, Val Loss: 0.0037\n",
      "Epoch [17/50], Train Loss: 0.0023, Val Loss: 0.0032\n",
      "Epoch [18/50], Train Loss: 0.0026, Val Loss: 0.0030\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=64, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0636, Val Loss: 0.1334\n",
      "Epoch [2/50], Train Loss: 0.0298, Val Loss: 0.0478\n",
      "Epoch [3/50], Train Loss: 0.0286, Val Loss: 0.0330\n",
      "Epoch [4/50], Train Loss: 0.0247, Val Loss: 0.0229\n",
      "Epoch [5/50], Train Loss: 0.0198, Val Loss: 0.0115\n",
      "Epoch [6/50], Train Loss: 0.0154, Val Loss: 0.0032\n",
      "Epoch [7/50], Train Loss: 0.0108, Val Loss: 0.0022\n",
      "Epoch [8/50], Train Loss: 0.0087, Val Loss: 0.0028\n",
      "Epoch [9/50], Train Loss: 0.0067, Val Loss: 0.0037\n",
      "Epoch [10/50], Train Loss: 0.0052, Val Loss: 0.0021\n",
      "Epoch [11/50], Train Loss: 0.0044, Val Loss: 0.0030\n",
      "Epoch [12/50], Train Loss: 0.0046, Val Loss: 0.0028\n",
      "Epoch [13/50], Train Loss: 0.0041, Val Loss: 0.0026\n",
      "Epoch [14/50], Train Loss: 0.0039, Val Loss: 0.0024\n",
      "Epoch [15/50], Train Loss: 0.0040, Val Loss: 0.0030\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=64, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0874, Val Loss: 0.1571\n",
      "Epoch [2/50], Train Loss: 0.0387, Val Loss: 0.0449\n",
      "Epoch [3/50], Train Loss: 0.0385, Val Loss: 0.0488\n",
      "Epoch [4/50], Train Loss: 0.0336, Val Loss: 0.0342\n",
      "Epoch [5/50], Train Loss: 0.0282, Val Loss: 0.0256\n",
      "Epoch [6/50], Train Loss: 0.0239, Val Loss: 0.0140\n",
      "Epoch [7/50], Train Loss: 0.0195, Val Loss: 0.0087\n",
      "Epoch [8/50], Train Loss: 0.0150, Val Loss: 0.0046\n",
      "Epoch [9/50], Train Loss: 0.0119, Val Loss: 0.0035\n",
      "Epoch [10/50], Train Loss: 0.0110, Val Loss: 0.0018\n",
      "Epoch [11/50], Train Loss: 0.0097, Val Loss: 0.0033\n",
      "Epoch [12/50], Train Loss: 0.0093, Val Loss: 0.0022\n",
      "Epoch [13/50], Train Loss: 0.0087, Val Loss: 0.0041\n",
      "Epoch [14/50], Train Loss: 0.0086, Val Loss: 0.0017\n",
      "Epoch [15/50], Train Loss: 0.0083, Val Loss: 0.0028\n",
      "Epoch [16/50], Train Loss: 0.0079, Val Loss: 0.0038\n",
      "Epoch [17/50], Train Loss: 0.0077, Val Loss: 0.0016\n",
      "Epoch [18/50], Train Loss: 0.0080, Val Loss: 0.0014\n",
      "Epoch [19/50], Train Loss: 0.0073, Val Loss: 0.0035\n",
      "Epoch [20/50], Train Loss: 0.0071, Val Loss: 0.0031\n",
      "Epoch [21/50], Train Loss: 0.0073, Val Loss: 0.0019\n",
      "Epoch [22/50], Train Loss: 0.0069, Val Loss: 0.0015\n",
      "Epoch [23/50], Train Loss: 0.0075, Val Loss: 0.0023\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=64, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0510, Val Loss: 0.0433\n",
      "Epoch [2/50], Train Loss: 0.0509, Val Loss: 0.0680\n",
      "Epoch [3/50], Train Loss: 0.0298, Val Loss: 0.0397\n",
      "Epoch [4/50], Train Loss: 0.0264, Val Loss: 0.0245\n",
      "Epoch [5/50], Train Loss: 0.0190, Val Loss: 0.0091\n",
      "Epoch [6/50], Train Loss: 0.0106, Val Loss: 0.0027\n",
      "Epoch [7/50], Train Loss: 0.0040, Val Loss: 0.0021\n",
      "Epoch [8/50], Train Loss: 0.0048, Val Loss: 0.0130\n",
      "Epoch [9/50], Train Loss: 0.0032, Val Loss: 0.0024\n",
      "Epoch [10/50], Train Loss: 0.0046, Val Loss: 0.0026\n",
      "Epoch [11/50], Train Loss: 0.0034, Val Loss: 0.0093\n",
      "Epoch [12/50], Train Loss: 0.0024, Val Loss: 0.0027\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=64, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0729, Val Loss: 0.0900\n",
      "Epoch [2/50], Train Loss: 0.0398, Val Loss: 0.0537\n",
      "Epoch [3/50], Train Loss: 0.0343, Val Loss: 0.0413\n",
      "Epoch [4/50], Train Loss: 0.0245, Val Loss: 0.0199\n",
      "Epoch [5/50], Train Loss: 0.0163, Val Loss: 0.0069\n",
      "Epoch [6/50], Train Loss: 0.0094, Val Loss: 0.0057\n",
      "Epoch [7/50], Train Loss: 0.0073, Val Loss: 0.0068\n",
      "Epoch [8/50], Train Loss: 0.0057, Val Loss: 0.0027\n",
      "Epoch [9/50], Train Loss: 0.0063, Val Loss: 0.0037\n",
      "Epoch [10/50], Train Loss: 0.0060, Val Loss: 0.0090\n",
      "Epoch [11/50], Train Loss: 0.0056, Val Loss: 0.0051\n",
      "Epoch [12/50], Train Loss: 0.0063, Val Loss: 0.0031\n",
      "Epoch [13/50], Train Loss: 0.0059, Val Loss: 0.0032\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=64, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0780, Val Loss: 0.0592\n",
      "Epoch [2/50], Train Loss: 0.0585, Val Loss: 0.0663\n",
      "Epoch [3/50], Train Loss: 0.0398, Val Loss: 0.0405\n",
      "Epoch [4/50], Train Loss: 0.0331, Val Loss: 0.0196\n",
      "Epoch [5/50], Train Loss: 0.0267, Val Loss: 0.0062\n",
      "Epoch [6/50], Train Loss: 0.0206, Val Loss: 0.0042\n",
      "Epoch [7/50], Train Loss: 0.0170, Val Loss: 0.0125\n",
      "Epoch [8/50], Train Loss: 0.0145, Val Loss: 0.0112\n",
      "Epoch [9/50], Train Loss: 0.0140, Val Loss: 0.0026\n",
      "Epoch [10/50], Train Loss: 0.0137, Val Loss: 0.0022\n",
      "Epoch [11/50], Train Loss: 0.0127, Val Loss: 0.0122\n",
      "Epoch [12/50], Train Loss: 0.0112, Val Loss: 0.0072\n",
      "Epoch [13/50], Train Loss: 0.0129, Val Loss: 0.0028\n",
      "Epoch [14/50], Train Loss: 0.0115, Val Loss: 0.0032\n",
      "Epoch [15/50], Train Loss: 0.0124, Val Loss: 0.0214\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=64, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0446, Val Loss: 0.0514\n",
      "Epoch [2/50], Train Loss: 0.0564, Val Loss: 0.0882\n",
      "Epoch [3/50], Train Loss: 0.0338, Val Loss: 0.0475\n",
      "Epoch [4/50], Train Loss: 0.0300, Val Loss: 0.0180\n",
      "Epoch [5/50], Train Loss: 0.0176, Val Loss: 0.0050\n",
      "Epoch [6/50], Train Loss: 0.0070, Val Loss: 0.0027\n",
      "Epoch [7/50], Train Loss: 0.0066, Val Loss: 0.0199\n",
      "Epoch [8/50], Train Loss: 0.0098, Val Loss: 0.0138\n",
      "Epoch [9/50], Train Loss: 0.0077, Val Loss: 0.0037\n",
      "Epoch [10/50], Train Loss: 0.0053, Val Loss: 0.0059\n",
      "Epoch [11/50], Train Loss: 0.0075, Val Loss: 0.0093\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=64, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0540, Val Loss: 0.0616\n",
      "Epoch [2/50], Train Loss: 0.0586, Val Loss: 0.0827\n",
      "Epoch [3/50], Train Loss: 0.0385, Val Loss: 0.0531\n",
      "Epoch [4/50], Train Loss: 0.0329, Val Loss: 0.0217\n",
      "Epoch [5/50], Train Loss: 0.0221, Val Loss: 0.0026\n",
      "Epoch [6/50], Train Loss: 0.0094, Val Loss: 0.0024\n",
      "Epoch [7/50], Train Loss: 0.0105, Val Loss: 0.0244\n",
      "Epoch [8/50], Train Loss: 0.0135, Val Loss: 0.0113\n",
      "Epoch [9/50], Train Loss: 0.0111, Val Loss: 0.0042\n",
      "Epoch [10/50], Train Loss: 0.0075, Val Loss: 0.0090\n",
      "Epoch [11/50], Train Loss: 0.0080, Val Loss: 0.0121\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=64, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0638, Val Loss: 0.0590\n",
      "Epoch [2/50], Train Loss: 0.0630, Val Loss: 0.0854\n",
      "Epoch [3/50], Train Loss: 0.0415, Val Loss: 0.0471\n",
      "Epoch [4/50], Train Loss: 0.0351, Val Loss: 0.0161\n",
      "Epoch [5/50], Train Loss: 0.0249, Val Loss: 0.0085\n",
      "Epoch [6/50], Train Loss: 0.0166, Val Loss: 0.0102\n",
      "Epoch [7/50], Train Loss: 0.0145, Val Loss: 0.0157\n",
      "Epoch [8/50], Train Loss: 0.0160, Val Loss: 0.0026\n",
      "Epoch [9/50], Train Loss: 0.0148, Val Loss: 0.0025\n",
      "Epoch [10/50], Train Loss: 0.0179, Val Loss: 0.0220\n",
      "Epoch [11/50], Train Loss: 0.0212, Val Loss: 0.0155\n",
      "Epoch [12/50], Train Loss: 0.0147, Val Loss: 0.0062\n",
      "Epoch [13/50], Train Loss: 0.0134, Val Loss: 0.0332\n",
      "Epoch [14/50], Train Loss: 0.0126, Val Loss: 0.0021\n",
      "Epoch [15/50], Train Loss: 0.0135, Val Loss: 0.0105\n",
      "Epoch [16/50], Train Loss: 0.0125, Val Loss: 0.0261\n",
      "Epoch [17/50], Train Loss: 0.0122, Val Loss: 0.0020\n",
      "Epoch [18/50], Train Loss: 0.0116, Val Loss: 0.0069\n",
      "Epoch [19/50], Train Loss: 0.0118, Val Loss: 0.0273\n",
      "Epoch [20/50], Train Loss: 0.0122, Val Loss: 0.0031\n",
      "Epoch [21/50], Train Loss: 0.0139, Val Loss: 0.0188\n",
      "Epoch [22/50], Train Loss: 0.0134, Val Loss: 0.0293\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=128, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0569, Val Loss: 0.0268\n",
      "Epoch [2/50], Train Loss: 0.0363, Val Loss: 0.0288\n",
      "Epoch [3/50], Train Loss: 0.0228, Val Loss: 0.0151\n",
      "Epoch [4/50], Train Loss: 0.0183, Val Loss: 0.0064\n",
      "Epoch [5/50], Train Loss: 0.0135, Val Loss: 0.0041\n",
      "Epoch [6/50], Train Loss: 0.0084, Val Loss: 0.0042\n",
      "Epoch [7/50], Train Loss: 0.0038, Val Loss: 0.0025\n",
      "Epoch [8/50], Train Loss: 0.0043, Val Loss: 0.0072\n",
      "Epoch [9/50], Train Loss: 0.0035, Val Loss: 0.0031\n",
      "Epoch [10/50], Train Loss: 0.0042, Val Loss: 0.0022\n",
      "Epoch [11/50], Train Loss: 0.0033, Val Loss: 0.0053\n",
      "Epoch [12/50], Train Loss: 0.0025, Val Loss: 0.0043\n",
      "Epoch [13/50], Train Loss: 0.0039, Val Loss: 0.0020\n",
      "Epoch [14/50], Train Loss: 0.0027, Val Loss: 0.0028\n",
      "Epoch [15/50], Train Loss: 0.0021, Val Loss: 0.0033\n",
      "Epoch [16/50], Train Loss: 0.0030, Val Loss: 0.0026\n",
      "Epoch [17/50], Train Loss: 0.0021, Val Loss: 0.0016\n",
      "Epoch [18/50], Train Loss: 0.0020, Val Loss: 0.0018\n",
      "Epoch [19/50], Train Loss: 0.0022, Val Loss: 0.0027\n",
      "Epoch [20/50], Train Loss: 0.0020, Val Loss: 0.0023\n",
      "Epoch [21/50], Train Loss: 0.0020, Val Loss: 0.0015\n",
      "Epoch [22/50], Train Loss: 0.0019, Val Loss: 0.0028\n",
      "Epoch [23/50], Train Loss: 0.0020, Val Loss: 0.0018\n",
      "Epoch [24/50], Train Loss: 0.0019, Val Loss: 0.0024\n",
      "Epoch [25/50], Train Loss: 0.0018, Val Loss: 0.0023\n",
      "Epoch [26/50], Train Loss: 0.0021, Val Loss: 0.0024\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=128, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0526, Val Loss: 0.0397\n",
      "Epoch [2/50], Train Loss: 0.0424, Val Loss: 0.0467\n",
      "Epoch [3/50], Train Loss: 0.0274, Val Loss: 0.0262\n",
      "Epoch [4/50], Train Loss: 0.0231, Val Loss: 0.0129\n",
      "Epoch [5/50], Train Loss: 0.0178, Val Loss: 0.0039\n",
      "Epoch [6/50], Train Loss: 0.0121, Val Loss: 0.0037\n",
      "Epoch [7/50], Train Loss: 0.0081, Val Loss: 0.0099\n",
      "Epoch [8/50], Train Loss: 0.0073, Val Loss: 0.0064\n",
      "Epoch [9/50], Train Loss: 0.0055, Val Loss: 0.0040\n",
      "Epoch [10/50], Train Loss: 0.0067, Val Loss: 0.0045\n",
      "Epoch [11/50], Train Loss: 0.0056, Val Loss: 0.0020\n",
      "Epoch [12/50], Train Loss: 0.0041, Val Loss: 0.0024\n",
      "Epoch [13/50], Train Loss: 0.0034, Val Loss: 0.0032\n",
      "Epoch [14/50], Train Loss: 0.0041, Val Loss: 0.0050\n",
      "Epoch [15/50], Train Loss: 0.0034, Val Loss: 0.0017\n",
      "Epoch [16/50], Train Loss: 0.0036, Val Loss: 0.0032\n",
      "Epoch [17/50], Train Loss: 0.0031, Val Loss: 0.0024\n",
      "Epoch [18/50], Train Loss: 0.0032, Val Loss: 0.0029\n",
      "Epoch [19/50], Train Loss: 0.0031, Val Loss: 0.0022\n",
      "Epoch [20/50], Train Loss: 0.0031, Val Loss: 0.0024\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=128, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0588, Val Loss: 0.0587\n",
      "Epoch [2/50], Train Loss: 0.0381, Val Loss: 0.0433\n",
      "Epoch [3/50], Train Loss: 0.0300, Val Loss: 0.0302\n",
      "Epoch [4/50], Train Loss: 0.0236, Val Loss: 0.0119\n",
      "Epoch [5/50], Train Loss: 0.0156, Val Loss: 0.0057\n",
      "Epoch [6/50], Train Loss: 0.0092, Val Loss: 0.0104\n",
      "Epoch [7/50], Train Loss: 0.0093, Val Loss: 0.0094\n",
      "Epoch [8/50], Train Loss: 0.0083, Val Loss: 0.0032\n",
      "Epoch [9/50], Train Loss: 0.0082, Val Loss: 0.0032\n",
      "Epoch [10/50], Train Loss: 0.0072, Val Loss: 0.0121\n",
      "Epoch [11/50], Train Loss: 0.0063, Val Loss: 0.0018\n",
      "Epoch [12/50], Train Loss: 0.0068, Val Loss: 0.0021\n",
      "Epoch [13/50], Train Loss: 0.0058, Val Loss: 0.0033\n",
      "Epoch [14/50], Train Loss: 0.0061, Val Loss: 0.0070\n",
      "Epoch [15/50], Train Loss: 0.0060, Val Loss: 0.0021\n",
      "Epoch [16/50], Train Loss: 0.0060, Val Loss: 0.0014\n",
      "Epoch [17/50], Train Loss: 0.0057, Val Loss: 0.0077\n",
      "Epoch [18/50], Train Loss: 0.0059, Val Loss: 0.0031\n",
      "Epoch [19/50], Train Loss: 0.0064, Val Loss: 0.0029\n",
      "Epoch [20/50], Train Loss: 0.0050, Val Loss: 0.0025\n",
      "Epoch [21/50], Train Loss: 0.0047, Val Loss: 0.0073\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=128, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0281, Val Loss: 0.0473\n",
      "Epoch [2/50], Train Loss: 0.0598, Val Loss: 0.0350\n",
      "Epoch [3/50], Train Loss: 0.0292, Val Loss: 0.0057\n",
      "Epoch [4/50], Train Loss: 0.0162, Val Loss: 0.0079\n",
      "Epoch [5/50], Train Loss: 0.0090, Val Loss: 0.0038\n",
      "Epoch [6/50], Train Loss: 0.0044, Val Loss: 0.0076\n",
      "Epoch [7/50], Train Loss: 0.0044, Val Loss: 0.0076\n",
      "Epoch [8/50], Train Loss: 0.0035, Val Loss: 0.0081\n",
      "Epoch [9/50], Train Loss: 0.0027, Val Loss: 0.0018\n",
      "Epoch [10/50], Train Loss: 0.0039, Val Loss: 0.0066\n",
      "Epoch [11/50], Train Loss: 0.0032, Val Loss: 0.0071\n",
      "Epoch [12/50], Train Loss: 0.0039, Val Loss: 0.0022\n",
      "Epoch [13/50], Train Loss: 0.0045, Val Loss: 0.0086\n",
      "Epoch [14/50], Train Loss: 0.0027, Val Loss: 0.0057\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=128, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0307, Val Loss: 0.0669\n",
      "Epoch [2/50], Train Loss: 0.0610, Val Loss: 0.0287\n",
      "Epoch [3/50], Train Loss: 0.0334, Val Loss: 0.0082\n",
      "Epoch [4/50], Train Loss: 0.0197, Val Loss: 0.0040\n",
      "Epoch [5/50], Train Loss: 0.0113, Val Loss: 0.0046\n",
      "Epoch [6/50], Train Loss: 0.0078, Val Loss: 0.0043\n",
      "Epoch [7/50], Train Loss: 0.0068, Val Loss: 0.0128\n",
      "Epoch [8/50], Train Loss: 0.0058, Val Loss: 0.0026\n",
      "Epoch [9/50], Train Loss: 0.0059, Val Loss: 0.0028\n",
      "Epoch [10/50], Train Loss: 0.0062, Val Loss: 0.0106\n",
      "Epoch [11/50], Train Loss: 0.0049, Val Loss: 0.0041\n",
      "Epoch [12/50], Train Loss: 0.0065, Val Loss: 0.0035\n",
      "Epoch [13/50], Train Loss: 0.0045, Val Loss: 0.0061\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=128, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0347, Val Loss: 0.0498\n",
      "Epoch [2/50], Train Loss: 0.0672, Val Loss: 0.0871\n",
      "Epoch [3/50], Train Loss: 0.0291, Val Loss: 0.0198\n",
      "Epoch [4/50], Train Loss: 0.0239, Val Loss: 0.0033\n",
      "Epoch [5/50], Train Loss: 0.0108, Val Loss: 0.0043\n",
      "Epoch [6/50], Train Loss: 0.0103, Val Loss: 0.0160\n",
      "Epoch [7/50], Train Loss: 0.0089, Val Loss: 0.0039\n",
      "Epoch [8/50], Train Loss: 0.0094, Val Loss: 0.0044\n",
      "Epoch [9/50], Train Loss: 0.0089, Val Loss: 0.0194\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=128, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0271, Val Loss: 0.1016\n",
      "Epoch [2/50], Train Loss: 0.0676, Val Loss: 0.0480\n",
      "Epoch [3/50], Train Loss: 0.0446, Val Loss: 0.0199\n",
      "Epoch [4/50], Train Loss: 0.0269, Val Loss: 0.0029\n",
      "Epoch [5/50], Train Loss: 0.0156, Val Loss: 0.0081\n",
      "Epoch [6/50], Train Loss: 0.0078, Val Loss: 0.0062\n",
      "Epoch [7/50], Train Loss: 0.0071, Val Loss: 0.0173\n",
      "Epoch [8/50], Train Loss: 0.0148, Val Loss: 0.0062\n",
      "Epoch [9/50], Train Loss: 0.0083, Val Loss: 0.0062\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=128, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0282, Val Loss: 0.0722\n",
      "Epoch [2/50], Train Loss: 0.0673, Val Loss: 0.0453\n",
      "Epoch [3/50], Train Loss: 0.0387, Val Loss: 0.0081\n",
      "Epoch [4/50], Train Loss: 0.0220, Val Loss: 0.0153\n",
      "Epoch [5/50], Train Loss: 0.0187, Val Loss: 0.0096\n",
      "Epoch [6/50], Train Loss: 0.0161, Val Loss: 0.0134\n",
      "Epoch [7/50], Train Loss: 0.0073, Val Loss: 0.0112\n",
      "Epoch [8/50], Train Loss: 0.0073, Val Loss: 0.0038\n",
      "Epoch [9/50], Train Loss: 0.0067, Val Loss: 0.0126\n",
      "Epoch [10/50], Train Loss: 0.0060, Val Loss: 0.0029\n",
      "Epoch [11/50], Train Loss: 0.0097, Val Loss: 0.0037\n",
      "Epoch [12/50], Train Loss: 0.0049, Val Loss: 0.0044\n",
      "Epoch [13/50], Train Loss: 0.0055, Val Loss: 0.0071\n",
      "Epoch [14/50], Train Loss: 0.0094, Val Loss: 0.0017\n",
      "Epoch [15/50], Train Loss: 0.0048, Val Loss: 0.0070\n",
      "Epoch [16/50], Train Loss: 0.0043, Val Loss: 0.0024\n",
      "Epoch [17/50], Train Loss: 0.0066, Val Loss: 0.0148\n",
      "Epoch [18/50], Train Loss: 0.0055, Val Loss: 0.0038\n",
      "Epoch [19/50], Train Loss: 0.0050, Val Loss: 0.0034\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0005, optimizer=adamw, hidden_size=128, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0310, Val Loss: 0.0992\n",
      "Epoch [2/50], Train Loss: 0.0709, Val Loss: 0.0516\n",
      "Epoch [3/50], Train Loss: 0.0425, Val Loss: 0.0112\n",
      "Epoch [4/50], Train Loss: 0.0256, Val Loss: 0.0053\n",
      "Epoch [5/50], Train Loss: 0.0183, Val Loss: 0.0101\n",
      "Epoch [6/50], Train Loss: 0.0175, Val Loss: 0.0158\n",
      "Epoch [7/50], Train Loss: 0.0132, Val Loss: 0.0038\n",
      "Epoch [8/50], Train Loss: 0.0098, Val Loss: 0.0067\n",
      "Epoch [9/50], Train Loss: 0.0101, Val Loss: 0.0067\n",
      "Epoch [10/50], Train Loss: 0.0142, Val Loss: 0.0031\n",
      "Epoch [11/50], Train Loss: 0.0094, Val Loss: 0.0046\n",
      "Epoch [12/50], Train Loss: 0.0095, Val Loss: 0.0082\n",
      "Epoch [13/50], Train Loss: 0.0177, Val Loss: 0.0142\n",
      "Epoch [14/50], Train Loss: 0.0085, Val Loss: 0.0051\n",
      "Epoch [15/50], Train Loss: 0.0075, Val Loss: 0.0078\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=16, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1020, Val Loss: 0.2886\n",
      "Epoch [2/50], Train Loss: 0.0944, Val Loss: 0.2716\n",
      "Epoch [3/50], Train Loss: 0.0879, Val Loss: 0.2559\n",
      "Epoch [4/50], Train Loss: 0.0820, Val Loss: 0.2414\n",
      "Epoch [5/50], Train Loss: 0.0765, Val Loss: 0.2278\n",
      "Epoch [6/50], Train Loss: 0.0716, Val Loss: 0.2151\n",
      "Epoch [7/50], Train Loss: 0.0671, Val Loss: 0.2032\n",
      "Epoch [8/50], Train Loss: 0.0629, Val Loss: 0.1921\n",
      "Epoch [9/50], Train Loss: 0.0592, Val Loss: 0.1816\n",
      "Epoch [10/50], Train Loss: 0.0558, Val Loss: 0.1718\n",
      "Epoch [11/50], Train Loss: 0.0527, Val Loss: 0.1626\n",
      "Epoch [12/50], Train Loss: 0.0500, Val Loss: 0.1541\n",
      "Epoch [13/50], Train Loss: 0.0475, Val Loss: 0.1462\n",
      "Epoch [14/50], Train Loss: 0.0453, Val Loss: 0.1389\n",
      "Epoch [15/50], Train Loss: 0.0434, Val Loss: 0.1322\n",
      "Epoch [16/50], Train Loss: 0.0417, Val Loss: 0.1260\n",
      "Epoch [17/50], Train Loss: 0.0402, Val Loss: 0.1203\n",
      "Epoch [18/50], Train Loss: 0.0389, Val Loss: 0.1151\n",
      "Epoch [19/50], Train Loss: 0.0377, Val Loss: 0.1104\n",
      "Epoch [20/50], Train Loss: 0.0367, Val Loss: 0.1061\n",
      "Epoch [21/50], Train Loss: 0.0358, Val Loss: 0.1021\n",
      "Epoch [22/50], Train Loss: 0.0349, Val Loss: 0.0985\n",
      "Epoch [23/50], Train Loss: 0.0342, Val Loss: 0.0951\n",
      "Epoch [24/50], Train Loss: 0.0335, Val Loss: 0.0920\n",
      "Epoch [25/50], Train Loss: 0.0329, Val Loss: 0.0891\n",
      "Epoch [26/50], Train Loss: 0.0323, Val Loss: 0.0865\n",
      "Epoch [27/50], Train Loss: 0.0317, Val Loss: 0.0840\n",
      "Epoch [28/50], Train Loss: 0.0312, Val Loss: 0.0816\n",
      "Epoch [29/50], Train Loss: 0.0307, Val Loss: 0.0793\n",
      "Epoch [30/50], Train Loss: 0.0302, Val Loss: 0.0772\n",
      "Epoch [31/50], Train Loss: 0.0297, Val Loss: 0.0751\n",
      "Epoch [32/50], Train Loss: 0.0292, Val Loss: 0.0731\n",
      "Epoch [33/50], Train Loss: 0.0287, Val Loss: 0.0711\n",
      "Epoch [34/50], Train Loss: 0.0283, Val Loss: 0.0693\n",
      "Epoch [35/50], Train Loss: 0.0278, Val Loss: 0.0674\n",
      "Epoch [36/50], Train Loss: 0.0273, Val Loss: 0.0656\n",
      "Epoch [37/50], Train Loss: 0.0269, Val Loss: 0.0638\n",
      "Epoch [38/50], Train Loss: 0.0264, Val Loss: 0.0620\n",
      "Epoch [39/50], Train Loss: 0.0260, Val Loss: 0.0602\n",
      "Epoch [40/50], Train Loss: 0.0255, Val Loss: 0.0584\n",
      "Epoch [41/50], Train Loss: 0.0250, Val Loss: 0.0567\n",
      "Epoch [42/50], Train Loss: 0.0246, Val Loss: 0.0549\n",
      "Epoch [43/50], Train Loss: 0.0241, Val Loss: 0.0532\n",
      "Epoch [44/50], Train Loss: 0.0237, Val Loss: 0.0514\n",
      "Epoch [45/50], Train Loss: 0.0232, Val Loss: 0.0497\n",
      "Epoch [46/50], Train Loss: 0.0227, Val Loss: 0.0480\n",
      "Epoch [47/50], Train Loss: 0.0223, Val Loss: 0.0462\n",
      "Epoch [48/50], Train Loss: 0.0218, Val Loss: 0.0445\n",
      "Epoch [49/50], Train Loss: 0.0213, Val Loss: 0.0428\n",
      "Epoch [50/50], Train Loss: 0.0208, Val Loss: 0.0411\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=16, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1308, Val Loss: 0.4112\n",
      "Epoch [2/50], Train Loss: 0.1187, Val Loss: 0.3845\n",
      "Epoch [3/50], Train Loss: 0.1101, Val Loss: 0.3605\n",
      "Epoch [4/50], Train Loss: 0.1004, Val Loss: 0.3382\n",
      "Epoch [5/50], Train Loss: 0.0937, Val Loss: 0.3176\n",
      "Epoch [6/50], Train Loss: 0.0861, Val Loss: 0.2985\n",
      "Epoch [7/50], Train Loss: 0.0798, Val Loss: 0.2809\n",
      "Epoch [8/50], Train Loss: 0.0737, Val Loss: 0.2645\n",
      "Epoch [9/50], Train Loss: 0.0708, Val Loss: 0.2492\n",
      "Epoch [10/50], Train Loss: 0.0665, Val Loss: 0.2350\n",
      "Epoch [11/50], Train Loss: 0.0621, Val Loss: 0.2217\n",
      "Epoch [12/50], Train Loss: 0.0593, Val Loss: 0.2095\n",
      "Epoch [13/50], Train Loss: 0.0553, Val Loss: 0.1979\n",
      "Epoch [14/50], Train Loss: 0.0534, Val Loss: 0.1873\n",
      "Epoch [15/50], Train Loss: 0.0509, Val Loss: 0.1774\n",
      "Epoch [16/50], Train Loss: 0.0483, Val Loss: 0.1682\n",
      "Epoch [17/50], Train Loss: 0.0455, Val Loss: 0.1597\n",
      "Epoch [18/50], Train Loss: 0.0449, Val Loss: 0.1518\n",
      "Epoch [19/50], Train Loss: 0.0432, Val Loss: 0.1446\n",
      "Epoch [20/50], Train Loss: 0.0424, Val Loss: 0.1380\n",
      "Epoch [21/50], Train Loss: 0.0409, Val Loss: 0.1318\n",
      "Epoch [22/50], Train Loss: 0.0398, Val Loss: 0.1262\n",
      "Epoch [23/50], Train Loss: 0.0378, Val Loss: 0.1211\n",
      "Epoch [24/50], Train Loss: 0.0372, Val Loss: 0.1163\n",
      "Epoch [25/50], Train Loss: 0.0372, Val Loss: 0.1119\n",
      "Epoch [26/50], Train Loss: 0.0354, Val Loss: 0.1078\n",
      "Epoch [27/50], Train Loss: 0.0352, Val Loss: 0.1038\n",
      "Epoch [28/50], Train Loss: 0.0349, Val Loss: 0.1000\n",
      "Epoch [29/50], Train Loss: 0.0339, Val Loss: 0.0967\n",
      "Epoch [30/50], Train Loss: 0.0322, Val Loss: 0.0936\n",
      "Epoch [31/50], Train Loss: 0.0321, Val Loss: 0.0904\n",
      "Epoch [32/50], Train Loss: 0.0307, Val Loss: 0.0878\n",
      "Epoch [33/50], Train Loss: 0.0314, Val Loss: 0.0848\n",
      "Epoch [34/50], Train Loss: 0.0306, Val Loss: 0.0820\n",
      "Epoch [35/50], Train Loss: 0.0298, Val Loss: 0.0795\n",
      "Epoch [36/50], Train Loss: 0.0297, Val Loss: 0.0770\n",
      "Epoch [37/50], Train Loss: 0.0285, Val Loss: 0.0749\n",
      "Epoch [38/50], Train Loss: 0.0288, Val Loss: 0.0727\n",
      "Epoch [39/50], Train Loss: 0.0292, Val Loss: 0.0704\n",
      "Epoch [40/50], Train Loss: 0.0269, Val Loss: 0.0681\n",
      "Epoch [41/50], Train Loss: 0.0259, Val Loss: 0.0661\n",
      "Epoch [42/50], Train Loss: 0.0253, Val Loss: 0.0640\n",
      "Epoch [43/50], Train Loss: 0.0252, Val Loss: 0.0618\n",
      "Epoch [44/50], Train Loss: 0.0253, Val Loss: 0.0598\n",
      "Epoch [45/50], Train Loss: 0.0249, Val Loss: 0.0577\n",
      "Epoch [46/50], Train Loss: 0.0236, Val Loss: 0.0560\n",
      "Epoch [47/50], Train Loss: 0.0237, Val Loss: 0.0542\n",
      "Epoch [48/50], Train Loss: 0.0223, Val Loss: 0.0523\n",
      "Epoch [49/50], Train Loss: 0.0221, Val Loss: 0.0504\n",
      "Epoch [50/50], Train Loss: 0.0217, Val Loss: 0.0489\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=16, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1328, Val Loss: 0.3058\n",
      "Epoch [2/50], Train Loss: 0.1211, Val Loss: 0.2882\n",
      "Epoch [3/50], Train Loss: 0.1124, Val Loss: 0.2725\n",
      "Epoch [4/50], Train Loss: 0.1084, Val Loss: 0.2577\n",
      "Epoch [5/50], Train Loss: 0.1019, Val Loss: 0.2440\n",
      "Epoch [6/50], Train Loss: 0.0952, Val Loss: 0.2313\n",
      "Epoch [7/50], Train Loss: 0.0904, Val Loss: 0.2193\n",
      "Epoch [8/50], Train Loss: 0.0855, Val Loss: 0.2080\n",
      "Epoch [9/50], Train Loss: 0.0819, Val Loss: 0.1974\n",
      "Epoch [10/50], Train Loss: 0.0773, Val Loss: 0.1874\n",
      "Epoch [11/50], Train Loss: 0.0732, Val Loss: 0.1780\n",
      "Epoch [12/50], Train Loss: 0.0711, Val Loss: 0.1691\n",
      "Epoch [13/50], Train Loss: 0.0689, Val Loss: 0.1610\n",
      "Epoch [14/50], Train Loss: 0.0672, Val Loss: 0.1533\n",
      "Epoch [15/50], Train Loss: 0.0644, Val Loss: 0.1462\n",
      "Epoch [16/50], Train Loss: 0.0613, Val Loss: 0.1395\n",
      "Epoch [17/50], Train Loss: 0.0576, Val Loss: 0.1331\n",
      "Epoch [18/50], Train Loss: 0.0575, Val Loss: 0.1279\n",
      "Epoch [19/50], Train Loss: 0.0576, Val Loss: 0.1228\n",
      "Epoch [20/50], Train Loss: 0.0573, Val Loss: 0.1181\n",
      "Epoch [21/50], Train Loss: 0.0547, Val Loss: 0.1138\n",
      "Epoch [22/50], Train Loss: 0.0527, Val Loss: 0.1098\n",
      "Epoch [23/50], Train Loss: 0.0531, Val Loss: 0.1063\n",
      "Epoch [24/50], Train Loss: 0.0522, Val Loss: 0.1028\n",
      "Epoch [25/50], Train Loss: 0.0502, Val Loss: 0.0998\n",
      "Epoch [26/50], Train Loss: 0.0480, Val Loss: 0.0968\n",
      "Epoch [27/50], Train Loss: 0.0471, Val Loss: 0.0938\n",
      "Epoch [28/50], Train Loss: 0.0476, Val Loss: 0.0915\n",
      "Epoch [29/50], Train Loss: 0.0462, Val Loss: 0.0895\n",
      "Epoch [30/50], Train Loss: 0.0456, Val Loss: 0.0875\n",
      "Epoch [31/50], Train Loss: 0.0463, Val Loss: 0.0858\n",
      "Epoch [32/50], Train Loss: 0.0444, Val Loss: 0.0843\n",
      "Epoch [33/50], Train Loss: 0.0457, Val Loss: 0.0830\n",
      "Epoch [34/50], Train Loss: 0.0452, Val Loss: 0.0814\n",
      "Epoch [35/50], Train Loss: 0.0425, Val Loss: 0.0798\n",
      "Epoch [36/50], Train Loss: 0.0417, Val Loss: 0.0788\n",
      "Epoch [37/50], Train Loss: 0.0424, Val Loss: 0.0779\n",
      "Epoch [38/50], Train Loss: 0.0417, Val Loss: 0.0765\n",
      "Epoch [39/50], Train Loss: 0.0416, Val Loss: 0.0750\n",
      "Epoch [40/50], Train Loss: 0.0410, Val Loss: 0.0741\n",
      "Epoch [41/50], Train Loss: 0.0407, Val Loss: 0.0728\n",
      "Epoch [42/50], Train Loss: 0.0397, Val Loss: 0.0722\n",
      "Epoch [43/50], Train Loss: 0.0389, Val Loss: 0.0715\n",
      "Epoch [44/50], Train Loss: 0.0404, Val Loss: 0.0708\n",
      "Epoch [45/50], Train Loss: 0.0379, Val Loss: 0.0697\n",
      "Epoch [46/50], Train Loss: 0.0396, Val Loss: 0.0690\n",
      "Epoch [47/50], Train Loss: 0.0369, Val Loss: 0.0682\n",
      "Epoch [48/50], Train Loss: 0.0374, Val Loss: 0.0673\n",
      "Epoch [49/50], Train Loss: 0.0361, Val Loss: 0.0662\n",
      "Epoch [50/50], Train Loss: 0.0363, Val Loss: 0.0655\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=16, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1017, Val Loss: 0.2811\n",
      "Epoch [2/50], Train Loss: 0.0936, Val Loss: 0.2647\n",
      "Epoch [3/50], Train Loss: 0.0871, Val Loss: 0.2505\n",
      "Epoch [4/50], Train Loss: 0.0814, Val Loss: 0.2377\n",
      "Epoch [5/50], Train Loss: 0.0761, Val Loss: 0.2256\n",
      "Epoch [6/50], Train Loss: 0.0712, Val Loss: 0.2139\n",
      "Epoch [7/50], Train Loss: 0.0665, Val Loss: 0.2025\n",
      "Epoch [8/50], Train Loss: 0.0620, Val Loss: 0.1913\n",
      "Epoch [9/50], Train Loss: 0.0578, Val Loss: 0.1804\n",
      "Epoch [10/50], Train Loss: 0.0538, Val Loss: 0.1696\n",
      "Epoch [11/50], Train Loss: 0.0500, Val Loss: 0.1591\n",
      "Epoch [12/50], Train Loss: 0.0466, Val Loss: 0.1491\n",
      "Epoch [13/50], Train Loss: 0.0435, Val Loss: 0.1395\n",
      "Epoch [14/50], Train Loss: 0.0408, Val Loss: 0.1305\n",
      "Epoch [15/50], Train Loss: 0.0384, Val Loss: 0.1223\n",
      "Epoch [16/50], Train Loss: 0.0365, Val Loss: 0.1149\n",
      "Epoch [17/50], Train Loss: 0.0350, Val Loss: 0.1084\n",
      "Epoch [18/50], Train Loss: 0.0338, Val Loss: 0.1028\n",
      "Epoch [19/50], Train Loss: 0.0329, Val Loss: 0.0979\n",
      "Epoch [20/50], Train Loss: 0.0322, Val Loss: 0.0938\n",
      "Epoch [21/50], Train Loss: 0.0317, Val Loss: 0.0903\n",
      "Epoch [22/50], Train Loss: 0.0312, Val Loss: 0.0872\n",
      "Epoch [23/50], Train Loss: 0.0307, Val Loss: 0.0845\n",
      "Epoch [24/50], Train Loss: 0.0303, Val Loss: 0.0821\n",
      "Epoch [25/50], Train Loss: 0.0298, Val Loss: 0.0798\n",
      "Epoch [26/50], Train Loss: 0.0294, Val Loss: 0.0777\n",
      "Epoch [27/50], Train Loss: 0.0290, Val Loss: 0.0756\n",
      "Epoch [28/50], Train Loss: 0.0285, Val Loss: 0.0736\n",
      "Epoch [29/50], Train Loss: 0.0281, Val Loss: 0.0717\n",
      "Epoch [30/50], Train Loss: 0.0276, Val Loss: 0.0697\n",
      "Epoch [31/50], Train Loss: 0.0271, Val Loss: 0.0676\n",
      "Epoch [32/50], Train Loss: 0.0266, Val Loss: 0.0656\n",
      "Epoch [33/50], Train Loss: 0.0261, Val Loss: 0.0636\n",
      "Epoch [34/50], Train Loss: 0.0255, Val Loss: 0.0615\n",
      "Epoch [35/50], Train Loss: 0.0250, Val Loss: 0.0593\n",
      "Epoch [36/50], Train Loss: 0.0244, Val Loss: 0.0571\n",
      "Epoch [37/50], Train Loss: 0.0238, Val Loss: 0.0549\n",
      "Epoch [38/50], Train Loss: 0.0232, Val Loss: 0.0527\n",
      "Epoch [39/50], Train Loss: 0.0226, Val Loss: 0.0504\n",
      "Epoch [40/50], Train Loss: 0.0220, Val Loss: 0.0481\n",
      "Epoch [41/50], Train Loss: 0.0213, Val Loss: 0.0457\n",
      "Epoch [42/50], Train Loss: 0.0207, Val Loss: 0.0434\n",
      "Epoch [43/50], Train Loss: 0.0200, Val Loss: 0.0410\n",
      "Epoch [44/50], Train Loss: 0.0194, Val Loss: 0.0387\n",
      "Epoch [45/50], Train Loss: 0.0187, Val Loss: 0.0364\n",
      "Epoch [46/50], Train Loss: 0.0180, Val Loss: 0.0342\n",
      "Epoch [47/50], Train Loss: 0.0174, Val Loss: 0.0320\n",
      "Epoch [48/50], Train Loss: 0.0167, Val Loss: 0.0298\n",
      "Epoch [49/50], Train Loss: 0.0161, Val Loss: 0.0278\n",
      "Epoch [50/50], Train Loss: 0.0155, Val Loss: 0.0259\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=16, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1833, Val Loss: 0.4524\n",
      "Epoch [2/50], Train Loss: 0.1670, Val Loss: 0.4257\n",
      "Epoch [3/50], Train Loss: 0.1551, Val Loss: 0.4018\n",
      "Epoch [4/50], Train Loss: 0.1412, Val Loss: 0.3798\n",
      "Epoch [5/50], Train Loss: 0.1312, Val Loss: 0.3585\n",
      "Epoch [6/50], Train Loss: 0.1219, Val Loss: 0.3379\n",
      "Epoch [7/50], Train Loss: 0.1121, Val Loss: 0.3175\n",
      "Epoch [8/50], Train Loss: 0.1021, Val Loss: 0.2970\n",
      "Epoch [9/50], Train Loss: 0.0941, Val Loss: 0.2765\n",
      "Epoch [10/50], Train Loss: 0.0844, Val Loss: 0.2557\n",
      "Epoch [11/50], Train Loss: 0.0765, Val Loss: 0.2350\n",
      "Epoch [12/50], Train Loss: 0.0696, Val Loss: 0.2149\n",
      "Epoch [13/50], Train Loss: 0.0645, Val Loss: 0.1955\n",
      "Epoch [14/50], Train Loss: 0.0575, Val Loss: 0.1771\n",
      "Epoch [15/50], Train Loss: 0.0511, Val Loss: 0.1601\n",
      "Epoch [16/50], Train Loss: 0.0474, Val Loss: 0.1448\n",
      "Epoch [17/50], Train Loss: 0.0461, Val Loss: 0.1317\n",
      "Epoch [18/50], Train Loss: 0.0428, Val Loss: 0.1199\n",
      "Epoch [19/50], Train Loss: 0.0419, Val Loss: 0.1093\n",
      "Epoch [20/50], Train Loss: 0.0386, Val Loss: 0.1004\n",
      "Epoch [21/50], Train Loss: 0.0374, Val Loss: 0.0928\n",
      "Epoch [22/50], Train Loss: 0.0363, Val Loss: 0.0862\n",
      "Epoch [23/50], Train Loss: 0.0353, Val Loss: 0.0799\n",
      "Epoch [24/50], Train Loss: 0.0346, Val Loss: 0.0744\n",
      "Epoch [25/50], Train Loss: 0.0330, Val Loss: 0.0693\n",
      "Epoch [26/50], Train Loss: 0.0316, Val Loss: 0.0647\n",
      "Epoch [27/50], Train Loss: 0.0325, Val Loss: 0.0599\n",
      "Epoch [28/50], Train Loss: 0.0305, Val Loss: 0.0559\n",
      "Epoch [29/50], Train Loss: 0.0294, Val Loss: 0.0521\n",
      "Epoch [30/50], Train Loss: 0.0275, Val Loss: 0.0481\n",
      "Epoch [31/50], Train Loss: 0.0284, Val Loss: 0.0448\n",
      "Epoch [32/50], Train Loss: 0.0270, Val Loss: 0.0410\n",
      "Epoch [33/50], Train Loss: 0.0265, Val Loss: 0.0378\n",
      "Epoch [34/50], Train Loss: 0.0261, Val Loss: 0.0356\n",
      "Epoch [35/50], Train Loss: 0.0252, Val Loss: 0.0328\n",
      "Epoch [36/50], Train Loss: 0.0249, Val Loss: 0.0299\n",
      "Epoch [37/50], Train Loss: 0.0238, Val Loss: 0.0278\n",
      "Epoch [38/50], Train Loss: 0.0247, Val Loss: 0.0257\n",
      "Epoch [39/50], Train Loss: 0.0228, Val Loss: 0.0241\n",
      "Epoch [40/50], Train Loss: 0.0221, Val Loss: 0.0225\n",
      "Epoch [41/50], Train Loss: 0.0219, Val Loss: 0.0214\n",
      "Epoch [42/50], Train Loss: 0.0222, Val Loss: 0.0202\n",
      "Epoch [43/50], Train Loss: 0.0208, Val Loss: 0.0191\n",
      "Epoch [44/50], Train Loss: 0.0209, Val Loss: 0.0181\n",
      "Epoch [45/50], Train Loss: 0.0203, Val Loss: 0.0172\n",
      "Epoch [46/50], Train Loss: 0.0203, Val Loss: 0.0160\n",
      "Epoch [47/50], Train Loss: 0.0197, Val Loss: 0.0154\n",
      "Epoch [48/50], Train Loss: 0.0196, Val Loss: 0.0151\n",
      "Epoch [49/50], Train Loss: 0.0186, Val Loss: 0.0145\n",
      "Epoch [50/50], Train Loss: 0.0184, Val Loss: 0.0141\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=16, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0993, Val Loss: 0.2483\n",
      "Epoch [2/50], Train Loss: 0.0923, Val Loss: 0.2321\n",
      "Epoch [3/50], Train Loss: 0.0868, Val Loss: 0.2173\n",
      "Epoch [4/50], Train Loss: 0.0809, Val Loss: 0.2040\n",
      "Epoch [5/50], Train Loss: 0.0749, Val Loss: 0.1913\n",
      "Epoch [6/50], Train Loss: 0.0709, Val Loss: 0.1794\n",
      "Epoch [7/50], Train Loss: 0.0677, Val Loss: 0.1685\n",
      "Epoch [8/50], Train Loss: 0.0664, Val Loss: 0.1594\n",
      "Epoch [9/50], Train Loss: 0.0633, Val Loss: 0.1510\n",
      "Epoch [10/50], Train Loss: 0.0613, Val Loss: 0.1431\n",
      "Epoch [11/50], Train Loss: 0.0600, Val Loss: 0.1358\n",
      "Epoch [12/50], Train Loss: 0.0589, Val Loss: 0.1293\n",
      "Epoch [13/50], Train Loss: 0.0571, Val Loss: 0.1241\n",
      "Epoch [14/50], Train Loss: 0.0557, Val Loss: 0.1190\n",
      "Epoch [15/50], Train Loss: 0.0557, Val Loss: 0.1144\n",
      "Epoch [16/50], Train Loss: 0.0532, Val Loss: 0.1102\n",
      "Epoch [17/50], Train Loss: 0.0528, Val Loss: 0.1065\n",
      "Epoch [18/50], Train Loss: 0.0507, Val Loss: 0.1022\n",
      "Epoch [19/50], Train Loss: 0.0515, Val Loss: 0.0987\n",
      "Epoch [20/50], Train Loss: 0.0494, Val Loss: 0.0949\n",
      "Epoch [21/50], Train Loss: 0.0493, Val Loss: 0.0916\n",
      "Epoch [22/50], Train Loss: 0.0475, Val Loss: 0.0883\n",
      "Epoch [23/50], Train Loss: 0.0472, Val Loss: 0.0856\n",
      "Epoch [24/50], Train Loss: 0.0474, Val Loss: 0.0838\n",
      "Epoch [25/50], Train Loss: 0.0468, Val Loss: 0.0813\n",
      "Epoch [26/50], Train Loss: 0.0451, Val Loss: 0.0782\n",
      "Epoch [27/50], Train Loss: 0.0432, Val Loss: 0.0753\n",
      "Epoch [28/50], Train Loss: 0.0435, Val Loss: 0.0730\n",
      "Epoch [29/50], Train Loss: 0.0434, Val Loss: 0.0709\n",
      "Epoch [30/50], Train Loss: 0.0431, Val Loss: 0.0690\n",
      "Epoch [31/50], Train Loss: 0.0421, Val Loss: 0.0672\n",
      "Epoch [32/50], Train Loss: 0.0415, Val Loss: 0.0647\n",
      "Epoch [33/50], Train Loss: 0.0400, Val Loss: 0.0623\n",
      "Epoch [34/50], Train Loss: 0.0375, Val Loss: 0.0600\n",
      "Epoch [35/50], Train Loss: 0.0396, Val Loss: 0.0583\n",
      "Epoch [36/50], Train Loss: 0.0384, Val Loss: 0.0558\n",
      "Epoch [37/50], Train Loss: 0.0382, Val Loss: 0.0531\n",
      "Epoch [38/50], Train Loss: 0.0353, Val Loss: 0.0505\n",
      "Epoch [39/50], Train Loss: 0.0361, Val Loss: 0.0500\n",
      "Epoch [40/50], Train Loss: 0.0373, Val Loss: 0.0476\n",
      "Epoch [41/50], Train Loss: 0.0369, Val Loss: 0.0455\n",
      "Epoch [42/50], Train Loss: 0.0360, Val Loss: 0.0438\n",
      "Epoch [43/50], Train Loss: 0.0358, Val Loss: 0.0421\n",
      "Epoch [44/50], Train Loss: 0.0343, Val Loss: 0.0401\n",
      "Epoch [45/50], Train Loss: 0.0325, Val Loss: 0.0381\n",
      "Epoch [46/50], Train Loss: 0.0333, Val Loss: 0.0368\n",
      "Epoch [47/50], Train Loss: 0.0322, Val Loss: 0.0357\n",
      "Epoch [48/50], Train Loss: 0.0325, Val Loss: 0.0333\n",
      "Epoch [49/50], Train Loss: 0.0309, Val Loss: 0.0318\n",
      "Epoch [50/50], Train Loss: 0.0320, Val Loss: 0.0306\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=16, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.2171, Val Loss: 0.4300\n",
      "Epoch [2/50], Train Loss: 0.1866, Val Loss: 0.3784\n",
      "Epoch [3/50], Train Loss: 0.1591, Val Loss: 0.3286\n",
      "Epoch [4/50], Train Loss: 0.1333, Val Loss: 0.2800\n",
      "Epoch [5/50], Train Loss: 0.1091, Val Loss: 0.2331\n",
      "Epoch [6/50], Train Loss: 0.0870, Val Loss: 0.1894\n",
      "Epoch [7/50], Train Loss: 0.0678, Val Loss: 0.1505\n",
      "Epoch [8/50], Train Loss: 0.0523, Val Loss: 0.1182\n",
      "Epoch [9/50], Train Loss: 0.0410, Val Loss: 0.0936\n",
      "Epoch [10/50], Train Loss: 0.0337, Val Loss: 0.0765\n",
      "Epoch [11/50], Train Loss: 0.0295, Val Loss: 0.0657\n",
      "Epoch [12/50], Train Loss: 0.0273, Val Loss: 0.0592\n",
      "Epoch [13/50], Train Loss: 0.0261, Val Loss: 0.0556\n",
      "Epoch [14/50], Train Loss: 0.0254, Val Loss: 0.0536\n",
      "Epoch [15/50], Train Loss: 0.0248, Val Loss: 0.0522\n",
      "Epoch [16/50], Train Loss: 0.0243, Val Loss: 0.0512\n",
      "Epoch [17/50], Train Loss: 0.0239, Val Loss: 0.0503\n",
      "Epoch [18/50], Train Loss: 0.0234, Val Loss: 0.0493\n",
      "Epoch [19/50], Train Loss: 0.0230, Val Loss: 0.0483\n",
      "Epoch [20/50], Train Loss: 0.0226, Val Loss: 0.0472\n",
      "Epoch [21/50], Train Loss: 0.0222, Val Loss: 0.0460\n",
      "Epoch [22/50], Train Loss: 0.0218, Val Loss: 0.0448\n",
      "Epoch [23/50], Train Loss: 0.0214, Val Loss: 0.0435\n",
      "Epoch [24/50], Train Loss: 0.0210, Val Loss: 0.0421\n",
      "Epoch [25/50], Train Loss: 0.0206, Val Loss: 0.0408\n",
      "Epoch [26/50], Train Loss: 0.0202, Val Loss: 0.0393\n",
      "Epoch [27/50], Train Loss: 0.0199, Val Loss: 0.0379\n",
      "Epoch [28/50], Train Loss: 0.0195, Val Loss: 0.0365\n",
      "Epoch [29/50], Train Loss: 0.0191, Val Loss: 0.0351\n",
      "Epoch [30/50], Train Loss: 0.0187, Val Loss: 0.0337\n",
      "Epoch [31/50], Train Loss: 0.0183, Val Loss: 0.0323\n",
      "Epoch [32/50], Train Loss: 0.0180, Val Loss: 0.0309\n",
      "Epoch [33/50], Train Loss: 0.0176, Val Loss: 0.0295\n",
      "Epoch [34/50], Train Loss: 0.0173, Val Loss: 0.0283\n",
      "Epoch [35/50], Train Loss: 0.0170, Val Loss: 0.0270\n",
      "Epoch [36/50], Train Loss: 0.0167, Val Loss: 0.0258\n",
      "Epoch [37/50], Train Loss: 0.0164, Val Loss: 0.0247\n",
      "Epoch [38/50], Train Loss: 0.0161, Val Loss: 0.0236\n",
      "Epoch [39/50], Train Loss: 0.0159, Val Loss: 0.0225\n",
      "Epoch [40/50], Train Loss: 0.0156, Val Loss: 0.0216\n",
      "Epoch [41/50], Train Loss: 0.0154, Val Loss: 0.0207\n",
      "Epoch [42/50], Train Loss: 0.0152, Val Loss: 0.0199\n",
      "Epoch [43/50], Train Loss: 0.0149, Val Loss: 0.0191\n",
      "Epoch [44/50], Train Loss: 0.0147, Val Loss: 0.0184\n",
      "Epoch [45/50], Train Loss: 0.0145, Val Loss: 0.0177\n",
      "Epoch [46/50], Train Loss: 0.0143, Val Loss: 0.0172\n",
      "Epoch [47/50], Train Loss: 0.0141, Val Loss: 0.0166\n",
      "Epoch [48/50], Train Loss: 0.0139, Val Loss: 0.0161\n",
      "Epoch [49/50], Train Loss: 0.0137, Val Loss: 0.0156\n",
      "Epoch [50/50], Train Loss: 0.0135, Val Loss: 0.0151\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=16, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.2881, Val Loss: 0.5849\n",
      "Epoch [2/50], Train Loss: 0.2547, Val Loss: 0.5307\n",
      "Epoch [3/50], Train Loss: 0.2266, Val Loss: 0.4820\n",
      "Epoch [4/50], Train Loss: 0.2029, Val Loss: 0.4361\n",
      "Epoch [5/50], Train Loss: 0.1791, Val Loss: 0.3921\n",
      "Epoch [6/50], Train Loss: 0.1601, Val Loss: 0.3497\n",
      "Epoch [7/50], Train Loss: 0.1413, Val Loss: 0.3093\n",
      "Epoch [8/50], Train Loss: 0.1221, Val Loss: 0.2713\n",
      "Epoch [9/50], Train Loss: 0.1106, Val Loss: 0.2371\n",
      "Epoch [10/50], Train Loss: 0.0979, Val Loss: 0.2078\n",
      "Epoch [11/50], Train Loss: 0.0864, Val Loss: 0.1823\n",
      "Epoch [12/50], Train Loss: 0.0776, Val Loss: 0.1614\n",
      "Epoch [13/50], Train Loss: 0.0721, Val Loss: 0.1445\n",
      "Epoch [14/50], Train Loss: 0.0676, Val Loss: 0.1314\n",
      "Epoch [15/50], Train Loss: 0.0611, Val Loss: 0.1206\n",
      "Epoch [16/50], Train Loss: 0.0596, Val Loss: 0.1116\n",
      "Epoch [17/50], Train Loss: 0.0550, Val Loss: 0.1046\n",
      "Epoch [18/50], Train Loss: 0.0515, Val Loss: 0.0988\n",
      "Epoch [19/50], Train Loss: 0.0503, Val Loss: 0.0939\n",
      "Epoch [20/50], Train Loss: 0.0476, Val Loss: 0.0903\n",
      "Epoch [21/50], Train Loss: 0.0454, Val Loss: 0.0878\n",
      "Epoch [22/50], Train Loss: 0.0468, Val Loss: 0.0856\n",
      "Epoch [23/50], Train Loss: 0.0452, Val Loss: 0.0822\n",
      "Epoch [24/50], Train Loss: 0.0446, Val Loss: 0.0800\n",
      "Epoch [25/50], Train Loss: 0.0441, Val Loss: 0.0788\n",
      "Epoch [26/50], Train Loss: 0.0428, Val Loss: 0.0758\n",
      "Epoch [27/50], Train Loss: 0.0405, Val Loss: 0.0752\n",
      "Epoch [28/50], Train Loss: 0.0410, Val Loss: 0.0735\n",
      "Epoch [29/50], Train Loss: 0.0400, Val Loss: 0.0721\n",
      "Epoch [30/50], Train Loss: 0.0400, Val Loss: 0.0697\n",
      "Epoch [31/50], Train Loss: 0.0394, Val Loss: 0.0679\n",
      "Epoch [32/50], Train Loss: 0.0381, Val Loss: 0.0662\n",
      "Epoch [33/50], Train Loss: 0.0393, Val Loss: 0.0644\n",
      "Epoch [34/50], Train Loss: 0.0382, Val Loss: 0.0618\n",
      "Epoch [35/50], Train Loss: 0.0363, Val Loss: 0.0597\n",
      "Epoch [36/50], Train Loss: 0.0350, Val Loss: 0.0572\n",
      "Epoch [37/50], Train Loss: 0.0362, Val Loss: 0.0550\n",
      "Epoch [38/50], Train Loss: 0.0346, Val Loss: 0.0523\n",
      "Epoch [39/50], Train Loss: 0.0335, Val Loss: 0.0505\n",
      "Epoch [40/50], Train Loss: 0.0331, Val Loss: 0.0480\n",
      "Epoch [41/50], Train Loss: 0.0334, Val Loss: 0.0458\n",
      "Epoch [42/50], Train Loss: 0.0312, Val Loss: 0.0432\n",
      "Epoch [43/50], Train Loss: 0.0315, Val Loss: 0.0415\n",
      "Epoch [44/50], Train Loss: 0.0322, Val Loss: 0.0386\n",
      "Epoch [45/50], Train Loss: 0.0306, Val Loss: 0.0369\n",
      "Epoch [46/50], Train Loss: 0.0300, Val Loss: 0.0343\n",
      "Epoch [47/50], Train Loss: 0.0294, Val Loss: 0.0320\n",
      "Epoch [48/50], Train Loss: 0.0290, Val Loss: 0.0296\n",
      "Epoch [49/50], Train Loss: 0.0280, Val Loss: 0.0277\n",
      "Epoch [50/50], Train Loss: 0.0268, Val Loss: 0.0260\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=16, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1806, Val Loss: 0.4083\n",
      "Epoch [2/50], Train Loss: 0.1571, Val Loss: 0.3731\n",
      "Epoch [3/50], Train Loss: 0.1422, Val Loss: 0.3414\n",
      "Epoch [4/50], Train Loss: 0.1312, Val Loss: 0.3126\n",
      "Epoch [5/50], Train Loss: 0.1167, Val Loss: 0.2857\n",
      "Epoch [6/50], Train Loss: 0.1084, Val Loss: 0.2604\n",
      "Epoch [7/50], Train Loss: 0.0984, Val Loss: 0.2361\n",
      "Epoch [8/50], Train Loss: 0.0930, Val Loss: 0.2141\n",
      "Epoch [9/50], Train Loss: 0.0851, Val Loss: 0.1951\n",
      "Epoch [10/50], Train Loss: 0.0808, Val Loss: 0.1780\n",
      "Epoch [11/50], Train Loss: 0.0778, Val Loss: 0.1639\n",
      "Epoch [12/50], Train Loss: 0.0750, Val Loss: 0.1521\n",
      "Epoch [13/50], Train Loss: 0.0717, Val Loss: 0.1441\n",
      "Epoch [14/50], Train Loss: 0.0718, Val Loss: 0.1355\n",
      "Epoch [15/50], Train Loss: 0.0729, Val Loss: 0.1303\n",
      "Epoch [16/50], Train Loss: 0.0673, Val Loss: 0.1250\n",
      "Epoch [17/50], Train Loss: 0.0679, Val Loss: 0.1226\n",
      "Epoch [18/50], Train Loss: 0.0670, Val Loss: 0.1195\n",
      "Epoch [19/50], Train Loss: 0.0658, Val Loss: 0.1154\n",
      "Epoch [20/50], Train Loss: 0.0653, Val Loss: 0.1126\n",
      "Epoch [21/50], Train Loss: 0.0594, Val Loss: 0.1091\n",
      "Epoch [22/50], Train Loss: 0.0618, Val Loss: 0.1080\n",
      "Epoch [23/50], Train Loss: 0.0622, Val Loss: 0.1078\n",
      "Epoch [24/50], Train Loss: 0.0608, Val Loss: 0.1063\n",
      "Epoch [25/50], Train Loss: 0.0610, Val Loss: 0.1041\n",
      "Epoch [26/50], Train Loss: 0.0570, Val Loss: 0.1018\n",
      "Epoch [27/50], Train Loss: 0.0588, Val Loss: 0.0978\n",
      "Epoch [28/50], Train Loss: 0.0583, Val Loss: 0.0958\n",
      "Epoch [29/50], Train Loss: 0.0563, Val Loss: 0.0942\n",
      "Epoch [30/50], Train Loss: 0.0557, Val Loss: 0.0920\n",
      "Epoch [31/50], Train Loss: 0.0522, Val Loss: 0.0910\n",
      "Epoch [32/50], Train Loss: 0.0563, Val Loss: 0.0882\n",
      "Epoch [33/50], Train Loss: 0.0529, Val Loss: 0.0863\n",
      "Epoch [34/50], Train Loss: 0.0539, Val Loss: 0.0840\n",
      "Epoch [35/50], Train Loss: 0.0523, Val Loss: 0.0823\n",
      "Epoch [36/50], Train Loss: 0.0495, Val Loss: 0.0821\n",
      "Epoch [37/50], Train Loss: 0.0512, Val Loss: 0.0816\n",
      "Epoch [38/50], Train Loss: 0.0496, Val Loss: 0.0774\n",
      "Epoch [39/50], Train Loss: 0.0506, Val Loss: 0.0752\n",
      "Epoch [40/50], Train Loss: 0.0475, Val Loss: 0.0723\n",
      "Epoch [41/50], Train Loss: 0.0488, Val Loss: 0.0698\n",
      "Epoch [42/50], Train Loss: 0.0454, Val Loss: 0.0672\n",
      "Epoch [43/50], Train Loss: 0.0490, Val Loss: 0.0640\n",
      "Epoch [44/50], Train Loss: 0.0459, Val Loss: 0.0622\n",
      "Epoch [45/50], Train Loss: 0.0445, Val Loss: 0.0597\n",
      "Epoch [46/50], Train Loss: 0.0456, Val Loss: 0.0569\n",
      "Epoch [47/50], Train Loss: 0.0437, Val Loss: 0.0544\n",
      "Epoch [48/50], Train Loss: 0.0440, Val Loss: 0.0539\n",
      "Epoch [49/50], Train Loss: 0.0432, Val Loss: 0.0523\n",
      "Epoch [50/50], Train Loss: 0.0426, Val Loss: 0.0493\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=32, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1221, Val Loss: 0.3947\n",
      "Epoch [2/50], Train Loss: 0.1072, Val Loss: 0.3588\n",
      "Epoch [3/50], Train Loss: 0.0949, Val Loss: 0.3267\n",
      "Epoch [4/50], Train Loss: 0.0842, Val Loss: 0.2974\n",
      "Epoch [5/50], Train Loss: 0.0747, Val Loss: 0.2700\n",
      "Epoch [6/50], Train Loss: 0.0662, Val Loss: 0.2442\n",
      "Epoch [7/50], Train Loss: 0.0588, Val Loss: 0.2199\n",
      "Epoch [8/50], Train Loss: 0.0523, Val Loss: 0.1972\n",
      "Epoch [9/50], Train Loss: 0.0467, Val Loss: 0.1761\n",
      "Epoch [10/50], Train Loss: 0.0421, Val Loss: 0.1570\n",
      "Epoch [11/50], Train Loss: 0.0384, Val Loss: 0.1399\n",
      "Epoch [12/50], Train Loss: 0.0355, Val Loss: 0.1251\n",
      "Epoch [13/50], Train Loss: 0.0333, Val Loss: 0.1124\n",
      "Epoch [14/50], Train Loss: 0.0316, Val Loss: 0.1018\n",
      "Epoch [15/50], Train Loss: 0.0303, Val Loss: 0.0930\n",
      "Epoch [16/50], Train Loss: 0.0293, Val Loss: 0.0858\n",
      "Epoch [17/50], Train Loss: 0.0284, Val Loss: 0.0798\n",
      "Epoch [18/50], Train Loss: 0.0277, Val Loss: 0.0748\n",
      "Epoch [19/50], Train Loss: 0.0270, Val Loss: 0.0705\n",
      "Epoch [20/50], Train Loss: 0.0264, Val Loss: 0.0668\n",
      "Epoch [21/50], Train Loss: 0.0258, Val Loss: 0.0636\n",
      "Epoch [22/50], Train Loss: 0.0252, Val Loss: 0.0606\n",
      "Epoch [23/50], Train Loss: 0.0247, Val Loss: 0.0579\n",
      "Epoch [24/50], Train Loss: 0.0241, Val Loss: 0.0554\n",
      "Epoch [25/50], Train Loss: 0.0236, Val Loss: 0.0531\n",
      "Epoch [26/50], Train Loss: 0.0230, Val Loss: 0.0508\n",
      "Epoch [27/50], Train Loss: 0.0225, Val Loss: 0.0487\n",
      "Epoch [28/50], Train Loss: 0.0219, Val Loss: 0.0466\n",
      "Epoch [29/50], Train Loss: 0.0214, Val Loss: 0.0446\n",
      "Epoch [30/50], Train Loss: 0.0208, Val Loss: 0.0427\n",
      "Epoch [31/50], Train Loss: 0.0203, Val Loss: 0.0408\n",
      "Epoch [32/50], Train Loss: 0.0198, Val Loss: 0.0389\n",
      "Epoch [33/50], Train Loss: 0.0192, Val Loss: 0.0371\n",
      "Epoch [34/50], Train Loss: 0.0187, Val Loss: 0.0353\n",
      "Epoch [35/50], Train Loss: 0.0182, Val Loss: 0.0335\n",
      "Epoch [36/50], Train Loss: 0.0176, Val Loss: 0.0318\n",
      "Epoch [37/50], Train Loss: 0.0171, Val Loss: 0.0301\n",
      "Epoch [38/50], Train Loss: 0.0166, Val Loss: 0.0284\n",
      "Epoch [39/50], Train Loss: 0.0161, Val Loss: 0.0268\n",
      "Epoch [40/50], Train Loss: 0.0155, Val Loss: 0.0252\n",
      "Epoch [41/50], Train Loss: 0.0150, Val Loss: 0.0236\n",
      "Epoch [42/50], Train Loss: 0.0145, Val Loss: 0.0221\n",
      "Epoch [43/50], Train Loss: 0.0140, Val Loss: 0.0206\n",
      "Epoch [44/50], Train Loss: 0.0135, Val Loss: 0.0191\n",
      "Epoch [45/50], Train Loss: 0.0129, Val Loss: 0.0177\n",
      "Epoch [46/50], Train Loss: 0.0124, Val Loss: 0.0164\n",
      "Epoch [47/50], Train Loss: 0.0119, Val Loss: 0.0151\n",
      "Epoch [48/50], Train Loss: 0.0115, Val Loss: 0.0139\n",
      "Epoch [49/50], Train Loss: 0.0110, Val Loss: 0.0127\n",
      "Epoch [50/50], Train Loss: 0.0105, Val Loss: 0.0116\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=32, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0674, Val Loss: 0.2134\n",
      "Epoch [2/50], Train Loss: 0.0603, Val Loss: 0.1924\n",
      "Epoch [3/50], Train Loss: 0.0547, Val Loss: 0.1736\n",
      "Epoch [4/50], Train Loss: 0.0495, Val Loss: 0.1565\n",
      "Epoch [5/50], Train Loss: 0.0453, Val Loss: 0.1408\n",
      "Epoch [6/50], Train Loss: 0.0412, Val Loss: 0.1263\n",
      "Epoch [7/50], Train Loss: 0.0390, Val Loss: 0.1132\n",
      "Epoch [8/50], Train Loss: 0.0355, Val Loss: 0.1013\n",
      "Epoch [9/50], Train Loss: 0.0330, Val Loss: 0.0909\n",
      "Epoch [10/50], Train Loss: 0.0316, Val Loss: 0.0818\n",
      "Epoch [11/50], Train Loss: 0.0302, Val Loss: 0.0739\n",
      "Epoch [12/50], Train Loss: 0.0292, Val Loss: 0.0673\n",
      "Epoch [13/50], Train Loss: 0.0285, Val Loss: 0.0619\n",
      "Epoch [14/50], Train Loss: 0.0277, Val Loss: 0.0571\n",
      "Epoch [15/50], Train Loss: 0.0263, Val Loss: 0.0532\n",
      "Epoch [16/50], Train Loss: 0.0256, Val Loss: 0.0497\n",
      "Epoch [17/50], Train Loss: 0.0254, Val Loss: 0.0470\n",
      "Epoch [18/50], Train Loss: 0.0244, Val Loss: 0.0443\n",
      "Epoch [19/50], Train Loss: 0.0240, Val Loss: 0.0420\n",
      "Epoch [20/50], Train Loss: 0.0231, Val Loss: 0.0400\n",
      "Epoch [21/50], Train Loss: 0.0225, Val Loss: 0.0382\n",
      "Epoch [22/50], Train Loss: 0.0224, Val Loss: 0.0366\n",
      "Epoch [23/50], Train Loss: 0.0214, Val Loss: 0.0349\n",
      "Epoch [24/50], Train Loss: 0.0205, Val Loss: 0.0331\n",
      "Epoch [25/50], Train Loss: 0.0207, Val Loss: 0.0313\n",
      "Epoch [26/50], Train Loss: 0.0195, Val Loss: 0.0293\n",
      "Epoch [27/50], Train Loss: 0.0195, Val Loss: 0.0278\n",
      "Epoch [28/50], Train Loss: 0.0184, Val Loss: 0.0260\n",
      "Epoch [29/50], Train Loss: 0.0179, Val Loss: 0.0244\n",
      "Epoch [30/50], Train Loss: 0.0170, Val Loss: 0.0226\n",
      "Epoch [31/50], Train Loss: 0.0164, Val Loss: 0.0210\n",
      "Epoch [32/50], Train Loss: 0.0157, Val Loss: 0.0195\n",
      "Epoch [33/50], Train Loss: 0.0156, Val Loss: 0.0178\n",
      "Epoch [34/50], Train Loss: 0.0142, Val Loss: 0.0158\n",
      "Epoch [35/50], Train Loss: 0.0144, Val Loss: 0.0146\n",
      "Epoch [36/50], Train Loss: 0.0131, Val Loss: 0.0132\n",
      "Epoch [37/50], Train Loss: 0.0127, Val Loss: 0.0116\n",
      "Epoch [38/50], Train Loss: 0.0115, Val Loss: 0.0103\n",
      "Epoch [39/50], Train Loss: 0.0108, Val Loss: 0.0092\n",
      "Epoch [40/50], Train Loss: 0.0103, Val Loss: 0.0078\n",
      "Epoch [41/50], Train Loss: 0.0099, Val Loss: 0.0069\n",
      "Epoch [42/50], Train Loss: 0.0095, Val Loss: 0.0061\n",
      "Epoch [43/50], Train Loss: 0.0084, Val Loss: 0.0055\n",
      "Epoch [44/50], Train Loss: 0.0085, Val Loss: 0.0050\n",
      "Epoch [45/50], Train Loss: 0.0078, Val Loss: 0.0044\n",
      "Epoch [46/50], Train Loss: 0.0073, Val Loss: 0.0040\n",
      "Epoch [47/50], Train Loss: 0.0071, Val Loss: 0.0038\n",
      "Epoch [48/50], Train Loss: 0.0068, Val Loss: 0.0036\n",
      "Epoch [49/50], Train Loss: 0.0068, Val Loss: 0.0033\n",
      "Epoch [50/50], Train Loss: 0.0066, Val Loss: 0.0029\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=32, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1433, Val Loss: 0.2497\n",
      "Epoch [2/50], Train Loss: 0.1279, Val Loss: 0.2193\n",
      "Epoch [3/50], Train Loss: 0.1145, Val Loss: 0.1917\n",
      "Epoch [4/50], Train Loss: 0.1000, Val Loss: 0.1666\n",
      "Epoch [5/50], Train Loss: 0.0868, Val Loss: 0.1436\n",
      "Epoch [6/50], Train Loss: 0.0798, Val Loss: 0.1227\n",
      "Epoch [7/50], Train Loss: 0.0673, Val Loss: 0.1040\n",
      "Epoch [8/50], Train Loss: 0.0625, Val Loss: 0.0878\n",
      "Epoch [9/50], Train Loss: 0.0566, Val Loss: 0.0746\n",
      "Epoch [10/50], Train Loss: 0.0508, Val Loss: 0.0642\n",
      "Epoch [11/50], Train Loss: 0.0466, Val Loss: 0.0561\n",
      "Epoch [12/50], Train Loss: 0.0454, Val Loss: 0.0500\n",
      "Epoch [13/50], Train Loss: 0.0429, Val Loss: 0.0460\n",
      "Epoch [14/50], Train Loss: 0.0406, Val Loss: 0.0429\n",
      "Epoch [15/50], Train Loss: 0.0385, Val Loss: 0.0412\n",
      "Epoch [16/50], Train Loss: 0.0387, Val Loss: 0.0401\n",
      "Epoch [17/50], Train Loss: 0.0375, Val Loss: 0.0392\n",
      "Epoch [18/50], Train Loss: 0.0372, Val Loss: 0.0381\n",
      "Epoch [19/50], Train Loss: 0.0355, Val Loss: 0.0373\n",
      "Epoch [20/50], Train Loss: 0.0356, Val Loss: 0.0368\n",
      "Epoch [21/50], Train Loss: 0.0333, Val Loss: 0.0354\n",
      "Epoch [22/50], Train Loss: 0.0314, Val Loss: 0.0347\n",
      "Epoch [23/50], Train Loss: 0.0326, Val Loss: 0.0338\n",
      "Epoch [24/50], Train Loss: 0.0305, Val Loss: 0.0330\n",
      "Epoch [25/50], Train Loss: 0.0317, Val Loss: 0.0321\n",
      "Epoch [26/50], Train Loss: 0.0303, Val Loss: 0.0314\n",
      "Epoch [27/50], Train Loss: 0.0296, Val Loss: 0.0304\n",
      "Epoch [28/50], Train Loss: 0.0295, Val Loss: 0.0295\n",
      "Epoch [29/50], Train Loss: 0.0285, Val Loss: 0.0284\n",
      "Epoch [30/50], Train Loss: 0.0283, Val Loss: 0.0274\n",
      "Epoch [31/50], Train Loss: 0.0276, Val Loss: 0.0265\n",
      "Epoch [32/50], Train Loss: 0.0265, Val Loss: 0.0260\n",
      "Epoch [33/50], Train Loss: 0.0254, Val Loss: 0.0247\n",
      "Epoch [34/50], Train Loss: 0.0259, Val Loss: 0.0237\n",
      "Epoch [35/50], Train Loss: 0.0259, Val Loss: 0.0227\n",
      "Epoch [36/50], Train Loss: 0.0257, Val Loss: 0.0215\n",
      "Epoch [37/50], Train Loss: 0.0248, Val Loss: 0.0203\n",
      "Epoch [38/50], Train Loss: 0.0252, Val Loss: 0.0198\n",
      "Epoch [39/50], Train Loss: 0.0232, Val Loss: 0.0190\n",
      "Epoch [40/50], Train Loss: 0.0220, Val Loss: 0.0185\n",
      "Epoch [41/50], Train Loss: 0.0227, Val Loss: 0.0174\n",
      "Epoch [42/50], Train Loss: 0.0222, Val Loss: 0.0159\n",
      "Epoch [43/50], Train Loss: 0.0209, Val Loss: 0.0149\n",
      "Epoch [44/50], Train Loss: 0.0213, Val Loss: 0.0139\n",
      "Epoch [45/50], Train Loss: 0.0208, Val Loss: 0.0133\n",
      "Epoch [46/50], Train Loss: 0.0203, Val Loss: 0.0126\n",
      "Epoch [47/50], Train Loss: 0.0196, Val Loss: 0.0121\n",
      "Epoch [48/50], Train Loss: 0.0191, Val Loss: 0.0109\n",
      "Epoch [49/50], Train Loss: 0.0190, Val Loss: 0.0106\n",
      "Epoch [50/50], Train Loss: 0.0189, Val Loss: 0.0100\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=32, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1442, Val Loss: 0.3596\n",
      "Epoch [2/50], Train Loss: 0.1210, Val Loss: 0.3155\n",
      "Epoch [3/50], Train Loss: 0.1021, Val Loss: 0.2751\n",
      "Epoch [4/50], Train Loss: 0.0851, Val Loss: 0.2363\n",
      "Epoch [5/50], Train Loss: 0.0697, Val Loss: 0.1982\n",
      "Epoch [6/50], Train Loss: 0.0560, Val Loss: 0.1615\n",
      "Epoch [7/50], Train Loss: 0.0446, Val Loss: 0.1283\n",
      "Epoch [8/50], Train Loss: 0.0362, Val Loss: 0.1012\n",
      "Epoch [9/50], Train Loss: 0.0311, Val Loss: 0.0818\n",
      "Epoch [10/50], Train Loss: 0.0283, Val Loss: 0.0695\n",
      "Epoch [11/50], Train Loss: 0.0268, Val Loss: 0.0620\n",
      "Epoch [12/50], Train Loss: 0.0257, Val Loss: 0.0569\n",
      "Epoch [13/50], Train Loss: 0.0247, Val Loss: 0.0530\n",
      "Epoch [14/50], Train Loss: 0.0238, Val Loss: 0.0496\n",
      "Epoch [15/50], Train Loss: 0.0228, Val Loss: 0.0463\n",
      "Epoch [16/50], Train Loss: 0.0219, Val Loss: 0.0431\n",
      "Epoch [17/50], Train Loss: 0.0209, Val Loss: 0.0400\n",
      "Epoch [18/50], Train Loss: 0.0200, Val Loss: 0.0369\n",
      "Epoch [19/50], Train Loss: 0.0190, Val Loss: 0.0338\n",
      "Epoch [20/50], Train Loss: 0.0180, Val Loss: 0.0308\n",
      "Epoch [21/50], Train Loss: 0.0171, Val Loss: 0.0279\n",
      "Epoch [22/50], Train Loss: 0.0161, Val Loss: 0.0251\n",
      "Epoch [23/50], Train Loss: 0.0151, Val Loss: 0.0223\n",
      "Epoch [24/50], Train Loss: 0.0141, Val Loss: 0.0197\n",
      "Epoch [25/50], Train Loss: 0.0132, Val Loss: 0.0173\n",
      "Epoch [26/50], Train Loss: 0.0122, Val Loss: 0.0150\n",
      "Epoch [27/50], Train Loss: 0.0113, Val Loss: 0.0129\n",
      "Epoch [28/50], Train Loss: 0.0104, Val Loss: 0.0111\n",
      "Epoch [29/50], Train Loss: 0.0096, Val Loss: 0.0095\n",
      "Epoch [30/50], Train Loss: 0.0089, Val Loss: 0.0082\n",
      "Epoch [31/50], Train Loss: 0.0082, Val Loss: 0.0071\n",
      "Epoch [32/50], Train Loss: 0.0075, Val Loss: 0.0063\n",
      "Epoch [33/50], Train Loss: 0.0069, Val Loss: 0.0056\n",
      "Epoch [34/50], Train Loss: 0.0064, Val Loss: 0.0052\n",
      "Epoch [35/50], Train Loss: 0.0059, Val Loss: 0.0048\n",
      "Epoch [36/50], Train Loss: 0.0055, Val Loss: 0.0046\n",
      "Epoch [37/50], Train Loss: 0.0051, Val Loss: 0.0044\n",
      "Epoch [38/50], Train Loss: 0.0047, Val Loss: 0.0043\n",
      "Epoch [39/50], Train Loss: 0.0043, Val Loss: 0.0041\n",
      "Epoch [40/50], Train Loss: 0.0040, Val Loss: 0.0040\n",
      "Epoch [41/50], Train Loss: 0.0037, Val Loss: 0.0039\n",
      "Epoch [42/50], Train Loss: 0.0034, Val Loss: 0.0038\n",
      "Epoch [43/50], Train Loss: 0.0032, Val Loss: 0.0036\n",
      "Epoch [44/50], Train Loss: 0.0030, Val Loss: 0.0034\n",
      "Epoch [45/50], Train Loss: 0.0028, Val Loss: 0.0033\n",
      "Epoch [46/50], Train Loss: 0.0027, Val Loss: 0.0031\n",
      "Epoch [47/50], Train Loss: 0.0026, Val Loss: 0.0030\n",
      "Epoch [48/50], Train Loss: 0.0025, Val Loss: 0.0028\n",
      "Epoch [49/50], Train Loss: 0.0024, Val Loss: 0.0027\n",
      "Epoch [50/50], Train Loss: 0.0023, Val Loss: 0.0026\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=32, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1491, Val Loss: 0.3558\n",
      "Epoch [2/50], Train Loss: 0.1252, Val Loss: 0.3116\n",
      "Epoch [3/50], Train Loss: 0.1049, Val Loss: 0.2713\n",
      "Epoch [4/50], Train Loss: 0.0880, Val Loss: 0.2325\n",
      "Epoch [5/50], Train Loss: 0.0715, Val Loss: 0.1939\n",
      "Epoch [6/50], Train Loss: 0.0586, Val Loss: 0.1574\n",
      "Epoch [7/50], Train Loss: 0.0475, Val Loss: 0.1258\n",
      "Epoch [8/50], Train Loss: 0.0434, Val Loss: 0.1016\n",
      "Epoch [9/50], Train Loss: 0.0393, Val Loss: 0.0849\n",
      "Epoch [10/50], Train Loss: 0.0375, Val Loss: 0.0751\n",
      "Epoch [11/50], Train Loss: 0.0362, Val Loss: 0.0685\n",
      "Epoch [12/50], Train Loss: 0.0354, Val Loss: 0.0647\n",
      "Epoch [13/50], Train Loss: 0.0336, Val Loss: 0.0619\n",
      "Epoch [14/50], Train Loss: 0.0340, Val Loss: 0.0588\n",
      "Epoch [15/50], Train Loss: 0.0331, Val Loss: 0.0554\n",
      "Epoch [16/50], Train Loss: 0.0313, Val Loss: 0.0529\n",
      "Epoch [17/50], Train Loss: 0.0311, Val Loss: 0.0497\n",
      "Epoch [18/50], Train Loss: 0.0293, Val Loss: 0.0469\n",
      "Epoch [19/50], Train Loss: 0.0301, Val Loss: 0.0440\n",
      "Epoch [20/50], Train Loss: 0.0277, Val Loss: 0.0412\n",
      "Epoch [21/50], Train Loss: 0.0267, Val Loss: 0.0386\n",
      "Epoch [22/50], Train Loss: 0.0259, Val Loss: 0.0358\n",
      "Epoch [23/50], Train Loss: 0.0248, Val Loss: 0.0332\n",
      "Epoch [24/50], Train Loss: 0.0253, Val Loss: 0.0308\n",
      "Epoch [25/50], Train Loss: 0.0229, Val Loss: 0.0283\n",
      "Epoch [26/50], Train Loss: 0.0218, Val Loss: 0.0258\n",
      "Epoch [27/50], Train Loss: 0.0213, Val Loss: 0.0233\n",
      "Epoch [28/50], Train Loss: 0.0208, Val Loss: 0.0209\n",
      "Epoch [29/50], Train Loss: 0.0193, Val Loss: 0.0192\n",
      "Epoch [30/50], Train Loss: 0.0190, Val Loss: 0.0177\n",
      "Epoch [31/50], Train Loss: 0.0178, Val Loss: 0.0161\n",
      "Epoch [32/50], Train Loss: 0.0168, Val Loss: 0.0149\n",
      "Epoch [33/50], Train Loss: 0.0166, Val Loss: 0.0136\n",
      "Epoch [34/50], Train Loss: 0.0154, Val Loss: 0.0133\n",
      "Epoch [35/50], Train Loss: 0.0152, Val Loss: 0.0121\n",
      "Epoch [36/50], Train Loss: 0.0150, Val Loss: 0.0111\n",
      "Epoch [37/50], Train Loss: 0.0137, Val Loss: 0.0108\n",
      "Epoch [38/50], Train Loss: 0.0129, Val Loss: 0.0104\n",
      "Epoch [39/50], Train Loss: 0.0133, Val Loss: 0.0098\n",
      "Epoch [40/50], Train Loss: 0.0125, Val Loss: 0.0095\n",
      "Epoch [41/50], Train Loss: 0.0115, Val Loss: 0.0095\n",
      "Epoch [42/50], Train Loss: 0.0116, Val Loss: 0.0086\n",
      "Epoch [43/50], Train Loss: 0.0114, Val Loss: 0.0080\n",
      "Epoch [44/50], Train Loss: 0.0109, Val Loss: 0.0081\n",
      "Epoch [45/50], Train Loss: 0.0103, Val Loss: 0.0081\n",
      "Epoch [46/50], Train Loss: 0.0100, Val Loss: 0.0073\n",
      "Epoch [47/50], Train Loss: 0.0096, Val Loss: 0.0069\n",
      "Epoch [48/50], Train Loss: 0.0101, Val Loss: 0.0067\n",
      "Epoch [49/50], Train Loss: 0.0095, Val Loss: 0.0064\n",
      "Epoch [50/50], Train Loss: 0.0092, Val Loss: 0.0066\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=32, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1398, Val Loss: 0.3602\n",
      "Epoch [2/50], Train Loss: 0.1225, Val Loss: 0.3205\n",
      "Epoch [3/50], Train Loss: 0.1059, Val Loss: 0.2850\n",
      "Epoch [4/50], Train Loss: 0.0940, Val Loss: 0.2517\n",
      "Epoch [5/50], Train Loss: 0.0830, Val Loss: 0.2197\n",
      "Epoch [6/50], Train Loss: 0.0736, Val Loss: 0.1902\n",
      "Epoch [7/50], Train Loss: 0.0629, Val Loss: 0.1629\n",
      "Epoch [8/50], Train Loss: 0.0618, Val Loss: 0.1405\n",
      "Epoch [9/50], Train Loss: 0.0539, Val Loss: 0.1224\n",
      "Epoch [10/50], Train Loss: 0.0542, Val Loss: 0.1102\n",
      "Epoch [11/50], Train Loss: 0.0509, Val Loss: 0.1012\n",
      "Epoch [12/50], Train Loss: 0.0476, Val Loss: 0.0929\n",
      "Epoch [13/50], Train Loss: 0.0468, Val Loss: 0.0874\n",
      "Epoch [14/50], Train Loss: 0.0473, Val Loss: 0.0826\n",
      "Epoch [15/50], Train Loss: 0.0431, Val Loss: 0.0768\n",
      "Epoch [16/50], Train Loss: 0.0454, Val Loss: 0.0709\n",
      "Epoch [17/50], Train Loss: 0.0405, Val Loss: 0.0647\n",
      "Epoch [18/50], Train Loss: 0.0406, Val Loss: 0.0593\n",
      "Epoch [19/50], Train Loss: 0.0389, Val Loss: 0.0533\n",
      "Epoch [20/50], Train Loss: 0.0379, Val Loss: 0.0497\n",
      "Epoch [21/50], Train Loss: 0.0369, Val Loss: 0.0470\n",
      "Epoch [22/50], Train Loss: 0.0344, Val Loss: 0.0417\n",
      "Epoch [23/50], Train Loss: 0.0333, Val Loss: 0.0354\n",
      "Epoch [24/50], Train Loss: 0.0319, Val Loss: 0.0323\n",
      "Epoch [25/50], Train Loss: 0.0310, Val Loss: 0.0282\n",
      "Epoch [26/50], Train Loss: 0.0295, Val Loss: 0.0245\n",
      "Epoch [27/50], Train Loss: 0.0293, Val Loss: 0.0206\n",
      "Epoch [28/50], Train Loss: 0.0279, Val Loss: 0.0177\n",
      "Epoch [29/50], Train Loss: 0.0271, Val Loss: 0.0148\n",
      "Epoch [30/50], Train Loss: 0.0257, Val Loss: 0.0131\n",
      "Epoch [31/50], Train Loss: 0.0250, Val Loss: 0.0133\n",
      "Epoch [32/50], Train Loss: 0.0254, Val Loss: 0.0121\n",
      "Epoch [33/50], Train Loss: 0.0233, Val Loss: 0.0103\n",
      "Epoch [34/50], Train Loss: 0.0240, Val Loss: 0.0089\n",
      "Epoch [35/50], Train Loss: 0.0207, Val Loss: 0.0093\n",
      "Epoch [36/50], Train Loss: 0.0218, Val Loss: 0.0088\n",
      "Epoch [37/50], Train Loss: 0.0216, Val Loss: 0.0090\n",
      "Epoch [38/50], Train Loss: 0.0197, Val Loss: 0.0083\n",
      "Epoch [39/50], Train Loss: 0.0207, Val Loss: 0.0076\n",
      "Epoch [40/50], Train Loss: 0.0186, Val Loss: 0.0077\n",
      "Epoch [41/50], Train Loss: 0.0185, Val Loss: 0.0062\n",
      "Epoch [42/50], Train Loss: 0.0187, Val Loss: 0.0068\n",
      "Epoch [43/50], Train Loss: 0.0185, Val Loss: 0.0068\n",
      "Epoch [44/50], Train Loss: 0.0180, Val Loss: 0.0072\n",
      "Epoch [45/50], Train Loss: 0.0171, Val Loss: 0.0068\n",
      "Epoch [46/50], Train Loss: 0.0174, Val Loss: 0.0065\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=32, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1056, Val Loss: 0.2654\n",
      "Epoch [2/50], Train Loss: 0.0838, Val Loss: 0.2216\n",
      "Epoch [3/50], Train Loss: 0.0649, Val Loss: 0.1773\n",
      "Epoch [4/50], Train Loss: 0.0486, Val Loss: 0.1364\n",
      "Epoch [5/50], Train Loss: 0.0377, Val Loss: 0.1057\n",
      "Epoch [6/50], Train Loss: 0.0328, Val Loss: 0.0877\n",
      "Epoch [7/50], Train Loss: 0.0313, Val Loss: 0.0787\n",
      "Epoch [8/50], Train Loss: 0.0306, Val Loss: 0.0739\n",
      "Epoch [9/50], Train Loss: 0.0298, Val Loss: 0.0706\n",
      "Epoch [10/50], Train Loss: 0.0290, Val Loss: 0.0677\n",
      "Epoch [11/50], Train Loss: 0.0281, Val Loss: 0.0648\n",
      "Epoch [12/50], Train Loss: 0.0271, Val Loss: 0.0619\n",
      "Epoch [13/50], Train Loss: 0.0261, Val Loss: 0.0588\n",
      "Epoch [14/50], Train Loss: 0.0251, Val Loss: 0.0554\n",
      "Epoch [15/50], Train Loss: 0.0239, Val Loss: 0.0520\n",
      "Epoch [16/50], Train Loss: 0.0228, Val Loss: 0.0483\n",
      "Epoch [17/50], Train Loss: 0.0215, Val Loss: 0.0444\n",
      "Epoch [18/50], Train Loss: 0.0202, Val Loss: 0.0405\n",
      "Epoch [19/50], Train Loss: 0.0188, Val Loss: 0.0364\n",
      "Epoch [20/50], Train Loss: 0.0174, Val Loss: 0.0323\n",
      "Epoch [21/50], Train Loss: 0.0160, Val Loss: 0.0282\n",
      "Epoch [22/50], Train Loss: 0.0146, Val Loss: 0.0243\n",
      "Epoch [23/50], Train Loss: 0.0132, Val Loss: 0.0208\n",
      "Epoch [24/50], Train Loss: 0.0120, Val Loss: 0.0176\n",
      "Epoch [25/50], Train Loss: 0.0108, Val Loss: 0.0149\n",
      "Epoch [26/50], Train Loss: 0.0097, Val Loss: 0.0127\n",
      "Epoch [27/50], Train Loss: 0.0087, Val Loss: 0.0110\n",
      "Epoch [28/50], Train Loss: 0.0078, Val Loss: 0.0099\n",
      "Epoch [29/50], Train Loss: 0.0069, Val Loss: 0.0091\n",
      "Epoch [30/50], Train Loss: 0.0061, Val Loss: 0.0087\n",
      "Epoch [31/50], Train Loss: 0.0054, Val Loss: 0.0085\n",
      "Epoch [32/50], Train Loss: 0.0047, Val Loss: 0.0083\n",
      "Epoch [33/50], Train Loss: 0.0042, Val Loss: 0.0082\n",
      "Epoch [34/50], Train Loss: 0.0038, Val Loss: 0.0081\n",
      "Epoch [35/50], Train Loss: 0.0034, Val Loss: 0.0080\n",
      "Epoch [36/50], Train Loss: 0.0031, Val Loss: 0.0078\n",
      "Epoch [37/50], Train Loss: 0.0029, Val Loss: 0.0075\n",
      "Epoch [38/50], Train Loss: 0.0028, Val Loss: 0.0073\n",
      "Epoch [39/50], Train Loss: 0.0026, Val Loss: 0.0071\n",
      "Epoch [40/50], Train Loss: 0.0026, Val Loss: 0.0070\n",
      "Epoch [41/50], Train Loss: 0.0025, Val Loss: 0.0069\n",
      "Epoch [42/50], Train Loss: 0.0025, Val Loss: 0.0068\n",
      "Epoch [43/50], Train Loss: 0.0024, Val Loss: 0.0067\n",
      "Epoch [44/50], Train Loss: 0.0024, Val Loss: 0.0066\n",
      "Epoch [45/50], Train Loss: 0.0024, Val Loss: 0.0065\n",
      "Epoch [46/50], Train Loss: 0.0024, Val Loss: 0.0065\n",
      "Epoch [47/50], Train Loss: 0.0024, Val Loss: 0.0064\n",
      "Epoch [48/50], Train Loss: 0.0023, Val Loss: 0.0064\n",
      "Epoch [49/50], Train Loss: 0.0023, Val Loss: 0.0064\n",
      "Epoch [50/50], Train Loss: 0.0023, Val Loss: 0.0063\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=32, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1556, Val Loss: 0.3805\n",
      "Epoch [2/50], Train Loss: 0.1194, Val Loss: 0.3102\n",
      "Epoch [3/50], Train Loss: 0.0896, Val Loss: 0.2426\n",
      "Epoch [4/50], Train Loss: 0.0642, Val Loss: 0.1817\n",
      "Epoch [5/50], Train Loss: 0.0477, Val Loss: 0.1344\n",
      "Epoch [6/50], Train Loss: 0.0411, Val Loss: 0.1061\n",
      "Epoch [7/50], Train Loss: 0.0409, Val Loss: 0.0930\n",
      "Epoch [8/50], Train Loss: 0.0402, Val Loss: 0.0867\n",
      "Epoch [9/50], Train Loss: 0.0392, Val Loss: 0.0834\n",
      "Epoch [10/50], Train Loss: 0.0397, Val Loss: 0.0815\n",
      "Epoch [11/50], Train Loss: 0.0382, Val Loss: 0.0790\n",
      "Epoch [12/50], Train Loss: 0.0369, Val Loss: 0.0757\n",
      "Epoch [13/50], Train Loss: 0.0374, Val Loss: 0.0735\n",
      "Epoch [14/50], Train Loss: 0.0359, Val Loss: 0.0713\n",
      "Epoch [15/50], Train Loss: 0.0368, Val Loss: 0.0691\n",
      "Epoch [16/50], Train Loss: 0.0338, Val Loss: 0.0655\n",
      "Epoch [17/50], Train Loss: 0.0338, Val Loss: 0.0632\n",
      "Epoch [18/50], Train Loss: 0.0326, Val Loss: 0.0611\n",
      "Epoch [19/50], Train Loss: 0.0318, Val Loss: 0.0585\n",
      "Epoch [20/50], Train Loss: 0.0306, Val Loss: 0.0562\n",
      "Epoch [21/50], Train Loss: 0.0292, Val Loss: 0.0526\n",
      "Epoch [22/50], Train Loss: 0.0299, Val Loss: 0.0502\n",
      "Epoch [23/50], Train Loss: 0.0285, Val Loss: 0.0470\n",
      "Epoch [24/50], Train Loss: 0.0275, Val Loss: 0.0441\n",
      "Epoch [25/50], Train Loss: 0.0269, Val Loss: 0.0418\n",
      "Epoch [26/50], Train Loss: 0.0263, Val Loss: 0.0376\n",
      "Epoch [27/50], Train Loss: 0.0255, Val Loss: 0.0352\n",
      "Epoch [28/50], Train Loss: 0.0250, Val Loss: 0.0329\n",
      "Epoch [29/50], Train Loss: 0.0228, Val Loss: 0.0298\n",
      "Epoch [30/50], Train Loss: 0.0227, Val Loss: 0.0278\n",
      "Epoch [31/50], Train Loss: 0.0212, Val Loss: 0.0253\n",
      "Epoch [32/50], Train Loss: 0.0209, Val Loss: 0.0227\n",
      "Epoch [33/50], Train Loss: 0.0192, Val Loss: 0.0205\n",
      "Epoch [34/50], Train Loss: 0.0184, Val Loss: 0.0191\n",
      "Epoch [35/50], Train Loss: 0.0182, Val Loss: 0.0167\n",
      "Epoch [36/50], Train Loss: 0.0163, Val Loss: 0.0147\n",
      "Epoch [37/50], Train Loss: 0.0158, Val Loss: 0.0133\n",
      "Epoch [38/50], Train Loss: 0.0142, Val Loss: 0.0117\n",
      "Epoch [39/50], Train Loss: 0.0134, Val Loss: 0.0102\n",
      "Epoch [40/50], Train Loss: 0.0122, Val Loss: 0.0095\n",
      "Epoch [41/50], Train Loss: 0.0118, Val Loss: 0.0081\n",
      "Epoch [42/50], Train Loss: 0.0108, Val Loss: 0.0078\n",
      "Epoch [43/50], Train Loss: 0.0110, Val Loss: 0.0067\n",
      "Epoch [44/50], Train Loss: 0.0101, Val Loss: 0.0071\n",
      "Epoch [45/50], Train Loss: 0.0104, Val Loss: 0.0070\n",
      "Epoch [46/50], Train Loss: 0.0097, Val Loss: 0.0065\n",
      "Epoch [47/50], Train Loss: 0.0096, Val Loss: 0.0066\n",
      "Epoch [48/50], Train Loss: 0.0096, Val Loss: 0.0065\n",
      "Epoch [49/50], Train Loss: 0.0091, Val Loss: 0.0061\n",
      "Epoch [50/50], Train Loss: 0.0095, Val Loss: 0.0056\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=32, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1455, Val Loss: 0.3305\n",
      "Epoch [2/50], Train Loss: 0.1189, Val Loss: 0.2880\n",
      "Epoch [3/50], Train Loss: 0.1002, Val Loss: 0.2494\n",
      "Epoch [4/50], Train Loss: 0.0844, Val Loss: 0.2143\n",
      "Epoch [5/50], Train Loss: 0.0726, Val Loss: 0.1844\n",
      "Epoch [6/50], Train Loss: 0.0669, Val Loss: 0.1621\n",
      "Epoch [7/50], Train Loss: 0.0631, Val Loss: 0.1460\n",
      "Epoch [8/50], Train Loss: 0.0589, Val Loss: 0.1335\n",
      "Epoch [9/50], Train Loss: 0.0576, Val Loss: 0.1243\n",
      "Epoch [10/50], Train Loss: 0.0557, Val Loss: 0.1185\n",
      "Epoch [11/50], Train Loss: 0.0561, Val Loss: 0.1147\n",
      "Epoch [12/50], Train Loss: 0.0545, Val Loss: 0.1093\n",
      "Epoch [13/50], Train Loss: 0.0539, Val Loss: 0.1069\n",
      "Epoch [14/50], Train Loss: 0.0501, Val Loss: 0.1017\n",
      "Epoch [15/50], Train Loss: 0.0508, Val Loss: 0.0978\n",
      "Epoch [16/50], Train Loss: 0.0513, Val Loss: 0.0938\n",
      "Epoch [17/50], Train Loss: 0.0469, Val Loss: 0.0902\n",
      "Epoch [18/50], Train Loss: 0.0474, Val Loss: 0.0869\n",
      "Epoch [19/50], Train Loss: 0.0470, Val Loss: 0.0835\n",
      "Epoch [20/50], Train Loss: 0.0447, Val Loss: 0.0803\n",
      "Epoch [21/50], Train Loss: 0.0421, Val Loss: 0.0756\n",
      "Epoch [22/50], Train Loss: 0.0416, Val Loss: 0.0708\n",
      "Epoch [23/50], Train Loss: 0.0407, Val Loss: 0.0656\n",
      "Epoch [24/50], Train Loss: 0.0388, Val Loss: 0.0603\n",
      "Epoch [25/50], Train Loss: 0.0389, Val Loss: 0.0561\n",
      "Epoch [26/50], Train Loss: 0.0384, Val Loss: 0.0521\n",
      "Epoch [27/50], Train Loss: 0.0346, Val Loss: 0.0478\n",
      "Epoch [28/50], Train Loss: 0.0336, Val Loss: 0.0420\n",
      "Epoch [29/50], Train Loss: 0.0328, Val Loss: 0.0359\n",
      "Epoch [30/50], Train Loss: 0.0308, Val Loss: 0.0321\n",
      "Epoch [31/50], Train Loss: 0.0302, Val Loss: 0.0278\n",
      "Epoch [32/50], Train Loss: 0.0276, Val Loss: 0.0213\n",
      "Epoch [33/50], Train Loss: 0.0271, Val Loss: 0.0195\n",
      "Epoch [34/50], Train Loss: 0.0245, Val Loss: 0.0183\n",
      "Epoch [35/50], Train Loss: 0.0243, Val Loss: 0.0160\n",
      "Epoch [36/50], Train Loss: 0.0240, Val Loss: 0.0137\n",
      "Epoch [37/50], Train Loss: 0.0229, Val Loss: 0.0159\n",
      "Epoch [38/50], Train Loss: 0.0224, Val Loss: 0.0122\n",
      "Epoch [39/50], Train Loss: 0.0212, Val Loss: 0.0126\n",
      "Epoch [40/50], Train Loss: 0.0215, Val Loss: 0.0120\n",
      "Epoch [41/50], Train Loss: 0.0202, Val Loss: 0.0117\n",
      "Epoch [42/50], Train Loss: 0.0212, Val Loss: 0.0131\n",
      "Epoch [43/50], Train Loss: 0.0202, Val Loss: 0.0120\n",
      "Epoch [44/50], Train Loss: 0.0208, Val Loss: 0.0112\n",
      "Epoch [45/50], Train Loss: 0.0202, Val Loss: 0.0115\n",
      "Epoch [46/50], Train Loss: 0.0199, Val Loss: 0.0092\n",
      "Epoch [47/50], Train Loss: 0.0192, Val Loss: 0.0108\n",
      "Epoch [48/50], Train Loss: 0.0201, Val Loss: 0.0108\n",
      "Epoch [49/50], Train Loss: 0.0191, Val Loss: 0.0117\n",
      "Epoch [50/50], Train Loss: 0.0196, Val Loss: 0.0079\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=64, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1358, Val Loss: 0.3429\n",
      "Epoch [2/50], Train Loss: 0.1103, Val Loss: 0.2908\n",
      "Epoch [3/50], Train Loss: 0.0891, Val Loss: 0.2432\n",
      "Epoch [4/50], Train Loss: 0.0703, Val Loss: 0.1978\n",
      "Epoch [5/50], Train Loss: 0.0537, Val Loss: 0.1539\n",
      "Epoch [6/50], Train Loss: 0.0397, Val Loss: 0.1130\n",
      "Epoch [7/50], Train Loss: 0.0298, Val Loss: 0.0794\n",
      "Epoch [8/50], Train Loss: 0.0248, Val Loss: 0.0574\n",
      "Epoch [9/50], Train Loss: 0.0232, Val Loss: 0.0463\n",
      "Epoch [10/50], Train Loss: 0.0227, Val Loss: 0.0413\n",
      "Epoch [11/50], Train Loss: 0.0222, Val Loss: 0.0383\n",
      "Epoch [12/50], Train Loss: 0.0216, Val Loss: 0.0360\n",
      "Epoch [13/50], Train Loss: 0.0210, Val Loss: 0.0340\n",
      "Epoch [14/50], Train Loss: 0.0204, Val Loss: 0.0320\n",
      "Epoch [15/50], Train Loss: 0.0198, Val Loss: 0.0301\n",
      "Epoch [16/50], Train Loss: 0.0192, Val Loss: 0.0283\n",
      "Epoch [17/50], Train Loss: 0.0185, Val Loss: 0.0265\n",
      "Epoch [18/50], Train Loss: 0.0179, Val Loss: 0.0247\n",
      "Epoch [19/50], Train Loss: 0.0173, Val Loss: 0.0230\n",
      "Epoch [20/50], Train Loss: 0.0167, Val Loss: 0.0212\n",
      "Epoch [21/50], Train Loss: 0.0160, Val Loss: 0.0195\n",
      "Epoch [22/50], Train Loss: 0.0154, Val Loss: 0.0178\n",
      "Epoch [23/50], Train Loss: 0.0147, Val Loss: 0.0162\n",
      "Epoch [24/50], Train Loss: 0.0141, Val Loss: 0.0145\n",
      "Epoch [25/50], Train Loss: 0.0134, Val Loss: 0.0129\n",
      "Epoch [26/50], Train Loss: 0.0127, Val Loss: 0.0114\n",
      "Epoch [27/50], Train Loss: 0.0120, Val Loss: 0.0099\n",
      "Epoch [28/50], Train Loss: 0.0113, Val Loss: 0.0085\n",
      "Epoch [29/50], Train Loss: 0.0106, Val Loss: 0.0072\n",
      "Epoch [30/50], Train Loss: 0.0099, Val Loss: 0.0060\n",
      "Epoch [31/50], Train Loss: 0.0092, Val Loss: 0.0049\n",
      "Epoch [32/50], Train Loss: 0.0085, Val Loss: 0.0040\n",
      "Epoch [33/50], Train Loss: 0.0078, Val Loss: 0.0034\n",
      "Epoch [34/50], Train Loss: 0.0072, Val Loss: 0.0029\n",
      "Epoch [35/50], Train Loss: 0.0066, Val Loss: 0.0027\n",
      "Epoch [36/50], Train Loss: 0.0060, Val Loss: 0.0026\n",
      "Epoch [37/50], Train Loss: 0.0054, Val Loss: 0.0027\n",
      "Epoch [38/50], Train Loss: 0.0049, Val Loss: 0.0030\n",
      "Epoch [39/50], Train Loss: 0.0045, Val Loss: 0.0032\n",
      "Epoch [40/50], Train Loss: 0.0041, Val Loss: 0.0034\n",
      "Epoch [41/50], Train Loss: 0.0038, Val Loss: 0.0036\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=64, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1259, Val Loss: 0.3181\n",
      "Epoch [2/50], Train Loss: 0.1047, Val Loss: 0.2735\n",
      "Epoch [3/50], Train Loss: 0.0869, Val Loss: 0.2336\n",
      "Epoch [4/50], Train Loss: 0.0734, Val Loss: 0.1952\n",
      "Epoch [5/50], Train Loss: 0.0593, Val Loss: 0.1571\n",
      "Epoch [6/50], Train Loss: 0.0470, Val Loss: 0.1198\n",
      "Epoch [7/50], Train Loss: 0.0375, Val Loss: 0.0859\n",
      "Epoch [8/50], Train Loss: 0.0304, Val Loss: 0.0594\n",
      "Epoch [9/50], Train Loss: 0.0267, Val Loss: 0.0430\n",
      "Epoch [10/50], Train Loss: 0.0252, Val Loss: 0.0346\n",
      "Epoch [11/50], Train Loss: 0.0246, Val Loss: 0.0310\n",
      "Epoch [12/50], Train Loss: 0.0238, Val Loss: 0.0281\n",
      "Epoch [13/50], Train Loss: 0.0228, Val Loss: 0.0260\n",
      "Epoch [14/50], Train Loss: 0.0217, Val Loss: 0.0238\n",
      "Epoch [15/50], Train Loss: 0.0218, Val Loss: 0.0217\n",
      "Epoch [16/50], Train Loss: 0.0206, Val Loss: 0.0196\n",
      "Epoch [17/50], Train Loss: 0.0198, Val Loss: 0.0175\n",
      "Epoch [18/50], Train Loss: 0.0189, Val Loss: 0.0150\n",
      "Epoch [19/50], Train Loss: 0.0182, Val Loss: 0.0125\n",
      "Epoch [20/50], Train Loss: 0.0178, Val Loss: 0.0108\n",
      "Epoch [21/50], Train Loss: 0.0166, Val Loss: 0.0092\n",
      "Epoch [22/50], Train Loss: 0.0157, Val Loss: 0.0074\n",
      "Epoch [23/50], Train Loss: 0.0150, Val Loss: 0.0061\n",
      "Epoch [24/50], Train Loss: 0.0141, Val Loss: 0.0046\n",
      "Epoch [25/50], Train Loss: 0.0134, Val Loss: 0.0035\n",
      "Epoch [26/50], Train Loss: 0.0125, Val Loss: 0.0030\n",
      "Epoch [27/50], Train Loss: 0.0120, Val Loss: 0.0025\n",
      "Epoch [28/50], Train Loss: 0.0112, Val Loss: 0.0025\n",
      "Epoch [29/50], Train Loss: 0.0106, Val Loss: 0.0028\n",
      "Epoch [30/50], Train Loss: 0.0096, Val Loss: 0.0036\n",
      "Epoch [31/50], Train Loss: 0.0088, Val Loss: 0.0044\n",
      "Epoch [32/50], Train Loss: 0.0083, Val Loss: 0.0059\n",
      "Epoch [33/50], Train Loss: 0.0078, Val Loss: 0.0065\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=64, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1195, Val Loss: 0.3009\n",
      "Epoch [2/50], Train Loss: 0.1014, Val Loss: 0.2606\n",
      "Epoch [3/50], Train Loss: 0.0857, Val Loss: 0.2243\n",
      "Epoch [4/50], Train Loss: 0.0721, Val Loss: 0.1896\n",
      "Epoch [5/50], Train Loss: 0.0607, Val Loss: 0.1558\n",
      "Epoch [6/50], Train Loss: 0.0504, Val Loss: 0.1244\n",
      "Epoch [7/50], Train Loss: 0.0419, Val Loss: 0.0971\n",
      "Epoch [8/50], Train Loss: 0.0373, Val Loss: 0.0758\n",
      "Epoch [9/50], Train Loss: 0.0356, Val Loss: 0.0620\n",
      "Epoch [10/50], Train Loss: 0.0344, Val Loss: 0.0536\n",
      "Epoch [11/50], Train Loss: 0.0326, Val Loss: 0.0487\n",
      "Epoch [12/50], Train Loss: 0.0323, Val Loss: 0.0446\n",
      "Epoch [13/50], Train Loss: 0.0307, Val Loss: 0.0420\n",
      "Epoch [14/50], Train Loss: 0.0301, Val Loss: 0.0399\n",
      "Epoch [15/50], Train Loss: 0.0292, Val Loss: 0.0385\n",
      "Epoch [16/50], Train Loss: 0.0278, Val Loss: 0.0352\n",
      "Epoch [17/50], Train Loss: 0.0266, Val Loss: 0.0327\n",
      "Epoch [18/50], Train Loss: 0.0262, Val Loss: 0.0311\n",
      "Epoch [19/50], Train Loss: 0.0247, Val Loss: 0.0287\n",
      "Epoch [20/50], Train Loss: 0.0246, Val Loss: 0.0253\n",
      "Epoch [21/50], Train Loss: 0.0228, Val Loss: 0.0240\n",
      "Epoch [22/50], Train Loss: 0.0228, Val Loss: 0.0223\n",
      "Epoch [23/50], Train Loss: 0.0217, Val Loss: 0.0201\n",
      "Epoch [24/50], Train Loss: 0.0216, Val Loss: 0.0172\n",
      "Epoch [25/50], Train Loss: 0.0202, Val Loss: 0.0136\n",
      "Epoch [26/50], Train Loss: 0.0195, Val Loss: 0.0117\n",
      "Epoch [27/50], Train Loss: 0.0187, Val Loss: 0.0103\n",
      "Epoch [28/50], Train Loss: 0.0173, Val Loss: 0.0096\n",
      "Epoch [29/50], Train Loss: 0.0172, Val Loss: 0.0076\n",
      "Epoch [30/50], Train Loss: 0.0166, Val Loss: 0.0067\n",
      "Epoch [31/50], Train Loss: 0.0161, Val Loss: 0.0059\n",
      "Epoch [32/50], Train Loss: 0.0157, Val Loss: 0.0054\n",
      "Epoch [33/50], Train Loss: 0.0143, Val Loss: 0.0044\n",
      "Epoch [34/50], Train Loss: 0.0141, Val Loss: 0.0041\n",
      "Epoch [35/50], Train Loss: 0.0133, Val Loss: 0.0046\n",
      "Epoch [36/50], Train Loss: 0.0126, Val Loss: 0.0040\n",
      "Epoch [37/50], Train Loss: 0.0126, Val Loss: 0.0039\n",
      "Epoch [38/50], Train Loss: 0.0118, Val Loss: 0.0044\n",
      "Epoch [39/50], Train Loss: 0.0115, Val Loss: 0.0037\n",
      "Epoch [40/50], Train Loss: 0.0112, Val Loss: 0.0041\n",
      "Epoch [41/50], Train Loss: 0.0104, Val Loss: 0.0046\n",
      "Epoch [42/50], Train Loss: 0.0103, Val Loss: 0.0042\n",
      "Epoch [43/50], Train Loss: 0.0101, Val Loss: 0.0044\n",
      "Epoch [44/50], Train Loss: 0.0098, Val Loss: 0.0043\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=64, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0893, Val Loss: 0.2590\n",
      "Epoch [2/50], Train Loss: 0.0628, Val Loss: 0.1945\n",
      "Epoch [3/50], Train Loss: 0.0440, Val Loss: 0.1365\n",
      "Epoch [4/50], Train Loss: 0.0327, Val Loss: 0.0921\n",
      "Epoch [5/50], Train Loss: 0.0282, Val Loss: 0.0661\n",
      "Epoch [6/50], Train Loss: 0.0267, Val Loss: 0.0529\n",
      "Epoch [7/50], Train Loss: 0.0253, Val Loss: 0.0448\n",
      "Epoch [8/50], Train Loss: 0.0236, Val Loss: 0.0383\n",
      "Epoch [9/50], Train Loss: 0.0217, Val Loss: 0.0322\n",
      "Epoch [10/50], Train Loss: 0.0198, Val Loss: 0.0263\n",
      "Epoch [11/50], Train Loss: 0.0178, Val Loss: 0.0207\n",
      "Epoch [12/50], Train Loss: 0.0158, Val Loss: 0.0156\n",
      "Epoch [13/50], Train Loss: 0.0138, Val Loss: 0.0112\n",
      "Epoch [14/50], Train Loss: 0.0119, Val Loss: 0.0077\n",
      "Epoch [15/50], Train Loss: 0.0100, Val Loss: 0.0053\n",
      "Epoch [16/50], Train Loss: 0.0084, Val Loss: 0.0040\n",
      "Epoch [17/50], Train Loss: 0.0070, Val Loss: 0.0035\n",
      "Epoch [18/50], Train Loss: 0.0057, Val Loss: 0.0036\n",
      "Epoch [19/50], Train Loss: 0.0047, Val Loss: 0.0038\n",
      "Epoch [20/50], Train Loss: 0.0038, Val Loss: 0.0040\n",
      "Epoch [21/50], Train Loss: 0.0032, Val Loss: 0.0040\n",
      "Epoch [22/50], Train Loss: 0.0027, Val Loss: 0.0038\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=64, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1091, Val Loss: 0.2938\n",
      "Epoch [2/50], Train Loss: 0.0814, Val Loss: 0.2312\n",
      "Epoch [3/50], Train Loss: 0.0593, Val Loss: 0.1713\n",
      "Epoch [4/50], Train Loss: 0.0430, Val Loss: 0.1173\n",
      "Epoch [5/50], Train Loss: 0.0336, Val Loss: 0.0794\n",
      "Epoch [6/50], Train Loss: 0.0306, Val Loss: 0.0610\n",
      "Epoch [7/50], Train Loss: 0.0294, Val Loss: 0.0522\n",
      "Epoch [8/50], Train Loss: 0.0275, Val Loss: 0.0441\n",
      "Epoch [9/50], Train Loss: 0.0259, Val Loss: 0.0376\n",
      "Epoch [10/50], Train Loss: 0.0235, Val Loss: 0.0304\n",
      "Epoch [11/50], Train Loss: 0.0213, Val Loss: 0.0248\n",
      "Epoch [12/50], Train Loss: 0.0195, Val Loss: 0.0192\n",
      "Epoch [13/50], Train Loss: 0.0170, Val Loss: 0.0141\n",
      "Epoch [14/50], Train Loss: 0.0160, Val Loss: 0.0103\n",
      "Epoch [15/50], Train Loss: 0.0145, Val Loss: 0.0070\n",
      "Epoch [16/50], Train Loss: 0.0128, Val Loss: 0.0047\n",
      "Epoch [17/50], Train Loss: 0.0120, Val Loss: 0.0039\n",
      "Epoch [18/50], Train Loss: 0.0110, Val Loss: 0.0033\n",
      "Epoch [19/50], Train Loss: 0.0101, Val Loss: 0.0031\n",
      "Epoch [20/50], Train Loss: 0.0096, Val Loss: 0.0032\n",
      "Epoch [21/50], Train Loss: 0.0084, Val Loss: 0.0033\n",
      "Epoch [22/50], Train Loss: 0.0080, Val Loss: 0.0032\n",
      "Epoch [23/50], Train Loss: 0.0074, Val Loss: 0.0033\n",
      "Epoch [24/50], Train Loss: 0.0067, Val Loss: 0.0036\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=64, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1099, Val Loss: 0.2806\n",
      "Epoch [2/50], Train Loss: 0.0789, Val Loss: 0.2150\n",
      "Epoch [3/50], Train Loss: 0.0582, Val Loss: 0.1568\n",
      "Epoch [4/50], Train Loss: 0.0442, Val Loss: 0.1109\n",
      "Epoch [5/50], Train Loss: 0.0388, Val Loss: 0.0833\n",
      "Epoch [6/50], Train Loss: 0.0366, Val Loss: 0.0702\n",
      "Epoch [7/50], Train Loss: 0.0347, Val Loss: 0.0609\n",
      "Epoch [8/50], Train Loss: 0.0336, Val Loss: 0.0530\n",
      "Epoch [9/50], Train Loss: 0.0315, Val Loss: 0.0473\n",
      "Epoch [10/50], Train Loss: 0.0302, Val Loss: 0.0429\n",
      "Epoch [11/50], Train Loss: 0.0279, Val Loss: 0.0383\n",
      "Epoch [12/50], Train Loss: 0.0259, Val Loss: 0.0325\n",
      "Epoch [13/50], Train Loss: 0.0249, Val Loss: 0.0276\n",
      "Epoch [14/50], Train Loss: 0.0227, Val Loss: 0.0227\n",
      "Epoch [15/50], Train Loss: 0.0212, Val Loss: 0.0181\n",
      "Epoch [16/50], Train Loss: 0.0196, Val Loss: 0.0136\n",
      "Epoch [17/50], Train Loss: 0.0179, Val Loss: 0.0112\n",
      "Epoch [18/50], Train Loss: 0.0168, Val Loss: 0.0090\n",
      "Epoch [19/50], Train Loss: 0.0155, Val Loss: 0.0081\n",
      "Epoch [20/50], Train Loss: 0.0143, Val Loss: 0.0060\n",
      "Epoch [21/50], Train Loss: 0.0134, Val Loss: 0.0071\n",
      "Epoch [22/50], Train Loss: 0.0134, Val Loss: 0.0055\n",
      "Epoch [23/50], Train Loss: 0.0119, Val Loss: 0.0056\n",
      "Epoch [24/50], Train Loss: 0.0125, Val Loss: 0.0048\n",
      "Epoch [25/50], Train Loss: 0.0121, Val Loss: 0.0046\n",
      "Epoch [26/50], Train Loss: 0.0115, Val Loss: 0.0044\n",
      "Epoch [27/50], Train Loss: 0.0121, Val Loss: 0.0040\n",
      "Epoch [28/50], Train Loss: 0.0110, Val Loss: 0.0045\n",
      "Epoch [29/50], Train Loss: 0.0114, Val Loss: 0.0041\n",
      "Epoch [30/50], Train Loss: 0.0111, Val Loss: 0.0041\n",
      "Epoch [31/50], Train Loss: 0.0108, Val Loss: 0.0038\n",
      "Epoch [32/50], Train Loss: 0.0112, Val Loss: 0.0046\n",
      "Epoch [33/50], Train Loss: 0.0109, Val Loss: 0.0033\n",
      "Epoch [34/50], Train Loss: 0.0106, Val Loss: 0.0040\n",
      "Epoch [35/50], Train Loss: 0.0105, Val Loss: 0.0039\n",
      "Epoch [36/50], Train Loss: 0.0104, Val Loss: 0.0033\n",
      "Epoch [37/50], Train Loss: 0.0100, Val Loss: 0.0031\n",
      "Epoch [38/50], Train Loss: 0.0099, Val Loss: 0.0039\n",
      "Epoch [39/50], Train Loss: 0.0100, Val Loss: 0.0031\n",
      "Epoch [40/50], Train Loss: 0.0104, Val Loss: 0.0034\n",
      "Epoch [41/50], Train Loss: 0.0100, Val Loss: 0.0034\n",
      "Epoch [42/50], Train Loss: 0.0100, Val Loss: 0.0039\n",
      "Epoch [43/50], Train Loss: 0.0097, Val Loss: 0.0039\n",
      "Epoch [44/50], Train Loss: 0.0104, Val Loss: 0.0029\n",
      "Epoch [45/50], Train Loss: 0.0095, Val Loss: 0.0038\n",
      "Epoch [46/50], Train Loss: 0.0092, Val Loss: 0.0031\n",
      "Epoch [47/50], Train Loss: 0.0094, Val Loss: 0.0034\n",
      "Epoch [48/50], Train Loss: 0.0093, Val Loss: 0.0035\n",
      "Epoch [49/50], Train Loss: 0.0088, Val Loss: 0.0027\n",
      "Epoch [50/50], Train Loss: 0.0094, Val Loss: 0.0029\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=64, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0966, Val Loss: 0.2555\n",
      "Epoch [2/50], Train Loss: 0.0565, Val Loss: 0.1617\n",
      "Epoch [3/50], Train Loss: 0.0370, Val Loss: 0.0985\n",
      "Epoch [4/50], Train Loss: 0.0344, Val Loss: 0.0764\n",
      "Epoch [5/50], Train Loss: 0.0343, Val Loss: 0.0700\n",
      "Epoch [6/50], Train Loss: 0.0327, Val Loss: 0.0639\n",
      "Epoch [7/50], Train Loss: 0.0309, Val Loss: 0.0568\n",
      "Epoch [8/50], Train Loss: 0.0289, Val Loss: 0.0491\n",
      "Epoch [9/50], Train Loss: 0.0267, Val Loss: 0.0407\n",
      "Epoch [10/50], Train Loss: 0.0242, Val Loss: 0.0318\n",
      "Epoch [11/50], Train Loss: 0.0214, Val Loss: 0.0228\n",
      "Epoch [12/50], Train Loss: 0.0183, Val Loss: 0.0147\n",
      "Epoch [13/50], Train Loss: 0.0151, Val Loss: 0.0089\n",
      "Epoch [14/50], Train Loss: 0.0123, Val Loss: 0.0060\n",
      "Epoch [15/50], Train Loss: 0.0101, Val Loss: 0.0052\n",
      "Epoch [16/50], Train Loss: 0.0084, Val Loss: 0.0052\n",
      "Epoch [17/50], Train Loss: 0.0068, Val Loss: 0.0052\n",
      "Epoch [18/50], Train Loss: 0.0053, Val Loss: 0.0049\n",
      "Epoch [19/50], Train Loss: 0.0040, Val Loss: 0.0044\n",
      "Epoch [20/50], Train Loss: 0.0032, Val Loss: 0.0039\n",
      "Epoch [21/50], Train Loss: 0.0028, Val Loss: 0.0037\n",
      "Epoch [22/50], Train Loss: 0.0026, Val Loss: 0.0036\n",
      "Epoch [23/50], Train Loss: 0.0026, Val Loss: 0.0036\n",
      "Epoch [24/50], Train Loss: 0.0025, Val Loss: 0.0036\n",
      "Epoch [25/50], Train Loss: 0.0025, Val Loss: 0.0036\n",
      "Epoch [26/50], Train Loss: 0.0025, Val Loss: 0.0036\n",
      "Epoch [27/50], Train Loss: 0.0024, Val Loss: 0.0035\n",
      "Epoch [28/50], Train Loss: 0.0024, Val Loss: 0.0035\n",
      "Epoch [29/50], Train Loss: 0.0024, Val Loss: 0.0036\n",
      "Epoch [30/50], Train Loss: 0.0024, Val Loss: 0.0035\n",
      "Epoch [31/50], Train Loss: 0.0024, Val Loss: 0.0035\n",
      "Epoch [32/50], Train Loss: 0.0024, Val Loss: 0.0035\n",
      "Epoch [33/50], Train Loss: 0.0023, Val Loss: 0.0035\n",
      "Epoch [34/50], Train Loss: 0.0023, Val Loss: 0.0035\n",
      "Epoch [35/50], Train Loss: 0.0023, Val Loss: 0.0035\n",
      "Epoch [36/50], Train Loss: 0.0023, Val Loss: 0.0035\n",
      "Epoch [37/50], Train Loss: 0.0023, Val Loss: 0.0035\n",
      "Epoch [38/50], Train Loss: 0.0023, Val Loss: 0.0035\n",
      "Epoch [39/50], Train Loss: 0.0023, Val Loss: 0.0035\n",
      "Epoch [40/50], Train Loss: 0.0023, Val Loss: 0.0035\n",
      "Epoch [41/50], Train Loss: 0.0023, Val Loss: 0.0035\n",
      "Epoch [42/50], Train Loss: 0.0022, Val Loss: 0.0035\n",
      "Epoch [43/50], Train Loss: 0.0022, Val Loss: 0.0035\n",
      "Epoch [44/50], Train Loss: 0.0022, Val Loss: 0.0035\n",
      "Epoch [45/50], Train Loss: 0.0022, Val Loss: 0.0035\n",
      "Epoch [46/50], Train Loss: 0.0022, Val Loss: 0.0034\n",
      "Epoch [47/50], Train Loss: 0.0022, Val Loss: 0.0034\n",
      "Epoch [48/50], Train Loss: 0.0022, Val Loss: 0.0034\n",
      "Epoch [49/50], Train Loss: 0.0022, Val Loss: 0.0034\n",
      "Epoch [50/50], Train Loss: 0.0022, Val Loss: 0.0034\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=64, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0966, Val Loss: 0.2394\n",
      "Epoch [2/50], Train Loss: 0.0601, Val Loss: 0.1658\n",
      "Epoch [3/50], Train Loss: 0.0412, Val Loss: 0.1120\n",
      "Epoch [4/50], Train Loss: 0.0362, Val Loss: 0.0882\n",
      "Epoch [5/50], Train Loss: 0.0360, Val Loss: 0.0810\n",
      "Epoch [6/50], Train Loss: 0.0340, Val Loss: 0.0753\n",
      "Epoch [7/50], Train Loss: 0.0319, Val Loss: 0.0692\n",
      "Epoch [8/50], Train Loss: 0.0315, Val Loss: 0.0618\n",
      "Epoch [9/50], Train Loss: 0.0282, Val Loss: 0.0525\n",
      "Epoch [10/50], Train Loss: 0.0261, Val Loss: 0.0417\n",
      "Epoch [11/50], Train Loss: 0.0228, Val Loss: 0.0299\n",
      "Epoch [12/50], Train Loss: 0.0188, Val Loss: 0.0181\n",
      "Epoch [13/50], Train Loss: 0.0148, Val Loss: 0.0091\n",
      "Epoch [14/50], Train Loss: 0.0116, Val Loss: 0.0064\n",
      "Epoch [15/50], Train Loss: 0.0097, Val Loss: 0.0058\n",
      "Epoch [16/50], Train Loss: 0.0088, Val Loss: 0.0049\n",
      "Epoch [17/50], Train Loss: 0.0079, Val Loss: 0.0046\n",
      "Epoch [18/50], Train Loss: 0.0070, Val Loss: 0.0038\n",
      "Epoch [19/50], Train Loss: 0.0068, Val Loss: 0.0036\n",
      "Epoch [20/50], Train Loss: 0.0063, Val Loss: 0.0036\n",
      "Epoch [21/50], Train Loss: 0.0061, Val Loss: 0.0039\n",
      "Epoch [22/50], Train Loss: 0.0060, Val Loss: 0.0032\n",
      "Epoch [23/50], Train Loss: 0.0059, Val Loss: 0.0032\n",
      "Epoch [24/50], Train Loss: 0.0059, Val Loss: 0.0035\n",
      "Epoch [25/50], Train Loss: 0.0059, Val Loss: 0.0039\n",
      "Epoch [26/50], Train Loss: 0.0060, Val Loss: 0.0030\n",
      "Epoch [27/50], Train Loss: 0.0057, Val Loss: 0.0034\n",
      "Epoch [28/50], Train Loss: 0.0058, Val Loss: 0.0035\n",
      "Epoch [29/50], Train Loss: 0.0055, Val Loss: 0.0034\n",
      "Epoch [30/50], Train Loss: 0.0054, Val Loss: 0.0031\n",
      "Epoch [31/50], Train Loss: 0.0055, Val Loss: 0.0031\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=64, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0871, Val Loss: 0.2228\n",
      "Epoch [2/50], Train Loss: 0.0552, Val Loss: 0.1418\n",
      "Epoch [3/50], Train Loss: 0.0458, Val Loss: 0.0967\n",
      "Epoch [4/50], Train Loss: 0.0456, Val Loss: 0.0812\n",
      "Epoch [5/50], Train Loss: 0.0445, Val Loss: 0.0750\n",
      "Epoch [6/50], Train Loss: 0.0424, Val Loss: 0.0684\n",
      "Epoch [7/50], Train Loss: 0.0402, Val Loss: 0.0603\n",
      "Epoch [8/50], Train Loss: 0.0388, Val Loss: 0.0525\n",
      "Epoch [9/50], Train Loss: 0.0372, Val Loss: 0.0421\n",
      "Epoch [10/50], Train Loss: 0.0337, Val Loss: 0.0365\n",
      "Epoch [11/50], Train Loss: 0.0310, Val Loss: 0.0274\n",
      "Epoch [12/50], Train Loss: 0.0285, Val Loss: 0.0223\n",
      "Epoch [13/50], Train Loss: 0.0278, Val Loss: 0.0152\n",
      "Epoch [14/50], Train Loss: 0.0249, Val Loss: 0.0131\n",
      "Epoch [15/50], Train Loss: 0.0233, Val Loss: 0.0088\n",
      "Epoch [16/50], Train Loss: 0.0229, Val Loss: 0.0085\n",
      "Epoch [17/50], Train Loss: 0.0213, Val Loss: 0.0075\n",
      "Epoch [18/50], Train Loss: 0.0201, Val Loss: 0.0064\n",
      "Epoch [19/50], Train Loss: 0.0181, Val Loss: 0.0060\n",
      "Epoch [20/50], Train Loss: 0.0167, Val Loss: 0.0061\n",
      "Epoch [21/50], Train Loss: 0.0159, Val Loss: 0.0055\n",
      "Epoch [22/50], Train Loss: 0.0153, Val Loss: 0.0047\n",
      "Epoch [23/50], Train Loss: 0.0143, Val Loss: 0.0057\n",
      "Epoch [24/50], Train Loss: 0.0127, Val Loss: 0.0046\n",
      "Epoch [25/50], Train Loss: 0.0137, Val Loss: 0.0056\n",
      "Epoch [26/50], Train Loss: 0.0134, Val Loss: 0.0049\n",
      "Epoch [27/50], Train Loss: 0.0128, Val Loss: 0.0049\n",
      "Epoch [28/50], Train Loss: 0.0123, Val Loss: 0.0044\n",
      "Epoch [29/50], Train Loss: 0.0116, Val Loss: 0.0050\n",
      "Epoch [30/50], Train Loss: 0.0120, Val Loss: 0.0046\n",
      "Epoch [31/50], Train Loss: 0.0124, Val Loss: 0.0047\n",
      "Epoch [32/50], Train Loss: 0.0116, Val Loss: 0.0032\n",
      "Epoch [33/50], Train Loss: 0.0118, Val Loss: 0.0048\n",
      "Epoch [34/50], Train Loss: 0.0118, Val Loss: 0.0051\n",
      "Epoch [35/50], Train Loss: 0.0113, Val Loss: 0.0043\n",
      "Epoch [36/50], Train Loss: 0.0117, Val Loss: 0.0047\n",
      "Epoch [37/50], Train Loss: 0.0108, Val Loss: 0.0036\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=128, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1332, Val Loss: 0.3323\n",
      "Epoch [2/50], Train Loss: 0.0897, Val Loss: 0.2369\n",
      "Epoch [3/50], Train Loss: 0.0555, Val Loss: 0.1465\n",
      "Epoch [4/50], Train Loss: 0.0315, Val Loss: 0.0723\n",
      "Epoch [5/50], Train Loss: 0.0257, Val Loss: 0.0470\n",
      "Epoch [6/50], Train Loss: 0.0259, Val Loss: 0.0446\n",
      "Epoch [7/50], Train Loss: 0.0242, Val Loss: 0.0405\n",
      "Epoch [8/50], Train Loss: 0.0227, Val Loss: 0.0360\n",
      "Epoch [9/50], Train Loss: 0.0214, Val Loss: 0.0318\n",
      "Epoch [10/50], Train Loss: 0.0201, Val Loss: 0.0277\n",
      "Epoch [11/50], Train Loss: 0.0188, Val Loss: 0.0237\n",
      "Epoch [12/50], Train Loss: 0.0175, Val Loss: 0.0199\n",
      "Epoch [13/50], Train Loss: 0.0161, Val Loss: 0.0164\n",
      "Epoch [14/50], Train Loss: 0.0148, Val Loss: 0.0131\n",
      "Epoch [15/50], Train Loss: 0.0135, Val Loss: 0.0102\n",
      "Epoch [16/50], Train Loss: 0.0121, Val Loss: 0.0079\n",
      "Epoch [17/50], Train Loss: 0.0108, Val Loss: 0.0060\n",
      "Epoch [18/50], Train Loss: 0.0095, Val Loss: 0.0048\n",
      "Epoch [19/50], Train Loss: 0.0083, Val Loss: 0.0040\n",
      "Epoch [20/50], Train Loss: 0.0070, Val Loss: 0.0036\n",
      "Epoch [21/50], Train Loss: 0.0058, Val Loss: 0.0037\n",
      "Epoch [22/50], Train Loss: 0.0046, Val Loss: 0.0044\n",
      "Epoch [23/50], Train Loss: 0.0036, Val Loss: 0.0054\n",
      "Epoch [24/50], Train Loss: 0.0030, Val Loss: 0.0062\n",
      "Epoch [25/50], Train Loss: 0.0028, Val Loss: 0.0066\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=128, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0815, Val Loss: 0.2235\n",
      "Epoch [2/50], Train Loss: 0.0585, Val Loss: 0.1657\n",
      "Epoch [3/50], Train Loss: 0.0421, Val Loss: 0.1145\n",
      "Epoch [4/50], Train Loss: 0.0320, Val Loss: 0.0749\n",
      "Epoch [5/50], Train Loss: 0.0286, Val Loss: 0.0531\n",
      "Epoch [6/50], Train Loss: 0.0271, Val Loss: 0.0434\n",
      "Epoch [7/50], Train Loss: 0.0258, Val Loss: 0.0382\n",
      "Epoch [8/50], Train Loss: 0.0248, Val Loss: 0.0334\n",
      "Epoch [9/50], Train Loss: 0.0233, Val Loss: 0.0285\n",
      "Epoch [10/50], Train Loss: 0.0216, Val Loss: 0.0235\n",
      "Epoch [11/50], Train Loss: 0.0203, Val Loss: 0.0189\n",
      "Epoch [12/50], Train Loss: 0.0185, Val Loss: 0.0141\n",
      "Epoch [13/50], Train Loss: 0.0170, Val Loss: 0.0101\n",
      "Epoch [14/50], Train Loss: 0.0154, Val Loss: 0.0064\n",
      "Epoch [15/50], Train Loss: 0.0136, Val Loss: 0.0038\n",
      "Epoch [16/50], Train Loss: 0.0120, Val Loss: 0.0025\n",
      "Epoch [17/50], Train Loss: 0.0108, Val Loss: 0.0030\n",
      "Epoch [18/50], Train Loss: 0.0094, Val Loss: 0.0046\n",
      "Epoch [19/50], Train Loss: 0.0083, Val Loss: 0.0070\n",
      "Epoch [20/50], Train Loss: 0.0077, Val Loss: 0.0083\n",
      "Epoch [21/50], Train Loss: 0.0066, Val Loss: 0.0091\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=128, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1051, Val Loss: 0.2836\n",
      "Epoch [2/50], Train Loss: 0.0784, Val Loss: 0.2192\n",
      "Epoch [3/50], Train Loss: 0.0575, Val Loss: 0.1565\n",
      "Epoch [4/50], Train Loss: 0.0401, Val Loss: 0.0992\n",
      "Epoch [5/50], Train Loss: 0.0335, Val Loss: 0.0648\n",
      "Epoch [6/50], Train Loss: 0.0322, Val Loss: 0.0546\n",
      "Epoch [7/50], Train Loss: 0.0304, Val Loss: 0.0501\n",
      "Epoch [8/50], Train Loss: 0.0288, Val Loss: 0.0471\n",
      "Epoch [9/50], Train Loss: 0.0281, Val Loss: 0.0395\n",
      "Epoch [10/50], Train Loss: 0.0265, Val Loss: 0.0334\n",
      "Epoch [11/50], Train Loss: 0.0249, Val Loss: 0.0299\n",
      "Epoch [12/50], Train Loss: 0.0232, Val Loss: 0.0232\n",
      "Epoch [13/50], Train Loss: 0.0219, Val Loss: 0.0189\n",
      "Epoch [14/50], Train Loss: 0.0206, Val Loss: 0.0161\n",
      "Epoch [15/50], Train Loss: 0.0187, Val Loss: 0.0121\n",
      "Epoch [16/50], Train Loss: 0.0174, Val Loss: 0.0091\n",
      "Epoch [17/50], Train Loss: 0.0158, Val Loss: 0.0058\n",
      "Epoch [18/50], Train Loss: 0.0148, Val Loss: 0.0042\n",
      "Epoch [19/50], Train Loss: 0.0123, Val Loss: 0.0028\n",
      "Epoch [20/50], Train Loss: 0.0111, Val Loss: 0.0027\n",
      "Epoch [21/50], Train Loss: 0.0100, Val Loss: 0.0041\n",
      "Epoch [22/50], Train Loss: 0.0091, Val Loss: 0.0042\n",
      "Epoch [23/50], Train Loss: 0.0086, Val Loss: 0.0050\n",
      "Epoch [24/50], Train Loss: 0.0083, Val Loss: 0.0047\n",
      "Epoch [25/50], Train Loss: 0.0076, Val Loss: 0.0047\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=128, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1282, Val Loss: 0.2952\n",
      "Epoch [2/50], Train Loss: 0.0645, Val Loss: 0.1521\n",
      "Epoch [3/50], Train Loss: 0.0298, Val Loss: 0.0585\n",
      "Epoch [4/50], Train Loss: 0.0305, Val Loss: 0.0503\n",
      "Epoch [5/50], Train Loss: 0.0283, Val Loss: 0.0455\n",
      "Epoch [6/50], Train Loss: 0.0255, Val Loss: 0.0387\n",
      "Epoch [7/50], Train Loss: 0.0230, Val Loss: 0.0322\n",
      "Epoch [8/50], Train Loss: 0.0203, Val Loss: 0.0256\n",
      "Epoch [9/50], Train Loss: 0.0175, Val Loss: 0.0191\n",
      "Epoch [10/50], Train Loss: 0.0144, Val Loss: 0.0130\n",
      "Epoch [11/50], Train Loss: 0.0111, Val Loss: 0.0078\n",
      "Epoch [12/50], Train Loss: 0.0078, Val Loss: 0.0046\n",
      "Epoch [13/50], Train Loss: 0.0051, Val Loss: 0.0036\n",
      "Epoch [14/50], Train Loss: 0.0037, Val Loss: 0.0030\n",
      "Epoch [15/50], Train Loss: 0.0030, Val Loss: 0.0027\n",
      "Epoch [16/50], Train Loss: 0.0026, Val Loss: 0.0025\n",
      "Epoch [17/50], Train Loss: 0.0024, Val Loss: 0.0024\n",
      "Epoch [18/50], Train Loss: 0.0024, Val Loss: 0.0024\n",
      "Epoch [19/50], Train Loss: 0.0023, Val Loss: 0.0025\n",
      "Epoch [20/50], Train Loss: 0.0023, Val Loss: 0.0025\n",
      "Epoch [21/50], Train Loss: 0.0023, Val Loss: 0.0025\n",
      "Epoch [22/50], Train Loss: 0.0023, Val Loss: 0.0025\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=128, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1058, Val Loss: 0.2490\n",
      "Epoch [2/50], Train Loss: 0.0530, Val Loss: 0.1297\n",
      "Epoch [3/50], Train Loss: 0.0334, Val Loss: 0.0668\n",
      "Epoch [4/50], Train Loss: 0.0326, Val Loss: 0.0561\n",
      "Epoch [5/50], Train Loss: 0.0293, Val Loss: 0.0467\n",
      "Epoch [6/50], Train Loss: 0.0262, Val Loss: 0.0369\n",
      "Epoch [7/50], Train Loss: 0.0233, Val Loss: 0.0276\n",
      "Epoch [8/50], Train Loss: 0.0196, Val Loss: 0.0184\n",
      "Epoch [9/50], Train Loss: 0.0162, Val Loss: 0.0106\n",
      "Epoch [10/50], Train Loss: 0.0123, Val Loss: 0.0049\n",
      "Epoch [11/50], Train Loss: 0.0094, Val Loss: 0.0028\n",
      "Epoch [12/50], Train Loss: 0.0073, Val Loss: 0.0026\n",
      "Epoch [13/50], Train Loss: 0.0061, Val Loss: 0.0024\n",
      "Epoch [14/50], Train Loss: 0.0050, Val Loss: 0.0021\n",
      "Epoch [15/50], Train Loss: 0.0046, Val Loss: 0.0024\n",
      "Epoch [16/50], Train Loss: 0.0043, Val Loss: 0.0021\n",
      "Epoch [17/50], Train Loss: 0.0041, Val Loss: 0.0023\n",
      "Epoch [18/50], Train Loss: 0.0043, Val Loss: 0.0024\n",
      "Epoch [19/50], Train Loss: 0.0042, Val Loss: 0.0022\n",
      "Epoch [20/50], Train Loss: 0.0039, Val Loss: 0.0022\n",
      "Epoch [21/50], Train Loss: 0.0040, Val Loss: 0.0024\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=128, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1114, Val Loss: 0.2601\n",
      "Epoch [2/50], Train Loss: 0.0566, Val Loss: 0.1235\n",
      "Epoch [3/50], Train Loss: 0.0361, Val Loss: 0.0557\n",
      "Epoch [4/50], Train Loss: 0.0362, Val Loss: 0.0514\n",
      "Epoch [5/50], Train Loss: 0.0326, Val Loss: 0.0440\n",
      "Epoch [6/50], Train Loss: 0.0307, Val Loss: 0.0367\n",
      "Epoch [7/50], Train Loss: 0.0284, Val Loss: 0.0299\n",
      "Epoch [8/50], Train Loss: 0.0250, Val Loss: 0.0215\n",
      "Epoch [9/50], Train Loss: 0.0229, Val Loss: 0.0162\n",
      "Epoch [10/50], Train Loss: 0.0199, Val Loss: 0.0103\n",
      "Epoch [11/50], Train Loss: 0.0181, Val Loss: 0.0062\n",
      "Epoch [12/50], Train Loss: 0.0159, Val Loss: 0.0038\n",
      "Epoch [13/50], Train Loss: 0.0136, Val Loss: 0.0029\n",
      "Epoch [14/50], Train Loss: 0.0120, Val Loss: 0.0028\n",
      "Epoch [15/50], Train Loss: 0.0112, Val Loss: 0.0034\n",
      "Epoch [16/50], Train Loss: 0.0100, Val Loss: 0.0027\n",
      "Epoch [17/50], Train Loss: 0.0092, Val Loss: 0.0040\n",
      "Epoch [18/50], Train Loss: 0.0087, Val Loss: 0.0033\n",
      "Epoch [19/50], Train Loss: 0.0084, Val Loss: 0.0023\n",
      "Epoch [20/50], Train Loss: 0.0082, Val Loss: 0.0034\n",
      "Epoch [21/50], Train Loss: 0.0083, Val Loss: 0.0034\n",
      "Epoch [22/50], Train Loss: 0.0079, Val Loss: 0.0019\n",
      "Epoch [23/50], Train Loss: 0.0083, Val Loss: 0.0037\n",
      "Epoch [24/50], Train Loss: 0.0077, Val Loss: 0.0028\n",
      "Epoch [25/50], Train Loss: 0.0077, Val Loss: 0.0023\n",
      "Epoch [26/50], Train Loss: 0.0075, Val Loss: 0.0025\n",
      "Epoch [27/50], Train Loss: 0.0078, Val Loss: 0.0031\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=128, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0817, Val Loss: 0.1826\n",
      "Epoch [2/50], Train Loss: 0.0337, Val Loss: 0.0746\n",
      "Epoch [3/50], Train Loss: 0.0370, Val Loss: 0.0697\n",
      "Epoch [4/50], Train Loss: 0.0329, Val Loss: 0.0596\n",
      "Epoch [5/50], Train Loss: 0.0298, Val Loss: 0.0491\n",
      "Epoch [6/50], Train Loss: 0.0264, Val Loss: 0.0377\n",
      "Epoch [7/50], Train Loss: 0.0225, Val Loss: 0.0256\n",
      "Epoch [8/50], Train Loss: 0.0178, Val Loss: 0.0137\n",
      "Epoch [9/50], Train Loss: 0.0123, Val Loss: 0.0048\n",
      "Epoch [10/50], Train Loss: 0.0069, Val Loss: 0.0020\n",
      "Epoch [11/50], Train Loss: 0.0038, Val Loss: 0.0033\n",
      "Epoch [12/50], Train Loss: 0.0032, Val Loss: 0.0033\n",
      "Epoch [13/50], Train Loss: 0.0026, Val Loss: 0.0028\n",
      "Epoch [14/50], Train Loss: 0.0028, Val Loss: 0.0032\n",
      "Epoch [15/50], Train Loss: 0.0027, Val Loss: 0.0039\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=128, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0819, Val Loss: 0.1645\n",
      "Epoch [2/50], Train Loss: 0.0342, Val Loss: 0.0557\n",
      "Epoch [3/50], Train Loss: 0.0417, Val Loss: 0.0612\n",
      "Epoch [4/50], Train Loss: 0.0348, Val Loss: 0.0494\n",
      "Epoch [5/50], Train Loss: 0.0322, Val Loss: 0.0373\n",
      "Epoch [6/50], Train Loss: 0.0284, Val Loss: 0.0262\n",
      "Epoch [7/50], Train Loss: 0.0244, Val Loss: 0.0136\n",
      "Epoch [8/50], Train Loss: 0.0203, Val Loss: 0.0060\n",
      "Epoch [9/50], Train Loss: 0.0152, Val Loss: 0.0030\n",
      "Epoch [10/50], Train Loss: 0.0122, Val Loss: 0.0036\n",
      "Epoch [11/50], Train Loss: 0.0103, Val Loss: 0.0049\n",
      "Epoch [12/50], Train Loss: 0.0080, Val Loss: 0.0036\n",
      "Epoch [13/50], Train Loss: 0.0063, Val Loss: 0.0018\n",
      "Epoch [14/50], Train Loss: 0.0058, Val Loss: 0.0024\n",
      "Epoch [15/50], Train Loss: 0.0057, Val Loss: 0.0059\n",
      "Epoch [16/50], Train Loss: 0.0055, Val Loss: 0.0034\n",
      "Epoch [17/50], Train Loss: 0.0055, Val Loss: 0.0025\n",
      "Epoch [18/50], Train Loss: 0.0055, Val Loss: 0.0031\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adam, hidden_size=128, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0832, Val Loss: 0.1924\n",
      "Epoch [2/50], Train Loss: 0.0429, Val Loss: 0.0921\n",
      "Epoch [3/50], Train Loss: 0.0405, Val Loss: 0.0697\n",
      "Epoch [4/50], Train Loss: 0.0388, Val Loss: 0.0593\n",
      "Epoch [5/50], Train Loss: 0.0348, Val Loss: 0.0470\n",
      "Epoch [6/50], Train Loss: 0.0313, Val Loss: 0.0323\n",
      "Epoch [7/50], Train Loss: 0.0260, Val Loss: 0.0182\n",
      "Epoch [8/50], Train Loss: 0.0210, Val Loss: 0.0073\n",
      "Epoch [9/50], Train Loss: 0.0159, Val Loss: 0.0043\n",
      "Epoch [10/50], Train Loss: 0.0129, Val Loss: 0.0041\n",
      "Epoch [11/50], Train Loss: 0.0109, Val Loss: 0.0056\n",
      "Epoch [12/50], Train Loss: 0.0099, Val Loss: 0.0029\n",
      "Epoch [13/50], Train Loss: 0.0095, Val Loss: 0.0030\n",
      "Epoch [14/50], Train Loss: 0.0091, Val Loss: 0.0050\n",
      "Epoch [15/50], Train Loss: 0.0089, Val Loss: 0.0035\n",
      "Epoch [16/50], Train Loss: 0.0090, Val Loss: 0.0025\n",
      "Epoch [17/50], Train Loss: 0.0082, Val Loss: 0.0046\n",
      "Epoch [18/50], Train Loss: 0.0088, Val Loss: 0.0041\n",
      "Epoch [19/50], Train Loss: 0.0083, Val Loss: 0.0027\n",
      "Epoch [20/50], Train Loss: 0.0083, Val Loss: 0.0038\n",
      "Epoch [21/50], Train Loss: 0.0083, Val Loss: 0.0069\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=16, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1541, Val Loss: 0.3823\n",
      "Epoch [2/50], Train Loss: 0.1534, Val Loss: 0.3811\n",
      "Epoch [3/50], Train Loss: 0.1527, Val Loss: 0.3799\n",
      "Epoch [4/50], Train Loss: 0.1520, Val Loss: 0.3787\n",
      "Epoch [5/50], Train Loss: 0.1514, Val Loss: 0.3776\n",
      "Epoch [6/50], Train Loss: 0.1507, Val Loss: 0.3764\n",
      "Epoch [7/50], Train Loss: 0.1500, Val Loss: 0.3752\n",
      "Epoch [8/50], Train Loss: 0.1493, Val Loss: 0.3740\n",
      "Epoch [9/50], Train Loss: 0.1487, Val Loss: 0.3728\n",
      "Epoch [10/50], Train Loss: 0.1480, Val Loss: 0.3716\n",
      "Epoch [11/50], Train Loss: 0.1473, Val Loss: 0.3705\n",
      "Epoch [12/50], Train Loss: 0.1467, Val Loss: 0.3693\n",
      "Epoch [13/50], Train Loss: 0.1460, Val Loss: 0.3682\n",
      "Epoch [14/50], Train Loss: 0.1454, Val Loss: 0.3670\n",
      "Epoch [15/50], Train Loss: 0.1447, Val Loss: 0.3659\n",
      "Epoch [16/50], Train Loss: 0.1441, Val Loss: 0.3647\n",
      "Epoch [17/50], Train Loss: 0.1434, Val Loss: 0.3636\n",
      "Epoch [18/50], Train Loss: 0.1428, Val Loss: 0.3625\n",
      "Epoch [19/50], Train Loss: 0.1422, Val Loss: 0.3614\n",
      "Epoch [20/50], Train Loss: 0.1415, Val Loss: 0.3603\n",
      "Epoch [21/50], Train Loss: 0.1409, Val Loss: 0.3591\n",
      "Epoch [22/50], Train Loss: 0.1403, Val Loss: 0.3580\n",
      "Epoch [23/50], Train Loss: 0.1397, Val Loss: 0.3569\n",
      "Epoch [24/50], Train Loss: 0.1391, Val Loss: 0.3558\n",
      "Epoch [25/50], Train Loss: 0.1384, Val Loss: 0.3547\n",
      "Epoch [26/50], Train Loss: 0.1378, Val Loss: 0.3537\n",
      "Epoch [27/50], Train Loss: 0.1372, Val Loss: 0.3525\n",
      "Epoch [28/50], Train Loss: 0.1366, Val Loss: 0.3515\n",
      "Epoch [29/50], Train Loss: 0.1360, Val Loss: 0.3504\n",
      "Epoch [30/50], Train Loss: 0.1354, Val Loss: 0.3493\n",
      "Epoch [31/50], Train Loss: 0.1348, Val Loss: 0.3482\n",
      "Epoch [32/50], Train Loss: 0.1342, Val Loss: 0.3472\n",
      "Epoch [33/50], Train Loss: 0.1336, Val Loss: 0.3461\n",
      "Epoch [34/50], Train Loss: 0.1331, Val Loss: 0.3451\n",
      "Epoch [35/50], Train Loss: 0.1325, Val Loss: 0.3440\n",
      "Epoch [36/50], Train Loss: 0.1319, Val Loss: 0.3430\n",
      "Epoch [37/50], Train Loss: 0.1313, Val Loss: 0.3420\n",
      "Epoch [38/50], Train Loss: 0.1308, Val Loss: 0.3409\n",
      "Epoch [39/50], Train Loss: 0.1302, Val Loss: 0.3399\n",
      "Epoch [40/50], Train Loss: 0.1296, Val Loss: 0.3389\n",
      "Epoch [41/50], Train Loss: 0.1291, Val Loss: 0.3378\n",
      "Epoch [42/50], Train Loss: 0.1285, Val Loss: 0.3368\n",
      "Epoch [43/50], Train Loss: 0.1279, Val Loss: 0.3358\n",
      "Epoch [44/50], Train Loss: 0.1274, Val Loss: 0.3348\n",
      "Epoch [45/50], Train Loss: 0.1268, Val Loss: 0.3338\n",
      "Epoch [46/50], Train Loss: 0.1263, Val Loss: 0.3328\n",
      "Epoch [47/50], Train Loss: 0.1257, Val Loss: 0.3318\n",
      "Epoch [48/50], Train Loss: 0.1252, Val Loss: 0.3308\n",
      "Epoch [49/50], Train Loss: 0.1247, Val Loss: 0.3298\n",
      "Epoch [50/50], Train Loss: 0.1241, Val Loss: 0.3288\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=16, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1534, Val Loss: 0.4198\n",
      "Epoch [2/50], Train Loss: 0.1526, Val Loss: 0.4186\n",
      "Epoch [3/50], Train Loss: 0.1531, Val Loss: 0.4174\n",
      "Epoch [4/50], Train Loss: 0.1507, Val Loss: 0.4163\n",
      "Epoch [5/50], Train Loss: 0.1519, Val Loss: 0.4152\n",
      "Epoch [6/50], Train Loss: 0.1492, Val Loss: 0.4140\n",
      "Epoch [7/50], Train Loss: 0.1500, Val Loss: 0.4129\n",
      "Epoch [8/50], Train Loss: 0.1474, Val Loss: 0.4118\n",
      "Epoch [9/50], Train Loss: 0.1484, Val Loss: 0.4106\n",
      "Epoch [10/50], Train Loss: 0.1479, Val Loss: 0.4095\n",
      "Epoch [11/50], Train Loss: 0.1477, Val Loss: 0.4084\n",
      "Epoch [12/50], Train Loss: 0.1464, Val Loss: 0.4073\n",
      "Epoch [13/50], Train Loss: 0.1462, Val Loss: 0.4061\n",
      "Epoch [14/50], Train Loss: 0.1437, Val Loss: 0.4050\n",
      "Epoch [15/50], Train Loss: 0.1443, Val Loss: 0.4039\n",
      "Epoch [16/50], Train Loss: 0.1458, Val Loss: 0.4028\n",
      "Epoch [17/50], Train Loss: 0.1441, Val Loss: 0.4017\n",
      "Epoch [18/50], Train Loss: 0.1431, Val Loss: 0.4006\n",
      "Epoch [19/50], Train Loss: 0.1432, Val Loss: 0.3995\n",
      "Epoch [20/50], Train Loss: 0.1415, Val Loss: 0.3984\n",
      "Epoch [21/50], Train Loss: 0.1421, Val Loss: 0.3973\n",
      "Epoch [22/50], Train Loss: 0.1414, Val Loss: 0.3962\n",
      "Epoch [23/50], Train Loss: 0.1396, Val Loss: 0.3951\n",
      "Epoch [24/50], Train Loss: 0.1389, Val Loss: 0.3941\n",
      "Epoch [25/50], Train Loss: 0.1388, Val Loss: 0.3930\n",
      "Epoch [26/50], Train Loss: 0.1390, Val Loss: 0.3920\n",
      "Epoch [27/50], Train Loss: 0.1390, Val Loss: 0.3909\n",
      "Epoch [28/50], Train Loss: 0.1372, Val Loss: 0.3898\n",
      "Epoch [29/50], Train Loss: 0.1387, Val Loss: 0.3888\n",
      "Epoch [30/50], Train Loss: 0.1374, Val Loss: 0.3877\n",
      "Epoch [31/50], Train Loss: 0.1372, Val Loss: 0.3866\n",
      "Epoch [32/50], Train Loss: 0.1362, Val Loss: 0.3856\n",
      "Epoch [33/50], Train Loss: 0.1345, Val Loss: 0.3846\n",
      "Epoch [34/50], Train Loss: 0.1339, Val Loss: 0.3835\n",
      "Epoch [35/50], Train Loss: 0.1345, Val Loss: 0.3825\n",
      "Epoch [36/50], Train Loss: 0.1348, Val Loss: 0.3814\n",
      "Epoch [37/50], Train Loss: 0.1344, Val Loss: 0.3804\n",
      "Epoch [38/50], Train Loss: 0.1333, Val Loss: 0.3794\n",
      "Epoch [39/50], Train Loss: 0.1327, Val Loss: 0.3784\n",
      "Epoch [40/50], Train Loss: 0.1334, Val Loss: 0.3774\n",
      "Epoch [41/50], Train Loss: 0.1317, Val Loss: 0.3763\n",
      "Epoch [42/50], Train Loss: 0.1302, Val Loss: 0.3753\n",
      "Epoch [43/50], Train Loss: 0.1305, Val Loss: 0.3743\n",
      "Epoch [44/50], Train Loss: 0.1314, Val Loss: 0.3733\n",
      "Epoch [45/50], Train Loss: 0.1288, Val Loss: 0.3723\n",
      "Epoch [46/50], Train Loss: 0.1300, Val Loss: 0.3713\n",
      "Epoch [47/50], Train Loss: 0.1297, Val Loss: 0.3703\n",
      "Epoch [48/50], Train Loss: 0.1284, Val Loss: 0.3693\n",
      "Epoch [49/50], Train Loss: 0.1281, Val Loss: 0.3684\n",
      "Epoch [50/50], Train Loss: 0.1283, Val Loss: 0.3673\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=16, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1685, Val Loss: 0.4442\n",
      "Epoch [2/50], Train Loss: 0.1674, Val Loss: 0.4431\n",
      "Epoch [3/50], Train Loss: 0.1670, Val Loss: 0.4421\n",
      "Epoch [4/50], Train Loss: 0.1658, Val Loss: 0.4410\n",
      "Epoch [5/50], Train Loss: 0.1660, Val Loss: 0.4399\n",
      "Epoch [6/50], Train Loss: 0.1646, Val Loss: 0.4389\n",
      "Epoch [7/50], Train Loss: 0.1646, Val Loss: 0.4378\n",
      "Epoch [8/50], Train Loss: 0.1648, Val Loss: 0.4367\n",
      "Epoch [9/50], Train Loss: 0.1616, Val Loss: 0.4357\n",
      "Epoch [10/50], Train Loss: 0.1631, Val Loss: 0.4347\n",
      "Epoch [11/50], Train Loss: 0.1639, Val Loss: 0.4336\n",
      "Epoch [12/50], Train Loss: 0.1640, Val Loss: 0.4326\n",
      "Epoch [13/50], Train Loss: 0.1619, Val Loss: 0.4315\n",
      "Epoch [14/50], Train Loss: 0.1605, Val Loss: 0.4305\n",
      "Epoch [15/50], Train Loss: 0.1621, Val Loss: 0.4295\n",
      "Epoch [16/50], Train Loss: 0.1600, Val Loss: 0.4284\n",
      "Epoch [17/50], Train Loss: 0.1597, Val Loss: 0.4274\n",
      "Epoch [18/50], Train Loss: 0.1591, Val Loss: 0.4264\n",
      "Epoch [19/50], Train Loss: 0.1574, Val Loss: 0.4254\n",
      "Epoch [20/50], Train Loss: 0.1576, Val Loss: 0.4244\n",
      "Epoch [21/50], Train Loss: 0.1571, Val Loss: 0.4234\n",
      "Epoch [22/50], Train Loss: 0.1578, Val Loss: 0.4225\n",
      "Epoch [23/50], Train Loss: 0.1581, Val Loss: 0.4214\n",
      "Epoch [24/50], Train Loss: 0.1576, Val Loss: 0.4205\n",
      "Epoch [25/50], Train Loss: 0.1556, Val Loss: 0.4195\n",
      "Epoch [26/50], Train Loss: 0.1554, Val Loss: 0.4185\n",
      "Epoch [27/50], Train Loss: 0.1558, Val Loss: 0.4175\n",
      "Epoch [28/50], Train Loss: 0.1555, Val Loss: 0.4165\n",
      "Epoch [29/50], Train Loss: 0.1528, Val Loss: 0.4156\n",
      "Epoch [30/50], Train Loss: 0.1556, Val Loss: 0.4146\n",
      "Epoch [31/50], Train Loss: 0.1532, Val Loss: 0.4136\n",
      "Epoch [32/50], Train Loss: 0.1525, Val Loss: 0.4127\n",
      "Epoch [33/50], Train Loss: 0.1525, Val Loss: 0.4117\n",
      "Epoch [34/50], Train Loss: 0.1515, Val Loss: 0.4108\n",
      "Epoch [35/50], Train Loss: 0.1507, Val Loss: 0.4098\n",
      "Epoch [36/50], Train Loss: 0.1509, Val Loss: 0.4089\n",
      "Epoch [37/50], Train Loss: 0.1508, Val Loss: 0.4080\n",
      "Epoch [38/50], Train Loss: 0.1500, Val Loss: 0.4070\n",
      "Epoch [39/50], Train Loss: 0.1497, Val Loss: 0.4061\n",
      "Epoch [40/50], Train Loss: 0.1493, Val Loss: 0.4052\n",
      "Epoch [41/50], Train Loss: 0.1495, Val Loss: 0.4042\n",
      "Epoch [42/50], Train Loss: 0.1466, Val Loss: 0.4033\n",
      "Epoch [43/50], Train Loss: 0.1488, Val Loss: 0.4024\n",
      "Epoch [44/50], Train Loss: 0.1476, Val Loss: 0.4015\n",
      "Epoch [45/50], Train Loss: 0.1456, Val Loss: 0.4006\n",
      "Epoch [46/50], Train Loss: 0.1444, Val Loss: 0.3997\n",
      "Epoch [47/50], Train Loss: 0.1462, Val Loss: 0.3988\n",
      "Epoch [48/50], Train Loss: 0.1446, Val Loss: 0.3979\n",
      "Epoch [49/50], Train Loss: 0.1443, Val Loss: 0.3970\n",
      "Epoch [50/50], Train Loss: 0.1450, Val Loss: 0.3961\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=16, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.2009, Val Loss: 0.4230\n",
      "Epoch [2/50], Train Loss: 0.1997, Val Loss: 0.4212\n",
      "Epoch [3/50], Train Loss: 0.1985, Val Loss: 0.4195\n",
      "Epoch [4/50], Train Loss: 0.1974, Val Loss: 0.4178\n",
      "Epoch [5/50], Train Loss: 0.1962, Val Loss: 0.4161\n",
      "Epoch [6/50], Train Loss: 0.1951, Val Loss: 0.4144\n",
      "Epoch [7/50], Train Loss: 0.1940, Val Loss: 0.4127\n",
      "Epoch [8/50], Train Loss: 0.1928, Val Loss: 0.4110\n",
      "Epoch [9/50], Train Loss: 0.1917, Val Loss: 0.4093\n",
      "Epoch [10/50], Train Loss: 0.1906, Val Loss: 0.4077\n",
      "Epoch [11/50], Train Loss: 0.1895, Val Loss: 0.4061\n",
      "Epoch [12/50], Train Loss: 0.1885, Val Loss: 0.4044\n",
      "Epoch [13/50], Train Loss: 0.1874, Val Loss: 0.4028\n",
      "Epoch [14/50], Train Loss: 0.1863, Val Loss: 0.4012\n",
      "Epoch [15/50], Train Loss: 0.1853, Val Loss: 0.3996\n",
      "Epoch [16/50], Train Loss: 0.1842, Val Loss: 0.3980\n",
      "Epoch [17/50], Train Loss: 0.1832, Val Loss: 0.3965\n",
      "Epoch [18/50], Train Loss: 0.1821, Val Loss: 0.3949\n",
      "Epoch [19/50], Train Loss: 0.1811, Val Loss: 0.3934\n",
      "Epoch [20/50], Train Loss: 0.1801, Val Loss: 0.3918\n",
      "Epoch [21/50], Train Loss: 0.1791, Val Loss: 0.3903\n",
      "Epoch [22/50], Train Loss: 0.1781, Val Loss: 0.3888\n",
      "Epoch [23/50], Train Loss: 0.1771, Val Loss: 0.3872\n",
      "Epoch [24/50], Train Loss: 0.1761, Val Loss: 0.3858\n",
      "Epoch [25/50], Train Loss: 0.1751, Val Loss: 0.3843\n",
      "Epoch [26/50], Train Loss: 0.1742, Val Loss: 0.3828\n",
      "Epoch [27/50], Train Loss: 0.1732, Val Loss: 0.3813\n",
      "Epoch [28/50], Train Loss: 0.1723, Val Loss: 0.3798\n",
      "Epoch [29/50], Train Loss: 0.1713, Val Loss: 0.3784\n",
      "Epoch [30/50], Train Loss: 0.1704, Val Loss: 0.3770\n",
      "Epoch [31/50], Train Loss: 0.1694, Val Loss: 0.3755\n",
      "Epoch [32/50], Train Loss: 0.1685, Val Loss: 0.3741\n",
      "Epoch [33/50], Train Loss: 0.1676, Val Loss: 0.3727\n",
      "Epoch [34/50], Train Loss: 0.1667, Val Loss: 0.3713\n",
      "Epoch [35/50], Train Loss: 0.1658, Val Loss: 0.3699\n",
      "Epoch [36/50], Train Loss: 0.1649, Val Loss: 0.3685\n",
      "Epoch [37/50], Train Loss: 0.1640, Val Loss: 0.3671\n",
      "Epoch [38/50], Train Loss: 0.1631, Val Loss: 0.3658\n",
      "Epoch [39/50], Train Loss: 0.1622, Val Loss: 0.3644\n",
      "Epoch [40/50], Train Loss: 0.1613, Val Loss: 0.3630\n",
      "Epoch [41/50], Train Loss: 0.1605, Val Loss: 0.3617\n",
      "Epoch [42/50], Train Loss: 0.1596, Val Loss: 0.3603\n",
      "Epoch [43/50], Train Loss: 0.1588, Val Loss: 0.3590\n",
      "Epoch [44/50], Train Loss: 0.1579, Val Loss: 0.3577\n",
      "Epoch [45/50], Train Loss: 0.1571, Val Loss: 0.3564\n",
      "Epoch [46/50], Train Loss: 0.1563, Val Loss: 0.3551\n",
      "Epoch [47/50], Train Loss: 0.1554, Val Loss: 0.3538\n",
      "Epoch [48/50], Train Loss: 0.1546, Val Loss: 0.3525\n",
      "Epoch [49/50], Train Loss: 0.1538, Val Loss: 0.3512\n",
      "Epoch [50/50], Train Loss: 0.1530, Val Loss: 0.3499\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=16, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1980, Val Loss: 0.4944\n",
      "Epoch [2/50], Train Loss: 0.1969, Val Loss: 0.4931\n",
      "Epoch [3/50], Train Loss: 0.1958, Val Loss: 0.4918\n",
      "Epoch [4/50], Train Loss: 0.1959, Val Loss: 0.4906\n",
      "Epoch [5/50], Train Loss: 0.1947, Val Loss: 0.4893\n",
      "Epoch [6/50], Train Loss: 0.1929, Val Loss: 0.4881\n",
      "Epoch [7/50], Train Loss: 0.1936, Val Loss: 0.4868\n",
      "Epoch [8/50], Train Loss: 0.1919, Val Loss: 0.4856\n",
      "Epoch [9/50], Train Loss: 0.1918, Val Loss: 0.4844\n",
      "Epoch [10/50], Train Loss: 0.1913, Val Loss: 0.4831\n",
      "Epoch [11/50], Train Loss: 0.1912, Val Loss: 0.4819\n",
      "Epoch [12/50], Train Loss: 0.1903, Val Loss: 0.4807\n",
      "Epoch [13/50], Train Loss: 0.1890, Val Loss: 0.4795\n",
      "Epoch [14/50], Train Loss: 0.1888, Val Loss: 0.4783\n",
      "Epoch [15/50], Train Loss: 0.1882, Val Loss: 0.4771\n",
      "Epoch [16/50], Train Loss: 0.1871, Val Loss: 0.4759\n",
      "Epoch [17/50], Train Loss: 0.1862, Val Loss: 0.4747\n",
      "Epoch [18/50], Train Loss: 0.1858, Val Loss: 0.4735\n",
      "Epoch [19/50], Train Loss: 0.1845, Val Loss: 0.4723\n",
      "Epoch [20/50], Train Loss: 0.1845, Val Loss: 0.4711\n",
      "Epoch [21/50], Train Loss: 0.1842, Val Loss: 0.4700\n",
      "Epoch [22/50], Train Loss: 0.1825, Val Loss: 0.4688\n",
      "Epoch [23/50], Train Loss: 0.1814, Val Loss: 0.4676\n",
      "Epoch [24/50], Train Loss: 0.1814, Val Loss: 0.4665\n",
      "Epoch [25/50], Train Loss: 0.1814, Val Loss: 0.4653\n",
      "Epoch [26/50], Train Loss: 0.1809, Val Loss: 0.4641\n",
      "Epoch [27/50], Train Loss: 0.1792, Val Loss: 0.4630\n",
      "Epoch [28/50], Train Loss: 0.1793, Val Loss: 0.4618\n",
      "Epoch [29/50], Train Loss: 0.1776, Val Loss: 0.4607\n",
      "Epoch [30/50], Train Loss: 0.1776, Val Loss: 0.4596\n",
      "Epoch [31/50], Train Loss: 0.1766, Val Loss: 0.4584\n",
      "Epoch [32/50], Train Loss: 0.1764, Val Loss: 0.4573\n",
      "Epoch [33/50], Train Loss: 0.1751, Val Loss: 0.4562\n",
      "Epoch [34/50], Train Loss: 0.1746, Val Loss: 0.4550\n",
      "Epoch [35/50], Train Loss: 0.1742, Val Loss: 0.4539\n",
      "Epoch [36/50], Train Loss: 0.1736, Val Loss: 0.4528\n",
      "Epoch [37/50], Train Loss: 0.1735, Val Loss: 0.4517\n",
      "Epoch [38/50], Train Loss: 0.1728, Val Loss: 0.4506\n",
      "Epoch [39/50], Train Loss: 0.1728, Val Loss: 0.4495\n",
      "Epoch [40/50], Train Loss: 0.1711, Val Loss: 0.4484\n",
      "Epoch [41/50], Train Loss: 0.1708, Val Loss: 0.4473\n",
      "Epoch [42/50], Train Loss: 0.1696, Val Loss: 0.4462\n",
      "Epoch [43/50], Train Loss: 0.1686, Val Loss: 0.4451\n",
      "Epoch [44/50], Train Loss: 0.1692, Val Loss: 0.4440\n",
      "Epoch [45/50], Train Loss: 0.1694, Val Loss: 0.4430\n",
      "Epoch [46/50], Train Loss: 0.1682, Val Loss: 0.4419\n",
      "Epoch [47/50], Train Loss: 0.1675, Val Loss: 0.4408\n",
      "Epoch [48/50], Train Loss: 0.1666, Val Loss: 0.4398\n",
      "Epoch [49/50], Train Loss: 0.1662, Val Loss: 0.4387\n",
      "Epoch [50/50], Train Loss: 0.1652, Val Loss: 0.4377\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=16, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1319, Val Loss: 0.3493\n",
      "Epoch [2/50], Train Loss: 0.1365, Val Loss: 0.3481\n",
      "Epoch [3/50], Train Loss: 0.1335, Val Loss: 0.3470\n",
      "Epoch [4/50], Train Loss: 0.1299, Val Loss: 0.3459\n",
      "Epoch [5/50], Train Loss: 0.1332, Val Loss: 0.3448\n",
      "Epoch [6/50], Train Loss: 0.1351, Val Loss: 0.3437\n",
      "Epoch [7/50], Train Loss: 0.1329, Val Loss: 0.3427\n",
      "Epoch [8/50], Train Loss: 0.1292, Val Loss: 0.3416\n",
      "Epoch [9/50], Train Loss: 0.1300, Val Loss: 0.3405\n",
      "Epoch [10/50], Train Loss: 0.1283, Val Loss: 0.3395\n",
      "Epoch [11/50], Train Loss: 0.1347, Val Loss: 0.3384\n",
      "Epoch [12/50], Train Loss: 0.1281, Val Loss: 0.3374\n",
      "Epoch [13/50], Train Loss: 0.1301, Val Loss: 0.3363\n",
      "Epoch [14/50], Train Loss: 0.1295, Val Loss: 0.3353\n",
      "Epoch [15/50], Train Loss: 0.1289, Val Loss: 0.3342\n",
      "Epoch [16/50], Train Loss: 0.1289, Val Loss: 0.3332\n",
      "Epoch [17/50], Train Loss: 0.1278, Val Loss: 0.3322\n",
      "Epoch [18/50], Train Loss: 0.1263, Val Loss: 0.3312\n",
      "Epoch [19/50], Train Loss: 0.1250, Val Loss: 0.3302\n",
      "Epoch [20/50], Train Loss: 0.1252, Val Loss: 0.3292\n",
      "Epoch [21/50], Train Loss: 0.1276, Val Loss: 0.3282\n",
      "Epoch [22/50], Train Loss: 0.1259, Val Loss: 0.3272\n",
      "Epoch [23/50], Train Loss: 0.1244, Val Loss: 0.3263\n",
      "Epoch [24/50], Train Loss: 0.1249, Val Loss: 0.3253\n",
      "Epoch [25/50], Train Loss: 0.1221, Val Loss: 0.3243\n",
      "Epoch [26/50], Train Loss: 0.1196, Val Loss: 0.3234\n",
      "Epoch [27/50], Train Loss: 0.1198, Val Loss: 0.3224\n",
      "Epoch [28/50], Train Loss: 0.1202, Val Loss: 0.3215\n",
      "Epoch [29/50], Train Loss: 0.1209, Val Loss: 0.3206\n",
      "Epoch [30/50], Train Loss: 0.1211, Val Loss: 0.3197\n",
      "Epoch [31/50], Train Loss: 0.1203, Val Loss: 0.3187\n",
      "Epoch [32/50], Train Loss: 0.1201, Val Loss: 0.3178\n",
      "Epoch [33/50], Train Loss: 0.1165, Val Loss: 0.3169\n",
      "Epoch [34/50], Train Loss: 0.1165, Val Loss: 0.3160\n",
      "Epoch [35/50], Train Loss: 0.1187, Val Loss: 0.3151\n",
      "Epoch [36/50], Train Loss: 0.1183, Val Loss: 0.3142\n",
      "Epoch [37/50], Train Loss: 0.1169, Val Loss: 0.3133\n",
      "Epoch [38/50], Train Loss: 0.1173, Val Loss: 0.3125\n",
      "Epoch [39/50], Train Loss: 0.1151, Val Loss: 0.3116\n",
      "Epoch [40/50], Train Loss: 0.1161, Val Loss: 0.3107\n",
      "Epoch [41/50], Train Loss: 0.1162, Val Loss: 0.3099\n",
      "Epoch [42/50], Train Loss: 0.1165, Val Loss: 0.3090\n",
      "Epoch [43/50], Train Loss: 0.1142, Val Loss: 0.3081\n",
      "Epoch [44/50], Train Loss: 0.1143, Val Loss: 0.3073\n",
      "Epoch [45/50], Train Loss: 0.1136, Val Loss: 0.3065\n",
      "Epoch [46/50], Train Loss: 0.1131, Val Loss: 0.3056\n",
      "Epoch [47/50], Train Loss: 0.1141, Val Loss: 0.3048\n",
      "Epoch [48/50], Train Loss: 0.1137, Val Loss: 0.3039\n",
      "Epoch [49/50], Train Loss: 0.1100, Val Loss: 0.3031\n",
      "Epoch [50/50], Train Loss: 0.1108, Val Loss: 0.3023\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=16, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1861, Val Loss: 0.4367\n",
      "Epoch [2/50], Train Loss: 0.1850, Val Loss: 0.4350\n",
      "Epoch [3/50], Train Loss: 0.1839, Val Loss: 0.4332\n",
      "Epoch [4/50], Train Loss: 0.1828, Val Loss: 0.4314\n",
      "Epoch [5/50], Train Loss: 0.1817, Val Loss: 0.4297\n",
      "Epoch [6/50], Train Loss: 0.1806, Val Loss: 0.4279\n",
      "Epoch [7/50], Train Loss: 0.1795, Val Loss: 0.4262\n",
      "Epoch [8/50], Train Loss: 0.1784, Val Loss: 0.4244\n",
      "Epoch [9/50], Train Loss: 0.1774, Val Loss: 0.4228\n",
      "Epoch [10/50], Train Loss: 0.1763, Val Loss: 0.4211\n",
      "Epoch [11/50], Train Loss: 0.1753, Val Loss: 0.4194\n",
      "Epoch [12/50], Train Loss: 0.1742, Val Loss: 0.4177\n",
      "Epoch [13/50], Train Loss: 0.1732, Val Loss: 0.4160\n",
      "Epoch [14/50], Train Loss: 0.1722, Val Loss: 0.4144\n",
      "Epoch [15/50], Train Loss: 0.1712, Val Loss: 0.4127\n",
      "Epoch [16/50], Train Loss: 0.1701, Val Loss: 0.4111\n",
      "Epoch [17/50], Train Loss: 0.1692, Val Loss: 0.4095\n",
      "Epoch [18/50], Train Loss: 0.1682, Val Loss: 0.4079\n",
      "Epoch [19/50], Train Loss: 0.1672, Val Loss: 0.4063\n",
      "Epoch [20/50], Train Loss: 0.1662, Val Loss: 0.4047\n",
      "Epoch [21/50], Train Loss: 0.1652, Val Loss: 0.4031\n",
      "Epoch [22/50], Train Loss: 0.1643, Val Loss: 0.4015\n",
      "Epoch [23/50], Train Loss: 0.1633, Val Loss: 0.4000\n",
      "Epoch [24/50], Train Loss: 0.1624, Val Loss: 0.3984\n",
      "Epoch [25/50], Train Loss: 0.1615, Val Loss: 0.3969\n",
      "Epoch [26/50], Train Loss: 0.1605, Val Loss: 0.3953\n",
      "Epoch [27/50], Train Loss: 0.1596, Val Loss: 0.3938\n",
      "Epoch [28/50], Train Loss: 0.1587, Val Loss: 0.3923\n",
      "Epoch [29/50], Train Loss: 0.1578, Val Loss: 0.3908\n",
      "Epoch [30/50], Train Loss: 0.1569, Val Loss: 0.3893\n",
      "Epoch [31/50], Train Loss: 0.1560, Val Loss: 0.3878\n",
      "Epoch [32/50], Train Loss: 0.1551, Val Loss: 0.3864\n",
      "Epoch [33/50], Train Loss: 0.1542, Val Loss: 0.3849\n",
      "Epoch [34/50], Train Loss: 0.1534, Val Loss: 0.3834\n",
      "Epoch [35/50], Train Loss: 0.1525, Val Loss: 0.3820\n",
      "Epoch [36/50], Train Loss: 0.1517, Val Loss: 0.3806\n",
      "Epoch [37/50], Train Loss: 0.1508, Val Loss: 0.3792\n",
      "Epoch [38/50], Train Loss: 0.1500, Val Loss: 0.3777\n",
      "Epoch [39/50], Train Loss: 0.1491, Val Loss: 0.3763\n",
      "Epoch [40/50], Train Loss: 0.1483, Val Loss: 0.3749\n",
      "Epoch [41/50], Train Loss: 0.1475, Val Loss: 0.3735\n",
      "Epoch [42/50], Train Loss: 0.1467, Val Loss: 0.3722\n",
      "Epoch [43/50], Train Loss: 0.1458, Val Loss: 0.3708\n",
      "Epoch [44/50], Train Loss: 0.1450, Val Loss: 0.3694\n",
      "Epoch [45/50], Train Loss: 0.1442, Val Loss: 0.3680\n",
      "Epoch [46/50], Train Loss: 0.1434, Val Loss: 0.3667\n",
      "Epoch [47/50], Train Loss: 0.1427, Val Loss: 0.3654\n",
      "Epoch [48/50], Train Loss: 0.1419, Val Loss: 0.3640\n",
      "Epoch [49/50], Train Loss: 0.1411, Val Loss: 0.3627\n",
      "Epoch [50/50], Train Loss: 0.1403, Val Loss: 0.3614\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=16, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.2173, Val Loss: 0.4391\n",
      "Epoch [2/50], Train Loss: 0.2157, Val Loss: 0.4376\n",
      "Epoch [3/50], Train Loss: 0.2155, Val Loss: 0.4361\n",
      "Epoch [4/50], Train Loss: 0.2135, Val Loss: 0.4346\n",
      "Epoch [5/50], Train Loss: 0.2120, Val Loss: 0.4331\n",
      "Epoch [6/50], Train Loss: 0.2121, Val Loss: 0.4316\n",
      "Epoch [7/50], Train Loss: 0.2106, Val Loss: 0.4302\n",
      "Epoch [8/50], Train Loss: 0.2098, Val Loss: 0.4287\n",
      "Epoch [9/50], Train Loss: 0.2083, Val Loss: 0.4273\n",
      "Epoch [10/50], Train Loss: 0.2082, Val Loss: 0.4258\n",
      "Epoch [11/50], Train Loss: 0.2068, Val Loss: 0.4244\n",
      "Epoch [12/50], Train Loss: 0.2063, Val Loss: 0.4229\n",
      "Epoch [13/50], Train Loss: 0.2048, Val Loss: 0.4215\n",
      "Epoch [14/50], Train Loss: 0.2029, Val Loss: 0.4201\n",
      "Epoch [15/50], Train Loss: 0.2030, Val Loss: 0.4187\n",
      "Epoch [16/50], Train Loss: 0.2011, Val Loss: 0.4174\n",
      "Epoch [17/50], Train Loss: 0.2015, Val Loss: 0.4160\n",
      "Epoch [18/50], Train Loss: 0.2002, Val Loss: 0.4146\n",
      "Epoch [19/50], Train Loss: 0.1987, Val Loss: 0.4133\n",
      "Epoch [20/50], Train Loss: 0.1988, Val Loss: 0.4119\n",
      "Epoch [21/50], Train Loss: 0.1960, Val Loss: 0.4105\n",
      "Epoch [22/50], Train Loss: 0.1959, Val Loss: 0.4092\n",
      "Epoch [23/50], Train Loss: 0.1961, Val Loss: 0.4079\n",
      "Epoch [24/50], Train Loss: 0.1924, Val Loss: 0.4066\n",
      "Epoch [25/50], Train Loss: 0.1934, Val Loss: 0.4053\n",
      "Epoch [26/50], Train Loss: 0.1923, Val Loss: 0.4040\n",
      "Epoch [27/50], Train Loss: 0.1899, Val Loss: 0.4027\n",
      "Epoch [28/50], Train Loss: 0.1897, Val Loss: 0.4014\n",
      "Epoch [29/50], Train Loss: 0.1882, Val Loss: 0.4001\n",
      "Epoch [30/50], Train Loss: 0.1877, Val Loss: 0.3988\n",
      "Epoch [31/50], Train Loss: 0.1866, Val Loss: 0.3976\n",
      "Epoch [32/50], Train Loss: 0.1860, Val Loss: 0.3963\n",
      "Epoch [33/50], Train Loss: 0.1859, Val Loss: 0.3951\n",
      "Epoch [34/50], Train Loss: 0.1838, Val Loss: 0.3938\n",
      "Epoch [35/50], Train Loss: 0.1828, Val Loss: 0.3926\n",
      "Epoch [36/50], Train Loss: 0.1824, Val Loss: 0.3914\n",
      "Epoch [37/50], Train Loss: 0.1826, Val Loss: 0.3902\n",
      "Epoch [38/50], Train Loss: 0.1825, Val Loss: 0.3890\n",
      "Epoch [39/50], Train Loss: 0.1803, Val Loss: 0.3878\n",
      "Epoch [40/50], Train Loss: 0.1801, Val Loss: 0.3866\n",
      "Epoch [41/50], Train Loss: 0.1786, Val Loss: 0.3854\n",
      "Epoch [42/50], Train Loss: 0.1773, Val Loss: 0.3842\n",
      "Epoch [43/50], Train Loss: 0.1756, Val Loss: 0.3830\n",
      "Epoch [44/50], Train Loss: 0.1747, Val Loss: 0.3819\n",
      "Epoch [45/50], Train Loss: 0.1757, Val Loss: 0.3807\n",
      "Epoch [46/50], Train Loss: 0.1750, Val Loss: 0.3796\n",
      "Epoch [47/50], Train Loss: 0.1741, Val Loss: 0.3784\n",
      "Epoch [48/50], Train Loss: 0.1733, Val Loss: 0.3773\n",
      "Epoch [49/50], Train Loss: 0.1722, Val Loss: 0.3761\n",
      "Epoch [50/50], Train Loss: 0.1704, Val Loss: 0.3750\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=16, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1598, Val Loss: 0.3864\n",
      "Epoch [2/50], Train Loss: 0.1552, Val Loss: 0.3854\n",
      "Epoch [3/50], Train Loss: 0.1546, Val Loss: 0.3843\n",
      "Epoch [4/50], Train Loss: 0.1566, Val Loss: 0.3833\n",
      "Epoch [5/50], Train Loss: 0.1585, Val Loss: 0.3823\n",
      "Epoch [6/50], Train Loss: 0.1533, Val Loss: 0.3813\n",
      "Epoch [7/50], Train Loss: 0.1541, Val Loss: 0.3803\n",
      "Epoch [8/50], Train Loss: 0.1563, Val Loss: 0.3792\n",
      "Epoch [9/50], Train Loss: 0.1530, Val Loss: 0.3782\n",
      "Epoch [10/50], Train Loss: 0.1519, Val Loss: 0.3772\n",
      "Epoch [11/50], Train Loss: 0.1547, Val Loss: 0.3762\n",
      "Epoch [12/50], Train Loss: 0.1520, Val Loss: 0.3752\n",
      "Epoch [13/50], Train Loss: 0.1523, Val Loss: 0.3742\n",
      "Epoch [14/50], Train Loss: 0.1475, Val Loss: 0.3733\n",
      "Epoch [15/50], Train Loss: 0.1491, Val Loss: 0.3723\n",
      "Epoch [16/50], Train Loss: 0.1491, Val Loss: 0.3713\n",
      "Epoch [17/50], Train Loss: 0.1483, Val Loss: 0.3704\n",
      "Epoch [18/50], Train Loss: 0.1460, Val Loss: 0.3694\n",
      "Epoch [19/50], Train Loss: 0.1471, Val Loss: 0.3685\n",
      "Epoch [20/50], Train Loss: 0.1487, Val Loss: 0.3675\n",
      "Epoch [21/50], Train Loss: 0.1472, Val Loss: 0.3666\n",
      "Epoch [22/50], Train Loss: 0.1448, Val Loss: 0.3657\n",
      "Epoch [23/50], Train Loss: 0.1487, Val Loss: 0.3647\n",
      "Epoch [24/50], Train Loss: 0.1456, Val Loss: 0.3638\n",
      "Epoch [25/50], Train Loss: 0.1458, Val Loss: 0.3629\n",
      "Epoch [26/50], Train Loss: 0.1434, Val Loss: 0.3620\n",
      "Epoch [27/50], Train Loss: 0.1413, Val Loss: 0.3611\n",
      "Epoch [28/50], Train Loss: 0.1418, Val Loss: 0.3602\n",
      "Epoch [29/50], Train Loss: 0.1434, Val Loss: 0.3593\n",
      "Epoch [30/50], Train Loss: 0.1422, Val Loss: 0.3584\n",
      "Epoch [31/50], Train Loss: 0.1428, Val Loss: 0.3575\n",
      "Epoch [32/50], Train Loss: 0.1444, Val Loss: 0.3566\n",
      "Epoch [33/50], Train Loss: 0.1385, Val Loss: 0.3557\n",
      "Epoch [34/50], Train Loss: 0.1426, Val Loss: 0.3548\n",
      "Epoch [35/50], Train Loss: 0.1377, Val Loss: 0.3539\n",
      "Epoch [36/50], Train Loss: 0.1382, Val Loss: 0.3531\n",
      "Epoch [37/50], Train Loss: 0.1393, Val Loss: 0.3522\n",
      "Epoch [38/50], Train Loss: 0.1376, Val Loss: 0.3514\n",
      "Epoch [39/50], Train Loss: 0.1369, Val Loss: 0.3505\n",
      "Epoch [40/50], Train Loss: 0.1387, Val Loss: 0.3497\n",
      "Epoch [41/50], Train Loss: 0.1339, Val Loss: 0.3488\n",
      "Epoch [42/50], Train Loss: 0.1369, Val Loss: 0.3480\n",
      "Epoch [43/50], Train Loss: 0.1364, Val Loss: 0.3472\n",
      "Epoch [44/50], Train Loss: 0.1356, Val Loss: 0.3463\n",
      "Epoch [45/50], Train Loss: 0.1361, Val Loss: 0.3455\n",
      "Epoch [46/50], Train Loss: 0.1334, Val Loss: 0.3447\n",
      "Epoch [47/50], Train Loss: 0.1344, Val Loss: 0.3438\n",
      "Epoch [48/50], Train Loss: 0.1303, Val Loss: 0.3430\n",
      "Epoch [49/50], Train Loss: 0.1326, Val Loss: 0.3422\n",
      "Epoch [50/50], Train Loss: 0.1318, Val Loss: 0.3414\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=32, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1813, Val Loss: 0.4863\n",
      "Epoch [2/50], Train Loss: 0.1806, Val Loss: 0.4848\n",
      "Epoch [3/50], Train Loss: 0.1798, Val Loss: 0.4834\n",
      "Epoch [4/50], Train Loss: 0.1791, Val Loss: 0.4819\n",
      "Epoch [5/50], Train Loss: 0.1784, Val Loss: 0.4805\n",
      "Epoch [6/50], Train Loss: 0.1777, Val Loss: 0.4790\n",
      "Epoch [7/50], Train Loss: 0.1770, Val Loss: 0.4776\n",
      "Epoch [8/50], Train Loss: 0.1763, Val Loss: 0.4762\n",
      "Epoch [9/50], Train Loss: 0.1755, Val Loss: 0.4747\n",
      "Epoch [10/50], Train Loss: 0.1748, Val Loss: 0.4734\n",
      "Epoch [11/50], Train Loss: 0.1741, Val Loss: 0.4719\n",
      "Epoch [12/50], Train Loss: 0.1735, Val Loss: 0.4705\n",
      "Epoch [13/50], Train Loss: 0.1728, Val Loss: 0.4691\n",
      "Epoch [14/50], Train Loss: 0.1721, Val Loss: 0.4678\n",
      "Epoch [15/50], Train Loss: 0.1714, Val Loss: 0.4664\n",
      "Epoch [16/50], Train Loss: 0.1707, Val Loss: 0.4650\n",
      "Epoch [17/50], Train Loss: 0.1700, Val Loss: 0.4636\n",
      "Epoch [18/50], Train Loss: 0.1693, Val Loss: 0.4622\n",
      "Epoch [19/50], Train Loss: 0.1687, Val Loss: 0.4609\n",
      "Epoch [20/50], Train Loss: 0.1680, Val Loss: 0.4595\n",
      "Epoch [21/50], Train Loss: 0.1673, Val Loss: 0.4582\n",
      "Epoch [22/50], Train Loss: 0.1667, Val Loss: 0.4569\n",
      "Epoch [23/50], Train Loss: 0.1660, Val Loss: 0.4555\n",
      "Epoch [24/50], Train Loss: 0.1654, Val Loss: 0.4542\n",
      "Epoch [25/50], Train Loss: 0.1647, Val Loss: 0.4528\n",
      "Epoch [26/50], Train Loss: 0.1641, Val Loss: 0.4515\n",
      "Epoch [27/50], Train Loss: 0.1634, Val Loss: 0.4502\n",
      "Epoch [28/50], Train Loss: 0.1628, Val Loss: 0.4489\n",
      "Epoch [29/50], Train Loss: 0.1621, Val Loss: 0.4476\n",
      "Epoch [30/50], Train Loss: 0.1615, Val Loss: 0.4462\n",
      "Epoch [31/50], Train Loss: 0.1608, Val Loss: 0.4449\n",
      "Epoch [32/50], Train Loss: 0.1602, Val Loss: 0.4437\n",
      "Epoch [33/50], Train Loss: 0.1596, Val Loss: 0.4424\n",
      "Epoch [34/50], Train Loss: 0.1590, Val Loss: 0.4411\n",
      "Epoch [35/50], Train Loss: 0.1583, Val Loss: 0.4398\n",
      "Epoch [36/50], Train Loss: 0.1577, Val Loss: 0.4385\n",
      "Epoch [37/50], Train Loss: 0.1571, Val Loss: 0.4372\n",
      "Epoch [38/50], Train Loss: 0.1565, Val Loss: 0.4360\n",
      "Epoch [39/50], Train Loss: 0.1559, Val Loss: 0.4347\n",
      "Epoch [40/50], Train Loss: 0.1553, Val Loss: 0.4335\n",
      "Epoch [41/50], Train Loss: 0.1547, Val Loss: 0.4322\n",
      "Epoch [42/50], Train Loss: 0.1541, Val Loss: 0.4310\n",
      "Epoch [43/50], Train Loss: 0.1535, Val Loss: 0.4297\n",
      "Epoch [44/50], Train Loss: 0.1529, Val Loss: 0.4285\n",
      "Epoch [45/50], Train Loss: 0.1523, Val Loss: 0.4273\n",
      "Epoch [46/50], Train Loss: 0.1517, Val Loss: 0.4260\n",
      "Epoch [47/50], Train Loss: 0.1511, Val Loss: 0.4248\n",
      "Epoch [48/50], Train Loss: 0.1505, Val Loss: 0.4236\n",
      "Epoch [49/50], Train Loss: 0.1499, Val Loss: 0.4224\n",
      "Epoch [50/50], Train Loss: 0.1493, Val Loss: 0.4212\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=32, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1455, Val Loss: 0.3703\n",
      "Epoch [2/50], Train Loss: 0.1450, Val Loss: 0.3692\n",
      "Epoch [3/50], Train Loss: 0.1446, Val Loss: 0.3682\n",
      "Epoch [4/50], Train Loss: 0.1435, Val Loss: 0.3671\n",
      "Epoch [5/50], Train Loss: 0.1450, Val Loss: 0.3661\n",
      "Epoch [6/50], Train Loss: 0.1432, Val Loss: 0.3650\n",
      "Epoch [7/50], Train Loss: 0.1423, Val Loss: 0.3640\n",
      "Epoch [8/50], Train Loss: 0.1424, Val Loss: 0.3630\n",
      "Epoch [9/50], Train Loss: 0.1421, Val Loss: 0.3619\n",
      "Epoch [10/50], Train Loss: 0.1416, Val Loss: 0.3609\n",
      "Epoch [11/50], Train Loss: 0.1410, Val Loss: 0.3599\n",
      "Epoch [12/50], Train Loss: 0.1402, Val Loss: 0.3589\n",
      "Epoch [13/50], Train Loss: 0.1402, Val Loss: 0.3579\n",
      "Epoch [14/50], Train Loss: 0.1389, Val Loss: 0.3569\n",
      "Epoch [15/50], Train Loss: 0.1383, Val Loss: 0.3559\n",
      "Epoch [16/50], Train Loss: 0.1381, Val Loss: 0.3549\n",
      "Epoch [17/50], Train Loss: 0.1380, Val Loss: 0.3539\n",
      "Epoch [18/50], Train Loss: 0.1372, Val Loss: 0.3529\n",
      "Epoch [19/50], Train Loss: 0.1358, Val Loss: 0.3519\n",
      "Epoch [20/50], Train Loss: 0.1364, Val Loss: 0.3509\n",
      "Epoch [21/50], Train Loss: 0.1357, Val Loss: 0.3499\n",
      "Epoch [22/50], Train Loss: 0.1355, Val Loss: 0.3490\n",
      "Epoch [23/50], Train Loss: 0.1347, Val Loss: 0.3480\n",
      "Epoch [24/50], Train Loss: 0.1346, Val Loss: 0.3470\n",
      "Epoch [25/50], Train Loss: 0.1338, Val Loss: 0.3461\n",
      "Epoch [26/50], Train Loss: 0.1332, Val Loss: 0.3451\n",
      "Epoch [27/50], Train Loss: 0.1328, Val Loss: 0.3442\n",
      "Epoch [28/50], Train Loss: 0.1320, Val Loss: 0.3432\n",
      "Epoch [29/50], Train Loss: 0.1312, Val Loss: 0.3422\n",
      "Epoch [30/50], Train Loss: 0.1315, Val Loss: 0.3413\n",
      "Epoch [31/50], Train Loss: 0.1314, Val Loss: 0.3404\n",
      "Epoch [32/50], Train Loss: 0.1305, Val Loss: 0.3394\n",
      "Epoch [33/50], Train Loss: 0.1299, Val Loss: 0.3385\n",
      "Epoch [34/50], Train Loss: 0.1294, Val Loss: 0.3376\n",
      "Epoch [35/50], Train Loss: 0.1294, Val Loss: 0.3366\n",
      "Epoch [36/50], Train Loss: 0.1294, Val Loss: 0.3357\n",
      "Epoch [37/50], Train Loss: 0.1282, Val Loss: 0.3348\n",
      "Epoch [38/50], Train Loss: 0.1280, Val Loss: 0.3339\n",
      "Epoch [39/50], Train Loss: 0.1274, Val Loss: 0.3330\n",
      "Epoch [40/50], Train Loss: 0.1275, Val Loss: 0.3320\n",
      "Epoch [41/50], Train Loss: 0.1265, Val Loss: 0.3311\n",
      "Epoch [42/50], Train Loss: 0.1262, Val Loss: 0.3302\n",
      "Epoch [43/50], Train Loss: 0.1258, Val Loss: 0.3293\n",
      "Epoch [44/50], Train Loss: 0.1251, Val Loss: 0.3284\n",
      "Epoch [45/50], Train Loss: 0.1252, Val Loss: 0.3275\n",
      "Epoch [46/50], Train Loss: 0.1244, Val Loss: 0.3266\n",
      "Epoch [47/50], Train Loss: 0.1245, Val Loss: 0.3257\n",
      "Epoch [48/50], Train Loss: 0.1238, Val Loss: 0.3249\n",
      "Epoch [49/50], Train Loss: 0.1236, Val Loss: 0.3240\n",
      "Epoch [50/50], Train Loss: 0.1232, Val Loss: 0.3231\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=32, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1298, Val Loss: 0.3563\n",
      "Epoch [2/50], Train Loss: 0.1330, Val Loss: 0.3553\n",
      "Epoch [3/50], Train Loss: 0.1287, Val Loss: 0.3543\n",
      "Epoch [4/50], Train Loss: 0.1297, Val Loss: 0.3533\n",
      "Epoch [5/50], Train Loss: 0.1282, Val Loss: 0.3523\n",
      "Epoch [6/50], Train Loss: 0.1295, Val Loss: 0.3514\n",
      "Epoch [7/50], Train Loss: 0.1285, Val Loss: 0.3504\n",
      "Epoch [8/50], Train Loss: 0.1280, Val Loss: 0.3494\n",
      "Epoch [9/50], Train Loss: 0.1264, Val Loss: 0.3485\n",
      "Epoch [10/50], Train Loss: 0.1285, Val Loss: 0.3476\n",
      "Epoch [11/50], Train Loss: 0.1269, Val Loss: 0.3466\n",
      "Epoch [12/50], Train Loss: 0.1261, Val Loss: 0.3456\n",
      "Epoch [13/50], Train Loss: 0.1266, Val Loss: 0.3447\n",
      "Epoch [14/50], Train Loss: 0.1257, Val Loss: 0.3438\n",
      "Epoch [15/50], Train Loss: 0.1253, Val Loss: 0.3428\n",
      "Epoch [16/50], Train Loss: 0.1252, Val Loss: 0.3419\n",
      "Epoch [17/50], Train Loss: 0.1235, Val Loss: 0.3409\n",
      "Epoch [18/50], Train Loss: 0.1229, Val Loss: 0.3401\n",
      "Epoch [19/50], Train Loss: 0.1240, Val Loss: 0.3391\n",
      "Epoch [20/50], Train Loss: 0.1222, Val Loss: 0.3382\n",
      "Epoch [21/50], Train Loss: 0.1218, Val Loss: 0.3373\n",
      "Epoch [22/50], Train Loss: 0.1206, Val Loss: 0.3364\n",
      "Epoch [23/50], Train Loss: 0.1218, Val Loss: 0.3355\n",
      "Epoch [24/50], Train Loss: 0.1222, Val Loss: 0.3346\n",
      "Epoch [25/50], Train Loss: 0.1200, Val Loss: 0.3337\n",
      "Epoch [26/50], Train Loss: 0.1205, Val Loss: 0.3328\n",
      "Epoch [27/50], Train Loss: 0.1203, Val Loss: 0.3319\n",
      "Epoch [28/50], Train Loss: 0.1198, Val Loss: 0.3310\n",
      "Epoch [29/50], Train Loss: 0.1200, Val Loss: 0.3301\n",
      "Epoch [30/50], Train Loss: 0.1179, Val Loss: 0.3292\n",
      "Epoch [31/50], Train Loss: 0.1181, Val Loss: 0.3283\n",
      "Epoch [32/50], Train Loss: 0.1176, Val Loss: 0.3275\n",
      "Epoch [33/50], Train Loss: 0.1181, Val Loss: 0.3266\n",
      "Epoch [34/50], Train Loss: 0.1166, Val Loss: 0.3257\n",
      "Epoch [35/50], Train Loss: 0.1167, Val Loss: 0.3249\n",
      "Epoch [36/50], Train Loss: 0.1163, Val Loss: 0.3240\n",
      "Epoch [37/50], Train Loss: 0.1170, Val Loss: 0.3231\n",
      "Epoch [38/50], Train Loss: 0.1162, Val Loss: 0.3223\n",
      "Epoch [39/50], Train Loss: 0.1169, Val Loss: 0.3214\n",
      "Epoch [40/50], Train Loss: 0.1135, Val Loss: 0.3206\n",
      "Epoch [41/50], Train Loss: 0.1154, Val Loss: 0.3197\n",
      "Epoch [42/50], Train Loss: 0.1158, Val Loss: 0.3189\n",
      "Epoch [43/50], Train Loss: 0.1138, Val Loss: 0.3180\n",
      "Epoch [44/50], Train Loss: 0.1155, Val Loss: 0.3172\n",
      "Epoch [45/50], Train Loss: 0.1138, Val Loss: 0.3164\n",
      "Epoch [46/50], Train Loss: 0.1132, Val Loss: 0.3155\n",
      "Epoch [47/50], Train Loss: 0.1133, Val Loss: 0.3147\n",
      "Epoch [48/50], Train Loss: 0.1123, Val Loss: 0.3139\n",
      "Epoch [49/50], Train Loss: 0.1121, Val Loss: 0.3131\n",
      "Epoch [50/50], Train Loss: 0.1108, Val Loss: 0.3123\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=32, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1353, Val Loss: 0.3651\n",
      "Epoch [2/50], Train Loss: 0.1347, Val Loss: 0.3639\n",
      "Epoch [3/50], Train Loss: 0.1341, Val Loss: 0.3627\n",
      "Epoch [4/50], Train Loss: 0.1335, Val Loss: 0.3616\n",
      "Epoch [5/50], Train Loss: 0.1329, Val Loss: 0.3604\n",
      "Epoch [6/50], Train Loss: 0.1322, Val Loss: 0.3592\n",
      "Epoch [7/50], Train Loss: 0.1316, Val Loss: 0.3581\n",
      "Epoch [8/50], Train Loss: 0.1310, Val Loss: 0.3569\n",
      "Epoch [9/50], Train Loss: 0.1304, Val Loss: 0.3557\n",
      "Epoch [10/50], Train Loss: 0.1298, Val Loss: 0.3546\n",
      "Epoch [11/50], Train Loss: 0.1292, Val Loss: 0.3535\n",
      "Epoch [12/50], Train Loss: 0.1286, Val Loss: 0.3524\n",
      "Epoch [13/50], Train Loss: 0.1281, Val Loss: 0.3512\n",
      "Epoch [14/50], Train Loss: 0.1275, Val Loss: 0.3501\n",
      "Epoch [15/50], Train Loss: 0.1269, Val Loss: 0.3490\n",
      "Epoch [16/50], Train Loss: 0.1263, Val Loss: 0.3479\n",
      "Epoch [17/50], Train Loss: 0.1258, Val Loss: 0.3468\n",
      "Epoch [18/50], Train Loss: 0.1252, Val Loss: 0.3457\n",
      "Epoch [19/50], Train Loss: 0.1246, Val Loss: 0.3446\n",
      "Epoch [20/50], Train Loss: 0.1241, Val Loss: 0.3435\n",
      "Epoch [21/50], Train Loss: 0.1235, Val Loss: 0.3425\n",
      "Epoch [22/50], Train Loss: 0.1230, Val Loss: 0.3414\n",
      "Epoch [23/50], Train Loss: 0.1224, Val Loss: 0.3403\n",
      "Epoch [24/50], Train Loss: 0.1219, Val Loss: 0.3393\n",
      "Epoch [25/50], Train Loss: 0.1213, Val Loss: 0.3382\n",
      "Epoch [26/50], Train Loss: 0.1208, Val Loss: 0.3371\n",
      "Epoch [27/50], Train Loss: 0.1203, Val Loss: 0.3361\n",
      "Epoch [28/50], Train Loss: 0.1197, Val Loss: 0.3350\n",
      "Epoch [29/50], Train Loss: 0.1192, Val Loss: 0.3340\n",
      "Epoch [30/50], Train Loss: 0.1187, Val Loss: 0.3330\n",
      "Epoch [31/50], Train Loss: 0.1181, Val Loss: 0.3320\n",
      "Epoch [32/50], Train Loss: 0.1176, Val Loss: 0.3309\n",
      "Epoch [33/50], Train Loss: 0.1171, Val Loss: 0.3299\n",
      "Epoch [34/50], Train Loss: 0.1166, Val Loss: 0.3289\n",
      "Epoch [35/50], Train Loss: 0.1161, Val Loss: 0.3279\n",
      "Epoch [36/50], Train Loss: 0.1156, Val Loss: 0.3269\n",
      "Epoch [37/50], Train Loss: 0.1151, Val Loss: 0.3259\n",
      "Epoch [38/50], Train Loss: 0.1146, Val Loss: 0.3249\n",
      "Epoch [39/50], Train Loss: 0.1141, Val Loss: 0.3239\n",
      "Epoch [40/50], Train Loss: 0.1136, Val Loss: 0.3229\n",
      "Epoch [41/50], Train Loss: 0.1131, Val Loss: 0.3219\n",
      "Epoch [42/50], Train Loss: 0.1126, Val Loss: 0.3210\n",
      "Epoch [43/50], Train Loss: 0.1121, Val Loss: 0.3200\n",
      "Epoch [44/50], Train Loss: 0.1116, Val Loss: 0.3190\n",
      "Epoch [45/50], Train Loss: 0.1112, Val Loss: 0.3181\n",
      "Epoch [46/50], Train Loss: 0.1107, Val Loss: 0.3171\n",
      "Epoch [47/50], Train Loss: 0.1102, Val Loss: 0.3161\n",
      "Epoch [48/50], Train Loss: 0.1097, Val Loss: 0.3152\n",
      "Epoch [49/50], Train Loss: 0.1093, Val Loss: 0.3143\n",
      "Epoch [50/50], Train Loss: 0.1088, Val Loss: 0.3133\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=32, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1623, Val Loss: 0.4375\n",
      "Epoch [2/50], Train Loss: 0.1616, Val Loss: 0.4362\n",
      "Epoch [3/50], Train Loss: 0.1610, Val Loss: 0.4349\n",
      "Epoch [4/50], Train Loss: 0.1603, Val Loss: 0.4336\n",
      "Epoch [5/50], Train Loss: 0.1594, Val Loss: 0.4324\n",
      "Epoch [6/50], Train Loss: 0.1586, Val Loss: 0.4311\n",
      "Epoch [7/50], Train Loss: 0.1582, Val Loss: 0.4298\n",
      "Epoch [8/50], Train Loss: 0.1580, Val Loss: 0.4286\n",
      "Epoch [9/50], Train Loss: 0.1574, Val Loss: 0.4273\n",
      "Epoch [10/50], Train Loss: 0.1562, Val Loss: 0.4261\n",
      "Epoch [11/50], Train Loss: 0.1554, Val Loss: 0.4248\n",
      "Epoch [12/50], Train Loss: 0.1551, Val Loss: 0.4236\n",
      "Epoch [13/50], Train Loss: 0.1540, Val Loss: 0.4223\n",
      "Epoch [14/50], Train Loss: 0.1533, Val Loss: 0.4211\n",
      "Epoch [15/50], Train Loss: 0.1533, Val Loss: 0.4199\n",
      "Epoch [16/50], Train Loss: 0.1528, Val Loss: 0.4187\n",
      "Epoch [17/50], Train Loss: 0.1519, Val Loss: 0.4175\n",
      "Epoch [18/50], Train Loss: 0.1519, Val Loss: 0.4162\n",
      "Epoch [19/50], Train Loss: 0.1510, Val Loss: 0.4151\n",
      "Epoch [20/50], Train Loss: 0.1503, Val Loss: 0.4139\n",
      "Epoch [21/50], Train Loss: 0.1495, Val Loss: 0.4127\n",
      "Epoch [22/50], Train Loss: 0.1494, Val Loss: 0.4115\n",
      "Epoch [23/50], Train Loss: 0.1489, Val Loss: 0.4103\n",
      "Epoch [24/50], Train Loss: 0.1477, Val Loss: 0.4091\n",
      "Epoch [25/50], Train Loss: 0.1477, Val Loss: 0.4079\n",
      "Epoch [26/50], Train Loss: 0.1462, Val Loss: 0.4068\n",
      "Epoch [27/50], Train Loss: 0.1463, Val Loss: 0.4056\n",
      "Epoch [28/50], Train Loss: 0.1456, Val Loss: 0.4044\n",
      "Epoch [29/50], Train Loss: 0.1440, Val Loss: 0.4033\n",
      "Epoch [30/50], Train Loss: 0.1437, Val Loss: 0.4021\n",
      "Epoch [31/50], Train Loss: 0.1439, Val Loss: 0.4010\n",
      "Epoch [32/50], Train Loss: 0.1432, Val Loss: 0.3998\n",
      "Epoch [33/50], Train Loss: 0.1426, Val Loss: 0.3987\n",
      "Epoch [34/50], Train Loss: 0.1422, Val Loss: 0.3975\n",
      "Epoch [35/50], Train Loss: 0.1426, Val Loss: 0.3964\n",
      "Epoch [36/50], Train Loss: 0.1410, Val Loss: 0.3953\n",
      "Epoch [37/50], Train Loss: 0.1395, Val Loss: 0.3942\n",
      "Epoch [38/50], Train Loss: 0.1395, Val Loss: 0.3931\n",
      "Epoch [39/50], Train Loss: 0.1387, Val Loss: 0.3919\n",
      "Epoch [40/50], Train Loss: 0.1388, Val Loss: 0.3908\n",
      "Epoch [41/50], Train Loss: 0.1378, Val Loss: 0.3897\n",
      "Epoch [42/50], Train Loss: 0.1378, Val Loss: 0.3886\n",
      "Epoch [43/50], Train Loss: 0.1367, Val Loss: 0.3875\n",
      "Epoch [44/50], Train Loss: 0.1359, Val Loss: 0.3864\n",
      "Epoch [45/50], Train Loss: 0.1360, Val Loss: 0.3854\n",
      "Epoch [46/50], Train Loss: 0.1348, Val Loss: 0.3843\n",
      "Epoch [47/50], Train Loss: 0.1350, Val Loss: 0.3832\n",
      "Epoch [48/50], Train Loss: 0.1343, Val Loss: 0.3821\n",
      "Epoch [49/50], Train Loss: 0.1336, Val Loss: 0.3810\n",
      "Epoch [50/50], Train Loss: 0.1332, Val Loss: 0.3800\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=32, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1490, Val Loss: 0.3605\n",
      "Epoch [2/50], Train Loss: 0.1477, Val Loss: 0.3595\n",
      "Epoch [3/50], Train Loss: 0.1484, Val Loss: 0.3584\n",
      "Epoch [4/50], Train Loss: 0.1464, Val Loss: 0.3574\n",
      "Epoch [5/50], Train Loss: 0.1456, Val Loss: 0.3564\n",
      "Epoch [6/50], Train Loss: 0.1430, Val Loss: 0.3554\n",
      "Epoch [7/50], Train Loss: 0.1441, Val Loss: 0.3544\n",
      "Epoch [8/50], Train Loss: 0.1418, Val Loss: 0.3534\n",
      "Epoch [9/50], Train Loss: 0.1426, Val Loss: 0.3524\n",
      "Epoch [10/50], Train Loss: 0.1429, Val Loss: 0.3514\n",
      "Epoch [11/50], Train Loss: 0.1423, Val Loss: 0.3505\n",
      "Epoch [12/50], Train Loss: 0.1412, Val Loss: 0.3495\n",
      "Epoch [13/50], Train Loss: 0.1392, Val Loss: 0.3485\n",
      "Epoch [14/50], Train Loss: 0.1417, Val Loss: 0.3475\n",
      "Epoch [15/50], Train Loss: 0.1402, Val Loss: 0.3465\n",
      "Epoch [16/50], Train Loss: 0.1392, Val Loss: 0.3456\n",
      "Epoch [17/50], Train Loss: 0.1420, Val Loss: 0.3446\n",
      "Epoch [18/50], Train Loss: 0.1395, Val Loss: 0.3436\n",
      "Epoch [19/50], Train Loss: 0.1358, Val Loss: 0.3427\n",
      "Epoch [20/50], Train Loss: 0.1377, Val Loss: 0.3418\n",
      "Epoch [21/50], Train Loss: 0.1382, Val Loss: 0.3408\n",
      "Epoch [22/50], Train Loss: 0.1364, Val Loss: 0.3399\n",
      "Epoch [23/50], Train Loss: 0.1365, Val Loss: 0.3389\n",
      "Epoch [24/50], Train Loss: 0.1345, Val Loss: 0.3380\n",
      "Epoch [25/50], Train Loss: 0.1344, Val Loss: 0.3370\n",
      "Epoch [26/50], Train Loss: 0.1356, Val Loss: 0.3361\n",
      "Epoch [27/50], Train Loss: 0.1348, Val Loss: 0.3352\n",
      "Epoch [28/50], Train Loss: 0.1330, Val Loss: 0.3343\n",
      "Epoch [29/50], Train Loss: 0.1355, Val Loss: 0.3334\n",
      "Epoch [30/50], Train Loss: 0.1305, Val Loss: 0.3325\n",
      "Epoch [31/50], Train Loss: 0.1312, Val Loss: 0.3316\n",
      "Epoch [32/50], Train Loss: 0.1332, Val Loss: 0.3307\n",
      "Epoch [33/50], Train Loss: 0.1315, Val Loss: 0.3298\n",
      "Epoch [34/50], Train Loss: 0.1305, Val Loss: 0.3289\n",
      "Epoch [35/50], Train Loss: 0.1313, Val Loss: 0.3280\n",
      "Epoch [36/50], Train Loss: 0.1296, Val Loss: 0.3271\n",
      "Epoch [37/50], Train Loss: 0.1306, Val Loss: 0.3263\n",
      "Epoch [38/50], Train Loss: 0.1295, Val Loss: 0.3254\n",
      "Epoch [39/50], Train Loss: 0.1299, Val Loss: 0.3245\n",
      "Epoch [40/50], Train Loss: 0.1273, Val Loss: 0.3236\n",
      "Epoch [41/50], Train Loss: 0.1269, Val Loss: 0.3228\n",
      "Epoch [42/50], Train Loss: 0.1263, Val Loss: 0.3219\n",
      "Epoch [43/50], Train Loss: 0.1252, Val Loss: 0.3211\n",
      "Epoch [44/50], Train Loss: 0.1274, Val Loss: 0.3202\n",
      "Epoch [45/50], Train Loss: 0.1266, Val Loss: 0.3193\n",
      "Epoch [46/50], Train Loss: 0.1235, Val Loss: 0.3185\n",
      "Epoch [47/50], Train Loss: 0.1243, Val Loss: 0.3177\n",
      "Epoch [48/50], Train Loss: 0.1235, Val Loss: 0.3168\n",
      "Epoch [49/50], Train Loss: 0.1230, Val Loss: 0.3160\n",
      "Epoch [50/50], Train Loss: 0.1229, Val Loss: 0.3152\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=32, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1731, Val Loss: 0.4167\n",
      "Epoch [2/50], Train Loss: 0.1723, Val Loss: 0.4155\n",
      "Epoch [3/50], Train Loss: 0.1715, Val Loss: 0.4143\n",
      "Epoch [4/50], Train Loss: 0.1707, Val Loss: 0.4130\n",
      "Epoch [5/50], Train Loss: 0.1700, Val Loss: 0.4118\n",
      "Epoch [6/50], Train Loss: 0.1692, Val Loss: 0.4106\n",
      "Epoch [7/50], Train Loss: 0.1684, Val Loss: 0.4094\n",
      "Epoch [8/50], Train Loss: 0.1677, Val Loss: 0.4082\n",
      "Epoch [9/50], Train Loss: 0.1669, Val Loss: 0.4070\n",
      "Epoch [10/50], Train Loss: 0.1662, Val Loss: 0.4058\n",
      "Epoch [11/50], Train Loss: 0.1655, Val Loss: 0.4046\n",
      "Epoch [12/50], Train Loss: 0.1647, Val Loss: 0.4035\n",
      "Epoch [13/50], Train Loss: 0.1640, Val Loss: 0.4023\n",
      "Epoch [14/50], Train Loss: 0.1633, Val Loss: 0.4011\n",
      "Epoch [15/50], Train Loss: 0.1625, Val Loss: 0.4000\n",
      "Epoch [16/50], Train Loss: 0.1618, Val Loss: 0.3988\n",
      "Epoch [17/50], Train Loss: 0.1611, Val Loss: 0.3977\n",
      "Epoch [18/50], Train Loss: 0.1604, Val Loss: 0.3965\n",
      "Epoch [19/50], Train Loss: 0.1597, Val Loss: 0.3954\n",
      "Epoch [20/50], Train Loss: 0.1590, Val Loss: 0.3942\n",
      "Epoch [21/50], Train Loss: 0.1583, Val Loss: 0.3931\n",
      "Epoch [22/50], Train Loss: 0.1576, Val Loss: 0.3920\n",
      "Epoch [23/50], Train Loss: 0.1569, Val Loss: 0.3909\n",
      "Epoch [24/50], Train Loss: 0.1562, Val Loss: 0.3898\n",
      "Epoch [25/50], Train Loss: 0.1555, Val Loss: 0.3887\n",
      "Epoch [26/50], Train Loss: 0.1549, Val Loss: 0.3875\n",
      "Epoch [27/50], Train Loss: 0.1542, Val Loss: 0.3865\n",
      "Epoch [28/50], Train Loss: 0.1535, Val Loss: 0.3854\n",
      "Epoch [29/50], Train Loss: 0.1529, Val Loss: 0.3843\n",
      "Epoch [30/50], Train Loss: 0.1522, Val Loss: 0.3832\n",
      "Epoch [31/50], Train Loss: 0.1515, Val Loss: 0.3821\n",
      "Epoch [32/50], Train Loss: 0.1509, Val Loss: 0.3810\n",
      "Epoch [33/50], Train Loss: 0.1502, Val Loss: 0.3800\n",
      "Epoch [34/50], Train Loss: 0.1496, Val Loss: 0.3789\n",
      "Epoch [35/50], Train Loss: 0.1489, Val Loss: 0.3779\n",
      "Epoch [36/50], Train Loss: 0.1483, Val Loss: 0.3768\n",
      "Epoch [37/50], Train Loss: 0.1477, Val Loss: 0.3757\n",
      "Epoch [38/50], Train Loss: 0.1470, Val Loss: 0.3747\n",
      "Epoch [39/50], Train Loss: 0.1464, Val Loss: 0.3737\n",
      "Epoch [40/50], Train Loss: 0.1458, Val Loss: 0.3726\n",
      "Epoch [41/50], Train Loss: 0.1452, Val Loss: 0.3716\n",
      "Epoch [42/50], Train Loss: 0.1445, Val Loss: 0.3706\n",
      "Epoch [43/50], Train Loss: 0.1439, Val Loss: 0.3696\n",
      "Epoch [44/50], Train Loss: 0.1433, Val Loss: 0.3685\n",
      "Epoch [45/50], Train Loss: 0.1427, Val Loss: 0.3675\n",
      "Epoch [46/50], Train Loss: 0.1421, Val Loss: 0.3665\n",
      "Epoch [47/50], Train Loss: 0.1415, Val Loss: 0.3655\n",
      "Epoch [48/50], Train Loss: 0.1409, Val Loss: 0.3645\n",
      "Epoch [49/50], Train Loss: 0.1403, Val Loss: 0.3635\n",
      "Epoch [50/50], Train Loss: 0.1397, Val Loss: 0.3625\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=32, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1598, Val Loss: 0.3635\n",
      "Epoch [2/50], Train Loss: 0.1603, Val Loss: 0.3624\n",
      "Epoch [3/50], Train Loss: 0.1589, Val Loss: 0.3612\n",
      "Epoch [4/50], Train Loss: 0.1580, Val Loss: 0.3601\n",
      "Epoch [5/50], Train Loss: 0.1573, Val Loss: 0.3590\n",
      "Epoch [6/50], Train Loss: 0.1559, Val Loss: 0.3579\n",
      "Epoch [7/50], Train Loss: 0.1552, Val Loss: 0.3568\n",
      "Epoch [8/50], Train Loss: 0.1548, Val Loss: 0.3557\n",
      "Epoch [9/50], Train Loss: 0.1534, Val Loss: 0.3547\n",
      "Epoch [10/50], Train Loss: 0.1525, Val Loss: 0.3536\n",
      "Epoch [11/50], Train Loss: 0.1522, Val Loss: 0.3525\n",
      "Epoch [12/50], Train Loss: 0.1521, Val Loss: 0.3514\n",
      "Epoch [13/50], Train Loss: 0.1517, Val Loss: 0.3504\n",
      "Epoch [14/50], Train Loss: 0.1510, Val Loss: 0.3493\n",
      "Epoch [15/50], Train Loss: 0.1503, Val Loss: 0.3483\n",
      "Epoch [16/50], Train Loss: 0.1503, Val Loss: 0.3472\n",
      "Epoch [17/50], Train Loss: 0.1491, Val Loss: 0.3462\n",
      "Epoch [18/50], Train Loss: 0.1474, Val Loss: 0.3452\n",
      "Epoch [19/50], Train Loss: 0.1480, Val Loss: 0.3441\n",
      "Epoch [20/50], Train Loss: 0.1461, Val Loss: 0.3431\n",
      "Epoch [21/50], Train Loss: 0.1460, Val Loss: 0.3421\n",
      "Epoch [22/50], Train Loss: 0.1449, Val Loss: 0.3411\n",
      "Epoch [23/50], Train Loss: 0.1442, Val Loss: 0.3401\n",
      "Epoch [24/50], Train Loss: 0.1431, Val Loss: 0.3391\n",
      "Epoch [25/50], Train Loss: 0.1432, Val Loss: 0.3381\n",
      "Epoch [26/50], Train Loss: 0.1431, Val Loss: 0.3371\n",
      "Epoch [27/50], Train Loss: 0.1426, Val Loss: 0.3361\n",
      "Epoch [28/50], Train Loss: 0.1417, Val Loss: 0.3351\n",
      "Epoch [29/50], Train Loss: 0.1406, Val Loss: 0.3341\n",
      "Epoch [30/50], Train Loss: 0.1402, Val Loss: 0.3332\n",
      "Epoch [31/50], Train Loss: 0.1400, Val Loss: 0.3322\n",
      "Epoch [32/50], Train Loss: 0.1393, Val Loss: 0.3312\n",
      "Epoch [33/50], Train Loss: 0.1387, Val Loss: 0.3303\n",
      "Epoch [34/50], Train Loss: 0.1371, Val Loss: 0.3293\n",
      "Epoch [35/50], Train Loss: 0.1372, Val Loss: 0.3284\n",
      "Epoch [36/50], Train Loss: 0.1369, Val Loss: 0.3275\n",
      "Epoch [37/50], Train Loss: 0.1360, Val Loss: 0.3265\n",
      "Epoch [38/50], Train Loss: 0.1353, Val Loss: 0.3256\n",
      "Epoch [39/50], Train Loss: 0.1353, Val Loss: 0.3247\n",
      "Epoch [40/50], Train Loss: 0.1339, Val Loss: 0.3237\n",
      "Epoch [41/50], Train Loss: 0.1326, Val Loss: 0.3228\n",
      "Epoch [42/50], Train Loss: 0.1328, Val Loss: 0.3219\n",
      "Epoch [43/50], Train Loss: 0.1324, Val Loss: 0.3210\n",
      "Epoch [44/50], Train Loss: 0.1329, Val Loss: 0.3201\n",
      "Epoch [45/50], Train Loss: 0.1318, Val Loss: 0.3192\n",
      "Epoch [46/50], Train Loss: 0.1306, Val Loss: 0.3183\n",
      "Epoch [47/50], Train Loss: 0.1301, Val Loss: 0.3174\n",
      "Epoch [48/50], Train Loss: 0.1299, Val Loss: 0.3165\n",
      "Epoch [49/50], Train Loss: 0.1287, Val Loss: 0.3156\n",
      "Epoch [50/50], Train Loss: 0.1294, Val Loss: 0.3148\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=32, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1641, Val Loss: 0.4131\n",
      "Epoch [2/50], Train Loss: 0.1620, Val Loss: 0.4116\n",
      "Epoch [3/50], Train Loss: 0.1604, Val Loss: 0.4100\n",
      "Epoch [4/50], Train Loss: 0.1582, Val Loss: 0.4085\n",
      "Epoch [5/50], Train Loss: 0.1598, Val Loss: 0.4069\n",
      "Epoch [6/50], Train Loss: 0.1583, Val Loss: 0.4055\n",
      "Epoch [7/50], Train Loss: 0.1552, Val Loss: 0.4040\n",
      "Epoch [8/50], Train Loss: 0.1574, Val Loss: 0.4025\n",
      "Epoch [9/50], Train Loss: 0.1568, Val Loss: 0.4010\n",
      "Epoch [10/50], Train Loss: 0.1534, Val Loss: 0.3996\n",
      "Epoch [11/50], Train Loss: 0.1551, Val Loss: 0.3981\n",
      "Epoch [12/50], Train Loss: 0.1528, Val Loss: 0.3966\n",
      "Epoch [13/50], Train Loss: 0.1497, Val Loss: 0.3952\n",
      "Epoch [14/50], Train Loss: 0.1529, Val Loss: 0.3938\n",
      "Epoch [15/50], Train Loss: 0.1514, Val Loss: 0.3924\n",
      "Epoch [16/50], Train Loss: 0.1491, Val Loss: 0.3910\n",
      "Epoch [17/50], Train Loss: 0.1481, Val Loss: 0.3896\n",
      "Epoch [18/50], Train Loss: 0.1507, Val Loss: 0.3882\n",
      "Epoch [19/50], Train Loss: 0.1496, Val Loss: 0.3868\n",
      "Epoch [20/50], Train Loss: 0.1485, Val Loss: 0.3854\n",
      "Epoch [21/50], Train Loss: 0.1471, Val Loss: 0.3841\n",
      "Epoch [22/50], Train Loss: 0.1469, Val Loss: 0.3827\n",
      "Epoch [23/50], Train Loss: 0.1438, Val Loss: 0.3814\n",
      "Epoch [24/50], Train Loss: 0.1462, Val Loss: 0.3801\n",
      "Epoch [25/50], Train Loss: 0.1443, Val Loss: 0.3787\n",
      "Epoch [26/50], Train Loss: 0.1438, Val Loss: 0.3774\n",
      "Epoch [27/50], Train Loss: 0.1432, Val Loss: 0.3761\n",
      "Epoch [28/50], Train Loss: 0.1440, Val Loss: 0.3748\n",
      "Epoch [29/50], Train Loss: 0.1409, Val Loss: 0.3735\n",
      "Epoch [30/50], Train Loss: 0.1399, Val Loss: 0.3722\n",
      "Epoch [31/50], Train Loss: 0.1423, Val Loss: 0.3709\n",
      "Epoch [32/50], Train Loss: 0.1389, Val Loss: 0.3697\n",
      "Epoch [33/50], Train Loss: 0.1394, Val Loss: 0.3684\n",
      "Epoch [34/50], Train Loss: 0.1390, Val Loss: 0.3671\n",
      "Epoch [35/50], Train Loss: 0.1397, Val Loss: 0.3659\n",
      "Epoch [36/50], Train Loss: 0.1381, Val Loss: 0.3646\n",
      "Epoch [37/50], Train Loss: 0.1358, Val Loss: 0.3634\n",
      "Epoch [38/50], Train Loss: 0.1352, Val Loss: 0.3622\n",
      "Epoch [39/50], Train Loss: 0.1352, Val Loss: 0.3610\n",
      "Epoch [40/50], Train Loss: 0.1342, Val Loss: 0.3598\n",
      "Epoch [41/50], Train Loss: 0.1342, Val Loss: 0.3586\n",
      "Epoch [42/50], Train Loss: 0.1341, Val Loss: 0.3574\n",
      "Epoch [43/50], Train Loss: 0.1321, Val Loss: 0.3562\n",
      "Epoch [44/50], Train Loss: 0.1301, Val Loss: 0.3550\n",
      "Epoch [45/50], Train Loss: 0.1326, Val Loss: 0.3538\n",
      "Epoch [46/50], Train Loss: 0.1321, Val Loss: 0.3527\n",
      "Epoch [47/50], Train Loss: 0.1291, Val Loss: 0.3515\n",
      "Epoch [48/50], Train Loss: 0.1304, Val Loss: 0.3504\n",
      "Epoch [49/50], Train Loss: 0.1309, Val Loss: 0.3492\n",
      "Epoch [50/50], Train Loss: 0.1296, Val Loss: 0.3480\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=64, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1486, Val Loss: 0.3808\n",
      "Epoch [2/50], Train Loss: 0.1479, Val Loss: 0.3795\n",
      "Epoch [3/50], Train Loss: 0.1473, Val Loss: 0.3782\n",
      "Epoch [4/50], Train Loss: 0.1466, Val Loss: 0.3769\n",
      "Epoch [5/50], Train Loss: 0.1459, Val Loss: 0.3756\n",
      "Epoch [6/50], Train Loss: 0.1453, Val Loss: 0.3743\n",
      "Epoch [7/50], Train Loss: 0.1446, Val Loss: 0.3730\n",
      "Epoch [8/50], Train Loss: 0.1439, Val Loss: 0.3717\n",
      "Epoch [9/50], Train Loss: 0.1433, Val Loss: 0.3705\n",
      "Epoch [10/50], Train Loss: 0.1426, Val Loss: 0.3692\n",
      "Epoch [11/50], Train Loss: 0.1420, Val Loss: 0.3680\n",
      "Epoch [12/50], Train Loss: 0.1414, Val Loss: 0.3667\n",
      "Epoch [13/50], Train Loss: 0.1407, Val Loss: 0.3655\n",
      "Epoch [14/50], Train Loss: 0.1401, Val Loss: 0.3643\n",
      "Epoch [15/50], Train Loss: 0.1395, Val Loss: 0.3630\n",
      "Epoch [16/50], Train Loss: 0.1388, Val Loss: 0.3618\n",
      "Epoch [17/50], Train Loss: 0.1382, Val Loss: 0.3606\n",
      "Epoch [18/50], Train Loss: 0.1376, Val Loss: 0.3594\n",
      "Epoch [19/50], Train Loss: 0.1370, Val Loss: 0.3582\n",
      "Epoch [20/50], Train Loss: 0.1364, Val Loss: 0.3570\n",
      "Epoch [21/50], Train Loss: 0.1358, Val Loss: 0.3558\n",
      "Epoch [22/50], Train Loss: 0.1352, Val Loss: 0.3546\n",
      "Epoch [23/50], Train Loss: 0.1346, Val Loss: 0.3534\n",
      "Epoch [24/50], Train Loss: 0.1340, Val Loss: 0.3522\n",
      "Epoch [25/50], Train Loss: 0.1334, Val Loss: 0.3510\n",
      "Epoch [26/50], Train Loss: 0.1328, Val Loss: 0.3499\n",
      "Epoch [27/50], Train Loss: 0.1322, Val Loss: 0.3487\n",
      "Epoch [28/50], Train Loss: 0.1316, Val Loss: 0.3475\n",
      "Epoch [29/50], Train Loss: 0.1310, Val Loss: 0.3464\n",
      "Epoch [30/50], Train Loss: 0.1304, Val Loss: 0.3452\n",
      "Epoch [31/50], Train Loss: 0.1299, Val Loss: 0.3441\n",
      "Epoch [32/50], Train Loss: 0.1293, Val Loss: 0.3430\n",
      "Epoch [33/50], Train Loss: 0.1287, Val Loss: 0.3418\n",
      "Epoch [34/50], Train Loss: 0.1282, Val Loss: 0.3407\n",
      "Epoch [35/50], Train Loss: 0.1276, Val Loss: 0.3396\n",
      "Epoch [36/50], Train Loss: 0.1270, Val Loss: 0.3385\n",
      "Epoch [37/50], Train Loss: 0.1265, Val Loss: 0.3373\n",
      "Epoch [38/50], Train Loss: 0.1259, Val Loss: 0.3362\n",
      "Epoch [39/50], Train Loss: 0.1254, Val Loss: 0.3351\n",
      "Epoch [40/50], Train Loss: 0.1248, Val Loss: 0.3340\n",
      "Epoch [41/50], Train Loss: 0.1243, Val Loss: 0.3329\n",
      "Epoch [42/50], Train Loss: 0.1238, Val Loss: 0.3319\n",
      "Epoch [43/50], Train Loss: 0.1232, Val Loss: 0.3308\n",
      "Epoch [44/50], Train Loss: 0.1227, Val Loss: 0.3297\n",
      "Epoch [45/50], Train Loss: 0.1221, Val Loss: 0.3286\n",
      "Epoch [46/50], Train Loss: 0.1216, Val Loss: 0.3276\n",
      "Epoch [47/50], Train Loss: 0.1211, Val Loss: 0.3265\n",
      "Epoch [48/50], Train Loss: 0.1206, Val Loss: 0.3255\n",
      "Epoch [49/50], Train Loss: 0.1201, Val Loss: 0.3244\n",
      "Epoch [50/50], Train Loss: 0.1195, Val Loss: 0.3233\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=64, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1555, Val Loss: 0.3864\n",
      "Epoch [2/50], Train Loss: 0.1540, Val Loss: 0.3852\n",
      "Epoch [3/50], Train Loss: 0.1536, Val Loss: 0.3841\n",
      "Epoch [4/50], Train Loss: 0.1528, Val Loss: 0.3829\n",
      "Epoch [5/50], Train Loss: 0.1532, Val Loss: 0.3818\n",
      "Epoch [6/50], Train Loss: 0.1521, Val Loss: 0.3807\n",
      "Epoch [7/50], Train Loss: 0.1508, Val Loss: 0.3795\n",
      "Epoch [8/50], Train Loss: 0.1511, Val Loss: 0.3784\n",
      "Epoch [9/50], Train Loss: 0.1498, Val Loss: 0.3773\n",
      "Epoch [10/50], Train Loss: 0.1494, Val Loss: 0.3762\n",
      "Epoch [11/50], Train Loss: 0.1490, Val Loss: 0.3751\n",
      "Epoch [12/50], Train Loss: 0.1486, Val Loss: 0.3740\n",
      "Epoch [13/50], Train Loss: 0.1479, Val Loss: 0.3729\n",
      "Epoch [14/50], Train Loss: 0.1475, Val Loss: 0.3718\n",
      "Epoch [15/50], Train Loss: 0.1470, Val Loss: 0.3707\n",
      "Epoch [16/50], Train Loss: 0.1453, Val Loss: 0.3696\n",
      "Epoch [17/50], Train Loss: 0.1455, Val Loss: 0.3685\n",
      "Epoch [18/50], Train Loss: 0.1461, Val Loss: 0.3674\n",
      "Epoch [19/50], Train Loss: 0.1442, Val Loss: 0.3663\n",
      "Epoch [20/50], Train Loss: 0.1433, Val Loss: 0.3653\n",
      "Epoch [21/50], Train Loss: 0.1434, Val Loss: 0.3642\n",
      "Epoch [22/50], Train Loss: 0.1426, Val Loss: 0.3631\n",
      "Epoch [23/50], Train Loss: 0.1429, Val Loss: 0.3621\n",
      "Epoch [24/50], Train Loss: 0.1421, Val Loss: 0.3610\n",
      "Epoch [25/50], Train Loss: 0.1407, Val Loss: 0.3600\n",
      "Epoch [26/50], Train Loss: 0.1409, Val Loss: 0.3589\n",
      "Epoch [27/50], Train Loss: 0.1402, Val Loss: 0.3579\n",
      "Epoch [28/50], Train Loss: 0.1386, Val Loss: 0.3568\n",
      "Epoch [29/50], Train Loss: 0.1391, Val Loss: 0.3558\n",
      "Epoch [30/50], Train Loss: 0.1377, Val Loss: 0.3547\n",
      "Epoch [31/50], Train Loss: 0.1380, Val Loss: 0.3537\n",
      "Epoch [32/50], Train Loss: 0.1379, Val Loss: 0.3527\n",
      "Epoch [33/50], Train Loss: 0.1375, Val Loss: 0.3517\n",
      "Epoch [34/50], Train Loss: 0.1370, Val Loss: 0.3506\n",
      "Epoch [35/50], Train Loss: 0.1363, Val Loss: 0.3496\n",
      "Epoch [36/50], Train Loss: 0.1357, Val Loss: 0.3486\n",
      "Epoch [37/50], Train Loss: 0.1347, Val Loss: 0.3476\n",
      "Epoch [38/50], Train Loss: 0.1342, Val Loss: 0.3466\n",
      "Epoch [39/50], Train Loss: 0.1341, Val Loss: 0.3456\n",
      "Epoch [40/50], Train Loss: 0.1338, Val Loss: 0.3446\n",
      "Epoch [41/50], Train Loss: 0.1332, Val Loss: 0.3436\n",
      "Epoch [42/50], Train Loss: 0.1332, Val Loss: 0.3426\n",
      "Epoch [43/50], Train Loss: 0.1328, Val Loss: 0.3416\n",
      "Epoch [44/50], Train Loss: 0.1315, Val Loss: 0.3406\n",
      "Epoch [45/50], Train Loss: 0.1315, Val Loss: 0.3397\n",
      "Epoch [46/50], Train Loss: 0.1311, Val Loss: 0.3387\n",
      "Epoch [47/50], Train Loss: 0.1299, Val Loss: 0.3377\n",
      "Epoch [48/50], Train Loss: 0.1297, Val Loss: 0.3368\n",
      "Epoch [49/50], Train Loss: 0.1290, Val Loss: 0.3358\n",
      "Epoch [50/50], Train Loss: 0.1289, Val Loss: 0.3348\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=64, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1671, Val Loss: 0.4362\n",
      "Epoch [2/50], Train Loss: 0.1668, Val Loss: 0.4348\n",
      "Epoch [3/50], Train Loss: 0.1672, Val Loss: 0.4335\n",
      "Epoch [4/50], Train Loss: 0.1657, Val Loss: 0.4322\n",
      "Epoch [5/50], Train Loss: 0.1670, Val Loss: 0.4308\n",
      "Epoch [6/50], Train Loss: 0.1654, Val Loss: 0.4295\n",
      "Epoch [7/50], Train Loss: 0.1655, Val Loss: 0.4281\n",
      "Epoch [8/50], Train Loss: 0.1638, Val Loss: 0.4268\n",
      "Epoch [9/50], Train Loss: 0.1625, Val Loss: 0.4255\n",
      "Epoch [10/50], Train Loss: 0.1634, Val Loss: 0.4242\n",
      "Epoch [11/50], Train Loss: 0.1617, Val Loss: 0.4229\n",
      "Epoch [12/50], Train Loss: 0.1607, Val Loss: 0.4216\n",
      "Epoch [13/50], Train Loss: 0.1602, Val Loss: 0.4203\n",
      "Epoch [14/50], Train Loss: 0.1593, Val Loss: 0.4190\n",
      "Epoch [15/50], Train Loss: 0.1600, Val Loss: 0.4177\n",
      "Epoch [16/50], Train Loss: 0.1572, Val Loss: 0.4165\n",
      "Epoch [17/50], Train Loss: 0.1576, Val Loss: 0.4152\n",
      "Epoch [18/50], Train Loss: 0.1579, Val Loss: 0.4139\n",
      "Epoch [19/50], Train Loss: 0.1574, Val Loss: 0.4127\n",
      "Epoch [20/50], Train Loss: 0.1555, Val Loss: 0.4115\n",
      "Epoch [21/50], Train Loss: 0.1544, Val Loss: 0.4102\n",
      "Epoch [22/50], Train Loss: 0.1539, Val Loss: 0.4090\n",
      "Epoch [23/50], Train Loss: 0.1545, Val Loss: 0.4077\n",
      "Epoch [24/50], Train Loss: 0.1532, Val Loss: 0.4065\n",
      "Epoch [25/50], Train Loss: 0.1548, Val Loss: 0.4053\n",
      "Epoch [26/50], Train Loss: 0.1522, Val Loss: 0.4041\n",
      "Epoch [27/50], Train Loss: 0.1517, Val Loss: 0.4028\n",
      "Epoch [28/50], Train Loss: 0.1512, Val Loss: 0.4016\n",
      "Epoch [29/50], Train Loss: 0.1521, Val Loss: 0.4004\n",
      "Epoch [30/50], Train Loss: 0.1503, Val Loss: 0.3992\n",
      "Epoch [31/50], Train Loss: 0.1493, Val Loss: 0.3980\n",
      "Epoch [32/50], Train Loss: 0.1490, Val Loss: 0.3968\n",
      "Epoch [33/50], Train Loss: 0.1483, Val Loss: 0.3957\n",
      "Epoch [34/50], Train Loss: 0.1481, Val Loss: 0.3945\n",
      "Epoch [35/50], Train Loss: 0.1465, Val Loss: 0.3933\n",
      "Epoch [36/50], Train Loss: 0.1478, Val Loss: 0.3921\n",
      "Epoch [37/50], Train Loss: 0.1472, Val Loss: 0.3910\n",
      "Epoch [38/50], Train Loss: 0.1453, Val Loss: 0.3898\n",
      "Epoch [39/50], Train Loss: 0.1452, Val Loss: 0.3886\n",
      "Epoch [40/50], Train Loss: 0.1454, Val Loss: 0.3875\n",
      "Epoch [41/50], Train Loss: 0.1455, Val Loss: 0.3863\n",
      "Epoch [42/50], Train Loss: 0.1428, Val Loss: 0.3852\n",
      "Epoch [43/50], Train Loss: 0.1424, Val Loss: 0.3840\n",
      "Epoch [44/50], Train Loss: 0.1418, Val Loss: 0.3829\n",
      "Epoch [45/50], Train Loss: 0.1426, Val Loss: 0.3818\n",
      "Epoch [46/50], Train Loss: 0.1413, Val Loss: 0.3806\n",
      "Epoch [47/50], Train Loss: 0.1417, Val Loss: 0.3795\n",
      "Epoch [48/50], Train Loss: 0.1411, Val Loss: 0.3784\n",
      "Epoch [49/50], Train Loss: 0.1377, Val Loss: 0.3773\n",
      "Epoch [50/50], Train Loss: 0.1396, Val Loss: 0.3762\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=64, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1875, Val Loss: 0.4576\n",
      "Epoch [2/50], Train Loss: 0.1865, Val Loss: 0.4559\n",
      "Epoch [3/50], Train Loss: 0.1856, Val Loss: 0.4542\n",
      "Epoch [4/50], Train Loss: 0.1846, Val Loss: 0.4525\n",
      "Epoch [5/50], Train Loss: 0.1837, Val Loss: 0.4508\n",
      "Epoch [6/50], Train Loss: 0.1828, Val Loss: 0.4492\n",
      "Epoch [7/50], Train Loss: 0.1818, Val Loss: 0.4475\n",
      "Epoch [8/50], Train Loss: 0.1809, Val Loss: 0.4458\n",
      "Epoch [9/50], Train Loss: 0.1800, Val Loss: 0.4442\n",
      "Epoch [10/50], Train Loss: 0.1791, Val Loss: 0.4426\n",
      "Epoch [11/50], Train Loss: 0.1782, Val Loss: 0.4409\n",
      "Epoch [12/50], Train Loss: 0.1773, Val Loss: 0.4393\n",
      "Epoch [13/50], Train Loss: 0.1764, Val Loss: 0.4377\n",
      "Epoch [14/50], Train Loss: 0.1755, Val Loss: 0.4361\n",
      "Epoch [15/50], Train Loss: 0.1746, Val Loss: 0.4345\n",
      "Epoch [16/50], Train Loss: 0.1737, Val Loss: 0.4329\n",
      "Epoch [17/50], Train Loss: 0.1729, Val Loss: 0.4313\n",
      "Epoch [18/50], Train Loss: 0.1720, Val Loss: 0.4297\n",
      "Epoch [19/50], Train Loss: 0.1711, Val Loss: 0.4282\n",
      "Epoch [20/50], Train Loss: 0.1703, Val Loss: 0.4266\n",
      "Epoch [21/50], Train Loss: 0.1694, Val Loss: 0.4251\n",
      "Epoch [22/50], Train Loss: 0.1686, Val Loss: 0.4235\n",
      "Epoch [23/50], Train Loss: 0.1677, Val Loss: 0.4220\n",
      "Epoch [24/50], Train Loss: 0.1669, Val Loss: 0.4205\n",
      "Epoch [25/50], Train Loss: 0.1661, Val Loss: 0.4190\n",
      "Epoch [26/50], Train Loss: 0.1652, Val Loss: 0.4175\n",
      "Epoch [27/50], Train Loss: 0.1644, Val Loss: 0.4159\n",
      "Epoch [28/50], Train Loss: 0.1636, Val Loss: 0.4145\n",
      "Epoch [29/50], Train Loss: 0.1628, Val Loss: 0.4130\n",
      "Epoch [30/50], Train Loss: 0.1620, Val Loss: 0.4115\n",
      "Epoch [31/50], Train Loss: 0.1612, Val Loss: 0.4100\n",
      "Epoch [32/50], Train Loss: 0.1604, Val Loss: 0.4085\n",
      "Epoch [33/50], Train Loss: 0.1596, Val Loss: 0.4071\n",
      "Epoch [34/50], Train Loss: 0.1588, Val Loss: 0.4056\n",
      "Epoch [35/50], Train Loss: 0.1580, Val Loss: 0.4042\n",
      "Epoch [36/50], Train Loss: 0.1573, Val Loss: 0.4027\n",
      "Epoch [37/50], Train Loss: 0.1565, Val Loss: 0.4013\n",
      "Epoch [38/50], Train Loss: 0.1557, Val Loss: 0.3999\n",
      "Epoch [39/50], Train Loss: 0.1550, Val Loss: 0.3985\n",
      "Epoch [40/50], Train Loss: 0.1542, Val Loss: 0.3971\n",
      "Epoch [41/50], Train Loss: 0.1535, Val Loss: 0.3957\n",
      "Epoch [42/50], Train Loss: 0.1527, Val Loss: 0.3943\n",
      "Epoch [43/50], Train Loss: 0.1520, Val Loss: 0.3929\n",
      "Epoch [44/50], Train Loss: 0.1512, Val Loss: 0.3915\n",
      "Epoch [45/50], Train Loss: 0.1505, Val Loss: 0.3901\n",
      "Epoch [46/50], Train Loss: 0.1498, Val Loss: 0.3888\n",
      "Epoch [47/50], Train Loss: 0.1490, Val Loss: 0.3874\n",
      "Epoch [48/50], Train Loss: 0.1483, Val Loss: 0.3860\n",
      "Epoch [49/50], Train Loss: 0.1476, Val Loss: 0.3847\n",
      "Epoch [50/50], Train Loss: 0.1469, Val Loss: 0.3834\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=64, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1932, Val Loss: 0.4806\n",
      "Epoch [2/50], Train Loss: 0.1918, Val Loss: 0.4789\n",
      "Epoch [3/50], Train Loss: 0.1913, Val Loss: 0.4771\n",
      "Epoch [4/50], Train Loss: 0.1909, Val Loss: 0.4754\n",
      "Epoch [5/50], Train Loss: 0.1901, Val Loss: 0.4736\n",
      "Epoch [6/50], Train Loss: 0.1894, Val Loss: 0.4719\n",
      "Epoch [7/50], Train Loss: 0.1869, Val Loss: 0.4701\n",
      "Epoch [8/50], Train Loss: 0.1864, Val Loss: 0.4684\n",
      "Epoch [9/50], Train Loss: 0.1854, Val Loss: 0.4667\n",
      "Epoch [10/50], Train Loss: 0.1843, Val Loss: 0.4650\n",
      "Epoch [11/50], Train Loss: 0.1836, Val Loss: 0.4634\n",
      "Epoch [12/50], Train Loss: 0.1822, Val Loss: 0.4617\n",
      "Epoch [13/50], Train Loss: 0.1818, Val Loss: 0.4601\n",
      "Epoch [14/50], Train Loss: 0.1813, Val Loss: 0.4584\n",
      "Epoch [15/50], Train Loss: 0.1804, Val Loss: 0.4568\n",
      "Epoch [16/50], Train Loss: 0.1791, Val Loss: 0.4551\n",
      "Epoch [17/50], Train Loss: 0.1789, Val Loss: 0.4535\n",
      "Epoch [18/50], Train Loss: 0.1780, Val Loss: 0.4519\n",
      "Epoch [19/50], Train Loss: 0.1767, Val Loss: 0.4503\n",
      "Epoch [20/50], Train Loss: 0.1759, Val Loss: 0.4487\n",
      "Epoch [21/50], Train Loss: 0.1752, Val Loss: 0.4471\n",
      "Epoch [22/50], Train Loss: 0.1731, Val Loss: 0.4456\n",
      "Epoch [23/50], Train Loss: 0.1738, Val Loss: 0.4440\n",
      "Epoch [24/50], Train Loss: 0.1725, Val Loss: 0.4424\n",
      "Epoch [25/50], Train Loss: 0.1723, Val Loss: 0.4409\n",
      "Epoch [26/50], Train Loss: 0.1714, Val Loss: 0.4394\n",
      "Epoch [27/50], Train Loss: 0.1702, Val Loss: 0.4378\n",
      "Epoch [28/50], Train Loss: 0.1691, Val Loss: 0.4363\n",
      "Epoch [29/50], Train Loss: 0.1682, Val Loss: 0.4348\n",
      "Epoch [30/50], Train Loss: 0.1683, Val Loss: 0.4333\n",
      "Epoch [31/50], Train Loss: 0.1667, Val Loss: 0.4318\n",
      "Epoch [32/50], Train Loss: 0.1660, Val Loss: 0.4304\n",
      "Epoch [33/50], Train Loss: 0.1656, Val Loss: 0.4289\n",
      "Epoch [34/50], Train Loss: 0.1641, Val Loss: 0.4274\n",
      "Epoch [35/50], Train Loss: 0.1634, Val Loss: 0.4260\n",
      "Epoch [36/50], Train Loss: 0.1631, Val Loss: 0.4245\n",
      "Epoch [37/50], Train Loss: 0.1617, Val Loss: 0.4231\n",
      "Epoch [38/50], Train Loss: 0.1616, Val Loss: 0.4217\n",
      "Epoch [39/50], Train Loss: 0.1603, Val Loss: 0.4203\n",
      "Epoch [40/50], Train Loss: 0.1599, Val Loss: 0.4189\n",
      "Epoch [41/50], Train Loss: 0.1593, Val Loss: 0.4175\n",
      "Epoch [42/50], Train Loss: 0.1583, Val Loss: 0.4161\n",
      "Epoch [43/50], Train Loss: 0.1588, Val Loss: 0.4147\n",
      "Epoch [44/50], Train Loss: 0.1574, Val Loss: 0.4133\n",
      "Epoch [45/50], Train Loss: 0.1569, Val Loss: 0.4119\n",
      "Epoch [46/50], Train Loss: 0.1554, Val Loss: 0.4106\n",
      "Epoch [47/50], Train Loss: 0.1544, Val Loss: 0.4092\n",
      "Epoch [48/50], Train Loss: 0.1540, Val Loss: 0.4079\n",
      "Epoch [49/50], Train Loss: 0.1533, Val Loss: 0.4065\n",
      "Epoch [50/50], Train Loss: 0.1521, Val Loss: 0.4052\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=64, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1588, Val Loss: 0.4134\n",
      "Epoch [2/50], Train Loss: 0.1582, Val Loss: 0.4119\n",
      "Epoch [3/50], Train Loss: 0.1601, Val Loss: 0.4103\n",
      "Epoch [4/50], Train Loss: 0.1572, Val Loss: 0.4089\n",
      "Epoch [5/50], Train Loss: 0.1562, Val Loss: 0.4074\n",
      "Epoch [6/50], Train Loss: 0.1553, Val Loss: 0.4059\n",
      "Epoch [7/50], Train Loss: 0.1552, Val Loss: 0.4045\n",
      "Epoch [8/50], Train Loss: 0.1543, Val Loss: 0.4030\n",
      "Epoch [9/50], Train Loss: 0.1568, Val Loss: 0.4015\n",
      "Epoch [10/50], Train Loss: 0.1518, Val Loss: 0.4001\n",
      "Epoch [11/50], Train Loss: 0.1526, Val Loss: 0.3987\n",
      "Epoch [12/50], Train Loss: 0.1519, Val Loss: 0.3972\n",
      "Epoch [13/50], Train Loss: 0.1505, Val Loss: 0.3958\n",
      "Epoch [14/50], Train Loss: 0.1501, Val Loss: 0.3944\n",
      "Epoch [15/50], Train Loss: 0.1506, Val Loss: 0.3930\n",
      "Epoch [16/50], Train Loss: 0.1491, Val Loss: 0.3917\n",
      "Epoch [17/50], Train Loss: 0.1489, Val Loss: 0.3903\n",
      "Epoch [18/50], Train Loss: 0.1480, Val Loss: 0.3889\n",
      "Epoch [19/50], Train Loss: 0.1486, Val Loss: 0.3875\n",
      "Epoch [20/50], Train Loss: 0.1467, Val Loss: 0.3862\n",
      "Epoch [21/50], Train Loss: 0.1450, Val Loss: 0.3848\n",
      "Epoch [22/50], Train Loss: 0.1435, Val Loss: 0.3835\n",
      "Epoch [23/50], Train Loss: 0.1445, Val Loss: 0.3822\n",
      "Epoch [24/50], Train Loss: 0.1420, Val Loss: 0.3809\n",
      "Epoch [25/50], Train Loss: 0.1422, Val Loss: 0.3796\n",
      "Epoch [26/50], Train Loss: 0.1431, Val Loss: 0.3783\n",
      "Epoch [27/50], Train Loss: 0.1403, Val Loss: 0.3770\n",
      "Epoch [28/50], Train Loss: 0.1418, Val Loss: 0.3757\n",
      "Epoch [29/50], Train Loss: 0.1394, Val Loss: 0.3745\n",
      "Epoch [30/50], Train Loss: 0.1398, Val Loss: 0.3732\n",
      "Epoch [31/50], Train Loss: 0.1388, Val Loss: 0.3719\n",
      "Epoch [32/50], Train Loss: 0.1389, Val Loss: 0.3707\n",
      "Epoch [33/50], Train Loss: 0.1379, Val Loss: 0.3695\n",
      "Epoch [34/50], Train Loss: 0.1364, Val Loss: 0.3682\n",
      "Epoch [35/50], Train Loss: 0.1376, Val Loss: 0.3670\n",
      "Epoch [36/50], Train Loss: 0.1363, Val Loss: 0.3657\n",
      "Epoch [37/50], Train Loss: 0.1359, Val Loss: 0.3645\n",
      "Epoch [38/50], Train Loss: 0.1346, Val Loss: 0.3633\n",
      "Epoch [39/50], Train Loss: 0.1340, Val Loss: 0.3621\n",
      "Epoch [40/50], Train Loss: 0.1330, Val Loss: 0.3610\n",
      "Epoch [41/50], Train Loss: 0.1312, Val Loss: 0.3598\n",
      "Epoch [42/50], Train Loss: 0.1331, Val Loss: 0.3586\n",
      "Epoch [43/50], Train Loss: 0.1314, Val Loss: 0.3574\n",
      "Epoch [44/50], Train Loss: 0.1313, Val Loss: 0.3563\n",
      "Epoch [45/50], Train Loss: 0.1324, Val Loss: 0.3551\n",
      "Epoch [46/50], Train Loss: 0.1297, Val Loss: 0.3539\n",
      "Epoch [47/50], Train Loss: 0.1274, Val Loss: 0.3528\n",
      "Epoch [48/50], Train Loss: 0.1307, Val Loss: 0.3517\n",
      "Epoch [49/50], Train Loss: 0.1278, Val Loss: 0.3505\n",
      "Epoch [50/50], Train Loss: 0.1277, Val Loss: 0.3494\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=64, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1649, Val Loss: 0.4088\n",
      "Epoch [2/50], Train Loss: 0.1641, Val Loss: 0.4075\n",
      "Epoch [3/50], Train Loss: 0.1634, Val Loss: 0.4062\n",
      "Epoch [4/50], Train Loss: 0.1626, Val Loss: 0.4049\n",
      "Epoch [5/50], Train Loss: 0.1619, Val Loss: 0.4037\n",
      "Epoch [6/50], Train Loss: 0.1611, Val Loss: 0.4024\n",
      "Epoch [7/50], Train Loss: 0.1604, Val Loss: 0.4011\n",
      "Epoch [8/50], Train Loss: 0.1596, Val Loss: 0.3999\n",
      "Epoch [9/50], Train Loss: 0.1589, Val Loss: 0.3986\n",
      "Epoch [10/50], Train Loss: 0.1582, Val Loss: 0.3974\n",
      "Epoch [11/50], Train Loss: 0.1574, Val Loss: 0.3962\n",
      "Epoch [12/50], Train Loss: 0.1567, Val Loss: 0.3950\n",
      "Epoch [13/50], Train Loss: 0.1560, Val Loss: 0.3937\n",
      "Epoch [14/50], Train Loss: 0.1553, Val Loss: 0.3925\n",
      "Epoch [15/50], Train Loss: 0.1546, Val Loss: 0.3913\n",
      "Epoch [16/50], Train Loss: 0.1539, Val Loss: 0.3901\n",
      "Epoch [17/50], Train Loss: 0.1532, Val Loss: 0.3889\n",
      "Epoch [18/50], Train Loss: 0.1525, Val Loss: 0.3877\n",
      "Epoch [19/50], Train Loss: 0.1518, Val Loss: 0.3865\n",
      "Epoch [20/50], Train Loss: 0.1511, Val Loss: 0.3854\n",
      "Epoch [21/50], Train Loss: 0.1504, Val Loss: 0.3842\n",
      "Epoch [22/50], Train Loss: 0.1498, Val Loss: 0.3830\n",
      "Epoch [23/50], Train Loss: 0.1491, Val Loss: 0.3819\n",
      "Epoch [24/50], Train Loss: 0.1484, Val Loss: 0.3807\n",
      "Epoch [25/50], Train Loss: 0.1478, Val Loss: 0.3796\n",
      "Epoch [26/50], Train Loss: 0.1471, Val Loss: 0.3784\n",
      "Epoch [27/50], Train Loss: 0.1464, Val Loss: 0.3773\n",
      "Epoch [28/50], Train Loss: 0.1458, Val Loss: 0.3762\n",
      "Epoch [29/50], Train Loss: 0.1451, Val Loss: 0.3750\n",
      "Epoch [30/50], Train Loss: 0.1445, Val Loss: 0.3739\n",
      "Epoch [31/50], Train Loss: 0.1439, Val Loss: 0.3728\n",
      "Epoch [32/50], Train Loss: 0.1432, Val Loss: 0.3717\n",
      "Epoch [33/50], Train Loss: 0.1426, Val Loss: 0.3706\n",
      "Epoch [34/50], Train Loss: 0.1420, Val Loss: 0.3695\n",
      "Epoch [35/50], Train Loss: 0.1413, Val Loss: 0.3684\n",
      "Epoch [36/50], Train Loss: 0.1407, Val Loss: 0.3673\n",
      "Epoch [37/50], Train Loss: 0.1401, Val Loss: 0.3663\n",
      "Epoch [38/50], Train Loss: 0.1395, Val Loss: 0.3652\n",
      "Epoch [39/50], Train Loss: 0.1389, Val Loss: 0.3641\n",
      "Epoch [40/50], Train Loss: 0.1383, Val Loss: 0.3630\n",
      "Epoch [41/50], Train Loss: 0.1377, Val Loss: 0.3620\n",
      "Epoch [42/50], Train Loss: 0.1371, Val Loss: 0.3609\n",
      "Epoch [43/50], Train Loss: 0.1365, Val Loss: 0.3599\n",
      "Epoch [44/50], Train Loss: 0.1359, Val Loss: 0.3588\n",
      "Epoch [45/50], Train Loss: 0.1353, Val Loss: 0.3578\n",
      "Epoch [46/50], Train Loss: 0.1347, Val Loss: 0.3568\n",
      "Epoch [47/50], Train Loss: 0.1341, Val Loss: 0.3557\n",
      "Epoch [48/50], Train Loss: 0.1335, Val Loss: 0.3547\n",
      "Epoch [49/50], Train Loss: 0.1330, Val Loss: 0.3537\n",
      "Epoch [50/50], Train Loss: 0.1324, Val Loss: 0.3527\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=64, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1153, Val Loss: 0.3150\n",
      "Epoch [2/50], Train Loss: 0.1143, Val Loss: 0.3141\n",
      "Epoch [3/50], Train Loss: 0.1139, Val Loss: 0.3132\n",
      "Epoch [4/50], Train Loss: 0.1135, Val Loss: 0.3123\n",
      "Epoch [5/50], Train Loss: 0.1141, Val Loss: 0.3114\n",
      "Epoch [6/50], Train Loss: 0.1133, Val Loss: 0.3105\n",
      "Epoch [7/50], Train Loss: 0.1128, Val Loss: 0.3097\n",
      "Epoch [8/50], Train Loss: 0.1115, Val Loss: 0.3088\n",
      "Epoch [9/50], Train Loss: 0.1118, Val Loss: 0.3079\n",
      "Epoch [10/50], Train Loss: 0.1114, Val Loss: 0.3071\n",
      "Epoch [11/50], Train Loss: 0.1109, Val Loss: 0.3062\n",
      "Epoch [12/50], Train Loss: 0.1101, Val Loss: 0.3054\n",
      "Epoch [13/50], Train Loss: 0.1096, Val Loss: 0.3045\n",
      "Epoch [14/50], Train Loss: 0.1096, Val Loss: 0.3037\n",
      "Epoch [15/50], Train Loss: 0.1084, Val Loss: 0.3029\n",
      "Epoch [16/50], Train Loss: 0.1076, Val Loss: 0.3020\n",
      "Epoch [17/50], Train Loss: 0.1083, Val Loss: 0.3012\n",
      "Epoch [18/50], Train Loss: 0.1069, Val Loss: 0.3004\n",
      "Epoch [19/50], Train Loss: 0.1071, Val Loss: 0.2996\n",
      "Epoch [20/50], Train Loss: 0.1061, Val Loss: 0.2987\n",
      "Epoch [21/50], Train Loss: 0.1051, Val Loss: 0.2979\n",
      "Epoch [22/50], Train Loss: 0.1058, Val Loss: 0.2971\n",
      "Epoch [23/50], Train Loss: 0.1058, Val Loss: 0.2963\n",
      "Epoch [24/50], Train Loss: 0.1052, Val Loss: 0.2955\n",
      "Epoch [25/50], Train Loss: 0.1042, Val Loss: 0.2947\n",
      "Epoch [26/50], Train Loss: 0.1036, Val Loss: 0.2939\n",
      "Epoch [27/50], Train Loss: 0.1041, Val Loss: 0.2931\n",
      "Epoch [28/50], Train Loss: 0.1027, Val Loss: 0.2923\n",
      "Epoch [29/50], Train Loss: 0.1026, Val Loss: 0.2916\n",
      "Epoch [30/50], Train Loss: 0.1020, Val Loss: 0.2908\n",
      "Epoch [31/50], Train Loss: 0.1014, Val Loss: 0.2900\n",
      "Epoch [32/50], Train Loss: 0.1013, Val Loss: 0.2893\n",
      "Epoch [33/50], Train Loss: 0.1013, Val Loss: 0.2885\n",
      "Epoch [34/50], Train Loss: 0.1007, Val Loss: 0.2877\n",
      "Epoch [35/50], Train Loss: 0.1004, Val Loss: 0.2870\n",
      "Epoch [36/50], Train Loss: 0.0997, Val Loss: 0.2862\n",
      "Epoch [37/50], Train Loss: 0.0997, Val Loss: 0.2854\n",
      "Epoch [38/50], Train Loss: 0.0987, Val Loss: 0.2847\n",
      "Epoch [39/50], Train Loss: 0.0988, Val Loss: 0.2840\n",
      "Epoch [40/50], Train Loss: 0.0982, Val Loss: 0.2832\n",
      "Epoch [41/50], Train Loss: 0.0979, Val Loss: 0.2825\n",
      "Epoch [42/50], Train Loss: 0.0969, Val Loss: 0.2818\n",
      "Epoch [43/50], Train Loss: 0.0972, Val Loss: 0.2810\n",
      "Epoch [44/50], Train Loss: 0.0971, Val Loss: 0.2803\n",
      "Epoch [45/50], Train Loss: 0.0972, Val Loss: 0.2796\n",
      "Epoch [46/50], Train Loss: 0.0956, Val Loss: 0.2788\n",
      "Epoch [47/50], Train Loss: 0.0957, Val Loss: 0.2781\n",
      "Epoch [48/50], Train Loss: 0.0949, Val Loss: 0.2774\n",
      "Epoch [49/50], Train Loss: 0.0952, Val Loss: 0.2767\n",
      "Epoch [50/50], Train Loss: 0.0947, Val Loss: 0.2760\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=64, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1655, Val Loss: 0.4268\n",
      "Epoch [2/50], Train Loss: 0.1658, Val Loss: 0.4253\n",
      "Epoch [3/50], Train Loss: 0.1669, Val Loss: 0.4237\n",
      "Epoch [4/50], Train Loss: 0.1656, Val Loss: 0.4221\n",
      "Epoch [5/50], Train Loss: 0.1634, Val Loss: 0.4205\n",
      "Epoch [6/50], Train Loss: 0.1636, Val Loss: 0.4189\n",
      "Epoch [7/50], Train Loss: 0.1617, Val Loss: 0.4174\n",
      "Epoch [8/50], Train Loss: 0.1608, Val Loss: 0.4158\n",
      "Epoch [9/50], Train Loss: 0.1595, Val Loss: 0.4143\n",
      "Epoch [10/50], Train Loss: 0.1580, Val Loss: 0.4128\n",
      "Epoch [11/50], Train Loss: 0.1589, Val Loss: 0.4113\n",
      "Epoch [12/50], Train Loss: 0.1572, Val Loss: 0.4098\n",
      "Epoch [13/50], Train Loss: 0.1573, Val Loss: 0.4083\n",
      "Epoch [14/50], Train Loss: 0.1561, Val Loss: 0.4069\n",
      "Epoch [15/50], Train Loss: 0.1558, Val Loss: 0.4054\n",
      "Epoch [16/50], Train Loss: 0.1546, Val Loss: 0.4039\n",
      "Epoch [17/50], Train Loss: 0.1531, Val Loss: 0.4024\n",
      "Epoch [18/50], Train Loss: 0.1534, Val Loss: 0.4010\n",
      "Epoch [19/50], Train Loss: 0.1526, Val Loss: 0.3995\n",
      "Epoch [20/50], Train Loss: 0.1509, Val Loss: 0.3981\n",
      "Epoch [21/50], Train Loss: 0.1492, Val Loss: 0.3967\n",
      "Epoch [22/50], Train Loss: 0.1499, Val Loss: 0.3953\n",
      "Epoch [23/50], Train Loss: 0.1484, Val Loss: 0.3939\n",
      "Epoch [24/50], Train Loss: 0.1476, Val Loss: 0.3925\n",
      "Epoch [25/50], Train Loss: 0.1479, Val Loss: 0.3911\n",
      "Epoch [26/50], Train Loss: 0.1473, Val Loss: 0.3897\n",
      "Epoch [27/50], Train Loss: 0.1467, Val Loss: 0.3883\n",
      "Epoch [28/50], Train Loss: 0.1446, Val Loss: 0.3870\n",
      "Epoch [29/50], Train Loss: 0.1449, Val Loss: 0.3856\n",
      "Epoch [30/50], Train Loss: 0.1443, Val Loss: 0.3843\n",
      "Epoch [31/50], Train Loss: 0.1437, Val Loss: 0.3829\n",
      "Epoch [32/50], Train Loss: 0.1437, Val Loss: 0.3816\n",
      "Epoch [33/50], Train Loss: 0.1423, Val Loss: 0.3802\n",
      "Epoch [34/50], Train Loss: 0.1421, Val Loss: 0.3789\n",
      "Epoch [35/50], Train Loss: 0.1400, Val Loss: 0.3776\n",
      "Epoch [36/50], Train Loss: 0.1404, Val Loss: 0.3763\n",
      "Epoch [37/50], Train Loss: 0.1405, Val Loss: 0.3750\n",
      "Epoch [38/50], Train Loss: 0.1379, Val Loss: 0.3737\n",
      "Epoch [39/50], Train Loss: 0.1385, Val Loss: 0.3724\n",
      "Epoch [40/50], Train Loss: 0.1363, Val Loss: 0.3711\n",
      "Epoch [41/50], Train Loss: 0.1378, Val Loss: 0.3698\n",
      "Epoch [42/50], Train Loss: 0.1363, Val Loss: 0.3686\n",
      "Epoch [43/50], Train Loss: 0.1352, Val Loss: 0.3673\n",
      "Epoch [44/50], Train Loss: 0.1347, Val Loss: 0.3660\n",
      "Epoch [45/50], Train Loss: 0.1339, Val Loss: 0.3648\n",
      "Epoch [46/50], Train Loss: 0.1325, Val Loss: 0.3636\n",
      "Epoch [47/50], Train Loss: 0.1340, Val Loss: 0.3623\n",
      "Epoch [48/50], Train Loss: 0.1324, Val Loss: 0.3611\n",
      "Epoch [49/50], Train Loss: 0.1326, Val Loss: 0.3599\n",
      "Epoch [50/50], Train Loss: 0.1311, Val Loss: 0.3587\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=128, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1308, Val Loss: 0.3618\n",
      "Epoch [2/50], Train Loss: 0.1303, Val Loss: 0.3608\n",
      "Epoch [3/50], Train Loss: 0.1298, Val Loss: 0.3598\n",
      "Epoch [4/50], Train Loss: 0.1293, Val Loss: 0.3587\n",
      "Epoch [5/50], Train Loss: 0.1288, Val Loss: 0.3577\n",
      "Epoch [6/50], Train Loss: 0.1283, Val Loss: 0.3567\n",
      "Epoch [7/50], Train Loss: 0.1278, Val Loss: 0.3557\n",
      "Epoch [8/50], Train Loss: 0.1273, Val Loss: 0.3547\n",
      "Epoch [9/50], Train Loss: 0.1269, Val Loss: 0.3537\n",
      "Epoch [10/50], Train Loss: 0.1264, Val Loss: 0.3527\n",
      "Epoch [11/50], Train Loss: 0.1259, Val Loss: 0.3517\n",
      "Epoch [12/50], Train Loss: 0.1254, Val Loss: 0.3507\n",
      "Epoch [13/50], Train Loss: 0.1249, Val Loss: 0.3497\n",
      "Epoch [14/50], Train Loss: 0.1245, Val Loss: 0.3487\n",
      "Epoch [15/50], Train Loss: 0.1240, Val Loss: 0.3477\n",
      "Epoch [16/50], Train Loss: 0.1235, Val Loss: 0.3467\n",
      "Epoch [17/50], Train Loss: 0.1231, Val Loss: 0.3458\n",
      "Epoch [18/50], Train Loss: 0.1226, Val Loss: 0.3448\n",
      "Epoch [19/50], Train Loss: 0.1221, Val Loss: 0.3438\n",
      "Epoch [20/50], Train Loss: 0.1217, Val Loss: 0.3429\n",
      "Epoch [21/50], Train Loss: 0.1212, Val Loss: 0.3419\n",
      "Epoch [22/50], Train Loss: 0.1208, Val Loss: 0.3410\n",
      "Epoch [23/50], Train Loss: 0.1203, Val Loss: 0.3400\n",
      "Epoch [24/50], Train Loss: 0.1199, Val Loss: 0.3391\n",
      "Epoch [25/50], Train Loss: 0.1194, Val Loss: 0.3382\n",
      "Epoch [26/50], Train Loss: 0.1190, Val Loss: 0.3372\n",
      "Epoch [27/50], Train Loss: 0.1185, Val Loss: 0.3363\n",
      "Epoch [28/50], Train Loss: 0.1181, Val Loss: 0.3354\n",
      "Epoch [29/50], Train Loss: 0.1177, Val Loss: 0.3344\n",
      "Epoch [30/50], Train Loss: 0.1172, Val Loss: 0.3335\n",
      "Epoch [31/50], Train Loss: 0.1168, Val Loss: 0.3326\n",
      "Epoch [32/50], Train Loss: 0.1164, Val Loss: 0.3317\n",
      "Epoch [33/50], Train Loss: 0.1159, Val Loss: 0.3308\n",
      "Epoch [34/50], Train Loss: 0.1155, Val Loss: 0.3299\n",
      "Epoch [35/50], Train Loss: 0.1151, Val Loss: 0.3290\n",
      "Epoch [36/50], Train Loss: 0.1147, Val Loss: 0.3281\n",
      "Epoch [37/50], Train Loss: 0.1142, Val Loss: 0.3272\n",
      "Epoch [38/50], Train Loss: 0.1138, Val Loss: 0.3263\n",
      "Epoch [39/50], Train Loss: 0.1134, Val Loss: 0.3254\n",
      "Epoch [40/50], Train Loss: 0.1130, Val Loss: 0.3245\n",
      "Epoch [41/50], Train Loss: 0.1126, Val Loss: 0.3237\n",
      "Epoch [42/50], Train Loss: 0.1122, Val Loss: 0.3228\n",
      "Epoch [43/50], Train Loss: 0.1118, Val Loss: 0.3219\n",
      "Epoch [44/50], Train Loss: 0.1114, Val Loss: 0.3211\n",
      "Epoch [45/50], Train Loss: 0.1110, Val Loss: 0.3202\n",
      "Epoch [46/50], Train Loss: 0.1106, Val Loss: 0.3194\n",
      "Epoch [47/50], Train Loss: 0.1102, Val Loss: 0.3185\n",
      "Epoch [48/50], Train Loss: 0.1098, Val Loss: 0.3176\n",
      "Epoch [49/50], Train Loss: 0.1094, Val Loss: 0.3168\n",
      "Epoch [50/50], Train Loss: 0.1090, Val Loss: 0.3159\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=128, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1122, Val Loss: 0.3374\n",
      "Epoch [2/50], Train Loss: 0.1119, Val Loss: 0.3364\n",
      "Epoch [3/50], Train Loss: 0.1114, Val Loss: 0.3354\n",
      "Epoch [4/50], Train Loss: 0.1104, Val Loss: 0.3344\n",
      "Epoch [5/50], Train Loss: 0.1104, Val Loss: 0.3334\n",
      "Epoch [6/50], Train Loss: 0.1099, Val Loss: 0.3325\n",
      "Epoch [7/50], Train Loss: 0.1095, Val Loss: 0.3315\n",
      "Epoch [8/50], Train Loss: 0.1092, Val Loss: 0.3305\n",
      "Epoch [9/50], Train Loss: 0.1093, Val Loss: 0.3296\n",
      "Epoch [10/50], Train Loss: 0.1083, Val Loss: 0.3286\n",
      "Epoch [11/50], Train Loss: 0.1083, Val Loss: 0.3277\n",
      "Epoch [12/50], Train Loss: 0.1074, Val Loss: 0.3267\n",
      "Epoch [13/50], Train Loss: 0.1070, Val Loss: 0.3258\n",
      "Epoch [14/50], Train Loss: 0.1070, Val Loss: 0.3248\n",
      "Epoch [15/50], Train Loss: 0.1060, Val Loss: 0.3239\n",
      "Epoch [16/50], Train Loss: 0.1054, Val Loss: 0.3230\n",
      "Epoch [17/50], Train Loss: 0.1052, Val Loss: 0.3220\n",
      "Epoch [18/50], Train Loss: 0.1053, Val Loss: 0.3211\n",
      "Epoch [19/50], Train Loss: 0.1047, Val Loss: 0.3202\n",
      "Epoch [20/50], Train Loss: 0.1047, Val Loss: 0.3193\n",
      "Epoch [21/50], Train Loss: 0.1041, Val Loss: 0.3183\n",
      "Epoch [22/50], Train Loss: 0.1038, Val Loss: 0.3174\n",
      "Epoch [23/50], Train Loss: 0.1034, Val Loss: 0.3165\n",
      "Epoch [24/50], Train Loss: 0.1029, Val Loss: 0.3156\n",
      "Epoch [25/50], Train Loss: 0.1023, Val Loss: 0.3147\n",
      "Epoch [26/50], Train Loss: 0.1015, Val Loss: 0.3139\n",
      "Epoch [27/50], Train Loss: 0.1012, Val Loss: 0.3130\n",
      "Epoch [28/50], Train Loss: 0.1013, Val Loss: 0.3121\n",
      "Epoch [29/50], Train Loss: 0.1013, Val Loss: 0.3112\n",
      "Epoch [30/50], Train Loss: 0.1003, Val Loss: 0.3103\n",
      "Epoch [31/50], Train Loss: 0.1005, Val Loss: 0.3095\n",
      "Epoch [32/50], Train Loss: 0.0993, Val Loss: 0.3086\n",
      "Epoch [33/50], Train Loss: 0.0996, Val Loss: 0.3077\n",
      "Epoch [34/50], Train Loss: 0.0988, Val Loss: 0.3069\n",
      "Epoch [35/50], Train Loss: 0.0984, Val Loss: 0.3060\n",
      "Epoch [36/50], Train Loss: 0.0987, Val Loss: 0.3052\n",
      "Epoch [37/50], Train Loss: 0.0975, Val Loss: 0.3043\n",
      "Epoch [38/50], Train Loss: 0.0980, Val Loss: 0.3035\n",
      "Epoch [39/50], Train Loss: 0.0975, Val Loss: 0.3026\n",
      "Epoch [40/50], Train Loss: 0.0965, Val Loss: 0.3018\n",
      "Epoch [41/50], Train Loss: 0.0966, Val Loss: 0.3009\n",
      "Epoch [42/50], Train Loss: 0.0958, Val Loss: 0.3001\n",
      "Epoch [43/50], Train Loss: 0.0959, Val Loss: 0.2993\n",
      "Epoch [44/50], Train Loss: 0.0956, Val Loss: 0.2984\n",
      "Epoch [45/50], Train Loss: 0.0949, Val Loss: 0.2976\n",
      "Epoch [46/50], Train Loss: 0.0946, Val Loss: 0.2968\n",
      "Epoch [47/50], Train Loss: 0.0946, Val Loss: 0.2960\n",
      "Epoch [48/50], Train Loss: 0.0941, Val Loss: 0.2952\n",
      "Epoch [49/50], Train Loss: 0.0939, Val Loss: 0.2944\n",
      "Epoch [50/50], Train Loss: 0.0938, Val Loss: 0.2936\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=128, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1331, Val Loss: 0.3396\n",
      "Epoch [2/50], Train Loss: 0.1329, Val Loss: 0.3386\n",
      "Epoch [3/50], Train Loss: 0.1336, Val Loss: 0.3375\n",
      "Epoch [4/50], Train Loss: 0.1311, Val Loss: 0.3365\n",
      "Epoch [5/50], Train Loss: 0.1315, Val Loss: 0.3356\n",
      "Epoch [6/50], Train Loss: 0.1303, Val Loss: 0.3346\n",
      "Epoch [7/50], Train Loss: 0.1291, Val Loss: 0.3336\n",
      "Epoch [8/50], Train Loss: 0.1290, Val Loss: 0.3326\n",
      "Epoch [9/50], Train Loss: 0.1289, Val Loss: 0.3316\n",
      "Epoch [10/50], Train Loss: 0.1279, Val Loss: 0.3306\n",
      "Epoch [11/50], Train Loss: 0.1271, Val Loss: 0.3297\n",
      "Epoch [12/50], Train Loss: 0.1264, Val Loss: 0.3287\n",
      "Epoch [13/50], Train Loss: 0.1269, Val Loss: 0.3278\n",
      "Epoch [14/50], Train Loss: 0.1256, Val Loss: 0.3268\n",
      "Epoch [15/50], Train Loss: 0.1262, Val Loss: 0.3258\n",
      "Epoch [16/50], Train Loss: 0.1260, Val Loss: 0.3249\n",
      "Epoch [17/50], Train Loss: 0.1249, Val Loss: 0.3240\n",
      "Epoch [18/50], Train Loss: 0.1243, Val Loss: 0.3230\n",
      "Epoch [19/50], Train Loss: 0.1247, Val Loss: 0.3221\n",
      "Epoch [20/50], Train Loss: 0.1226, Val Loss: 0.3212\n",
      "Epoch [21/50], Train Loss: 0.1232, Val Loss: 0.3202\n",
      "Epoch [22/50], Train Loss: 0.1227, Val Loss: 0.3193\n",
      "Epoch [23/50], Train Loss: 0.1220, Val Loss: 0.3184\n",
      "Epoch [24/50], Train Loss: 0.1204, Val Loss: 0.3175\n",
      "Epoch [25/50], Train Loss: 0.1212, Val Loss: 0.3166\n",
      "Epoch [26/50], Train Loss: 0.1200, Val Loss: 0.3157\n",
      "Epoch [27/50], Train Loss: 0.1200, Val Loss: 0.3147\n",
      "Epoch [28/50], Train Loss: 0.1189, Val Loss: 0.3138\n",
      "Epoch [29/50], Train Loss: 0.1196, Val Loss: 0.3129\n",
      "Epoch [30/50], Train Loss: 0.1191, Val Loss: 0.3120\n",
      "Epoch [31/50], Train Loss: 0.1185, Val Loss: 0.3111\n",
      "Epoch [32/50], Train Loss: 0.1179, Val Loss: 0.3103\n",
      "Epoch [33/50], Train Loss: 0.1181, Val Loss: 0.3094\n",
      "Epoch [34/50], Train Loss: 0.1167, Val Loss: 0.3085\n",
      "Epoch [35/50], Train Loss: 0.1180, Val Loss: 0.3076\n",
      "Epoch [36/50], Train Loss: 0.1159, Val Loss: 0.3067\n",
      "Epoch [37/50], Train Loss: 0.1153, Val Loss: 0.3059\n",
      "Epoch [38/50], Train Loss: 0.1147, Val Loss: 0.3050\n",
      "Epoch [39/50], Train Loss: 0.1153, Val Loss: 0.3042\n",
      "Epoch [40/50], Train Loss: 0.1138, Val Loss: 0.3033\n",
      "Epoch [41/50], Train Loss: 0.1138, Val Loss: 0.3024\n",
      "Epoch [42/50], Train Loss: 0.1137, Val Loss: 0.3016\n",
      "Epoch [43/50], Train Loss: 0.1134, Val Loss: 0.3007\n",
      "Epoch [44/50], Train Loss: 0.1120, Val Loss: 0.2999\n",
      "Epoch [45/50], Train Loss: 0.1111, Val Loss: 0.2991\n",
      "Epoch [46/50], Train Loss: 0.1124, Val Loss: 0.2982\n",
      "Epoch [47/50], Train Loss: 0.1114, Val Loss: 0.2974\n",
      "Epoch [48/50], Train Loss: 0.1114, Val Loss: 0.2965\n",
      "Epoch [49/50], Train Loss: 0.1102, Val Loss: 0.2957\n",
      "Epoch [50/50], Train Loss: 0.1110, Val Loss: 0.2949\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=128, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1233, Val Loss: 0.3695\n",
      "Epoch [2/50], Train Loss: 0.1229, Val Loss: 0.3685\n",
      "Epoch [3/50], Train Loss: 0.1224, Val Loss: 0.3675\n",
      "Epoch [4/50], Train Loss: 0.1219, Val Loss: 0.3666\n",
      "Epoch [5/50], Train Loss: 0.1214, Val Loss: 0.3656\n",
      "Epoch [6/50], Train Loss: 0.1210, Val Loss: 0.3646\n",
      "Epoch [7/50], Train Loss: 0.1205, Val Loss: 0.3636\n",
      "Epoch [8/50], Train Loss: 0.1201, Val Loss: 0.3627\n",
      "Epoch [9/50], Train Loss: 0.1196, Val Loss: 0.3617\n",
      "Epoch [10/50], Train Loss: 0.1191, Val Loss: 0.3607\n",
      "Epoch [11/50], Train Loss: 0.1187, Val Loss: 0.3598\n",
      "Epoch [12/50], Train Loss: 0.1182, Val Loss: 0.3588\n",
      "Epoch [13/50], Train Loss: 0.1178, Val Loss: 0.3579\n",
      "Epoch [14/50], Train Loss: 0.1173, Val Loss: 0.3569\n",
      "Epoch [15/50], Train Loss: 0.1169, Val Loss: 0.3560\n",
      "Epoch [16/50], Train Loss: 0.1164, Val Loss: 0.3550\n",
      "Epoch [17/50], Train Loss: 0.1160, Val Loss: 0.3541\n",
      "Epoch [18/50], Train Loss: 0.1156, Val Loss: 0.3532\n",
      "Epoch [19/50], Train Loss: 0.1151, Val Loss: 0.3522\n",
      "Epoch [20/50], Train Loss: 0.1147, Val Loss: 0.3513\n",
      "Epoch [21/50], Train Loss: 0.1143, Val Loss: 0.3504\n",
      "Epoch [22/50], Train Loss: 0.1138, Val Loss: 0.3495\n",
      "Epoch [23/50], Train Loss: 0.1134, Val Loss: 0.3486\n",
      "Epoch [24/50], Train Loss: 0.1130, Val Loss: 0.3477\n",
      "Epoch [25/50], Train Loss: 0.1126, Val Loss: 0.3468\n",
      "Epoch [26/50], Train Loss: 0.1122, Val Loss: 0.3459\n",
      "Epoch [27/50], Train Loss: 0.1117, Val Loss: 0.3449\n",
      "Epoch [28/50], Train Loss: 0.1113, Val Loss: 0.3441\n",
      "Epoch [29/50], Train Loss: 0.1109, Val Loss: 0.3432\n",
      "Epoch [30/50], Train Loss: 0.1105, Val Loss: 0.3423\n",
      "Epoch [31/50], Train Loss: 0.1101, Val Loss: 0.3414\n",
      "Epoch [32/50], Train Loss: 0.1097, Val Loss: 0.3405\n",
      "Epoch [33/50], Train Loss: 0.1093, Val Loss: 0.3396\n",
      "Epoch [34/50], Train Loss: 0.1089, Val Loss: 0.3388\n",
      "Epoch [35/50], Train Loss: 0.1085, Val Loss: 0.3379\n",
      "Epoch [36/50], Train Loss: 0.1081, Val Loss: 0.3370\n",
      "Epoch [37/50], Train Loss: 0.1077, Val Loss: 0.3362\n",
      "Epoch [38/50], Train Loss: 0.1073, Val Loss: 0.3353\n",
      "Epoch [39/50], Train Loss: 0.1069, Val Loss: 0.3344\n",
      "Epoch [40/50], Train Loss: 0.1065, Val Loss: 0.3336\n",
      "Epoch [41/50], Train Loss: 0.1061, Val Loss: 0.3327\n",
      "Epoch [42/50], Train Loss: 0.1057, Val Loss: 0.3319\n",
      "Epoch [43/50], Train Loss: 0.1054, Val Loss: 0.3310\n",
      "Epoch [44/50], Train Loss: 0.1050, Val Loss: 0.3302\n",
      "Epoch [45/50], Train Loss: 0.1046, Val Loss: 0.3294\n",
      "Epoch [46/50], Train Loss: 0.1042, Val Loss: 0.3285\n",
      "Epoch [47/50], Train Loss: 0.1038, Val Loss: 0.3277\n",
      "Epoch [48/50], Train Loss: 0.1035, Val Loss: 0.3269\n",
      "Epoch [49/50], Train Loss: 0.1031, Val Loss: 0.3260\n",
      "Epoch [50/50], Train Loss: 0.1027, Val Loss: 0.3252\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=128, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1411, Val Loss: 0.4004\n",
      "Epoch [2/50], Train Loss: 0.1403, Val Loss: 0.3993\n",
      "Epoch [3/50], Train Loss: 0.1402, Val Loss: 0.3981\n",
      "Epoch [4/50], Train Loss: 0.1390, Val Loss: 0.3969\n",
      "Epoch [5/50], Train Loss: 0.1389, Val Loss: 0.3958\n",
      "Epoch [6/50], Train Loss: 0.1381, Val Loss: 0.3946\n",
      "Epoch [7/50], Train Loss: 0.1375, Val Loss: 0.3935\n",
      "Epoch [8/50], Train Loss: 0.1368, Val Loss: 0.3924\n",
      "Epoch [9/50], Train Loss: 0.1362, Val Loss: 0.3913\n",
      "Epoch [10/50], Train Loss: 0.1352, Val Loss: 0.3901\n",
      "Epoch [11/50], Train Loss: 0.1352, Val Loss: 0.3890\n",
      "Epoch [12/50], Train Loss: 0.1351, Val Loss: 0.3879\n",
      "Epoch [13/50], Train Loss: 0.1347, Val Loss: 0.3868\n",
      "Epoch [14/50], Train Loss: 0.1338, Val Loss: 0.3857\n",
      "Epoch [15/50], Train Loss: 0.1329, Val Loss: 0.3846\n",
      "Epoch [16/50], Train Loss: 0.1327, Val Loss: 0.3835\n",
      "Epoch [17/50], Train Loss: 0.1323, Val Loss: 0.3824\n",
      "Epoch [18/50], Train Loss: 0.1316, Val Loss: 0.3813\n",
      "Epoch [19/50], Train Loss: 0.1313, Val Loss: 0.3802\n",
      "Epoch [20/50], Train Loss: 0.1308, Val Loss: 0.3792\n",
      "Epoch [21/50], Train Loss: 0.1299, Val Loss: 0.3781\n",
      "Epoch [22/50], Train Loss: 0.1293, Val Loss: 0.3770\n",
      "Epoch [23/50], Train Loss: 0.1289, Val Loss: 0.3760\n",
      "Epoch [24/50], Train Loss: 0.1288, Val Loss: 0.3749\n",
      "Epoch [25/50], Train Loss: 0.1281, Val Loss: 0.3738\n",
      "Epoch [26/50], Train Loss: 0.1271, Val Loss: 0.3728\n",
      "Epoch [27/50], Train Loss: 0.1267, Val Loss: 0.3718\n",
      "Epoch [28/50], Train Loss: 0.1266, Val Loss: 0.3707\n",
      "Epoch [29/50], Train Loss: 0.1260, Val Loss: 0.3697\n",
      "Epoch [30/50], Train Loss: 0.1252, Val Loss: 0.3687\n",
      "Epoch [31/50], Train Loss: 0.1251, Val Loss: 0.3676\n",
      "Epoch [32/50], Train Loss: 0.1248, Val Loss: 0.3666\n",
      "Epoch [33/50], Train Loss: 0.1238, Val Loss: 0.3656\n",
      "Epoch [34/50], Train Loss: 0.1232, Val Loss: 0.3646\n",
      "Epoch [35/50], Train Loss: 0.1231, Val Loss: 0.3636\n",
      "Epoch [36/50], Train Loss: 0.1222, Val Loss: 0.3626\n",
      "Epoch [37/50], Train Loss: 0.1222, Val Loss: 0.3616\n",
      "Epoch [38/50], Train Loss: 0.1219, Val Loss: 0.3606\n",
      "Epoch [39/50], Train Loss: 0.1212, Val Loss: 0.3596\n",
      "Epoch [40/50], Train Loss: 0.1202, Val Loss: 0.3586\n",
      "Epoch [41/50], Train Loss: 0.1203, Val Loss: 0.3576\n",
      "Epoch [42/50], Train Loss: 0.1195, Val Loss: 0.3566\n",
      "Epoch [43/50], Train Loss: 0.1195, Val Loss: 0.3557\n",
      "Epoch [44/50], Train Loss: 0.1190, Val Loss: 0.3547\n",
      "Epoch [45/50], Train Loss: 0.1186, Val Loss: 0.3537\n",
      "Epoch [46/50], Train Loss: 0.1179, Val Loss: 0.3527\n",
      "Epoch [47/50], Train Loss: 0.1179, Val Loss: 0.3518\n",
      "Epoch [48/50], Train Loss: 0.1171, Val Loss: 0.3508\n",
      "Epoch [49/50], Train Loss: 0.1162, Val Loss: 0.3499\n",
      "Epoch [50/50], Train Loss: 0.1160, Val Loss: 0.3489\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=128, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1402, Val Loss: 0.3916\n",
      "Epoch [2/50], Train Loss: 0.1399, Val Loss: 0.3903\n",
      "Epoch [3/50], Train Loss: 0.1397, Val Loss: 0.3890\n",
      "Epoch [4/50], Train Loss: 0.1395, Val Loss: 0.3877\n",
      "Epoch [5/50], Train Loss: 0.1379, Val Loss: 0.3865\n",
      "Epoch [6/50], Train Loss: 0.1379, Val Loss: 0.3852\n",
      "Epoch [7/50], Train Loss: 0.1376, Val Loss: 0.3839\n",
      "Epoch [8/50], Train Loss: 0.1379, Val Loss: 0.3826\n",
      "Epoch [9/50], Train Loss: 0.1377, Val Loss: 0.3814\n",
      "Epoch [10/50], Train Loss: 0.1354, Val Loss: 0.3801\n",
      "Epoch [11/50], Train Loss: 0.1348, Val Loss: 0.3789\n",
      "Epoch [12/50], Train Loss: 0.1333, Val Loss: 0.3777\n",
      "Epoch [13/50], Train Loss: 0.1334, Val Loss: 0.3764\n",
      "Epoch [14/50], Train Loss: 0.1332, Val Loss: 0.3752\n",
      "Epoch [15/50], Train Loss: 0.1331, Val Loss: 0.3740\n",
      "Epoch [16/50], Train Loss: 0.1321, Val Loss: 0.3728\n",
      "Epoch [17/50], Train Loss: 0.1315, Val Loss: 0.3715\n",
      "Epoch [18/50], Train Loss: 0.1302, Val Loss: 0.3703\n",
      "Epoch [19/50], Train Loss: 0.1299, Val Loss: 0.3691\n",
      "Epoch [20/50], Train Loss: 0.1300, Val Loss: 0.3679\n",
      "Epoch [21/50], Train Loss: 0.1295, Val Loss: 0.3667\n",
      "Epoch [22/50], Train Loss: 0.1287, Val Loss: 0.3656\n",
      "Epoch [23/50], Train Loss: 0.1270, Val Loss: 0.3644\n",
      "Epoch [24/50], Train Loss: 0.1274, Val Loss: 0.3632\n",
      "Epoch [25/50], Train Loss: 0.1269, Val Loss: 0.3620\n",
      "Epoch [26/50], Train Loss: 0.1264, Val Loss: 0.3609\n",
      "Epoch [27/50], Train Loss: 0.1263, Val Loss: 0.3597\n",
      "Epoch [28/50], Train Loss: 0.1262, Val Loss: 0.3586\n",
      "Epoch [29/50], Train Loss: 0.1243, Val Loss: 0.3574\n",
      "Epoch [30/50], Train Loss: 0.1242, Val Loss: 0.3563\n",
      "Epoch [31/50], Train Loss: 0.1245, Val Loss: 0.3552\n",
      "Epoch [32/50], Train Loss: 0.1241, Val Loss: 0.3540\n",
      "Epoch [33/50], Train Loss: 0.1220, Val Loss: 0.3529\n",
      "Epoch [34/50], Train Loss: 0.1214, Val Loss: 0.3518\n",
      "Epoch [35/50], Train Loss: 0.1216, Val Loss: 0.3507\n",
      "Epoch [36/50], Train Loss: 0.1210, Val Loss: 0.3496\n",
      "Epoch [37/50], Train Loss: 0.1203, Val Loss: 0.3485\n",
      "Epoch [38/50], Train Loss: 0.1200, Val Loss: 0.3473\n",
      "Epoch [39/50], Train Loss: 0.1202, Val Loss: 0.3462\n",
      "Epoch [40/50], Train Loss: 0.1183, Val Loss: 0.3451\n",
      "Epoch [41/50], Train Loss: 0.1189, Val Loss: 0.3441\n",
      "Epoch [42/50], Train Loss: 0.1186, Val Loss: 0.3430\n",
      "Epoch [43/50], Train Loss: 0.1178, Val Loss: 0.3419\n",
      "Epoch [44/50], Train Loss: 0.1178, Val Loss: 0.3408\n",
      "Epoch [45/50], Train Loss: 0.1168, Val Loss: 0.3397\n",
      "Epoch [46/50], Train Loss: 0.1156, Val Loss: 0.3387\n",
      "Epoch [47/50], Train Loss: 0.1157, Val Loss: 0.3376\n",
      "Epoch [48/50], Train Loss: 0.1152, Val Loss: 0.3365\n",
      "Epoch [49/50], Train Loss: 0.1142, Val Loss: 0.3355\n",
      "Epoch [50/50], Train Loss: 0.1141, Val Loss: 0.3345\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=128, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1213, Val Loss: 0.3486\n",
      "Epoch [2/50], Train Loss: 0.1208, Val Loss: 0.3475\n",
      "Epoch [3/50], Train Loss: 0.1203, Val Loss: 0.3465\n",
      "Epoch [4/50], Train Loss: 0.1197, Val Loss: 0.3454\n",
      "Epoch [5/50], Train Loss: 0.1192, Val Loss: 0.3444\n",
      "Epoch [6/50], Train Loss: 0.1187, Val Loss: 0.3433\n",
      "Epoch [7/50], Train Loss: 0.1181, Val Loss: 0.3423\n",
      "Epoch [8/50], Train Loss: 0.1176, Val Loss: 0.3413\n",
      "Epoch [9/50], Train Loss: 0.1171, Val Loss: 0.3403\n",
      "Epoch [10/50], Train Loss: 0.1166, Val Loss: 0.3392\n",
      "Epoch [11/50], Train Loss: 0.1161, Val Loss: 0.3382\n",
      "Epoch [12/50], Train Loss: 0.1156, Val Loss: 0.3372\n",
      "Epoch [13/50], Train Loss: 0.1151, Val Loss: 0.3362\n",
      "Epoch [14/50], Train Loss: 0.1146, Val Loss: 0.3352\n",
      "Epoch [15/50], Train Loss: 0.1141, Val Loss: 0.3342\n",
      "Epoch [16/50], Train Loss: 0.1136, Val Loss: 0.3332\n",
      "Epoch [17/50], Train Loss: 0.1131, Val Loss: 0.3322\n",
      "Epoch [18/50], Train Loss: 0.1126, Val Loss: 0.3313\n",
      "Epoch [19/50], Train Loss: 0.1121, Val Loss: 0.3303\n",
      "Epoch [20/50], Train Loss: 0.1117, Val Loss: 0.3293\n",
      "Epoch [21/50], Train Loss: 0.1112, Val Loss: 0.3284\n",
      "Epoch [22/50], Train Loss: 0.1107, Val Loss: 0.3274\n",
      "Epoch [23/50], Train Loss: 0.1102, Val Loss: 0.3265\n",
      "Epoch [24/50], Train Loss: 0.1098, Val Loss: 0.3255\n",
      "Epoch [25/50], Train Loss: 0.1093, Val Loss: 0.3246\n",
      "Epoch [26/50], Train Loss: 0.1088, Val Loss: 0.3236\n",
      "Epoch [27/50], Train Loss: 0.1084, Val Loss: 0.3227\n",
      "Epoch [28/50], Train Loss: 0.1079, Val Loss: 0.3217\n",
      "Epoch [29/50], Train Loss: 0.1074, Val Loss: 0.3208\n",
      "Epoch [30/50], Train Loss: 0.1070, Val Loss: 0.3199\n",
      "Epoch [31/50], Train Loss: 0.1065, Val Loss: 0.3190\n",
      "Epoch [32/50], Train Loss: 0.1061, Val Loss: 0.3180\n",
      "Epoch [33/50], Train Loss: 0.1057, Val Loss: 0.3171\n",
      "Epoch [34/50], Train Loss: 0.1052, Val Loss: 0.3162\n",
      "Epoch [35/50], Train Loss: 0.1048, Val Loss: 0.3153\n",
      "Epoch [36/50], Train Loss: 0.1043, Val Loss: 0.3144\n",
      "Epoch [37/50], Train Loss: 0.1039, Val Loss: 0.3135\n",
      "Epoch [38/50], Train Loss: 0.1035, Val Loss: 0.3126\n",
      "Epoch [39/50], Train Loss: 0.1031, Val Loss: 0.3117\n",
      "Epoch [40/50], Train Loss: 0.1026, Val Loss: 0.3109\n",
      "Epoch [41/50], Train Loss: 0.1022, Val Loss: 0.3100\n",
      "Epoch [42/50], Train Loss: 0.1018, Val Loss: 0.3091\n",
      "Epoch [43/50], Train Loss: 0.1014, Val Loss: 0.3082\n",
      "Epoch [44/50], Train Loss: 0.1010, Val Loss: 0.3074\n",
      "Epoch [45/50], Train Loss: 0.1005, Val Loss: 0.3065\n",
      "Epoch [46/50], Train Loss: 0.1001, Val Loss: 0.3056\n",
      "Epoch [47/50], Train Loss: 0.0997, Val Loss: 0.3048\n",
      "Epoch [48/50], Train Loss: 0.0993, Val Loss: 0.3039\n",
      "Epoch [49/50], Train Loss: 0.0989, Val Loss: 0.3031\n",
      "Epoch [50/50], Train Loss: 0.0985, Val Loss: 0.3022\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=128, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1425, Val Loss: 0.3950\n",
      "Epoch [2/50], Train Loss: 0.1418, Val Loss: 0.3938\n",
      "Epoch [3/50], Train Loss: 0.1410, Val Loss: 0.3925\n",
      "Epoch [4/50], Train Loss: 0.1406, Val Loss: 0.3913\n",
      "Epoch [5/50], Train Loss: 0.1397, Val Loss: 0.3901\n",
      "Epoch [6/50], Train Loss: 0.1392, Val Loss: 0.3889\n",
      "Epoch [7/50], Train Loss: 0.1385, Val Loss: 0.3877\n",
      "Epoch [8/50], Train Loss: 0.1375, Val Loss: 0.3865\n",
      "Epoch [9/50], Train Loss: 0.1372, Val Loss: 0.3853\n",
      "Epoch [10/50], Train Loss: 0.1365, Val Loss: 0.3841\n",
      "Epoch [11/50], Train Loss: 0.1357, Val Loss: 0.3829\n",
      "Epoch [12/50], Train Loss: 0.1354, Val Loss: 0.3818\n",
      "Epoch [13/50], Train Loss: 0.1344, Val Loss: 0.3806\n",
      "Epoch [14/50], Train Loss: 0.1339, Val Loss: 0.3794\n",
      "Epoch [15/50], Train Loss: 0.1334, Val Loss: 0.3783\n",
      "Epoch [16/50], Train Loss: 0.1330, Val Loss: 0.3771\n",
      "Epoch [17/50], Train Loss: 0.1319, Val Loss: 0.3760\n",
      "Epoch [18/50], Train Loss: 0.1316, Val Loss: 0.3749\n",
      "Epoch [19/50], Train Loss: 0.1315, Val Loss: 0.3737\n",
      "Epoch [20/50], Train Loss: 0.1304, Val Loss: 0.3726\n",
      "Epoch [21/50], Train Loss: 0.1301, Val Loss: 0.3715\n",
      "Epoch [22/50], Train Loss: 0.1298, Val Loss: 0.3703\n",
      "Epoch [23/50], Train Loss: 0.1286, Val Loss: 0.3692\n",
      "Epoch [24/50], Train Loss: 0.1282, Val Loss: 0.3681\n",
      "Epoch [25/50], Train Loss: 0.1278, Val Loss: 0.3670\n",
      "Epoch [26/50], Train Loss: 0.1266, Val Loss: 0.3659\n",
      "Epoch [27/50], Train Loss: 0.1266, Val Loss: 0.3648\n",
      "Epoch [28/50], Train Loss: 0.1261, Val Loss: 0.3637\n",
      "Epoch [29/50], Train Loss: 0.1254, Val Loss: 0.3627\n",
      "Epoch [30/50], Train Loss: 0.1252, Val Loss: 0.3616\n",
      "Epoch [31/50], Train Loss: 0.1245, Val Loss: 0.3605\n",
      "Epoch [32/50], Train Loss: 0.1240, Val Loss: 0.3595\n",
      "Epoch [33/50], Train Loss: 0.1233, Val Loss: 0.3584\n",
      "Epoch [34/50], Train Loss: 0.1230, Val Loss: 0.3573\n",
      "Epoch [35/50], Train Loss: 0.1226, Val Loss: 0.3563\n",
      "Epoch [36/50], Train Loss: 0.1216, Val Loss: 0.3552\n",
      "Epoch [37/50], Train Loss: 0.1214, Val Loss: 0.3542\n",
      "Epoch [38/50], Train Loss: 0.1209, Val Loss: 0.3531\n",
      "Epoch [39/50], Train Loss: 0.1201, Val Loss: 0.3521\n",
      "Epoch [40/50], Train Loss: 0.1193, Val Loss: 0.3511\n",
      "Epoch [41/50], Train Loss: 0.1192, Val Loss: 0.3501\n",
      "Epoch [42/50], Train Loss: 0.1185, Val Loss: 0.3490\n",
      "Epoch [43/50], Train Loss: 0.1183, Val Loss: 0.3480\n",
      "Epoch [44/50], Train Loss: 0.1179, Val Loss: 0.3470\n",
      "Epoch [45/50], Train Loss: 0.1165, Val Loss: 0.3460\n",
      "Epoch [46/50], Train Loss: 0.1165, Val Loss: 0.3450\n",
      "Epoch [47/50], Train Loss: 0.1163, Val Loss: 0.3440\n",
      "Epoch [48/50], Train Loss: 0.1160, Val Loss: 0.3430\n",
      "Epoch [49/50], Train Loss: 0.1153, Val Loss: 0.3420\n",
      "Epoch [50/50], Train Loss: 0.1144, Val Loss: 0.3410\n",
      "Testing parameters: lr=0.0001, optimizer=sgd, hidden_size=128, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1413, Val Loss: 0.3644\n",
      "Epoch [2/50], Train Loss: 0.1411, Val Loss: 0.3632\n",
      "Epoch [3/50], Train Loss: 0.1390, Val Loss: 0.3620\n",
      "Epoch [4/50], Train Loss: 0.1389, Val Loss: 0.3609\n",
      "Epoch [5/50], Train Loss: 0.1375, Val Loss: 0.3597\n",
      "Epoch [6/50], Train Loss: 0.1373, Val Loss: 0.3585\n",
      "Epoch [7/50], Train Loss: 0.1377, Val Loss: 0.3574\n",
      "Epoch [8/50], Train Loss: 0.1357, Val Loss: 0.3562\n",
      "Epoch [9/50], Train Loss: 0.1356, Val Loss: 0.3551\n",
      "Epoch [10/50], Train Loss: 0.1343, Val Loss: 0.3540\n",
      "Epoch [11/50], Train Loss: 0.1336, Val Loss: 0.3528\n",
      "Epoch [12/50], Train Loss: 0.1327, Val Loss: 0.3517\n",
      "Epoch [13/50], Train Loss: 0.1340, Val Loss: 0.3506\n",
      "Epoch [14/50], Train Loss: 0.1326, Val Loss: 0.3495\n",
      "Epoch [15/50], Train Loss: 0.1320, Val Loss: 0.3484\n",
      "Epoch [16/50], Train Loss: 0.1315, Val Loss: 0.3473\n",
      "Epoch [17/50], Train Loss: 0.1312, Val Loss: 0.3462\n",
      "Epoch [18/50], Train Loss: 0.1303, Val Loss: 0.3451\n",
      "Epoch [19/50], Train Loss: 0.1296, Val Loss: 0.3440\n",
      "Epoch [20/50], Train Loss: 0.1301, Val Loss: 0.3429\n",
      "Epoch [21/50], Train Loss: 0.1293, Val Loss: 0.3419\n",
      "Epoch [22/50], Train Loss: 0.1275, Val Loss: 0.3408\n",
      "Epoch [23/50], Train Loss: 0.1272, Val Loss: 0.3397\n",
      "Epoch [24/50], Train Loss: 0.1269, Val Loss: 0.3387\n",
      "Epoch [25/50], Train Loss: 0.1271, Val Loss: 0.3376\n",
      "Epoch [26/50], Train Loss: 0.1252, Val Loss: 0.3366\n",
      "Epoch [27/50], Train Loss: 0.1248, Val Loss: 0.3355\n",
      "Epoch [28/50], Train Loss: 0.1240, Val Loss: 0.3345\n",
      "Epoch [29/50], Train Loss: 0.1252, Val Loss: 0.3335\n",
      "Epoch [30/50], Train Loss: 0.1241, Val Loss: 0.3325\n",
      "Epoch [31/50], Train Loss: 0.1232, Val Loss: 0.3314\n",
      "Epoch [32/50], Train Loss: 0.1225, Val Loss: 0.3304\n",
      "Epoch [33/50], Train Loss: 0.1226, Val Loss: 0.3294\n",
      "Epoch [34/50], Train Loss: 0.1212, Val Loss: 0.3284\n",
      "Epoch [35/50], Train Loss: 0.1209, Val Loss: 0.3274\n",
      "Epoch [36/50], Train Loss: 0.1203, Val Loss: 0.3264\n",
      "Epoch [37/50], Train Loss: 0.1199, Val Loss: 0.3254\n",
      "Epoch [38/50], Train Loss: 0.1197, Val Loss: 0.3244\n",
      "Epoch [39/50], Train Loss: 0.1185, Val Loss: 0.3234\n",
      "Epoch [40/50], Train Loss: 0.1183, Val Loss: 0.3225\n",
      "Epoch [41/50], Train Loss: 0.1186, Val Loss: 0.3215\n",
      "Epoch [42/50], Train Loss: 0.1174, Val Loss: 0.3205\n",
      "Epoch [43/50], Train Loss: 0.1169, Val Loss: 0.3195\n",
      "Epoch [44/50], Train Loss: 0.1171, Val Loss: 0.3186\n",
      "Epoch [45/50], Train Loss: 0.1163, Val Loss: 0.3176\n",
      "Epoch [46/50], Train Loss: 0.1154, Val Loss: 0.3167\n",
      "Epoch [47/50], Train Loss: 0.1150, Val Loss: 0.3157\n",
      "Epoch [48/50], Train Loss: 0.1139, Val Loss: 0.3148\n",
      "Epoch [49/50], Train Loss: 0.1142, Val Loss: 0.3138\n",
      "Epoch [50/50], Train Loss: 0.1135, Val Loss: 0.3129\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=16, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.3213, Val Loss: 0.7283\n",
      "Epoch [2/50], Train Loss: 0.3047, Val Loss: 0.6997\n",
      "Epoch [3/50], Train Loss: 0.2900, Val Loss: 0.6733\n",
      "Epoch [4/50], Train Loss: 0.2763, Val Loss: 0.6482\n",
      "Epoch [5/50], Train Loss: 0.2632, Val Loss: 0.6240\n",
      "Epoch [6/50], Train Loss: 0.2504, Val Loss: 0.6001\n",
      "Epoch [7/50], Train Loss: 0.2379, Val Loss: 0.5762\n",
      "Epoch [8/50], Train Loss: 0.2254, Val Loss: 0.5520\n",
      "Epoch [9/50], Train Loss: 0.2128, Val Loss: 0.5272\n",
      "Epoch [10/50], Train Loss: 0.2000, Val Loss: 0.5014\n",
      "Epoch [11/50], Train Loss: 0.1868, Val Loss: 0.4744\n",
      "Epoch [12/50], Train Loss: 0.1731, Val Loss: 0.4460\n",
      "Epoch [13/50], Train Loss: 0.1590, Val Loss: 0.4158\n",
      "Epoch [14/50], Train Loss: 0.1444, Val Loss: 0.3839\n",
      "Epoch [15/50], Train Loss: 0.1293, Val Loss: 0.3501\n",
      "Epoch [16/50], Train Loss: 0.1139, Val Loss: 0.3149\n",
      "Epoch [17/50], Train Loss: 0.0985, Val Loss: 0.2787\n",
      "Epoch [18/50], Train Loss: 0.0837, Val Loss: 0.2428\n",
      "Epoch [19/50], Train Loss: 0.0701, Val Loss: 0.2086\n",
      "Epoch [20/50], Train Loss: 0.0584, Val Loss: 0.1777\n",
      "Epoch [21/50], Train Loss: 0.0492, Val Loss: 0.1514\n",
      "Epoch [22/50], Train Loss: 0.0425, Val Loss: 0.1303\n",
      "Epoch [23/50], Train Loss: 0.0380, Val Loss: 0.1142\n",
      "Epoch [24/50], Train Loss: 0.0351, Val Loss: 0.1024\n",
      "Epoch [25/50], Train Loss: 0.0334, Val Loss: 0.0938\n",
      "Epoch [26/50], Train Loss: 0.0323, Val Loss: 0.0876\n",
      "Epoch [27/50], Train Loss: 0.0315, Val Loss: 0.0830\n",
      "Epoch [28/50], Train Loss: 0.0309, Val Loss: 0.0796\n",
      "Epoch [29/50], Train Loss: 0.0305, Val Loss: 0.0769\n",
      "Epoch [30/50], Train Loss: 0.0301, Val Loss: 0.0747\n",
      "Epoch [31/50], Train Loss: 0.0297, Val Loss: 0.0730\n",
      "Epoch [32/50], Train Loss: 0.0294, Val Loss: 0.0714\n",
      "Epoch [33/50], Train Loss: 0.0291, Val Loss: 0.0701\n",
      "Epoch [34/50], Train Loss: 0.0288, Val Loss: 0.0688\n",
      "Epoch [35/50], Train Loss: 0.0285, Val Loss: 0.0677\n",
      "Epoch [36/50], Train Loss: 0.0282, Val Loss: 0.0665\n",
      "Epoch [37/50], Train Loss: 0.0279, Val Loss: 0.0655\n",
      "Epoch [38/50], Train Loss: 0.0276, Val Loss: 0.0644\n",
      "Epoch [39/50], Train Loss: 0.0274, Val Loss: 0.0635\n",
      "Epoch [40/50], Train Loss: 0.0271, Val Loss: 0.0625\n",
      "Epoch [41/50], Train Loss: 0.0268, Val Loss: 0.0615\n",
      "Epoch [42/50], Train Loss: 0.0266, Val Loss: 0.0606\n",
      "Epoch [43/50], Train Loss: 0.0263, Val Loss: 0.0596\n",
      "Epoch [44/50], Train Loss: 0.0260, Val Loss: 0.0587\n",
      "Epoch [45/50], Train Loss: 0.0258, Val Loss: 0.0578\n",
      "Epoch [46/50], Train Loss: 0.0255, Val Loss: 0.0569\n",
      "Epoch [47/50], Train Loss: 0.0252, Val Loss: 0.0560\n",
      "Epoch [48/50], Train Loss: 0.0250, Val Loss: 0.0551\n",
      "Epoch [49/50], Train Loss: 0.0247, Val Loss: 0.0542\n",
      "Epoch [50/50], Train Loss: 0.0245, Val Loss: 0.0533\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=16, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.2202, Val Loss: 0.4944\n",
      "Epoch [2/50], Train Loss: 0.2024, Val Loss: 0.4612\n",
      "Epoch [3/50], Train Loss: 0.1869, Val Loss: 0.4305\n",
      "Epoch [4/50], Train Loss: 0.1723, Val Loss: 0.4016\n",
      "Epoch [5/50], Train Loss: 0.1599, Val Loss: 0.3740\n",
      "Epoch [6/50], Train Loss: 0.1472, Val Loss: 0.3475\n",
      "Epoch [7/50], Train Loss: 0.1356, Val Loss: 0.3220\n",
      "Epoch [8/50], Train Loss: 0.1246, Val Loss: 0.2974\n",
      "Epoch [9/50], Train Loss: 0.1162, Val Loss: 0.2738\n",
      "Epoch [10/50], Train Loss: 0.1044, Val Loss: 0.2514\n",
      "Epoch [11/50], Train Loss: 0.0988, Val Loss: 0.2302\n",
      "Epoch [12/50], Train Loss: 0.0878, Val Loss: 0.2102\n",
      "Epoch [13/50], Train Loss: 0.0800, Val Loss: 0.1912\n",
      "Epoch [14/50], Train Loss: 0.0772, Val Loss: 0.1736\n",
      "Epoch [15/50], Train Loss: 0.0679, Val Loss: 0.1574\n",
      "Epoch [16/50], Train Loss: 0.0625, Val Loss: 0.1426\n",
      "Epoch [17/50], Train Loss: 0.0575, Val Loss: 0.1293\n",
      "Epoch [18/50], Train Loss: 0.0552, Val Loss: 0.1176\n",
      "Epoch [19/50], Train Loss: 0.0513, Val Loss: 0.1072\n",
      "Epoch [20/50], Train Loss: 0.0475, Val Loss: 0.0979\n",
      "Epoch [21/50], Train Loss: 0.0454, Val Loss: 0.0897\n",
      "Epoch [22/50], Train Loss: 0.0430, Val Loss: 0.0823\n",
      "Epoch [23/50], Train Loss: 0.0401, Val Loss: 0.0762\n",
      "Epoch [24/50], Train Loss: 0.0397, Val Loss: 0.0710\n",
      "Epoch [25/50], Train Loss: 0.0369, Val Loss: 0.0663\n",
      "Epoch [26/50], Train Loss: 0.0378, Val Loss: 0.0624\n",
      "Epoch [27/50], Train Loss: 0.0351, Val Loss: 0.0591\n",
      "Epoch [28/50], Train Loss: 0.0359, Val Loss: 0.0560\n",
      "Epoch [29/50], Train Loss: 0.0343, Val Loss: 0.0535\n",
      "Epoch [30/50], Train Loss: 0.0341, Val Loss: 0.0513\n",
      "Epoch [31/50], Train Loss: 0.0330, Val Loss: 0.0492\n",
      "Epoch [32/50], Train Loss: 0.0331, Val Loss: 0.0473\n",
      "Epoch [33/50], Train Loss: 0.0335, Val Loss: 0.0456\n",
      "Epoch [34/50], Train Loss: 0.0309, Val Loss: 0.0443\n",
      "Epoch [35/50], Train Loss: 0.0301, Val Loss: 0.0435\n",
      "Epoch [36/50], Train Loss: 0.0306, Val Loss: 0.0424\n",
      "Epoch [37/50], Train Loss: 0.0293, Val Loss: 0.0414\n",
      "Epoch [38/50], Train Loss: 0.0282, Val Loss: 0.0408\n",
      "Epoch [39/50], Train Loss: 0.0283, Val Loss: 0.0402\n",
      "Epoch [40/50], Train Loss: 0.0280, Val Loss: 0.0392\n",
      "Epoch [41/50], Train Loss: 0.0278, Val Loss: 0.0380\n",
      "Epoch [42/50], Train Loss: 0.0277, Val Loss: 0.0370\n",
      "Epoch [43/50], Train Loss: 0.0275, Val Loss: 0.0359\n",
      "Epoch [44/50], Train Loss: 0.0260, Val Loss: 0.0351\n",
      "Epoch [45/50], Train Loss: 0.0267, Val Loss: 0.0341\n",
      "Epoch [46/50], Train Loss: 0.0266, Val Loss: 0.0335\n",
      "Epoch [47/50], Train Loss: 0.0266, Val Loss: 0.0331\n",
      "Epoch [48/50], Train Loss: 0.0259, Val Loss: 0.0326\n",
      "Epoch [49/50], Train Loss: 0.0259, Val Loss: 0.0318\n",
      "Epoch [50/50], Train Loss: 0.0247, Val Loss: 0.0311\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=16, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1716, Val Loss: 0.4278\n",
      "Epoch [2/50], Train Loss: 0.1650, Val Loss: 0.4099\n",
      "Epoch [3/50], Train Loss: 0.1571, Val Loss: 0.3933\n",
      "Epoch [4/50], Train Loss: 0.1503, Val Loss: 0.3773\n",
      "Epoch [5/50], Train Loss: 0.1409, Val Loss: 0.3617\n",
      "Epoch [6/50], Train Loss: 0.1329, Val Loss: 0.3461\n",
      "Epoch [7/50], Train Loss: 0.1286, Val Loss: 0.3306\n",
      "Epoch [8/50], Train Loss: 0.1234, Val Loss: 0.3150\n",
      "Epoch [9/50], Train Loss: 0.1162, Val Loss: 0.2994\n",
      "Epoch [10/50], Train Loss: 0.1066, Val Loss: 0.2836\n",
      "Epoch [11/50], Train Loss: 0.1031, Val Loss: 0.2677\n",
      "Epoch [12/50], Train Loss: 0.0976, Val Loss: 0.2519\n",
      "Epoch [13/50], Train Loss: 0.0958, Val Loss: 0.2363\n",
      "Epoch [14/50], Train Loss: 0.0874, Val Loss: 0.2211\n",
      "Epoch [15/50], Train Loss: 0.0800, Val Loss: 0.2062\n",
      "Epoch [16/50], Train Loss: 0.0758, Val Loss: 0.1922\n",
      "Epoch [17/50], Train Loss: 0.0699, Val Loss: 0.1783\n",
      "Epoch [18/50], Train Loss: 0.0685, Val Loss: 0.1656\n",
      "Epoch [19/50], Train Loss: 0.0652, Val Loss: 0.1536\n",
      "Epoch [20/50], Train Loss: 0.0611, Val Loss: 0.1426\n",
      "Epoch [21/50], Train Loss: 0.0604, Val Loss: 0.1331\n",
      "Epoch [22/50], Train Loss: 0.0579, Val Loss: 0.1246\n",
      "Epoch [23/50], Train Loss: 0.0557, Val Loss: 0.1166\n",
      "Epoch [24/50], Train Loss: 0.0561, Val Loss: 0.1100\n",
      "Epoch [25/50], Train Loss: 0.0511, Val Loss: 0.1050\n",
      "Epoch [26/50], Train Loss: 0.0506, Val Loss: 0.1004\n",
      "Epoch [27/50], Train Loss: 0.0507, Val Loss: 0.0962\n",
      "Epoch [28/50], Train Loss: 0.0478, Val Loss: 0.0924\n",
      "Epoch [29/50], Train Loss: 0.0516, Val Loss: 0.0889\n",
      "Epoch [30/50], Train Loss: 0.0478, Val Loss: 0.0854\n",
      "Epoch [31/50], Train Loss: 0.0474, Val Loss: 0.0821\n",
      "Epoch [32/50], Train Loss: 0.0470, Val Loss: 0.0793\n",
      "Epoch [33/50], Train Loss: 0.0468, Val Loss: 0.0767\n",
      "Epoch [34/50], Train Loss: 0.0459, Val Loss: 0.0744\n",
      "Epoch [35/50], Train Loss: 0.0443, Val Loss: 0.0726\n",
      "Epoch [36/50], Train Loss: 0.0453, Val Loss: 0.0705\n",
      "Epoch [37/50], Train Loss: 0.0449, Val Loss: 0.0689\n",
      "Epoch [38/50], Train Loss: 0.0435, Val Loss: 0.0673\n",
      "Epoch [39/50], Train Loss: 0.0419, Val Loss: 0.0658\n",
      "Epoch [40/50], Train Loss: 0.0425, Val Loss: 0.0643\n",
      "Epoch [41/50], Train Loss: 0.0427, Val Loss: 0.0630\n",
      "Epoch [42/50], Train Loss: 0.0424, Val Loss: 0.0609\n",
      "Epoch [43/50], Train Loss: 0.0414, Val Loss: 0.0598\n",
      "Epoch [44/50], Train Loss: 0.0404, Val Loss: 0.0597\n",
      "Epoch [45/50], Train Loss: 0.0384, Val Loss: 0.0590\n",
      "Epoch [46/50], Train Loss: 0.0387, Val Loss: 0.0575\n",
      "Epoch [47/50], Train Loss: 0.0390, Val Loss: 0.0562\n",
      "Epoch [48/50], Train Loss: 0.0382, Val Loss: 0.0555\n",
      "Epoch [49/50], Train Loss: 0.0387, Val Loss: 0.0547\n",
      "Epoch [50/50], Train Loss: 0.0377, Val Loss: 0.0532\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=16, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1251, Val Loss: 0.2767\n",
      "Epoch [2/50], Train Loss: 0.1118, Val Loss: 0.2564\n",
      "Epoch [3/50], Train Loss: 0.1007, Val Loss: 0.2381\n",
      "Epoch [4/50], Train Loss: 0.0909, Val Loss: 0.2214\n",
      "Epoch [5/50], Train Loss: 0.0822, Val Loss: 0.2057\n",
      "Epoch [6/50], Train Loss: 0.0742, Val Loss: 0.1907\n",
      "Epoch [7/50], Train Loss: 0.0671, Val Loss: 0.1766\n",
      "Epoch [8/50], Train Loss: 0.0606, Val Loss: 0.1631\n",
      "Epoch [9/50], Train Loss: 0.0549, Val Loss: 0.1505\n",
      "Epoch [10/50], Train Loss: 0.0499, Val Loss: 0.1387\n",
      "Epoch [11/50], Train Loss: 0.0456, Val Loss: 0.1280\n",
      "Epoch [12/50], Train Loss: 0.0420, Val Loss: 0.1183\n",
      "Epoch [13/50], Train Loss: 0.0391, Val Loss: 0.1098\n",
      "Epoch [14/50], Train Loss: 0.0367, Val Loss: 0.1023\n",
      "Epoch [15/50], Train Loss: 0.0348, Val Loss: 0.0959\n",
      "Epoch [16/50], Train Loss: 0.0334, Val Loss: 0.0905\n",
      "Epoch [17/50], Train Loss: 0.0321, Val Loss: 0.0858\n",
      "Epoch [18/50], Train Loss: 0.0311, Val Loss: 0.0817\n",
      "Epoch [19/50], Train Loss: 0.0302, Val Loss: 0.0782\n",
      "Epoch [20/50], Train Loss: 0.0295, Val Loss: 0.0751\n",
      "Epoch [21/50], Train Loss: 0.0287, Val Loss: 0.0723\n",
      "Epoch [22/50], Train Loss: 0.0280, Val Loss: 0.0697\n",
      "Epoch [23/50], Train Loss: 0.0274, Val Loss: 0.0673\n",
      "Epoch [24/50], Train Loss: 0.0267, Val Loss: 0.0650\n",
      "Epoch [25/50], Train Loss: 0.0260, Val Loss: 0.0628\n",
      "Epoch [26/50], Train Loss: 0.0254, Val Loss: 0.0607\n",
      "Epoch [27/50], Train Loss: 0.0247, Val Loss: 0.0586\n",
      "Epoch [28/50], Train Loss: 0.0241, Val Loss: 0.0565\n",
      "Epoch [29/50], Train Loss: 0.0235, Val Loss: 0.0544\n",
      "Epoch [30/50], Train Loss: 0.0228, Val Loss: 0.0524\n",
      "Epoch [31/50], Train Loss: 0.0222, Val Loss: 0.0503\n",
      "Epoch [32/50], Train Loss: 0.0215, Val Loss: 0.0483\n",
      "Epoch [33/50], Train Loss: 0.0209, Val Loss: 0.0463\n",
      "Epoch [34/50], Train Loss: 0.0203, Val Loss: 0.0443\n",
      "Epoch [35/50], Train Loss: 0.0197, Val Loss: 0.0423\n",
      "Epoch [36/50], Train Loss: 0.0191, Val Loss: 0.0404\n",
      "Epoch [37/50], Train Loss: 0.0185, Val Loss: 0.0385\n",
      "Epoch [38/50], Train Loss: 0.0180, Val Loss: 0.0367\n",
      "Epoch [39/50], Train Loss: 0.0175, Val Loss: 0.0350\n",
      "Epoch [40/50], Train Loss: 0.0170, Val Loss: 0.0334\n",
      "Epoch [41/50], Train Loss: 0.0165, Val Loss: 0.0319\n",
      "Epoch [42/50], Train Loss: 0.0161, Val Loss: 0.0305\n",
      "Epoch [43/50], Train Loss: 0.0157, Val Loss: 0.0291\n",
      "Epoch [44/50], Train Loss: 0.0153, Val Loss: 0.0279\n",
      "Epoch [45/50], Train Loss: 0.0150, Val Loss: 0.0268\n",
      "Epoch [46/50], Train Loss: 0.0147, Val Loss: 0.0258\n",
      "Epoch [47/50], Train Loss: 0.0144, Val Loss: 0.0248\n",
      "Epoch [48/50], Train Loss: 0.0141, Val Loss: 0.0240\n",
      "Epoch [49/50], Train Loss: 0.0138, Val Loss: 0.0232\n",
      "Epoch [50/50], Train Loss: 0.0136, Val Loss: 0.0225\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=16, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1017, Val Loss: 0.2579\n",
      "Epoch [2/50], Train Loss: 0.0903, Val Loss: 0.2316\n",
      "Epoch [3/50], Train Loss: 0.0800, Val Loss: 0.2084\n",
      "Epoch [4/50], Train Loss: 0.0737, Val Loss: 0.1877\n",
      "Epoch [5/50], Train Loss: 0.0662, Val Loss: 0.1690\n",
      "Epoch [6/50], Train Loss: 0.0605, Val Loss: 0.1523\n",
      "Epoch [7/50], Train Loss: 0.0571, Val Loss: 0.1374\n",
      "Epoch [8/50], Train Loss: 0.0517, Val Loss: 0.1240\n",
      "Epoch [9/50], Train Loss: 0.0469, Val Loss: 0.1122\n",
      "Epoch [10/50], Train Loss: 0.0452, Val Loss: 0.1019\n",
      "Epoch [11/50], Train Loss: 0.0419, Val Loss: 0.0928\n",
      "Epoch [12/50], Train Loss: 0.0410, Val Loss: 0.0851\n",
      "Epoch [13/50], Train Loss: 0.0381, Val Loss: 0.0784\n",
      "Epoch [14/50], Train Loss: 0.0372, Val Loss: 0.0733\n",
      "Epoch [15/50], Train Loss: 0.0347, Val Loss: 0.0685\n",
      "Epoch [16/50], Train Loss: 0.0338, Val Loss: 0.0641\n",
      "Epoch [17/50], Train Loss: 0.0333, Val Loss: 0.0609\n",
      "Epoch [18/50], Train Loss: 0.0316, Val Loss: 0.0583\n",
      "Epoch [19/50], Train Loss: 0.0314, Val Loss: 0.0556\n",
      "Epoch [20/50], Train Loss: 0.0309, Val Loss: 0.0532\n",
      "Epoch [21/50], Train Loss: 0.0296, Val Loss: 0.0506\n",
      "Epoch [22/50], Train Loss: 0.0288, Val Loss: 0.0488\n",
      "Epoch [23/50], Train Loss: 0.0276, Val Loss: 0.0470\n",
      "Epoch [24/50], Train Loss: 0.0274, Val Loss: 0.0449\n",
      "Epoch [25/50], Train Loss: 0.0282, Val Loss: 0.0432\n",
      "Epoch [26/50], Train Loss: 0.0261, Val Loss: 0.0418\n",
      "Epoch [27/50], Train Loss: 0.0260, Val Loss: 0.0404\n",
      "Epoch [28/50], Train Loss: 0.0251, Val Loss: 0.0393\n",
      "Epoch [29/50], Train Loss: 0.0249, Val Loss: 0.0375\n",
      "Epoch [30/50], Train Loss: 0.0237, Val Loss: 0.0359\n",
      "Epoch [31/50], Train Loss: 0.0241, Val Loss: 0.0344\n",
      "Epoch [32/50], Train Loss: 0.0229, Val Loss: 0.0328\n",
      "Epoch [33/50], Train Loss: 0.0228, Val Loss: 0.0316\n",
      "Epoch [34/50], Train Loss: 0.0217, Val Loss: 0.0303\n",
      "Epoch [35/50], Train Loss: 0.0220, Val Loss: 0.0286\n",
      "Epoch [36/50], Train Loss: 0.0202, Val Loss: 0.0276\n",
      "Epoch [37/50], Train Loss: 0.0208, Val Loss: 0.0264\n",
      "Epoch [38/50], Train Loss: 0.0199, Val Loss: 0.0248\n",
      "Epoch [39/50], Train Loss: 0.0194, Val Loss: 0.0238\n",
      "Epoch [40/50], Train Loss: 0.0190, Val Loss: 0.0225\n",
      "Epoch [41/50], Train Loss: 0.0187, Val Loss: 0.0212\n",
      "Epoch [42/50], Train Loss: 0.0196, Val Loss: 0.0202\n",
      "Epoch [43/50], Train Loss: 0.0183, Val Loss: 0.0195\n",
      "Epoch [44/50], Train Loss: 0.0178, Val Loss: 0.0181\n",
      "Epoch [45/50], Train Loss: 0.0173, Val Loss: 0.0172\n",
      "Epoch [46/50], Train Loss: 0.0173, Val Loss: 0.0163\n",
      "Epoch [47/50], Train Loss: 0.0165, Val Loss: 0.0156\n",
      "Epoch [48/50], Train Loss: 0.0158, Val Loss: 0.0151\n",
      "Epoch [49/50], Train Loss: 0.0155, Val Loss: 0.0141\n",
      "Epoch [50/50], Train Loss: 0.0153, Val Loss: 0.0137\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=16, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.2188, Val Loss: 0.4925\n",
      "Epoch [2/50], Train Loss: 0.2022, Val Loss: 0.4607\n",
      "Epoch [3/50], Train Loss: 0.1831, Val Loss: 0.4317\n",
      "Epoch [4/50], Train Loss: 0.1752, Val Loss: 0.4041\n",
      "Epoch [5/50], Train Loss: 0.1630, Val Loss: 0.3787\n",
      "Epoch [6/50], Train Loss: 0.1502, Val Loss: 0.3546\n",
      "Epoch [7/50], Train Loss: 0.1403, Val Loss: 0.3321\n",
      "Epoch [8/50], Train Loss: 0.1335, Val Loss: 0.3102\n",
      "Epoch [9/50], Train Loss: 0.1208, Val Loss: 0.2889\n",
      "Epoch [10/50], Train Loss: 0.1170, Val Loss: 0.2693\n",
      "Epoch [11/50], Train Loss: 0.1123, Val Loss: 0.2507\n",
      "Epoch [12/50], Train Loss: 0.1014, Val Loss: 0.2322\n",
      "Epoch [13/50], Train Loss: 0.0970, Val Loss: 0.2153\n",
      "Epoch [14/50], Train Loss: 0.0891, Val Loss: 0.1993\n",
      "Epoch [15/50], Train Loss: 0.0881, Val Loss: 0.1846\n",
      "Epoch [16/50], Train Loss: 0.0829, Val Loss: 0.1707\n",
      "Epoch [17/50], Train Loss: 0.0780, Val Loss: 0.1586\n",
      "Epoch [18/50], Train Loss: 0.0776, Val Loss: 0.1480\n",
      "Epoch [19/50], Train Loss: 0.0768, Val Loss: 0.1382\n",
      "Epoch [20/50], Train Loss: 0.0708, Val Loss: 0.1298\n",
      "Epoch [21/50], Train Loss: 0.0694, Val Loss: 0.1224\n",
      "Epoch [22/50], Train Loss: 0.0667, Val Loss: 0.1156\n",
      "Epoch [23/50], Train Loss: 0.0650, Val Loss: 0.1097\n",
      "Epoch [24/50], Train Loss: 0.0629, Val Loss: 0.1046\n",
      "Epoch [25/50], Train Loss: 0.0625, Val Loss: 0.0993\n",
      "Epoch [26/50], Train Loss: 0.0606, Val Loss: 0.0948\n",
      "Epoch [27/50], Train Loss: 0.0581, Val Loss: 0.0923\n",
      "Epoch [28/50], Train Loss: 0.0585, Val Loss: 0.0886\n",
      "Epoch [29/50], Train Loss: 0.0573, Val Loss: 0.0842\n",
      "Epoch [30/50], Train Loss: 0.0575, Val Loss: 0.0808\n",
      "Epoch [31/50], Train Loss: 0.0540, Val Loss: 0.0784\n",
      "Epoch [32/50], Train Loss: 0.0557, Val Loss: 0.0754\n",
      "Epoch [33/50], Train Loss: 0.0541, Val Loss: 0.0731\n",
      "Epoch [34/50], Train Loss: 0.0541, Val Loss: 0.0703\n",
      "Epoch [35/50], Train Loss: 0.0534, Val Loss: 0.0678\n",
      "Epoch [36/50], Train Loss: 0.0506, Val Loss: 0.0651\n",
      "Epoch [37/50], Train Loss: 0.0528, Val Loss: 0.0639\n",
      "Epoch [38/50], Train Loss: 0.0506, Val Loss: 0.0622\n",
      "Epoch [39/50], Train Loss: 0.0494, Val Loss: 0.0602\n",
      "Epoch [40/50], Train Loss: 0.0511, Val Loss: 0.0575\n",
      "Epoch [41/50], Train Loss: 0.0467, Val Loss: 0.0552\n",
      "Epoch [42/50], Train Loss: 0.0482, Val Loss: 0.0525\n",
      "Epoch [43/50], Train Loss: 0.0455, Val Loss: 0.0508\n",
      "Epoch [44/50], Train Loss: 0.0458, Val Loss: 0.0488\n",
      "Epoch [45/50], Train Loss: 0.0455, Val Loss: 0.0458\n",
      "Epoch [46/50], Train Loss: 0.0437, Val Loss: 0.0447\n",
      "Epoch [47/50], Train Loss: 0.0431, Val Loss: 0.0433\n",
      "Epoch [48/50], Train Loss: 0.0427, Val Loss: 0.0424\n",
      "Epoch [49/50], Train Loss: 0.0412, Val Loss: 0.0406\n",
      "Epoch [50/50], Train Loss: 0.0422, Val Loss: 0.0397\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=16, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.2174, Val Loss: 0.5492\n",
      "Epoch [2/50], Train Loss: 0.1983, Val Loss: 0.5166\n",
      "Epoch [3/50], Train Loss: 0.1809, Val Loss: 0.4849\n",
      "Epoch [4/50], Train Loss: 0.1638, Val Loss: 0.4523\n",
      "Epoch [5/50], Train Loss: 0.1465, Val Loss: 0.4176\n",
      "Epoch [6/50], Train Loss: 0.1285, Val Loss: 0.3800\n",
      "Epoch [7/50], Train Loss: 0.1098, Val Loss: 0.3389\n",
      "Epoch [8/50], Train Loss: 0.0911, Val Loss: 0.2952\n",
      "Epoch [9/50], Train Loss: 0.0737, Val Loss: 0.2513\n",
      "Epoch [10/50], Train Loss: 0.0591, Val Loss: 0.2107\n",
      "Epoch [11/50], Train Loss: 0.0486, Val Loss: 0.1767\n",
      "Epoch [12/50], Train Loss: 0.0422, Val Loss: 0.1508\n",
      "Epoch [13/50], Train Loss: 0.0388, Val Loss: 0.1326\n",
      "Epoch [14/50], Train Loss: 0.0372, Val Loss: 0.1204\n",
      "Epoch [15/50], Train Loss: 0.0364, Val Loss: 0.1123\n",
      "Epoch [16/50], Train Loss: 0.0359, Val Loss: 0.1068\n",
      "Epoch [17/50], Train Loss: 0.0356, Val Loss: 0.1029\n",
      "Epoch [18/50], Train Loss: 0.0352, Val Loss: 0.1000\n",
      "Epoch [19/50], Train Loss: 0.0349, Val Loss: 0.0977\n",
      "Epoch [20/50], Train Loss: 0.0345, Val Loss: 0.0958\n",
      "Epoch [21/50], Train Loss: 0.0342, Val Loss: 0.0941\n",
      "Epoch [22/50], Train Loss: 0.0339, Val Loss: 0.0926\n",
      "Epoch [23/50], Train Loss: 0.0335, Val Loss: 0.0911\n",
      "Epoch [24/50], Train Loss: 0.0332, Val Loss: 0.0896\n",
      "Epoch [25/50], Train Loss: 0.0328, Val Loss: 0.0882\n",
      "Epoch [26/50], Train Loss: 0.0324, Val Loss: 0.0868\n",
      "Epoch [27/50], Train Loss: 0.0321, Val Loss: 0.0854\n",
      "Epoch [28/50], Train Loss: 0.0317, Val Loss: 0.0840\n",
      "Epoch [29/50], Train Loss: 0.0313, Val Loss: 0.0825\n",
      "Epoch [30/50], Train Loss: 0.0309, Val Loss: 0.0811\n",
      "Epoch [31/50], Train Loss: 0.0305, Val Loss: 0.0796\n",
      "Epoch [32/50], Train Loss: 0.0301, Val Loss: 0.0780\n",
      "Epoch [33/50], Train Loss: 0.0296, Val Loss: 0.0765\n",
      "Epoch [34/50], Train Loss: 0.0292, Val Loss: 0.0749\n",
      "Epoch [35/50], Train Loss: 0.0288, Val Loss: 0.0732\n",
      "Epoch [36/50], Train Loss: 0.0283, Val Loss: 0.0716\n",
      "Epoch [37/50], Train Loss: 0.0278, Val Loss: 0.0698\n",
      "Epoch [38/50], Train Loss: 0.0273, Val Loss: 0.0681\n",
      "Epoch [39/50], Train Loss: 0.0268, Val Loss: 0.0662\n",
      "Epoch [40/50], Train Loss: 0.0262, Val Loss: 0.0643\n",
      "Epoch [41/50], Train Loss: 0.0257, Val Loss: 0.0624\n",
      "Epoch [42/50], Train Loss: 0.0251, Val Loss: 0.0604\n",
      "Epoch [43/50], Train Loss: 0.0245, Val Loss: 0.0584\n",
      "Epoch [44/50], Train Loss: 0.0239, Val Loss: 0.0563\n",
      "Epoch [45/50], Train Loss: 0.0233, Val Loss: 0.0541\n",
      "Epoch [46/50], Train Loss: 0.0226, Val Loss: 0.0519\n",
      "Epoch [47/50], Train Loss: 0.0220, Val Loss: 0.0496\n",
      "Epoch [48/50], Train Loss: 0.0213, Val Loss: 0.0473\n",
      "Epoch [49/50], Train Loss: 0.0206, Val Loss: 0.0450\n",
      "Epoch [50/50], Train Loss: 0.0199, Val Loss: 0.0426\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=16, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1541, Val Loss: 0.3615\n",
      "Epoch [2/50], Train Loss: 0.1312, Val Loss: 0.3203\n",
      "Epoch [3/50], Train Loss: 0.1145, Val Loss: 0.2835\n",
      "Epoch [4/50], Train Loss: 0.1006, Val Loss: 0.2505\n",
      "Epoch [5/50], Train Loss: 0.0885, Val Loss: 0.2200\n",
      "Epoch [6/50], Train Loss: 0.0768, Val Loss: 0.1922\n",
      "Epoch [7/50], Train Loss: 0.0691, Val Loss: 0.1673\n",
      "Epoch [8/50], Train Loss: 0.0612, Val Loss: 0.1469\n",
      "Epoch [9/50], Train Loss: 0.0557, Val Loss: 0.1299\n",
      "Epoch [10/50], Train Loss: 0.0514, Val Loss: 0.1174\n",
      "Epoch [11/50], Train Loss: 0.0492, Val Loss: 0.1080\n",
      "Epoch [12/50], Train Loss: 0.0479, Val Loss: 0.1010\n",
      "Epoch [13/50], Train Loss: 0.0460, Val Loss: 0.0956\n",
      "Epoch [14/50], Train Loss: 0.0440, Val Loss: 0.0917\n",
      "Epoch [15/50], Train Loss: 0.0444, Val Loss: 0.0889\n",
      "Epoch [16/50], Train Loss: 0.0431, Val Loss: 0.0877\n",
      "Epoch [17/50], Train Loss: 0.0426, Val Loss: 0.0858\n",
      "Epoch [18/50], Train Loss: 0.0418, Val Loss: 0.0847\n",
      "Epoch [19/50], Train Loss: 0.0398, Val Loss: 0.0828\n",
      "Epoch [20/50], Train Loss: 0.0404, Val Loss: 0.0820\n",
      "Epoch [21/50], Train Loss: 0.0389, Val Loss: 0.0807\n",
      "Epoch [22/50], Train Loss: 0.0390, Val Loss: 0.0795\n",
      "Epoch [23/50], Train Loss: 0.0392, Val Loss: 0.0781\n",
      "Epoch [24/50], Train Loss: 0.0377, Val Loss: 0.0763\n",
      "Epoch [25/50], Train Loss: 0.0374, Val Loss: 0.0745\n",
      "Epoch [26/50], Train Loss: 0.0375, Val Loss: 0.0731\n",
      "Epoch [27/50], Train Loss: 0.0369, Val Loss: 0.0717\n",
      "Epoch [28/50], Train Loss: 0.0357, Val Loss: 0.0702\n",
      "Epoch [29/50], Train Loss: 0.0353, Val Loss: 0.0681\n",
      "Epoch [30/50], Train Loss: 0.0362, Val Loss: 0.0659\n",
      "Epoch [31/50], Train Loss: 0.0352, Val Loss: 0.0635\n",
      "Epoch [32/50], Train Loss: 0.0342, Val Loss: 0.0616\n",
      "Epoch [33/50], Train Loss: 0.0335, Val Loss: 0.0598\n",
      "Epoch [34/50], Train Loss: 0.0328, Val Loss: 0.0576\n",
      "Epoch [35/50], Train Loss: 0.0320, Val Loss: 0.0556\n",
      "Epoch [36/50], Train Loss: 0.0328, Val Loss: 0.0530\n",
      "Epoch [37/50], Train Loss: 0.0314, Val Loss: 0.0504\n",
      "Epoch [38/50], Train Loss: 0.0299, Val Loss: 0.0483\n",
      "Epoch [39/50], Train Loss: 0.0301, Val Loss: 0.0463\n",
      "Epoch [40/50], Train Loss: 0.0297, Val Loss: 0.0436\n",
      "Epoch [41/50], Train Loss: 0.0284, Val Loss: 0.0416\n",
      "Epoch [42/50], Train Loss: 0.0285, Val Loss: 0.0392\n",
      "Epoch [43/50], Train Loss: 0.0273, Val Loss: 0.0358\n",
      "Epoch [44/50], Train Loss: 0.0271, Val Loss: 0.0338\n",
      "Epoch [45/50], Train Loss: 0.0268, Val Loss: 0.0318\n",
      "Epoch [46/50], Train Loss: 0.0264, Val Loss: 0.0298\n",
      "Epoch [47/50], Train Loss: 0.0256, Val Loss: 0.0281\n",
      "Epoch [48/50], Train Loss: 0.0252, Val Loss: 0.0266\n",
      "Epoch [49/50], Train Loss: 0.0248, Val Loss: 0.0252\n",
      "Epoch [50/50], Train Loss: 0.0238, Val Loss: 0.0231\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=16, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1594, Val Loss: 0.3261\n",
      "Epoch [2/50], Train Loss: 0.1441, Val Loss: 0.2982\n",
      "Epoch [3/50], Train Loss: 0.1292, Val Loss: 0.2715\n",
      "Epoch [4/50], Train Loss: 0.1175, Val Loss: 0.2463\n",
      "Epoch [5/50], Train Loss: 0.1029, Val Loss: 0.2222\n",
      "Epoch [6/50], Train Loss: 0.0946, Val Loss: 0.2000\n",
      "Epoch [7/50], Train Loss: 0.0894, Val Loss: 0.1804\n",
      "Epoch [8/50], Train Loss: 0.0829, Val Loss: 0.1628\n",
      "Epoch [9/50], Train Loss: 0.0779, Val Loss: 0.1483\n",
      "Epoch [10/50], Train Loss: 0.0715, Val Loss: 0.1365\n",
      "Epoch [11/50], Train Loss: 0.0719, Val Loss: 0.1274\n",
      "Epoch [12/50], Train Loss: 0.0711, Val Loss: 0.1204\n",
      "Epoch [13/50], Train Loss: 0.0718, Val Loss: 0.1153\n",
      "Epoch [14/50], Train Loss: 0.0675, Val Loss: 0.1102\n",
      "Epoch [15/50], Train Loss: 0.0658, Val Loss: 0.1058\n",
      "Epoch [16/50], Train Loss: 0.0628, Val Loss: 0.1016\n",
      "Epoch [17/50], Train Loss: 0.0615, Val Loss: 0.0968\n",
      "Epoch [18/50], Train Loss: 0.0581, Val Loss: 0.0931\n",
      "Epoch [19/50], Train Loss: 0.0597, Val Loss: 0.0887\n",
      "Epoch [20/50], Train Loss: 0.0597, Val Loss: 0.0870\n",
      "Epoch [21/50], Train Loss: 0.0568, Val Loss: 0.0842\n",
      "Epoch [22/50], Train Loss: 0.0572, Val Loss: 0.0819\n",
      "Epoch [23/50], Train Loss: 0.0549, Val Loss: 0.0788\n",
      "Epoch [24/50], Train Loss: 0.0541, Val Loss: 0.0741\n",
      "Epoch [25/50], Train Loss: 0.0507, Val Loss: 0.0705\n",
      "Epoch [26/50], Train Loss: 0.0533, Val Loss: 0.0677\n",
      "Epoch [27/50], Train Loss: 0.0534, Val Loss: 0.0653\n",
      "Epoch [28/50], Train Loss: 0.0491, Val Loss: 0.0622\n",
      "Epoch [29/50], Train Loss: 0.0486, Val Loss: 0.0598\n",
      "Epoch [30/50], Train Loss: 0.0478, Val Loss: 0.0573\n",
      "Epoch [31/50], Train Loss: 0.0488, Val Loss: 0.0548\n",
      "Epoch [32/50], Train Loss: 0.0464, Val Loss: 0.0523\n",
      "Epoch [33/50], Train Loss: 0.0471, Val Loss: 0.0500\n",
      "Epoch [34/50], Train Loss: 0.0478, Val Loss: 0.0485\n",
      "Epoch [35/50], Train Loss: 0.0464, Val Loss: 0.0474\n",
      "Epoch [36/50], Train Loss: 0.0427, Val Loss: 0.0470\n",
      "Epoch [37/50], Train Loss: 0.0429, Val Loss: 0.0442\n",
      "Epoch [38/50], Train Loss: 0.0435, Val Loss: 0.0424\n",
      "Epoch [39/50], Train Loss: 0.0421, Val Loss: 0.0405\n",
      "Epoch [40/50], Train Loss: 0.0436, Val Loss: 0.0394\n",
      "Epoch [41/50], Train Loss: 0.0419, Val Loss: 0.0388\n",
      "Epoch [42/50], Train Loss: 0.0433, Val Loss: 0.0376\n",
      "Epoch [43/50], Train Loss: 0.0396, Val Loss: 0.0358\n",
      "Epoch [44/50], Train Loss: 0.0389, Val Loss: 0.0339\n",
      "Epoch [45/50], Train Loss: 0.0400, Val Loss: 0.0324\n",
      "Epoch [46/50], Train Loss: 0.0388, Val Loss: 0.0334\n",
      "Epoch [47/50], Train Loss: 0.0380, Val Loss: 0.0342\n",
      "Epoch [48/50], Train Loss: 0.0382, Val Loss: 0.0329\n",
      "Epoch [49/50], Train Loss: 0.0373, Val Loss: 0.0314\n",
      "Epoch [50/50], Train Loss: 0.0376, Val Loss: 0.0307\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=32, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1344, Val Loss: 0.3614\n",
      "Epoch [2/50], Train Loss: 0.1176, Val Loss: 0.3269\n",
      "Epoch [3/50], Train Loss: 0.1040, Val Loss: 0.2970\n",
      "Epoch [4/50], Train Loss: 0.0922, Val Loss: 0.2700\n",
      "Epoch [5/50], Train Loss: 0.0816, Val Loss: 0.2452\n",
      "Epoch [6/50], Train Loss: 0.0721, Val Loss: 0.2220\n",
      "Epoch [7/50], Train Loss: 0.0635, Val Loss: 0.2000\n",
      "Epoch [8/50], Train Loss: 0.0557, Val Loss: 0.1792\n",
      "Epoch [9/50], Train Loss: 0.0487, Val Loss: 0.1595\n",
      "Epoch [10/50], Train Loss: 0.0426, Val Loss: 0.1411\n",
      "Epoch [11/50], Train Loss: 0.0374, Val Loss: 0.1242\n",
      "Epoch [12/50], Train Loss: 0.0330, Val Loss: 0.1090\n",
      "Epoch [13/50], Train Loss: 0.0295, Val Loss: 0.0957\n",
      "Epoch [14/50], Train Loss: 0.0269, Val Loss: 0.0843\n",
      "Epoch [15/50], Train Loss: 0.0250, Val Loss: 0.0749\n",
      "Epoch [16/50], Train Loss: 0.0236, Val Loss: 0.0673\n",
      "Epoch [17/50], Train Loss: 0.0226, Val Loss: 0.0613\n",
      "Epoch [18/50], Train Loss: 0.0219, Val Loss: 0.0565\n",
      "Epoch [19/50], Train Loss: 0.0213, Val Loss: 0.0528\n",
      "Epoch [20/50], Train Loss: 0.0208, Val Loss: 0.0497\n",
      "Epoch [21/50], Train Loss: 0.0203, Val Loss: 0.0472\n",
      "Epoch [22/50], Train Loss: 0.0199, Val Loss: 0.0452\n",
      "Epoch [23/50], Train Loss: 0.0195, Val Loss: 0.0433\n",
      "Epoch [24/50], Train Loss: 0.0191, Val Loss: 0.0417\n",
      "Epoch [25/50], Train Loss: 0.0187, Val Loss: 0.0402\n",
      "Epoch [26/50], Train Loss: 0.0182, Val Loss: 0.0388\n",
      "Epoch [27/50], Train Loss: 0.0178, Val Loss: 0.0374\n",
      "Epoch [28/50], Train Loss: 0.0174, Val Loss: 0.0361\n",
      "Epoch [29/50], Train Loss: 0.0170, Val Loss: 0.0349\n",
      "Epoch [30/50], Train Loss: 0.0165, Val Loss: 0.0336\n",
      "Epoch [31/50], Train Loss: 0.0161, Val Loss: 0.0324\n",
      "Epoch [32/50], Train Loss: 0.0156, Val Loss: 0.0312\n",
      "Epoch [33/50], Train Loss: 0.0152, Val Loss: 0.0300\n",
      "Epoch [34/50], Train Loss: 0.0147, Val Loss: 0.0287\n",
      "Epoch [35/50], Train Loss: 0.0143, Val Loss: 0.0275\n",
      "Epoch [36/50], Train Loss: 0.0138, Val Loss: 0.0263\n",
      "Epoch [37/50], Train Loss: 0.0134, Val Loss: 0.0250\n",
      "Epoch [38/50], Train Loss: 0.0129, Val Loss: 0.0238\n",
      "Epoch [39/50], Train Loss: 0.0124, Val Loss: 0.0226\n",
      "Epoch [40/50], Train Loss: 0.0119, Val Loss: 0.0213\n",
      "Epoch [41/50], Train Loss: 0.0114, Val Loss: 0.0201\n",
      "Epoch [42/50], Train Loss: 0.0109, Val Loss: 0.0188\n",
      "Epoch [43/50], Train Loss: 0.0104, Val Loss: 0.0176\n",
      "Epoch [44/50], Train Loss: 0.0099, Val Loss: 0.0163\n",
      "Epoch [45/50], Train Loss: 0.0093, Val Loss: 0.0150\n",
      "Epoch [46/50], Train Loss: 0.0088, Val Loss: 0.0138\n",
      "Epoch [47/50], Train Loss: 0.0083, Val Loss: 0.0126\n",
      "Epoch [48/50], Train Loss: 0.0078, Val Loss: 0.0114\n",
      "Epoch [49/50], Train Loss: 0.0072, Val Loss: 0.0102\n",
      "Epoch [50/50], Train Loss: 0.0067, Val Loss: 0.0090\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=32, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1484, Val Loss: 0.4166\n",
      "Epoch [2/50], Train Loss: 0.1323, Val Loss: 0.3801\n",
      "Epoch [3/50], Train Loss: 0.1181, Val Loss: 0.3462\n",
      "Epoch [4/50], Train Loss: 0.1040, Val Loss: 0.3136\n",
      "Epoch [5/50], Train Loss: 0.0908, Val Loss: 0.2819\n",
      "Epoch [6/50], Train Loss: 0.0792, Val Loss: 0.2512\n",
      "Epoch [7/50], Train Loss: 0.0696, Val Loss: 0.2217\n",
      "Epoch [8/50], Train Loss: 0.0591, Val Loss: 0.1934\n",
      "Epoch [9/50], Train Loss: 0.0514, Val Loss: 0.1676\n",
      "Epoch [10/50], Train Loss: 0.0452, Val Loss: 0.1451\n",
      "Epoch [11/50], Train Loss: 0.0398, Val Loss: 0.1267\n",
      "Epoch [12/50], Train Loss: 0.0375, Val Loss: 0.1120\n",
      "Epoch [13/50], Train Loss: 0.0355, Val Loss: 0.1009\n",
      "Epoch [14/50], Train Loss: 0.0341, Val Loss: 0.0921\n",
      "Epoch [15/50], Train Loss: 0.0322, Val Loss: 0.0857\n",
      "Epoch [16/50], Train Loss: 0.0323, Val Loss: 0.0806\n",
      "Epoch [17/50], Train Loss: 0.0306, Val Loss: 0.0765\n",
      "Epoch [18/50], Train Loss: 0.0313, Val Loss: 0.0733\n",
      "Epoch [19/50], Train Loss: 0.0303, Val Loss: 0.0707\n",
      "Epoch [20/50], Train Loss: 0.0303, Val Loss: 0.0680\n",
      "Epoch [21/50], Train Loss: 0.0294, Val Loss: 0.0653\n",
      "Epoch [22/50], Train Loss: 0.0276, Val Loss: 0.0629\n",
      "Epoch [23/50], Train Loss: 0.0272, Val Loss: 0.0610\n",
      "Epoch [24/50], Train Loss: 0.0272, Val Loss: 0.0587\n",
      "Epoch [25/50], Train Loss: 0.0266, Val Loss: 0.0567\n",
      "Epoch [26/50], Train Loss: 0.0264, Val Loss: 0.0544\n",
      "Epoch [27/50], Train Loss: 0.0249, Val Loss: 0.0524\n",
      "Epoch [28/50], Train Loss: 0.0241, Val Loss: 0.0503\n",
      "Epoch [29/50], Train Loss: 0.0245, Val Loss: 0.0480\n",
      "Epoch [30/50], Train Loss: 0.0234, Val Loss: 0.0461\n",
      "Epoch [31/50], Train Loss: 0.0233, Val Loss: 0.0445\n",
      "Epoch [32/50], Train Loss: 0.0230, Val Loss: 0.0428\n",
      "Epoch [33/50], Train Loss: 0.0217, Val Loss: 0.0411\n",
      "Epoch [34/50], Train Loss: 0.0214, Val Loss: 0.0390\n",
      "Epoch [35/50], Train Loss: 0.0209, Val Loss: 0.0372\n",
      "Epoch [36/50], Train Loss: 0.0197, Val Loss: 0.0359\n",
      "Epoch [37/50], Train Loss: 0.0200, Val Loss: 0.0346\n",
      "Epoch [38/50], Train Loss: 0.0194, Val Loss: 0.0330\n",
      "Epoch [39/50], Train Loss: 0.0188, Val Loss: 0.0312\n",
      "Epoch [40/50], Train Loss: 0.0182, Val Loss: 0.0293\n",
      "Epoch [41/50], Train Loss: 0.0176, Val Loss: 0.0274\n",
      "Epoch [42/50], Train Loss: 0.0166, Val Loss: 0.0257\n",
      "Epoch [43/50], Train Loss: 0.0165, Val Loss: 0.0240\n",
      "Epoch [44/50], Train Loss: 0.0165, Val Loss: 0.0226\n",
      "Epoch [45/50], Train Loss: 0.0157, Val Loss: 0.0207\n",
      "Epoch [46/50], Train Loss: 0.0149, Val Loss: 0.0195\n",
      "Epoch [47/50], Train Loss: 0.0141, Val Loss: 0.0184\n",
      "Epoch [48/50], Train Loss: 0.0145, Val Loss: 0.0166\n",
      "Epoch [49/50], Train Loss: 0.0131, Val Loss: 0.0151\n",
      "Epoch [50/50], Train Loss: 0.0121, Val Loss: 0.0140\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=32, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1641, Val Loss: 0.4006\n",
      "Epoch [2/50], Train Loss: 0.1461, Val Loss: 0.3681\n",
      "Epoch [3/50], Train Loss: 0.1318, Val Loss: 0.3383\n",
      "Epoch [4/50], Train Loss: 0.1187, Val Loss: 0.3101\n",
      "Epoch [5/50], Train Loss: 0.1074, Val Loss: 0.2824\n",
      "Epoch [6/50], Train Loss: 0.0978, Val Loss: 0.2555\n",
      "Epoch [7/50], Train Loss: 0.0860, Val Loss: 0.2291\n",
      "Epoch [8/50], Train Loss: 0.0766, Val Loss: 0.2030\n",
      "Epoch [9/50], Train Loss: 0.0685, Val Loss: 0.1780\n",
      "Epoch [10/50], Train Loss: 0.0611, Val Loss: 0.1551\n",
      "Epoch [11/50], Train Loss: 0.0556, Val Loss: 0.1344\n",
      "Epoch [12/50], Train Loss: 0.0498, Val Loss: 0.1168\n",
      "Epoch [13/50], Train Loss: 0.0470, Val Loss: 0.1031\n",
      "Epoch [14/50], Train Loss: 0.0456, Val Loss: 0.0933\n",
      "Epoch [15/50], Train Loss: 0.0426, Val Loss: 0.0853\n",
      "Epoch [16/50], Train Loss: 0.0410, Val Loss: 0.0786\n",
      "Epoch [17/50], Train Loss: 0.0418, Val Loss: 0.0732\n",
      "Epoch [18/50], Train Loss: 0.0401, Val Loss: 0.0702\n",
      "Epoch [19/50], Train Loss: 0.0372, Val Loss: 0.0674\n",
      "Epoch [20/50], Train Loss: 0.0377, Val Loss: 0.0656\n",
      "Epoch [21/50], Train Loss: 0.0389, Val Loss: 0.0633\n",
      "Epoch [22/50], Train Loss: 0.0365, Val Loss: 0.0608\n",
      "Epoch [23/50], Train Loss: 0.0374, Val Loss: 0.0594\n",
      "Epoch [24/50], Train Loss: 0.0361, Val Loss: 0.0576\n",
      "Epoch [25/50], Train Loss: 0.0362, Val Loss: 0.0550\n",
      "Epoch [26/50], Train Loss: 0.0346, Val Loss: 0.0526\n",
      "Epoch [27/50], Train Loss: 0.0334, Val Loss: 0.0507\n",
      "Epoch [28/50], Train Loss: 0.0335, Val Loss: 0.0491\n",
      "Epoch [29/50], Train Loss: 0.0327, Val Loss: 0.0477\n",
      "Epoch [30/50], Train Loss: 0.0329, Val Loss: 0.0466\n",
      "Epoch [31/50], Train Loss: 0.0312, Val Loss: 0.0435\n",
      "Epoch [32/50], Train Loss: 0.0306, Val Loss: 0.0423\n",
      "Epoch [33/50], Train Loss: 0.0295, Val Loss: 0.0408\n",
      "Epoch [34/50], Train Loss: 0.0296, Val Loss: 0.0394\n",
      "Epoch [35/50], Train Loss: 0.0297, Val Loss: 0.0372\n",
      "Epoch [36/50], Train Loss: 0.0297, Val Loss: 0.0365\n",
      "Epoch [37/50], Train Loss: 0.0273, Val Loss: 0.0338\n",
      "Epoch [38/50], Train Loss: 0.0277, Val Loss: 0.0325\n",
      "Epoch [39/50], Train Loss: 0.0262, Val Loss: 0.0316\n",
      "Epoch [40/50], Train Loss: 0.0274, Val Loss: 0.0310\n",
      "Epoch [41/50], Train Loss: 0.0266, Val Loss: 0.0303\n",
      "Epoch [42/50], Train Loss: 0.0262, Val Loss: 0.0278\n",
      "Epoch [43/50], Train Loss: 0.0252, Val Loss: 0.0263\n",
      "Epoch [44/50], Train Loss: 0.0249, Val Loss: 0.0250\n",
      "Epoch [45/50], Train Loss: 0.0247, Val Loss: 0.0235\n",
      "Epoch [46/50], Train Loss: 0.0243, Val Loss: 0.0222\n",
      "Epoch [47/50], Train Loss: 0.0226, Val Loss: 0.0207\n",
      "Epoch [48/50], Train Loss: 0.0238, Val Loss: 0.0196\n",
      "Epoch [49/50], Train Loss: 0.0233, Val Loss: 0.0181\n",
      "Epoch [50/50], Train Loss: 0.0220, Val Loss: 0.0170\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=32, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1487, Val Loss: 0.3787\n",
      "Epoch [2/50], Train Loss: 0.1231, Val Loss: 0.3312\n",
      "Epoch [3/50], Train Loss: 0.1028, Val Loss: 0.2887\n",
      "Epoch [4/50], Train Loss: 0.0844, Val Loss: 0.2471\n",
      "Epoch [5/50], Train Loss: 0.0676, Val Loss: 0.2053\n",
      "Epoch [6/50], Train Loss: 0.0528, Val Loss: 0.1651\n",
      "Epoch [7/50], Train Loss: 0.0414, Val Loss: 0.1300\n",
      "Epoch [8/50], Train Loss: 0.0343, Val Loss: 0.1036\n",
      "Epoch [9/50], Train Loss: 0.0310, Val Loss: 0.0869\n",
      "Epoch [10/50], Train Loss: 0.0298, Val Loss: 0.0776\n",
      "Epoch [11/50], Train Loss: 0.0293, Val Loss: 0.0725\n",
      "Epoch [12/50], Train Loss: 0.0287, Val Loss: 0.0693\n",
      "Epoch [13/50], Train Loss: 0.0281, Val Loss: 0.0668\n",
      "Epoch [14/50], Train Loss: 0.0275, Val Loss: 0.0647\n",
      "Epoch [15/50], Train Loss: 0.0268, Val Loss: 0.0626\n",
      "Epoch [16/50], Train Loss: 0.0261, Val Loss: 0.0605\n",
      "Epoch [17/50], Train Loss: 0.0255, Val Loss: 0.0584\n",
      "Epoch [18/50], Train Loss: 0.0248, Val Loss: 0.0563\n",
      "Epoch [19/50], Train Loss: 0.0241, Val Loss: 0.0542\n",
      "Epoch [20/50], Train Loss: 0.0233, Val Loss: 0.0520\n",
      "Epoch [21/50], Train Loss: 0.0226, Val Loss: 0.0498\n",
      "Epoch [22/50], Train Loss: 0.0219, Val Loss: 0.0476\n",
      "Epoch [23/50], Train Loss: 0.0211, Val Loss: 0.0453\n",
      "Epoch [24/50], Train Loss: 0.0203, Val Loss: 0.0429\n",
      "Epoch [25/50], Train Loss: 0.0195, Val Loss: 0.0405\n",
      "Epoch [26/50], Train Loss: 0.0186, Val Loss: 0.0381\n",
      "Epoch [27/50], Train Loss: 0.0177, Val Loss: 0.0356\n",
      "Epoch [28/50], Train Loss: 0.0168, Val Loss: 0.0330\n",
      "Epoch [29/50], Train Loss: 0.0159, Val Loss: 0.0305\n",
      "Epoch [30/50], Train Loss: 0.0150, Val Loss: 0.0279\n",
      "Epoch [31/50], Train Loss: 0.0140, Val Loss: 0.0252\n",
      "Epoch [32/50], Train Loss: 0.0130, Val Loss: 0.0227\n",
      "Epoch [33/50], Train Loss: 0.0120, Val Loss: 0.0201\n",
      "Epoch [34/50], Train Loss: 0.0110, Val Loss: 0.0176\n",
      "Epoch [35/50], Train Loss: 0.0100, Val Loss: 0.0153\n",
      "Epoch [36/50], Train Loss: 0.0090, Val Loss: 0.0131\n",
      "Epoch [37/50], Train Loss: 0.0080, Val Loss: 0.0111\n",
      "Epoch [38/50], Train Loss: 0.0071, Val Loss: 0.0095\n",
      "Epoch [39/50], Train Loss: 0.0063, Val Loss: 0.0081\n",
      "Epoch [40/50], Train Loss: 0.0056, Val Loss: 0.0071\n",
      "Epoch [41/50], Train Loss: 0.0050, Val Loss: 0.0064\n",
      "Epoch [42/50], Train Loss: 0.0044, Val Loss: 0.0059\n",
      "Epoch [43/50], Train Loss: 0.0040, Val Loss: 0.0056\n",
      "Epoch [44/50], Train Loss: 0.0037, Val Loss: 0.0055\n",
      "Epoch [45/50], Train Loss: 0.0034, Val Loss: 0.0054\n",
      "Epoch [46/50], Train Loss: 0.0032, Val Loss: 0.0053\n",
      "Epoch [47/50], Train Loss: 0.0030, Val Loss: 0.0052\n",
      "Epoch [48/50], Train Loss: 0.0028, Val Loss: 0.0052\n",
      "Epoch [49/50], Train Loss: 0.0027, Val Loss: 0.0051\n",
      "Epoch [50/50], Train Loss: 0.0026, Val Loss: 0.0049\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=32, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1824, Val Loss: 0.4386\n",
      "Epoch [2/50], Train Loss: 0.1621, Val Loss: 0.4021\n",
      "Epoch [3/50], Train Loss: 0.1447, Val Loss: 0.3681\n",
      "Epoch [4/50], Train Loss: 0.1280, Val Loss: 0.3342\n",
      "Epoch [5/50], Train Loss: 0.1131, Val Loss: 0.2983\n",
      "Epoch [6/50], Train Loss: 0.0957, Val Loss: 0.2597\n",
      "Epoch [7/50], Train Loss: 0.0802, Val Loss: 0.2191\n",
      "Epoch [8/50], Train Loss: 0.0653, Val Loss: 0.1787\n",
      "Epoch [9/50], Train Loss: 0.0537, Val Loss: 0.1426\n",
      "Epoch [10/50], Train Loss: 0.0453, Val Loss: 0.1149\n",
      "Epoch [11/50], Train Loss: 0.0401, Val Loss: 0.0954\n",
      "Epoch [12/50], Train Loss: 0.0372, Val Loss: 0.0823\n",
      "Epoch [13/50], Train Loss: 0.0360, Val Loss: 0.0731\n",
      "Epoch [14/50], Train Loss: 0.0346, Val Loss: 0.0675\n",
      "Epoch [15/50], Train Loss: 0.0344, Val Loss: 0.0628\n",
      "Epoch [16/50], Train Loss: 0.0340, Val Loss: 0.0594\n",
      "Epoch [17/50], Train Loss: 0.0333, Val Loss: 0.0568\n",
      "Epoch [18/50], Train Loss: 0.0310, Val Loss: 0.0539\n",
      "Epoch [19/50], Train Loss: 0.0305, Val Loss: 0.0509\n",
      "Epoch [20/50], Train Loss: 0.0298, Val Loss: 0.0486\n",
      "Epoch [21/50], Train Loss: 0.0301, Val Loss: 0.0455\n",
      "Epoch [22/50], Train Loss: 0.0289, Val Loss: 0.0435\n",
      "Epoch [23/50], Train Loss: 0.0282, Val Loss: 0.0411\n",
      "Epoch [24/50], Train Loss: 0.0269, Val Loss: 0.0384\n",
      "Epoch [25/50], Train Loss: 0.0265, Val Loss: 0.0360\n",
      "Epoch [26/50], Train Loss: 0.0249, Val Loss: 0.0331\n",
      "Epoch [27/50], Train Loss: 0.0253, Val Loss: 0.0308\n",
      "Epoch [28/50], Train Loss: 0.0223, Val Loss: 0.0284\n",
      "Epoch [29/50], Train Loss: 0.0221, Val Loss: 0.0260\n",
      "Epoch [30/50], Train Loss: 0.0219, Val Loss: 0.0238\n",
      "Epoch [31/50], Train Loss: 0.0209, Val Loss: 0.0213\n",
      "Epoch [32/50], Train Loss: 0.0199, Val Loss: 0.0194\n",
      "Epoch [33/50], Train Loss: 0.0189, Val Loss: 0.0170\n",
      "Epoch [34/50], Train Loss: 0.0183, Val Loss: 0.0151\n",
      "Epoch [35/50], Train Loss: 0.0175, Val Loss: 0.0133\n",
      "Epoch [36/50], Train Loss: 0.0165, Val Loss: 0.0114\n",
      "Epoch [37/50], Train Loss: 0.0149, Val Loss: 0.0106\n",
      "Epoch [38/50], Train Loss: 0.0152, Val Loss: 0.0095\n",
      "Epoch [39/50], Train Loss: 0.0152, Val Loss: 0.0082\n",
      "Epoch [40/50], Train Loss: 0.0144, Val Loss: 0.0079\n",
      "Epoch [41/50], Train Loss: 0.0139, Val Loss: 0.0070\n",
      "Epoch [42/50], Train Loss: 0.0134, Val Loss: 0.0061\n",
      "Epoch [43/50], Train Loss: 0.0125, Val Loss: 0.0059\n",
      "Epoch [44/50], Train Loss: 0.0113, Val Loss: 0.0064\n",
      "Epoch [45/50], Train Loss: 0.0117, Val Loss: 0.0064\n",
      "Epoch [46/50], Train Loss: 0.0110, Val Loss: 0.0062\n",
      "Epoch [47/50], Train Loss: 0.0113, Val Loss: 0.0057\n",
      "Epoch [48/50], Train Loss: 0.0109, Val Loss: 0.0056\n",
      "Epoch [49/50], Train Loss: 0.0099, Val Loss: 0.0055\n",
      "Epoch [50/50], Train Loss: 0.0102, Val Loss: 0.0061\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=32, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1266, Val Loss: 0.3181\n",
      "Epoch [2/50], Train Loss: 0.1052, Val Loss: 0.2710\n",
      "Epoch [3/50], Train Loss: 0.0873, Val Loss: 0.2299\n",
      "Epoch [4/50], Train Loss: 0.0734, Val Loss: 0.1941\n",
      "Epoch [5/50], Train Loss: 0.0658, Val Loss: 0.1635\n",
      "Epoch [6/50], Train Loss: 0.0560, Val Loss: 0.1392\n",
      "Epoch [7/50], Train Loss: 0.0523, Val Loss: 0.1202\n",
      "Epoch [8/50], Train Loss: 0.0497, Val Loss: 0.1060\n",
      "Epoch [9/50], Train Loss: 0.0492, Val Loss: 0.0961\n",
      "Epoch [10/50], Train Loss: 0.0477, Val Loss: 0.0884\n",
      "Epoch [11/50], Train Loss: 0.0450, Val Loss: 0.0826\n",
      "Epoch [12/50], Train Loss: 0.0451, Val Loss: 0.0779\n",
      "Epoch [13/50], Train Loss: 0.0428, Val Loss: 0.0737\n",
      "Epoch [14/50], Train Loss: 0.0431, Val Loss: 0.0698\n",
      "Epoch [15/50], Train Loss: 0.0428, Val Loss: 0.0657\n",
      "Epoch [16/50], Train Loss: 0.0407, Val Loss: 0.0629\n",
      "Epoch [17/50], Train Loss: 0.0396, Val Loss: 0.0585\n",
      "Epoch [18/50], Train Loss: 0.0392, Val Loss: 0.0546\n",
      "Epoch [19/50], Train Loss: 0.0370, Val Loss: 0.0505\n",
      "Epoch [20/50], Train Loss: 0.0357, Val Loss: 0.0481\n",
      "Epoch [21/50], Train Loss: 0.0370, Val Loss: 0.0446\n",
      "Epoch [22/50], Train Loss: 0.0347, Val Loss: 0.0405\n",
      "Epoch [23/50], Train Loss: 0.0337, Val Loss: 0.0381\n",
      "Epoch [24/50], Train Loss: 0.0327, Val Loss: 0.0362\n",
      "Epoch [25/50], Train Loss: 0.0317, Val Loss: 0.0328\n",
      "Epoch [26/50], Train Loss: 0.0312, Val Loss: 0.0291\n",
      "Epoch [27/50], Train Loss: 0.0304, Val Loss: 0.0257\n",
      "Epoch [28/50], Train Loss: 0.0290, Val Loss: 0.0235\n",
      "Epoch [29/50], Train Loss: 0.0298, Val Loss: 0.0221\n",
      "Epoch [30/50], Train Loss: 0.0283, Val Loss: 0.0212\n",
      "Epoch [31/50], Train Loss: 0.0270, Val Loss: 0.0179\n",
      "Epoch [32/50], Train Loss: 0.0261, Val Loss: 0.0160\n",
      "Epoch [33/50], Train Loss: 0.0259, Val Loss: 0.0153\n",
      "Epoch [34/50], Train Loss: 0.0252, Val Loss: 0.0150\n",
      "Epoch [35/50], Train Loss: 0.0257, Val Loss: 0.0132\n",
      "Epoch [36/50], Train Loss: 0.0247, Val Loss: 0.0130\n",
      "Epoch [37/50], Train Loss: 0.0236, Val Loss: 0.0140\n",
      "Epoch [38/50], Train Loss: 0.0240, Val Loss: 0.0128\n",
      "Epoch [39/50], Train Loss: 0.0223, Val Loss: 0.0131\n",
      "Epoch [40/50], Train Loss: 0.0218, Val Loss: 0.0113\n",
      "Epoch [41/50], Train Loss: 0.0220, Val Loss: 0.0101\n",
      "Epoch [42/50], Train Loss: 0.0216, Val Loss: 0.0104\n",
      "Epoch [43/50], Train Loss: 0.0207, Val Loss: 0.0099\n",
      "Epoch [44/50], Train Loss: 0.0212, Val Loss: 0.0103\n",
      "Epoch [45/50], Train Loss: 0.0194, Val Loss: 0.0111\n",
      "Epoch [46/50], Train Loss: 0.0200, Val Loss: 0.0093\n",
      "Epoch [47/50], Train Loss: 0.0193, Val Loss: 0.0095\n",
      "Epoch [48/50], Train Loss: 0.0190, Val Loss: 0.0090\n",
      "Epoch [49/50], Train Loss: 0.0177, Val Loss: 0.0088\n",
      "Epoch [50/50], Train Loss: 0.0174, Val Loss: 0.0081\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=32, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0757, Val Loss: 0.2377\n",
      "Epoch [2/50], Train Loss: 0.0599, Val Loss: 0.2033\n",
      "Epoch [3/50], Train Loss: 0.0491, Val Loss: 0.1725\n",
      "Epoch [4/50], Train Loss: 0.0413, Val Loss: 0.1451\n",
      "Epoch [5/50], Train Loss: 0.0363, Val Loss: 0.1219\n",
      "Epoch [6/50], Train Loss: 0.0335, Val Loss: 0.1041\n",
      "Epoch [7/50], Train Loss: 0.0320, Val Loss: 0.0913\n",
      "Epoch [8/50], Train Loss: 0.0310, Val Loss: 0.0821\n",
      "Epoch [9/50], Train Loss: 0.0301, Val Loss: 0.0751\n",
      "Epoch [10/50], Train Loss: 0.0291, Val Loss: 0.0692\n",
      "Epoch [11/50], Train Loss: 0.0281, Val Loss: 0.0638\n",
      "Epoch [12/50], Train Loss: 0.0268, Val Loss: 0.0584\n",
      "Epoch [13/50], Train Loss: 0.0255, Val Loss: 0.0529\n",
      "Epoch [14/50], Train Loss: 0.0241, Val Loss: 0.0472\n",
      "Epoch [15/50], Train Loss: 0.0225, Val Loss: 0.0414\n",
      "Epoch [16/50], Train Loss: 0.0209, Val Loss: 0.0355\n",
      "Epoch [17/50], Train Loss: 0.0192, Val Loss: 0.0297\n",
      "Epoch [18/50], Train Loss: 0.0175, Val Loss: 0.0243\n",
      "Epoch [19/50], Train Loss: 0.0159, Val Loss: 0.0195\n",
      "Epoch [20/50], Train Loss: 0.0144, Val Loss: 0.0156\n",
      "Epoch [21/50], Train Loss: 0.0132, Val Loss: 0.0126\n",
      "Epoch [22/50], Train Loss: 0.0121, Val Loss: 0.0104\n",
      "Epoch [23/50], Train Loss: 0.0112, Val Loss: 0.0090\n",
      "Epoch [24/50], Train Loss: 0.0104, Val Loss: 0.0080\n",
      "Epoch [25/50], Train Loss: 0.0096, Val Loss: 0.0074\n",
      "Epoch [26/50], Train Loss: 0.0087, Val Loss: 0.0070\n",
      "Epoch [27/50], Train Loss: 0.0078, Val Loss: 0.0067\n",
      "Epoch [28/50], Train Loss: 0.0070, Val Loss: 0.0066\n",
      "Epoch [29/50], Train Loss: 0.0061, Val Loss: 0.0065\n",
      "Epoch [30/50], Train Loss: 0.0052, Val Loss: 0.0064\n",
      "Epoch [31/50], Train Loss: 0.0045, Val Loss: 0.0063\n",
      "Epoch [32/50], Train Loss: 0.0038, Val Loss: 0.0062\n",
      "Epoch [33/50], Train Loss: 0.0033, Val Loss: 0.0059\n",
      "Epoch [34/50], Train Loss: 0.0030, Val Loss: 0.0056\n",
      "Epoch [35/50], Train Loss: 0.0028, Val Loss: 0.0053\n",
      "Epoch [36/50], Train Loss: 0.0026, Val Loss: 0.0051\n",
      "Epoch [37/50], Train Loss: 0.0026, Val Loss: 0.0050\n",
      "Epoch [38/50], Train Loss: 0.0025, Val Loss: 0.0049\n",
      "Epoch [39/50], Train Loss: 0.0025, Val Loss: 0.0048\n",
      "Epoch [40/50], Train Loss: 0.0025, Val Loss: 0.0048\n",
      "Epoch [41/50], Train Loss: 0.0025, Val Loss: 0.0048\n",
      "Epoch [42/50], Train Loss: 0.0024, Val Loss: 0.0047\n",
      "Epoch [43/50], Train Loss: 0.0024, Val Loss: 0.0047\n",
      "Epoch [44/50], Train Loss: 0.0024, Val Loss: 0.0047\n",
      "Epoch [45/50], Train Loss: 0.0024, Val Loss: 0.0047\n",
      "Epoch [46/50], Train Loss: 0.0024, Val Loss: 0.0047\n",
      "Epoch [47/50], Train Loss: 0.0024, Val Loss: 0.0047\n",
      "Epoch [48/50], Train Loss: 0.0024, Val Loss: 0.0047\n",
      "Epoch [49/50], Train Loss: 0.0024, Val Loss: 0.0047\n",
      "Epoch [50/50], Train Loss: 0.0024, Val Loss: 0.0047\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=32, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0843, Val Loss: 0.2704\n",
      "Epoch [2/50], Train Loss: 0.0710, Val Loss: 0.2363\n",
      "Epoch [3/50], Train Loss: 0.0615, Val Loss: 0.2066\n",
      "Epoch [4/50], Train Loss: 0.0538, Val Loss: 0.1795\n",
      "Epoch [5/50], Train Loss: 0.0477, Val Loss: 0.1548\n",
      "Epoch [6/50], Train Loss: 0.0432, Val Loss: 0.1339\n",
      "Epoch [7/50], Train Loss: 0.0403, Val Loss: 0.1169\n",
      "Epoch [8/50], Train Loss: 0.0373, Val Loss: 0.1032\n",
      "Epoch [9/50], Train Loss: 0.0364, Val Loss: 0.0931\n",
      "Epoch [10/50], Train Loss: 0.0351, Val Loss: 0.0856\n",
      "Epoch [11/50], Train Loss: 0.0337, Val Loss: 0.0795\n",
      "Epoch [12/50], Train Loss: 0.0327, Val Loss: 0.0732\n",
      "Epoch [13/50], Train Loss: 0.0310, Val Loss: 0.0675\n",
      "Epoch [14/50], Train Loss: 0.0301, Val Loss: 0.0609\n",
      "Epoch [15/50], Train Loss: 0.0286, Val Loss: 0.0545\n",
      "Epoch [16/50], Train Loss: 0.0271, Val Loss: 0.0480\n",
      "Epoch [17/50], Train Loss: 0.0255, Val Loss: 0.0415\n",
      "Epoch [18/50], Train Loss: 0.0225, Val Loss: 0.0343\n",
      "Epoch [19/50], Train Loss: 0.0210, Val Loss: 0.0281\n",
      "Epoch [20/50], Train Loss: 0.0197, Val Loss: 0.0215\n",
      "Epoch [21/50], Train Loss: 0.0174, Val Loss: 0.0164\n",
      "Epoch [22/50], Train Loss: 0.0161, Val Loss: 0.0119\n",
      "Epoch [23/50], Train Loss: 0.0146, Val Loss: 0.0093\n",
      "Epoch [24/50], Train Loss: 0.0142, Val Loss: 0.0072\n",
      "Epoch [25/50], Train Loss: 0.0130, Val Loss: 0.0065\n",
      "Epoch [26/50], Train Loss: 0.0120, Val Loss: 0.0056\n",
      "Epoch [27/50], Train Loss: 0.0110, Val Loss: 0.0057\n",
      "Epoch [28/50], Train Loss: 0.0109, Val Loss: 0.0057\n",
      "Epoch [29/50], Train Loss: 0.0103, Val Loss: 0.0053\n",
      "Epoch [30/50], Train Loss: 0.0094, Val Loss: 0.0056\n",
      "Epoch [31/50], Train Loss: 0.0089, Val Loss: 0.0054\n",
      "Epoch [32/50], Train Loss: 0.0086, Val Loss: 0.0050\n",
      "Epoch [33/50], Train Loss: 0.0083, Val Loss: 0.0046\n",
      "Epoch [34/50], Train Loss: 0.0077, Val Loss: 0.0051\n",
      "Epoch [35/50], Train Loss: 0.0074, Val Loss: 0.0043\n",
      "Epoch [36/50], Train Loss: 0.0076, Val Loss: 0.0045\n",
      "Epoch [37/50], Train Loss: 0.0069, Val Loss: 0.0044\n",
      "Epoch [38/50], Train Loss: 0.0072, Val Loss: 0.0040\n",
      "Epoch [39/50], Train Loss: 0.0068, Val Loss: 0.0045\n",
      "Epoch [40/50], Train Loss: 0.0068, Val Loss: 0.0046\n",
      "Epoch [41/50], Train Loss: 0.0065, Val Loss: 0.0039\n",
      "Epoch [42/50], Train Loss: 0.0067, Val Loss: 0.0042\n",
      "Epoch [43/50], Train Loss: 0.0067, Val Loss: 0.0042\n",
      "Epoch [44/50], Train Loss: 0.0065, Val Loss: 0.0044\n",
      "Epoch [45/50], Train Loss: 0.0067, Val Loss: 0.0042\n",
      "Epoch [46/50], Train Loss: 0.0066, Val Loss: 0.0038\n",
      "Epoch [47/50], Train Loss: 0.0062, Val Loss: 0.0042\n",
      "Epoch [48/50], Train Loss: 0.0065, Val Loss: 0.0040\n",
      "Epoch [49/50], Train Loss: 0.0062, Val Loss: 0.0037\n",
      "Epoch [50/50], Train Loss: 0.0064, Val Loss: 0.0043\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=32, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1353, Val Loss: 0.3130\n",
      "Epoch [2/50], Train Loss: 0.1059, Val Loss: 0.2605\n",
      "Epoch [3/50], Train Loss: 0.0857, Val Loss: 0.2150\n",
      "Epoch [4/50], Train Loss: 0.0727, Val Loss: 0.1762\n",
      "Epoch [5/50], Train Loss: 0.0647, Val Loss: 0.1471\n",
      "Epoch [6/50], Train Loss: 0.0619, Val Loss: 0.1291\n",
      "Epoch [7/50], Train Loss: 0.0596, Val Loss: 0.1178\n",
      "Epoch [8/50], Train Loss: 0.0561, Val Loss: 0.1115\n",
      "Epoch [9/50], Train Loss: 0.0577, Val Loss: 0.1084\n",
      "Epoch [10/50], Train Loss: 0.0536, Val Loss: 0.1031\n",
      "Epoch [11/50], Train Loss: 0.0545, Val Loss: 0.1005\n",
      "Epoch [12/50], Train Loss: 0.0522, Val Loss: 0.0960\n",
      "Epoch [13/50], Train Loss: 0.0507, Val Loss: 0.0923\n",
      "Epoch [14/50], Train Loss: 0.0503, Val Loss: 0.0874\n",
      "Epoch [15/50], Train Loss: 0.0476, Val Loss: 0.0847\n",
      "Epoch [16/50], Train Loss: 0.0479, Val Loss: 0.0816\n",
      "Epoch [17/50], Train Loss: 0.0465, Val Loss: 0.0782\n",
      "Epoch [18/50], Train Loss: 0.0464, Val Loss: 0.0741\n",
      "Epoch [19/50], Train Loss: 0.0443, Val Loss: 0.0703\n",
      "Epoch [20/50], Train Loss: 0.0430, Val Loss: 0.0646\n",
      "Epoch [21/50], Train Loss: 0.0424, Val Loss: 0.0613\n",
      "Epoch [22/50], Train Loss: 0.0416, Val Loss: 0.0579\n",
      "Epoch [23/50], Train Loss: 0.0398, Val Loss: 0.0528\n",
      "Epoch [24/50], Train Loss: 0.0392, Val Loss: 0.0503\n",
      "Epoch [25/50], Train Loss: 0.0382, Val Loss: 0.0458\n",
      "Epoch [26/50], Train Loss: 0.0374, Val Loss: 0.0395\n",
      "Epoch [27/50], Train Loss: 0.0361, Val Loss: 0.0372\n",
      "Epoch [28/50], Train Loss: 0.0350, Val Loss: 0.0316\n",
      "Epoch [29/50], Train Loss: 0.0334, Val Loss: 0.0275\n",
      "Epoch [30/50], Train Loss: 0.0321, Val Loss: 0.0265\n",
      "Epoch [31/50], Train Loss: 0.0311, Val Loss: 0.0216\n",
      "Epoch [32/50], Train Loss: 0.0300, Val Loss: 0.0187\n",
      "Epoch [33/50], Train Loss: 0.0301, Val Loss: 0.0167\n",
      "Epoch [34/50], Train Loss: 0.0294, Val Loss: 0.0151\n",
      "Epoch [35/50], Train Loss: 0.0280, Val Loss: 0.0142\n",
      "Epoch [36/50], Train Loss: 0.0268, Val Loss: 0.0136\n",
      "Epoch [37/50], Train Loss: 0.0259, Val Loss: 0.0136\n",
      "Epoch [38/50], Train Loss: 0.0262, Val Loss: 0.0140\n",
      "Epoch [39/50], Train Loss: 0.0261, Val Loss: 0.0116\n",
      "Epoch [40/50], Train Loss: 0.0252, Val Loss: 0.0128\n",
      "Epoch [41/50], Train Loss: 0.0240, Val Loss: 0.0123\n",
      "Epoch [42/50], Train Loss: 0.0234, Val Loss: 0.0118\n",
      "Epoch [43/50], Train Loss: 0.0234, Val Loss: 0.0101\n",
      "Epoch [44/50], Train Loss: 0.0222, Val Loss: 0.0106\n",
      "Epoch [45/50], Train Loss: 0.0221, Val Loss: 0.0106\n",
      "Epoch [46/50], Train Loss: 0.0214, Val Loss: 0.0106\n",
      "Epoch [47/50], Train Loss: 0.0208, Val Loss: 0.0082\n",
      "Epoch [48/50], Train Loss: 0.0203, Val Loss: 0.0097\n",
      "Epoch [49/50], Train Loss: 0.0203, Val Loss: 0.0074\n",
      "Epoch [50/50], Train Loss: 0.0191, Val Loss: 0.0084\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=64, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1645, Val Loss: 0.3985\n",
      "Epoch [2/50], Train Loss: 0.1380, Val Loss: 0.3446\n",
      "Epoch [3/50], Train Loss: 0.1154, Val Loss: 0.2941\n",
      "Epoch [4/50], Train Loss: 0.0942, Val Loss: 0.2440\n",
      "Epoch [5/50], Train Loss: 0.0739, Val Loss: 0.1930\n",
      "Epoch [6/50], Train Loss: 0.0549, Val Loss: 0.1418\n",
      "Epoch [7/50], Train Loss: 0.0390, Val Loss: 0.0954\n",
      "Epoch [8/50], Train Loss: 0.0289, Val Loss: 0.0625\n",
      "Epoch [9/50], Train Loss: 0.0250, Val Loss: 0.0467\n",
      "Epoch [10/50], Train Loss: 0.0238, Val Loss: 0.0407\n",
      "Epoch [11/50], Train Loss: 0.0229, Val Loss: 0.0374\n",
      "Epoch [12/50], Train Loss: 0.0220, Val Loss: 0.0348\n",
      "Epoch [13/50], Train Loss: 0.0212, Val Loss: 0.0324\n",
      "Epoch [14/50], Train Loss: 0.0204, Val Loss: 0.0301\n",
      "Epoch [15/50], Train Loss: 0.0196, Val Loss: 0.0280\n",
      "Epoch [16/50], Train Loss: 0.0189, Val Loss: 0.0259\n",
      "Epoch [17/50], Train Loss: 0.0181, Val Loss: 0.0239\n",
      "Epoch [18/50], Train Loss: 0.0173, Val Loss: 0.0220\n",
      "Epoch [19/50], Train Loss: 0.0166, Val Loss: 0.0201\n",
      "Epoch [20/50], Train Loss: 0.0158, Val Loss: 0.0182\n",
      "Epoch [21/50], Train Loss: 0.0151, Val Loss: 0.0164\n",
      "Epoch [22/50], Train Loss: 0.0143, Val Loss: 0.0147\n",
      "Epoch [23/50], Train Loss: 0.0135, Val Loss: 0.0130\n",
      "Epoch [24/50], Train Loss: 0.0127, Val Loss: 0.0113\n",
      "Epoch [25/50], Train Loss: 0.0119, Val Loss: 0.0097\n",
      "Epoch [26/50], Train Loss: 0.0111, Val Loss: 0.0082\n",
      "Epoch [27/50], Train Loss: 0.0103, Val Loss: 0.0068\n",
      "Epoch [28/50], Train Loss: 0.0094, Val Loss: 0.0055\n",
      "Epoch [29/50], Train Loss: 0.0085, Val Loss: 0.0044\n",
      "Epoch [30/50], Train Loss: 0.0076, Val Loss: 0.0035\n",
      "Epoch [31/50], Train Loss: 0.0067, Val Loss: 0.0029\n",
      "Epoch [32/50], Train Loss: 0.0057, Val Loss: 0.0027\n",
      "Epoch [33/50], Train Loss: 0.0048, Val Loss: 0.0031\n",
      "Epoch [34/50], Train Loss: 0.0040, Val Loss: 0.0040\n",
      "Epoch [35/50], Train Loss: 0.0034, Val Loss: 0.0052\n",
      "Epoch [36/50], Train Loss: 0.0031, Val Loss: 0.0060\n",
      "Epoch [37/50], Train Loss: 0.0030, Val Loss: 0.0063\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=64, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1303, Val Loss: 0.3496\n",
      "Epoch [2/50], Train Loss: 0.1044, Val Loss: 0.2957\n",
      "Epoch [3/50], Train Loss: 0.0848, Val Loss: 0.2484\n",
      "Epoch [4/50], Train Loss: 0.0674, Val Loss: 0.2051\n",
      "Epoch [5/50], Train Loss: 0.0531, Val Loss: 0.1655\n",
      "Epoch [6/50], Train Loss: 0.0423, Val Loss: 0.1303\n",
      "Epoch [7/50], Train Loss: 0.0352, Val Loss: 0.1016\n",
      "Epoch [8/50], Train Loss: 0.0302, Val Loss: 0.0816\n",
      "Epoch [9/50], Train Loss: 0.0290, Val Loss: 0.0682\n",
      "Epoch [10/50], Train Loss: 0.0278, Val Loss: 0.0603\n",
      "Epoch [11/50], Train Loss: 0.0272, Val Loss: 0.0565\n",
      "Epoch [12/50], Train Loss: 0.0269, Val Loss: 0.0531\n",
      "Epoch [13/50], Train Loss: 0.0259, Val Loss: 0.0494\n",
      "Epoch [14/50], Train Loss: 0.0251, Val Loss: 0.0465\n",
      "Epoch [15/50], Train Loss: 0.0246, Val Loss: 0.0441\n",
      "Epoch [16/50], Train Loss: 0.0234, Val Loss: 0.0412\n",
      "Epoch [17/50], Train Loss: 0.0230, Val Loss: 0.0384\n",
      "Epoch [18/50], Train Loss: 0.0218, Val Loss: 0.0360\n",
      "Epoch [19/50], Train Loss: 0.0212, Val Loss: 0.0336\n",
      "Epoch [20/50], Train Loss: 0.0206, Val Loss: 0.0306\n",
      "Epoch [21/50], Train Loss: 0.0196, Val Loss: 0.0283\n",
      "Epoch [22/50], Train Loss: 0.0185, Val Loss: 0.0251\n",
      "Epoch [23/50], Train Loss: 0.0179, Val Loss: 0.0223\n",
      "Epoch [24/50], Train Loss: 0.0173, Val Loss: 0.0202\n",
      "Epoch [25/50], Train Loss: 0.0161, Val Loss: 0.0176\n",
      "Epoch [26/50], Train Loss: 0.0154, Val Loss: 0.0149\n",
      "Epoch [27/50], Train Loss: 0.0147, Val Loss: 0.0128\n",
      "Epoch [28/50], Train Loss: 0.0136, Val Loss: 0.0104\n",
      "Epoch [29/50], Train Loss: 0.0129, Val Loss: 0.0082\n",
      "Epoch [30/50], Train Loss: 0.0122, Val Loss: 0.0065\n",
      "Epoch [31/50], Train Loss: 0.0114, Val Loss: 0.0051\n",
      "Epoch [32/50], Train Loss: 0.0109, Val Loss: 0.0043\n",
      "Epoch [33/50], Train Loss: 0.0106, Val Loss: 0.0036\n",
      "Epoch [34/50], Train Loss: 0.0100, Val Loss: 0.0030\n",
      "Epoch [35/50], Train Loss: 0.0094, Val Loss: 0.0028\n",
      "Epoch [36/50], Train Loss: 0.0089, Val Loss: 0.0029\n",
      "Epoch [37/50], Train Loss: 0.0081, Val Loss: 0.0030\n",
      "Epoch [38/50], Train Loss: 0.0079, Val Loss: 0.0030\n",
      "Epoch [39/50], Train Loss: 0.0076, Val Loss: 0.0029\n",
      "Epoch [40/50], Train Loss: 0.0074, Val Loss: 0.0031\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=64, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1498, Val Loss: 0.3583\n",
      "Epoch [2/50], Train Loss: 0.1240, Val Loss: 0.3045\n",
      "Epoch [3/50], Train Loss: 0.1015, Val Loss: 0.2532\n",
      "Epoch [4/50], Train Loss: 0.0807, Val Loss: 0.2035\n",
      "Epoch [5/50], Train Loss: 0.0646, Val Loss: 0.1564\n",
      "Epoch [6/50], Train Loss: 0.0526, Val Loss: 0.1164\n",
      "Epoch [7/50], Train Loss: 0.0435, Val Loss: 0.0882\n",
      "Epoch [8/50], Train Loss: 0.0411, Val Loss: 0.0723\n",
      "Epoch [9/50], Train Loss: 0.0391, Val Loss: 0.0650\n",
      "Epoch [10/50], Train Loss: 0.0376, Val Loss: 0.0592\n",
      "Epoch [11/50], Train Loss: 0.0370, Val Loss: 0.0554\n",
      "Epoch [12/50], Train Loss: 0.0344, Val Loss: 0.0518\n",
      "Epoch [13/50], Train Loss: 0.0340, Val Loss: 0.0489\n",
      "Epoch [14/50], Train Loss: 0.0316, Val Loss: 0.0454\n",
      "Epoch [15/50], Train Loss: 0.0329, Val Loss: 0.0415\n",
      "Epoch [16/50], Train Loss: 0.0301, Val Loss: 0.0380\n",
      "Epoch [17/50], Train Loss: 0.0294, Val Loss: 0.0354\n",
      "Epoch [18/50], Train Loss: 0.0276, Val Loss: 0.0330\n",
      "Epoch [19/50], Train Loss: 0.0267, Val Loss: 0.0300\n",
      "Epoch [20/50], Train Loss: 0.0258, Val Loss: 0.0266\n",
      "Epoch [21/50], Train Loss: 0.0253, Val Loss: 0.0235\n",
      "Epoch [22/50], Train Loss: 0.0247, Val Loss: 0.0198\n",
      "Epoch [23/50], Train Loss: 0.0220, Val Loss: 0.0171\n",
      "Epoch [24/50], Train Loss: 0.0218, Val Loss: 0.0148\n",
      "Epoch [25/50], Train Loss: 0.0201, Val Loss: 0.0119\n",
      "Epoch [26/50], Train Loss: 0.0190, Val Loss: 0.0091\n",
      "Epoch [27/50], Train Loss: 0.0176, Val Loss: 0.0075\n",
      "Epoch [28/50], Train Loss: 0.0172, Val Loss: 0.0057\n",
      "Epoch [29/50], Train Loss: 0.0164, Val Loss: 0.0045\n",
      "Epoch [30/50], Train Loss: 0.0159, Val Loss: 0.0047\n",
      "Epoch [31/50], Train Loss: 0.0154, Val Loss: 0.0044\n",
      "Epoch [32/50], Train Loss: 0.0143, Val Loss: 0.0043\n",
      "Epoch [33/50], Train Loss: 0.0141, Val Loss: 0.0045\n",
      "Epoch [34/50], Train Loss: 0.0136, Val Loss: 0.0047\n",
      "Epoch [35/50], Train Loss: 0.0134, Val Loss: 0.0040\n",
      "Epoch [36/50], Train Loss: 0.0131, Val Loss: 0.0039\n",
      "Epoch [37/50], Train Loss: 0.0124, Val Loss: 0.0049\n",
      "Epoch [38/50], Train Loss: 0.0122, Val Loss: 0.0042\n",
      "Epoch [39/50], Train Loss: 0.0126, Val Loss: 0.0033\n",
      "Epoch [40/50], Train Loss: 0.0126, Val Loss: 0.0033\n",
      "Epoch [41/50], Train Loss: 0.0118, Val Loss: 0.0041\n",
      "Epoch [42/50], Train Loss: 0.0118, Val Loss: 0.0036\n",
      "Epoch [43/50], Train Loss: 0.0118, Val Loss: 0.0039\n",
      "Epoch [44/50], Train Loss: 0.0118, Val Loss: 0.0029\n",
      "Epoch [45/50], Train Loss: 0.0112, Val Loss: 0.0038\n",
      "Epoch [46/50], Train Loss: 0.0111, Val Loss: 0.0035\n",
      "Epoch [47/50], Train Loss: 0.0107, Val Loss: 0.0044\n",
      "Epoch [48/50], Train Loss: 0.0108, Val Loss: 0.0036\n",
      "Epoch [49/50], Train Loss: 0.0106, Val Loss: 0.0037\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=64, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1273, Val Loss: 0.3229\n",
      "Epoch [2/50], Train Loss: 0.0912, Val Loss: 0.2548\n",
      "Epoch [3/50], Train Loss: 0.0644, Val Loss: 0.1918\n",
      "Epoch [4/50], Train Loss: 0.0442, Val Loss: 0.1351\n",
      "Epoch [5/50], Train Loss: 0.0328, Val Loss: 0.0939\n",
      "Epoch [6/50], Train Loss: 0.0291, Val Loss: 0.0717\n",
      "Epoch [7/50], Train Loss: 0.0279, Val Loss: 0.0611\n",
      "Epoch [8/50], Train Loss: 0.0264, Val Loss: 0.0541\n",
      "Epoch [9/50], Train Loss: 0.0247, Val Loss: 0.0480\n",
      "Epoch [10/50], Train Loss: 0.0229, Val Loss: 0.0421\n",
      "Epoch [11/50], Train Loss: 0.0211, Val Loss: 0.0363\n",
      "Epoch [12/50], Train Loss: 0.0193, Val Loss: 0.0308\n",
      "Epoch [13/50], Train Loss: 0.0174, Val Loss: 0.0255\n",
      "Epoch [14/50], Train Loss: 0.0156, Val Loss: 0.0206\n",
      "Epoch [15/50], Train Loss: 0.0139, Val Loss: 0.0163\n",
      "Epoch [16/50], Train Loss: 0.0122, Val Loss: 0.0126\n",
      "Epoch [17/50], Train Loss: 0.0107, Val Loss: 0.0097\n",
      "Epoch [18/50], Train Loss: 0.0093, Val Loss: 0.0075\n",
      "Epoch [19/50], Train Loss: 0.0081, Val Loss: 0.0059\n",
      "Epoch [20/50], Train Loss: 0.0071, Val Loss: 0.0048\n",
      "Epoch [21/50], Train Loss: 0.0061, Val Loss: 0.0041\n",
      "Epoch [22/50], Train Loss: 0.0052, Val Loss: 0.0036\n",
      "Epoch [23/50], Train Loss: 0.0044, Val Loss: 0.0034\n",
      "Epoch [24/50], Train Loss: 0.0037, Val Loss: 0.0032\n",
      "Epoch [25/50], Train Loss: 0.0032, Val Loss: 0.0031\n",
      "Epoch [26/50], Train Loss: 0.0028, Val Loss: 0.0030\n",
      "Epoch [27/50], Train Loss: 0.0026, Val Loss: 0.0029\n",
      "Epoch [28/50], Train Loss: 0.0024, Val Loss: 0.0028\n",
      "Epoch [29/50], Train Loss: 0.0023, Val Loss: 0.0028\n",
      "Epoch [30/50], Train Loss: 0.0023, Val Loss: 0.0027\n",
      "Epoch [31/50], Train Loss: 0.0022, Val Loss: 0.0027\n",
      "Epoch [32/50], Train Loss: 0.0022, Val Loss: 0.0027\n",
      "Epoch [33/50], Train Loss: 0.0022, Val Loss: 0.0027\n",
      "Epoch [34/50], Train Loss: 0.0022, Val Loss: 0.0027\n",
      "Epoch [35/50], Train Loss: 0.0021, Val Loss: 0.0027\n",
      "Epoch [36/50], Train Loss: 0.0021, Val Loss: 0.0027\n",
      "Epoch [37/50], Train Loss: 0.0021, Val Loss: 0.0027\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=64, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1153, Val Loss: 0.2912\n",
      "Epoch [2/50], Train Loss: 0.0807, Val Loss: 0.2175\n",
      "Epoch [3/50], Train Loss: 0.0541, Val Loss: 0.1487\n",
      "Epoch [4/50], Train Loss: 0.0370, Val Loss: 0.0950\n",
      "Epoch [5/50], Train Loss: 0.0314, Val Loss: 0.0686\n",
      "Epoch [6/50], Train Loss: 0.0313, Val Loss: 0.0590\n",
      "Epoch [7/50], Train Loss: 0.0297, Val Loss: 0.0528\n",
      "Epoch [8/50], Train Loss: 0.0279, Val Loss: 0.0473\n",
      "Epoch [9/50], Train Loss: 0.0268, Val Loss: 0.0434\n",
      "Epoch [10/50], Train Loss: 0.0247, Val Loss: 0.0376\n",
      "Epoch [11/50], Train Loss: 0.0234, Val Loss: 0.0311\n",
      "Epoch [12/50], Train Loss: 0.0222, Val Loss: 0.0266\n",
      "Epoch [13/50], Train Loss: 0.0206, Val Loss: 0.0215\n",
      "Epoch [14/50], Train Loss: 0.0182, Val Loss: 0.0175\n",
      "Epoch [15/50], Train Loss: 0.0170, Val Loss: 0.0137\n",
      "Epoch [16/50], Train Loss: 0.0155, Val Loss: 0.0097\n",
      "Epoch [17/50], Train Loss: 0.0141, Val Loss: 0.0073\n",
      "Epoch [18/50], Train Loss: 0.0131, Val Loss: 0.0056\n",
      "Epoch [19/50], Train Loss: 0.0117, Val Loss: 0.0045\n",
      "Epoch [20/50], Train Loss: 0.0107, Val Loss: 0.0040\n",
      "Epoch [21/50], Train Loss: 0.0100, Val Loss: 0.0040\n",
      "Epoch [22/50], Train Loss: 0.0096, Val Loss: 0.0037\n",
      "Epoch [23/50], Train Loss: 0.0086, Val Loss: 0.0040\n",
      "Epoch [24/50], Train Loss: 0.0083, Val Loss: 0.0040\n",
      "Epoch [25/50], Train Loss: 0.0077, Val Loss: 0.0040\n",
      "Epoch [26/50], Train Loss: 0.0068, Val Loss: 0.0040\n",
      "Epoch [27/50], Train Loss: 0.0065, Val Loss: 0.0039\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=64, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1284, Val Loss: 0.2916\n",
      "Epoch [2/50], Train Loss: 0.0876, Val Loss: 0.2169\n",
      "Epoch [3/50], Train Loss: 0.0623, Val Loss: 0.1557\n",
      "Epoch [4/50], Train Loss: 0.0503, Val Loss: 0.1147\n",
      "Epoch [5/50], Train Loss: 0.0470, Val Loss: 0.0947\n",
      "Epoch [6/50], Train Loss: 0.0429, Val Loss: 0.0838\n",
      "Epoch [7/50], Train Loss: 0.0428, Val Loss: 0.0766\n",
      "Epoch [8/50], Train Loss: 0.0397, Val Loss: 0.0697\n",
      "Epoch [9/50], Train Loss: 0.0398, Val Loss: 0.0640\n",
      "Epoch [10/50], Train Loss: 0.0371, Val Loss: 0.0591\n",
      "Epoch [11/50], Train Loss: 0.0350, Val Loss: 0.0539\n",
      "Epoch [12/50], Train Loss: 0.0339, Val Loss: 0.0495\n",
      "Epoch [13/50], Train Loss: 0.0334, Val Loss: 0.0452\n",
      "Epoch [14/50], Train Loss: 0.0305, Val Loss: 0.0417\n",
      "Epoch [15/50], Train Loss: 0.0296, Val Loss: 0.0368\n",
      "Epoch [16/50], Train Loss: 0.0274, Val Loss: 0.0319\n",
      "Epoch [17/50], Train Loss: 0.0258, Val Loss: 0.0284\n",
      "Epoch [18/50], Train Loss: 0.0243, Val Loss: 0.0237\n",
      "Epoch [19/50], Train Loss: 0.0223, Val Loss: 0.0199\n",
      "Epoch [20/50], Train Loss: 0.0209, Val Loss: 0.0171\n",
      "Epoch [21/50], Train Loss: 0.0200, Val Loss: 0.0143\n",
      "Epoch [22/50], Train Loss: 0.0182, Val Loss: 0.0112\n",
      "Epoch [23/50], Train Loss: 0.0168, Val Loss: 0.0086\n",
      "Epoch [24/50], Train Loss: 0.0152, Val Loss: 0.0054\n",
      "Epoch [25/50], Train Loss: 0.0143, Val Loss: 0.0053\n",
      "Epoch [26/50], Train Loss: 0.0144, Val Loss: 0.0041\n",
      "Epoch [27/50], Train Loss: 0.0138, Val Loss: 0.0039\n",
      "Epoch [28/50], Train Loss: 0.0138, Val Loss: 0.0039\n",
      "Epoch [29/50], Train Loss: 0.0130, Val Loss: 0.0033\n",
      "Epoch [30/50], Train Loss: 0.0133, Val Loss: 0.0038\n",
      "Epoch [31/50], Train Loss: 0.0124, Val Loss: 0.0040\n",
      "Epoch [32/50], Train Loss: 0.0130, Val Loss: 0.0036\n",
      "Epoch [33/50], Train Loss: 0.0123, Val Loss: 0.0042\n",
      "Epoch [34/50], Train Loss: 0.0124, Val Loss: 0.0048\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=64, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0980, Val Loss: 0.2483\n",
      "Epoch [2/50], Train Loss: 0.0568, Val Loss: 0.1619\n",
      "Epoch [3/50], Train Loss: 0.0370, Val Loss: 0.1027\n",
      "Epoch [4/50], Train Loss: 0.0320, Val Loss: 0.0763\n",
      "Epoch [5/50], Train Loss: 0.0311, Val Loss: 0.0660\n",
      "Epoch [6/50], Train Loss: 0.0297, Val Loss: 0.0595\n",
      "Epoch [7/50], Train Loss: 0.0280, Val Loss: 0.0536\n",
      "Epoch [8/50], Train Loss: 0.0261, Val Loss: 0.0476\n",
      "Epoch [9/50], Train Loss: 0.0240, Val Loss: 0.0414\n",
      "Epoch [10/50], Train Loss: 0.0218, Val Loss: 0.0349\n",
      "Epoch [11/50], Train Loss: 0.0192, Val Loss: 0.0280\n",
      "Epoch [12/50], Train Loss: 0.0164, Val Loss: 0.0211\n",
      "Epoch [13/50], Train Loss: 0.0132, Val Loss: 0.0147\n",
      "Epoch [14/50], Train Loss: 0.0100, Val Loss: 0.0098\n",
      "Epoch [15/50], Train Loss: 0.0072, Val Loss: 0.0071\n",
      "Epoch [16/50], Train Loss: 0.0053, Val Loss: 0.0059\n",
      "Epoch [17/50], Train Loss: 0.0042, Val Loss: 0.0050\n",
      "Epoch [18/50], Train Loss: 0.0035, Val Loss: 0.0044\n",
      "Epoch [19/50], Train Loss: 0.0031, Val Loss: 0.0039\n",
      "Epoch [20/50], Train Loss: 0.0028, Val Loss: 0.0036\n",
      "Epoch [21/50], Train Loss: 0.0027, Val Loss: 0.0035\n",
      "Epoch [22/50], Train Loss: 0.0026, Val Loss: 0.0035\n",
      "Epoch [23/50], Train Loss: 0.0025, Val Loss: 0.0034\n",
      "Epoch [24/50], Train Loss: 0.0025, Val Loss: 0.0034\n",
      "Epoch [25/50], Train Loss: 0.0024, Val Loss: 0.0034\n",
      "Epoch [26/50], Train Loss: 0.0024, Val Loss: 0.0034\n",
      "Epoch [27/50], Train Loss: 0.0024, Val Loss: 0.0034\n",
      "Epoch [28/50], Train Loss: 0.0023, Val Loss: 0.0035\n",
      "Epoch [29/50], Train Loss: 0.0023, Val Loss: 0.0035\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=64, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1529, Val Loss: 0.3384\n",
      "Epoch [2/50], Train Loss: 0.0959, Val Loss: 0.2296\n",
      "Epoch [3/50], Train Loss: 0.0525, Val Loss: 0.1255\n",
      "Epoch [4/50], Train Loss: 0.0363, Val Loss: 0.0768\n",
      "Epoch [5/50], Train Loss: 0.0358, Val Loss: 0.0712\n",
      "Epoch [6/50], Train Loss: 0.0360, Val Loss: 0.0682\n",
      "Epoch [7/50], Train Loss: 0.0330, Val Loss: 0.0629\n",
      "Epoch [8/50], Train Loss: 0.0308, Val Loss: 0.0567\n",
      "Epoch [9/50], Train Loss: 0.0305, Val Loss: 0.0516\n",
      "Epoch [10/50], Train Loss: 0.0279, Val Loss: 0.0452\n",
      "Epoch [11/50], Train Loss: 0.0264, Val Loss: 0.0384\n",
      "Epoch [12/50], Train Loss: 0.0247, Val Loss: 0.0339\n",
      "Epoch [13/50], Train Loss: 0.0229, Val Loss: 0.0275\n",
      "Epoch [14/50], Train Loss: 0.0210, Val Loss: 0.0228\n",
      "Epoch [15/50], Train Loss: 0.0190, Val Loss: 0.0195\n",
      "Epoch [16/50], Train Loss: 0.0174, Val Loss: 0.0159\n",
      "Epoch [17/50], Train Loss: 0.0167, Val Loss: 0.0126\n",
      "Epoch [18/50], Train Loss: 0.0155, Val Loss: 0.0105\n",
      "Epoch [19/50], Train Loss: 0.0131, Val Loss: 0.0078\n",
      "Epoch [20/50], Train Loss: 0.0118, Val Loss: 0.0048\n",
      "Epoch [21/50], Train Loss: 0.0100, Val Loss: 0.0038\n",
      "Epoch [22/50], Train Loss: 0.0086, Val Loss: 0.0033\n",
      "Epoch [23/50], Train Loss: 0.0070, Val Loss: 0.0036\n",
      "Epoch [24/50], Train Loss: 0.0069, Val Loss: 0.0034\n",
      "Epoch [25/50], Train Loss: 0.0064, Val Loss: 0.0033\n",
      "Epoch [26/50], Train Loss: 0.0062, Val Loss: 0.0036\n",
      "Epoch [27/50], Train Loss: 0.0063, Val Loss: 0.0030\n",
      "Epoch [28/50], Train Loss: 0.0063, Val Loss: 0.0035\n",
      "Epoch [29/50], Train Loss: 0.0058, Val Loss: 0.0032\n",
      "Epoch [30/50], Train Loss: 0.0063, Val Loss: 0.0030\n",
      "Epoch [31/50], Train Loss: 0.0064, Val Loss: 0.0032\n",
      "Epoch [32/50], Train Loss: 0.0063, Val Loss: 0.0032\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=64, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1275, Val Loss: 0.3013\n",
      "Epoch [2/50], Train Loss: 0.0869, Val Loss: 0.2157\n",
      "Epoch [3/50], Train Loss: 0.0595, Val Loss: 0.1411\n",
      "Epoch [4/50], Train Loss: 0.0487, Val Loss: 0.0969\n",
      "Epoch [5/50], Train Loss: 0.0478, Val Loss: 0.0830\n",
      "Epoch [6/50], Train Loss: 0.0439, Val Loss: 0.0742\n",
      "Epoch [7/50], Train Loss: 0.0428, Val Loss: 0.0695\n",
      "Epoch [8/50], Train Loss: 0.0408, Val Loss: 0.0639\n",
      "Epoch [9/50], Train Loss: 0.0384, Val Loss: 0.0589\n",
      "Epoch [10/50], Train Loss: 0.0371, Val Loss: 0.0514\n",
      "Epoch [11/50], Train Loss: 0.0326, Val Loss: 0.0429\n",
      "Epoch [12/50], Train Loss: 0.0294, Val Loss: 0.0350\n",
      "Epoch [13/50], Train Loss: 0.0276, Val Loss: 0.0283\n",
      "Epoch [14/50], Train Loss: 0.0253, Val Loss: 0.0231\n",
      "Epoch [15/50], Train Loss: 0.0221, Val Loss: 0.0156\n",
      "Epoch [16/50], Train Loss: 0.0204, Val Loss: 0.0112\n",
      "Epoch [17/50], Train Loss: 0.0181, Val Loss: 0.0082\n",
      "Epoch [18/50], Train Loss: 0.0179, Val Loss: 0.0061\n",
      "Epoch [19/50], Train Loss: 0.0167, Val Loss: 0.0063\n",
      "Epoch [20/50], Train Loss: 0.0166, Val Loss: 0.0054\n",
      "Epoch [21/50], Train Loss: 0.0162, Val Loss: 0.0051\n",
      "Epoch [22/50], Train Loss: 0.0152, Val Loss: 0.0051\n",
      "Epoch [23/50], Train Loss: 0.0148, Val Loss: 0.0051\n",
      "Epoch [24/50], Train Loss: 0.0159, Val Loss: 0.0056\n",
      "Epoch [25/50], Train Loss: 0.0149, Val Loss: 0.0054\n",
      "Epoch [26/50], Train Loss: 0.0143, Val Loss: 0.0051\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=128, num_layers=1, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1215, Val Loss: 0.3047\n",
      "Epoch [2/50], Train Loss: 0.0871, Val Loss: 0.2271\n",
      "Epoch [3/50], Train Loss: 0.0591, Val Loss: 0.1512\n",
      "Epoch [4/50], Train Loss: 0.0367, Val Loss: 0.0808\n",
      "Epoch [5/50], Train Loss: 0.0273, Val Loss: 0.0444\n",
      "Epoch [6/50], Train Loss: 0.0271, Val Loss: 0.0386\n",
      "Epoch [7/50], Train Loss: 0.0255, Val Loss: 0.0344\n",
      "Epoch [8/50], Train Loss: 0.0238, Val Loss: 0.0294\n",
      "Epoch [9/50], Train Loss: 0.0223, Val Loss: 0.0247\n",
      "Epoch [10/50], Train Loss: 0.0208, Val Loss: 0.0203\n",
      "Epoch [11/50], Train Loss: 0.0193, Val Loss: 0.0161\n",
      "Epoch [12/50], Train Loss: 0.0178, Val Loss: 0.0124\n",
      "Epoch [13/50], Train Loss: 0.0163, Val Loss: 0.0091\n",
      "Epoch [14/50], Train Loss: 0.0148, Val Loss: 0.0066\n",
      "Epoch [15/50], Train Loss: 0.0134, Val Loss: 0.0049\n",
      "Epoch [16/50], Train Loss: 0.0121, Val Loss: 0.0040\n",
      "Epoch [17/50], Train Loss: 0.0109, Val Loss: 0.0036\n",
      "Epoch [18/50], Train Loss: 0.0097, Val Loss: 0.0037\n",
      "Epoch [19/50], Train Loss: 0.0086, Val Loss: 0.0039\n",
      "Epoch [20/50], Train Loss: 0.0076, Val Loss: 0.0044\n",
      "Epoch [21/50], Train Loss: 0.0064, Val Loss: 0.0053\n",
      "Epoch [22/50], Train Loss: 0.0053, Val Loss: 0.0066\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=128, num_layers=1, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.0941, Val Loss: 0.2359\n",
      "Epoch [2/50], Train Loss: 0.0689, Val Loss: 0.1813\n",
      "Epoch [3/50], Train Loss: 0.0496, Val Loss: 0.1298\n",
      "Epoch [4/50], Train Loss: 0.0345, Val Loss: 0.0839\n",
      "Epoch [5/50], Train Loss: 0.0276, Val Loss: 0.0557\n",
      "Epoch [6/50], Train Loss: 0.0262, Val Loss: 0.0456\n",
      "Epoch [7/50], Train Loss: 0.0246, Val Loss: 0.0406\n",
      "Epoch [8/50], Train Loss: 0.0234, Val Loss: 0.0366\n",
      "Epoch [9/50], Train Loss: 0.0222, Val Loss: 0.0326\n",
      "Epoch [10/50], Train Loss: 0.0205, Val Loss: 0.0283\n",
      "Epoch [11/50], Train Loss: 0.0194, Val Loss: 0.0238\n",
      "Epoch [12/50], Train Loss: 0.0178, Val Loss: 0.0199\n",
      "Epoch [13/50], Train Loss: 0.0163, Val Loss: 0.0158\n",
      "Epoch [14/50], Train Loss: 0.0150, Val Loss: 0.0123\n",
      "Epoch [15/50], Train Loss: 0.0136, Val Loss: 0.0092\n",
      "Epoch [16/50], Train Loss: 0.0120, Val Loss: 0.0068\n",
      "Epoch [17/50], Train Loss: 0.0106, Val Loss: 0.0049\n",
      "Epoch [18/50], Train Loss: 0.0094, Val Loss: 0.0039\n",
      "Epoch [19/50], Train Loss: 0.0079, Val Loss: 0.0036\n",
      "Epoch [20/50], Train Loss: 0.0069, Val Loss: 0.0034\n",
      "Epoch [21/50], Train Loss: 0.0054, Val Loss: 0.0056\n",
      "Epoch [22/50], Train Loss: 0.0051, Val Loss: 0.0055\n",
      "Epoch [23/50], Train Loss: 0.0046, Val Loss: 0.0062\n",
      "Epoch [24/50], Train Loss: 0.0043, Val Loss: 0.0058\n",
      "Epoch [25/50], Train Loss: 0.0042, Val Loss: 0.0058\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=128, num_layers=1, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1341, Val Loss: 0.3497\n",
      "Epoch [2/50], Train Loss: 0.1017, Val Loss: 0.2795\n",
      "Epoch [3/50], Train Loss: 0.0751, Val Loss: 0.2093\n",
      "Epoch [4/50], Train Loss: 0.0510, Val Loss: 0.1368\n",
      "Epoch [5/50], Train Loss: 0.0360, Val Loss: 0.0797\n",
      "Epoch [6/50], Train Loss: 0.0335, Val Loss: 0.0600\n",
      "Epoch [7/50], Train Loss: 0.0322, Val Loss: 0.0547\n",
      "Epoch [8/50], Train Loss: 0.0301, Val Loss: 0.0492\n",
      "Epoch [9/50], Train Loss: 0.0294, Val Loss: 0.0454\n",
      "Epoch [10/50], Train Loss: 0.0277, Val Loss: 0.0403\n",
      "Epoch [11/50], Train Loss: 0.0262, Val Loss: 0.0361\n",
      "Epoch [12/50], Train Loss: 0.0248, Val Loss: 0.0318\n",
      "Epoch [13/50], Train Loss: 0.0242, Val Loss: 0.0287\n",
      "Epoch [14/50], Train Loss: 0.0226, Val Loss: 0.0249\n",
      "Epoch [15/50], Train Loss: 0.0207, Val Loss: 0.0225\n",
      "Epoch [16/50], Train Loss: 0.0198, Val Loss: 0.0187\n",
      "Epoch [17/50], Train Loss: 0.0183, Val Loss: 0.0155\n",
      "Epoch [18/50], Train Loss: 0.0172, Val Loss: 0.0130\n",
      "Epoch [19/50], Train Loss: 0.0157, Val Loss: 0.0096\n",
      "Epoch [20/50], Train Loss: 0.0146, Val Loss: 0.0077\n",
      "Epoch [21/50], Train Loss: 0.0125, Val Loss: 0.0054\n",
      "Epoch [22/50], Train Loss: 0.0116, Val Loss: 0.0037\n",
      "Epoch [23/50], Train Loss: 0.0099, Val Loss: 0.0020\n",
      "Epoch [24/50], Train Loss: 0.0095, Val Loss: 0.0019\n",
      "Epoch [25/50], Train Loss: 0.0085, Val Loss: 0.0033\n",
      "Epoch [26/50], Train Loss: 0.0084, Val Loss: 0.0041\n",
      "Epoch [27/50], Train Loss: 0.0079, Val Loss: 0.0042\n",
      "Epoch [28/50], Train Loss: 0.0078, Val Loss: 0.0033\n",
      "Epoch [29/50], Train Loss: 0.0075, Val Loss: 0.0044\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=128, num_layers=2, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.1130, Val Loss: 0.2715\n",
      "Epoch [2/50], Train Loss: 0.0529, Val Loss: 0.1261\n",
      "Epoch [3/50], Train Loss: 0.0280, Val Loss: 0.0529\n",
      "Epoch [4/50], Train Loss: 0.0296, Val Loss: 0.0464\n",
      "Epoch [5/50], Train Loss: 0.0261, Val Loss: 0.0371\n",
      "Epoch [6/50], Train Loss: 0.0234, Val Loss: 0.0286\n",
      "Epoch [7/50], Train Loss: 0.0206, Val Loss: 0.0210\n",
      "Epoch [8/50], Train Loss: 0.0177, Val Loss: 0.0142\n",
      "Epoch [9/50], Train Loss: 0.0148, Val Loss: 0.0088\n",
      "Epoch [10/50], Train Loss: 0.0120, Val Loss: 0.0053\n",
      "Epoch [11/50], Train Loss: 0.0095, Val Loss: 0.0035\n",
      "Epoch [12/50], Train Loss: 0.0073, Val Loss: 0.0030\n",
      "Epoch [13/50], Train Loss: 0.0054, Val Loss: 0.0032\n",
      "Epoch [14/50], Train Loss: 0.0039, Val Loss: 0.0033\n",
      "Epoch [15/50], Train Loss: 0.0030, Val Loss: 0.0030\n",
      "Epoch [16/50], Train Loss: 0.0026, Val Loss: 0.0029\n",
      "Epoch [17/50], Train Loss: 0.0024, Val Loss: 0.0029\n",
      "Epoch [18/50], Train Loss: 0.0023, Val Loss: 0.0028\n",
      "Epoch [19/50], Train Loss: 0.0023, Val Loss: 0.0028\n",
      "Epoch [20/50], Train Loss: 0.0023, Val Loss: 0.0029\n",
      "Epoch [21/50], Train Loss: 0.0023, Val Loss: 0.0029\n",
      "Epoch [22/50], Train Loss: 0.0022, Val Loss: 0.0028\n",
      "Epoch [23/50], Train Loss: 0.0022, Val Loss: 0.0028\n",
      "Epoch [24/50], Train Loss: 0.0022, Val Loss: 0.0028\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=128, num_layers=2, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1306, Val Loss: 0.2873\n",
      "Epoch [2/50], Train Loss: 0.0669, Val Loss: 0.1432\n",
      "Epoch [3/50], Train Loss: 0.0310, Val Loss: 0.0504\n",
      "Epoch [4/50], Train Loss: 0.0318, Val Loss: 0.0485\n",
      "Epoch [5/50], Train Loss: 0.0281, Val Loss: 0.0426\n",
      "Epoch [6/50], Train Loss: 0.0259, Val Loss: 0.0340\n",
      "Epoch [7/50], Train Loss: 0.0231, Val Loss: 0.0279\n",
      "Epoch [8/50], Train Loss: 0.0209, Val Loss: 0.0198\n",
      "Epoch [9/50], Train Loss: 0.0176, Val Loss: 0.0134\n",
      "Epoch [10/50], Train Loss: 0.0146, Val Loss: 0.0077\n",
      "Epoch [11/50], Train Loss: 0.0116, Val Loss: 0.0051\n",
      "Epoch [12/50], Train Loss: 0.0088, Val Loss: 0.0044\n",
      "Epoch [13/50], Train Loss: 0.0078, Val Loss: 0.0049\n",
      "Epoch [14/50], Train Loss: 0.0069, Val Loss: 0.0041\n",
      "Epoch [15/50], Train Loss: 0.0060, Val Loss: 0.0037\n",
      "Epoch [16/50], Train Loss: 0.0055, Val Loss: 0.0032\n",
      "Epoch [17/50], Train Loss: 0.0048, Val Loss: 0.0029\n",
      "Epoch [18/50], Train Loss: 0.0046, Val Loss: 0.0030\n",
      "Epoch [19/50], Train Loss: 0.0045, Val Loss: 0.0026\n",
      "Epoch [20/50], Train Loss: 0.0044, Val Loss: 0.0030\n",
      "Epoch [21/50], Train Loss: 0.0044, Val Loss: 0.0028\n",
      "Epoch [22/50], Train Loss: 0.0044, Val Loss: 0.0026\n",
      "Epoch [23/50], Train Loss: 0.0043, Val Loss: 0.0028\n",
      "Epoch [24/50], Train Loss: 0.0043, Val Loss: 0.0028\n",
      "Epoch [25/50], Train Loss: 0.0043, Val Loss: 0.0030\n",
      "Epoch [26/50], Train Loss: 0.0043, Val Loss: 0.0026\n",
      "Epoch [27/50], Train Loss: 0.0042, Val Loss: 0.0027\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=128, num_layers=2, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.1035, Val Loss: 0.2523\n",
      "Epoch [2/50], Train Loss: 0.0550, Val Loss: 0.1329\n",
      "Epoch [3/50], Train Loss: 0.0369, Val Loss: 0.0678\n",
      "Epoch [4/50], Train Loss: 0.0370, Val Loss: 0.0560\n",
      "Epoch [5/50], Train Loss: 0.0331, Val Loss: 0.0463\n",
      "Epoch [6/50], Train Loss: 0.0301, Val Loss: 0.0344\n",
      "Epoch [7/50], Train Loss: 0.0260, Val Loss: 0.0254\n",
      "Epoch [8/50], Train Loss: 0.0238, Val Loss: 0.0151\n",
      "Epoch [9/50], Train Loss: 0.0205, Val Loss: 0.0093\n",
      "Epoch [10/50], Train Loss: 0.0179, Val Loss: 0.0060\n",
      "Epoch [11/50], Train Loss: 0.0154, Val Loss: 0.0037\n",
      "Epoch [12/50], Train Loss: 0.0138, Val Loss: 0.0033\n",
      "Epoch [13/50], Train Loss: 0.0119, Val Loss: 0.0032\n",
      "Epoch [14/50], Train Loss: 0.0111, Val Loss: 0.0031\n",
      "Epoch [15/50], Train Loss: 0.0100, Val Loss: 0.0035\n",
      "Epoch [16/50], Train Loss: 0.0095, Val Loss: 0.0037\n",
      "Epoch [17/50], Train Loss: 0.0083, Val Loss: 0.0031\n",
      "Epoch [18/50], Train Loss: 0.0083, Val Loss: 0.0023\n",
      "Epoch [19/50], Train Loss: 0.0079, Val Loss: 0.0032\n",
      "Epoch [20/50], Train Loss: 0.0082, Val Loss: 0.0029\n",
      "Epoch [21/50], Train Loss: 0.0078, Val Loss: 0.0024\n",
      "Epoch [22/50], Train Loss: 0.0077, Val Loss: 0.0033\n",
      "Epoch [23/50], Train Loss: 0.0078, Val Loss: 0.0026\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=128, num_layers=3, dropout=0.0\n",
      "Epoch [1/50], Train Loss: 0.0983, Val Loss: 0.2208\n",
      "Epoch [2/50], Train Loss: 0.0357, Val Loss: 0.0738\n",
      "Epoch [3/50], Train Loss: 0.0348, Val Loss: 0.0581\n",
      "Epoch [4/50], Train Loss: 0.0318, Val Loss: 0.0481\n",
      "Epoch [5/50], Train Loss: 0.0280, Val Loss: 0.0357\n",
      "Epoch [6/50], Train Loss: 0.0242, Val Loss: 0.0231\n",
      "Epoch [7/50], Train Loss: 0.0198, Val Loss: 0.0119\n",
      "Epoch [8/50], Train Loss: 0.0152, Val Loss: 0.0049\n",
      "Epoch [9/50], Train Loss: 0.0112, Val Loss: 0.0031\n",
      "Epoch [10/50], Train Loss: 0.0083, Val Loss: 0.0039\n",
      "Epoch [11/50], Train Loss: 0.0055, Val Loss: 0.0039\n",
      "Epoch [12/50], Train Loss: 0.0037, Val Loss: 0.0025\n",
      "Epoch [13/50], Train Loss: 0.0028, Val Loss: 0.0023\n",
      "Epoch [14/50], Train Loss: 0.0026, Val Loss: 0.0037\n",
      "Epoch [15/50], Train Loss: 0.0026, Val Loss: 0.0040\n",
      "Epoch [16/50], Train Loss: 0.0025, Val Loss: 0.0024\n",
      "Epoch [17/50], Train Loss: 0.0025, Val Loss: 0.0024\n",
      "Epoch [18/50], Train Loss: 0.0026, Val Loss: 0.0047\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=128, num_layers=3, dropout=0.2\n",
      "Epoch [1/50], Train Loss: 0.1003, Val Loss: 0.2201\n",
      "Epoch [2/50], Train Loss: 0.0406, Val Loss: 0.0845\n",
      "Epoch [3/50], Train Loss: 0.0364, Val Loss: 0.0624\n",
      "Epoch [4/50], Train Loss: 0.0358, Val Loss: 0.0571\n",
      "Epoch [5/50], Train Loss: 0.0326, Val Loss: 0.0468\n",
      "Epoch [6/50], Train Loss: 0.0297, Val Loss: 0.0371\n",
      "Epoch [7/50], Train Loss: 0.0258, Val Loss: 0.0245\n",
      "Epoch [8/50], Train Loss: 0.0220, Val Loss: 0.0127\n",
      "Epoch [9/50], Train Loss: 0.0166, Val Loss: 0.0052\n",
      "Epoch [10/50], Train Loss: 0.0121, Val Loss: 0.0036\n",
      "Epoch [11/50], Train Loss: 0.0085, Val Loss: 0.0050\n",
      "Epoch [12/50], Train Loss: 0.0065, Val Loss: 0.0036\n",
      "Epoch [13/50], Train Loss: 0.0056, Val Loss: 0.0029\n",
      "Epoch [14/50], Train Loss: 0.0050, Val Loss: 0.0041\n",
      "Epoch [15/50], Train Loss: 0.0051, Val Loss: 0.0041\n",
      "Epoch [16/50], Train Loss: 0.0048, Val Loss: 0.0028\n",
      "Epoch [17/50], Train Loss: 0.0046, Val Loss: 0.0032\n",
      "Epoch [18/50], Train Loss: 0.0046, Val Loss: 0.0040\n",
      "Epoch [19/50], Train Loss: 0.0047, Val Loss: 0.0047\n",
      "Epoch [20/50], Train Loss: 0.0046, Val Loss: 0.0034\n",
      "Epoch [21/50], Train Loss: 0.0046, Val Loss: 0.0026\n",
      "Epoch [22/50], Train Loss: 0.0046, Val Loss: 0.0046\n",
      "Epoch [23/50], Train Loss: 0.0049, Val Loss: 0.0064\n",
      "Epoch [24/50], Train Loss: 0.0048, Val Loss: 0.0025\n",
      "Epoch [25/50], Train Loss: 0.0047, Val Loss: 0.0026\n",
      "Epoch [26/50], Train Loss: 0.0044, Val Loss: 0.0041\n",
      "Epoch [27/50], Train Loss: 0.0047, Val Loss: 0.0082\n",
      "Epoch [28/50], Train Loss: 0.0048, Val Loss: 0.0032\n",
      "Epoch [29/50], Train Loss: 0.0047, Val Loss: 0.0026\n",
      "Early stopping triggered.\n",
      "Testing parameters: lr=0.0001, optimizer=adamw, hidden_size=128, num_layers=3, dropout=0.5\n",
      "Epoch [1/50], Train Loss: 0.0956, Val Loss: 0.2258\n",
      "Epoch [2/50], Train Loss: 0.0465, Val Loss: 0.1052\n",
      "Epoch [3/50], Train Loss: 0.0415, Val Loss: 0.0757\n",
      "Epoch [4/50], Train Loss: 0.0397, Val Loss: 0.0661\n",
      "Epoch [5/50], Train Loss: 0.0351, Val Loss: 0.0498\n",
      "Epoch [6/50], Train Loss: 0.0309, Val Loss: 0.0323\n",
      "Epoch [7/50], Train Loss: 0.0272, Val Loss: 0.0186\n",
      "Epoch [8/50], Train Loss: 0.0201, Val Loss: 0.0065\n",
      "Epoch [9/50], Train Loss: 0.0159, Val Loss: 0.0051\n",
      "Epoch [10/50], Train Loss: 0.0129, Val Loss: 0.0055\n",
      "Epoch [11/50], Train Loss: 0.0110, Val Loss: 0.0044\n",
      "Epoch [12/50], Train Loss: 0.0110, Val Loss: 0.0035\n",
      "Epoch [13/50], Train Loss: 0.0101, Val Loss: 0.0031\n",
      "Epoch [14/50], Train Loss: 0.0093, Val Loss: 0.0039\n",
      "Epoch [15/50], Train Loss: 0.0096, Val Loss: 0.0033\n",
      "Epoch [16/50], Train Loss: 0.0092, Val Loss: 0.0053\n",
      "Epoch [17/50], Train Loss: 0.0089, Val Loss: 0.0042\n",
      "Epoch [18/50], Train Loss: 0.0091, Val Loss: 0.0032\n",
      "Early stopping triggered.\n",
      "Best Parameters: (0.001, 'adamw', 32, 1, 0.0), Best Validation Loss: 0.0010\n"
     ]
    }
   ],
   "source": [
    "# Transform to PyTorch Tensor\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32).to(device)\n",
    "\n",
    "# GRU Model\n",
    "class MyGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout):\n",
    "        super(MyGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(\n",
    "            input_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.dropout(out[:, -1, :])  # Dropout on the last hidden state\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Hyperparameter Grid\n",
    "param_grid = {\n",
    "    \"learning_rate\": [0.005, 0.001, 0.0005, 0.0001],\n",
    "    \"optimizer\": [\"adam\", \"sgd\", \"adamw\"],\n",
    "    \"hidden_size\": [16, 32, 64, 128],\n",
    "    \"num_layers\": [1, 2, 3],\n",
    "    \"dropout\": [0.0, 0.2, 0.5]\n",
    "}\n",
    "\n",
    "# Generate all combinations of hyperparameters\n",
    "param_combinations = list(product(*param_grid.values()))\n",
    "best_val_loss = float(\"inf\")\n",
    "best_params_GRU = None\n",
    "\n",
    "# Grid Search\n",
    "for params in param_combinations:\n",
    "    learning_rate, optimizer_name, hidden_size, num_layers, dropout = params\n",
    "\n",
    "    print(f\"Testing parameters: lr={learning_rate}, optimizer={optimizer_name}, hidden_size={hidden_size}, num_layers={num_layers}, dropout={dropout}\")\n",
    "\n",
    "    # Initialize model\n",
    "    model = MyGRU(input_size=5, hidden_size=hidden_size, num_layers=num_layers, output_size=5, dropout=dropout).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Initialize optimizer\n",
    "    if optimizer_name == \"adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    elif optimizer_name == \"sgd\":\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    elif optimizer_name == \"adamw\":\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Early Stopping\n",
    "    patience = 5\n",
    "    early_stop_counter = 0\n",
    "    best_model_val_loss = float(\"inf\")\n",
    "\n",
    "    # Training Loop\n",
    "    for epoch in range(50):  # Max epochs\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for i in range(0, len(X_train), 32):  # Batch size = 32\n",
    "            x_batch = X_train[i:i+32]\n",
    "            y_batch = y_train[i:i+32]\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(x_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss /= len(X_train) / 32  # Average train loss\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val)\n",
    "            val_loss = criterion(val_outputs, y_val).item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/50], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early Stopping Check\n",
    "        if val_loss < best_model_val_loss:\n",
    "            best_model_val_loss = val_loss\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            if early_stop_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    # Update best parameters if current combination is better\n",
    "    if best_model_val_loss < best_val_loss:\n",
    "        best_val_loss = best_model_val_loss\n",
    "        best_params_GRU = params\n",
    "        # Save best model\n",
    "        torch.save(model.state_dict(), \"best_model_GRU.pth\")\n",
    "\n",
    "print(f\"Best Parameters: {best_params_GRU}, Best Validation Loss: {best_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open - MSE: 0.0005, MAE: 0.0183, R: 0.8849\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High - MSE: 0.0006, MAE: 0.0199, R: 0.8577\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low - MSE: 0.0003, MAE: 0.0150, R: 0.9196\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Close - MSE: 0.0002, MAE: 0.0111, R: 0.9223\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Volume - MSE: 0.0014, MAE: 0.0288, R: -0.8167\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Metrics (All Features):\n",
      "Mean Squared Error (MSE): 0.0006\n",
      "Mean Absolute Error (MAE): 0.0186\n",
      "R Score: 0.5536\n"
     ]
    }
   ],
   "source": [
    "# Load Best Model\n",
    "best_lr, best_optimizer_name, best_hidden_size, best_num_layers, best_dropout = best_params_GRU\n",
    "model = MyGRU(\n",
    "    input_size=5,\n",
    "    hidden_size=best_hidden_size,\n",
    "    num_layers=best_num_layers,\n",
    "    output_size=5,\n",
    "    dropout=best_dropout\n",
    ").to(device)\n",
    "\n",
    "# Load the saved state_dict\n",
    "model.load_state_dict(torch.load(\"best_model_GRU.pth\"))\n",
    "model.eval()\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions_tensor = model(X_test_tensor)\n",
    "\n",
    "predictions = predictions_tensor.cpu().numpy()\n",
    "y_test = y_test_tensor.cpu().numpy()\n",
    "\n",
    "\n",
    "features = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    mse = mean_squared_error(y_test[:, i], predictions[:, i])\n",
    "    mae = mean_absolute_error(y_test[:, i], predictions[:, i])\n",
    "    r2 = r2_score(y_test[:, i], predictions[:, i])\n",
    "    print(f\"{feature} - MSE: {mse:.4f}, MAE: {mae:.4f}, R: {r2:.4f}\")\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(y_test[:, i], label=f\"True {feature}\", color=\"blue\")\n",
    "    plt.plot(predictions[:, i], label=f\"Predicted {feature}\", color=\"red\")\n",
    "    plt.title(f\"Predicted vs True {feature}\")\n",
    "    plt.xlabel(\"Sample Index\")\n",
    "    plt.ylabel(f\"{feature} Value\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "mse_total = mean_squared_error(y_test, predictions)\n",
    "mae_total = mean_absolute_error(y_test, predictions)\n",
    "r2_total = r2_score(y_test, predictions)\n",
    "\n",
    "print(\"\\nOverall Metrics (All Features):\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_total:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_total:.4f}\")\n",
    "print(f\"R Score: {r2_total:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the GRU model was the most consistent and performed the best across most features, demonstrating its ability to handle sequential data well without succumbing to the vanishing gradient problem as significantly as the LSTM model. The Vanilla RNN also showed decent performance, particularly with the \"Open\" and \"Low\" features, benefiting from its simplicity and being less prone to overfitting, especially given the relatively straightforward structure of this dataset. The LSTM model, while theoretically capable of capturing long-term dependencies, appeared to struggle with the simplicity and length of this dataset, leading to poor performance, especially in predicting \"Close\" and \"Volume.\" This might indicate overfitting or an inability to handle the specific short time dependencies present in the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mobile",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
